{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]', diff_loss_weight=1.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]_result.json', stream_id=4, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4210, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["fall of 1937", "The Skirmish of the Brick Church", "beliefs of Sunni Islamic thinkers", "\"The Lodger\"", "Londonistan", "a high-level marketing manager", "Houston, Texas", "cone-shaped", "San Francisco Bay Area's Levi's Stadium", "Ren\u00e9 Lalique", "absolution", "$105 billion", "ABC Cable News", "trial division", "their belief in the validity of the social contract", "Hyde Park", "four years", "Grey Street", "most of the items in the collection, unless those were newly accessioned into the collection", "literacy and numeracy", "Luther", "prime elements", "the Aveo", "one week", "Steymann v Staatssecretaris van Justitie", "1937", "The governments of the United States, Britain, Germany and France", "mother-of-pearl", "cholera", "Tower District", "ring theory", "Euclid's fundamental theorem of arithmetic", "Tony Hawk", "Beyonc\u00e9", "The Book of Discipline", "USSR", "Schmalkaldic League", "2006", "70%", "Einstein", "Genghis Khan", "four half-courses per term", "2011", "Brownlee", "Tracy Wolfson", "the wisdom and prudence of certain decisions of procurement", "1971", "the Uighurs surrendered to the Mongols first", "cnidarians", "CBS", "842 pounds", "two of Tesla's uncles", "up to \u00a332,583", "the City council", "three", "shopping", "4 weeks", "propulsion, electrical power and life support", "William Smilie", "George Westinghouse", "1279", "complexity classes", "\"everything that smacks of sacrifice\"", "a system to function"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7404040404040404}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9630", "mrqa_squad-validation-7687", "mrqa_squad-validation-4836", "mrqa_squad-validation-131", "mrqa_squad-validation-2297", "mrqa_squad-validation-5505", "mrqa_squad-validation-1802", "mrqa_squad-validation-9136", "mrqa_squad-validation-9061", "mrqa_squad-validation-116", "mrqa_squad-validation-5877", "mrqa_squad-validation-6294", "mrqa_squad-validation-7214", "mrqa_squad-validation-3699", "mrqa_squad-validation-8247", "mrqa_squad-validation-4419", "mrqa_squad-validation-553", "mrqa_squad-validation-3811", "mrqa_squad-validation-2092"], "SR": 0.703125, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 1, "before_eval_results": {"predictions": ["the Pulfrich effect", "semi-legal", "Westinghouse Electric", "Pax Mongolica", "The date of 2035", "internal strife", "the Marburg Colloquy", "Northumbria University", "non-cryogenic", "the defense and justification of empire-building", "the Carm Michael numbers", "1999", "Scorpion", "October 16, 2012", "a commune", "the metal locking screw on the camera lens", "Eldon Square Shopping Centre", "type III secretion system", "$680 billion", "296", "New Collegiate Division", "four levels", "18 million volumes", "15,100", "Warner Bros. Presents", "V\u03b39/V\u03b42 T cells", "1985", "the Augustinian friars", "third", "2012", "gold", "force model that is independent of any macroscale position vector", "378", "tourism", "the Jews", "all", "many celebrated seasons", "Charles-Fer Ferdinand University", "The Nationals", "a computational problem where a single output (of a total function) is expected for every input", "Katharina von Bora", "1888", "the middle of the continent", "Schmalkaldic League", "4:51", "Knaurs Lexikon", "constant pressure", "detective shows", "the southern and central parts of France", "Maria Fold and thrust Belt", "making it seem like climate change is more serious by overstating the impact", "1945", "1876", "Elway", "spring of 1349", "Extreme Makeover: Home Edition", "the Wesleyan Holiness Consortium", "it has settled as one of the pillars of history", "Pennsylvania Railroad, B&O Railroad, Reading Railroad and Short Line Railroad", "The Sphinx would devour anyone who could not answer her riddle", "The Smashing Pumpkins are an American alternative rock band from Chicago, Illinois, formed", "the 107th justice to serve on the United States Supreme Court", "D. B. Cooper is a media epithet popularly used to refer to an unidentified man who hijacked a Boeing 727 aircraft in the airspace between Portland, Oregon, and Seattle, Washington", "If the citizen's heart was heavier than a feather they would face torment in a lake of fire"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8429135215113834}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.2666666666666667, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.33333333333333337, 0.18181818181818182, 0.3076923076923077, 0.0, 0.07142857142857144, 0.2105263157894737]}}, "before_error_ids": ["mrqa_squad-validation-8546", "mrqa_squad-validation-9024", "mrqa_squad-validation-10466", "mrqa_squad-validation-1030", "mrqa_squad-validation-1189", "mrqa_squad-validation-1600", "mrqa_squad-validation-4287", "mrqa_squad-validation-2166", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-1274", "mrqa_hotpotqa-validation-3713"], "SR": 0.78125, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 2, "before_eval_results": {"predictions": ["Solim\u00f5es Basin", "the seal of the Federal Communications Commission", "Islamization", "the E. W. Scripps Company", "Grand Canal d'Alsace", "food security", "exothermic", "Newcastle Diamonds", "the wisdom and prudence of certain decisions of procurement", "D\u00fcrer", "Grover Cleveland", "Erg\u00e4nzungsschulen", "concrete", "the National Science Foundation (NSF) beginning in 1985 to promote advanced research and education networking in the United States", "The Newlywed Game", "the German-Swiss border", "the \"blurring of theological and confessional differences in the interests of unity.\"", "microbes", "300", "the electrostatic force", "Jim Nantz and Phil Simms", "1530", "from 12:00 to 6:00 p.m. Eastern Time", "employ consultant pharmacists and/or provide consulting services", "the murder of Christ", "1708", "the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)", "the Religious Coalition for Reproductive Choice", "The Arrow", "intractable problems", "yellow fever", "silver and inlaid with gold", "Richard Wilkinson and Kate Pickett", "elsewhere in the Northern United Kingdom", "president and CEO", "Von Miller", "beta decay", "a diverse phylum of bacteria capable of carrying out photosynthesis", "vaccination", "the plague theory", "the loss of soil fertility and weed invasion", "the fact (Fermat's little theorem)", "11th", "the most popular show at the time", "Robert of Jumi\u00e8ges", "student populations", "German", "Persia", "superheaters", "two", "Johann von Staupitz", "lectures", "the land gets more", "What a wonderful World", "His wife Frances", "Dugout canoe", "Ganges", "against", "Britney Spears", "the title My Fair Lady", "Don Bradman", "Ford Motor Co.", "Fidenza", "Charles Scribner's"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7426522435897436}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6399999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9768", "mrqa_squad-validation-5521", "mrqa_squad-validation-4847", "mrqa_squad-validation-597", "mrqa_squad-validation-5828", "mrqa_squad-validation-8777", "mrqa_squad-validation-4974", "mrqa_squad-validation-9023", "mrqa_squad-validation-1188", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-9859", "mrqa_searchqa-validation-6857", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-3869"], "SR": 0.6875, "CSR": 0.7239583333333333, "EFR": 1.0, "Overall": 0.8619791666666666}, {"timecode": 3, "before_eval_results": {"predictions": ["Winter Film Capital of the World", "sarcasm and attempts to humiliate pupils", "collenchyma tissue", "24 March 1879", "Scottish Constitutional Convention", "constant factors and smaller terms", "1996", "CBS", "genetic branches", "religious", "14,000", "Killer T cells", "11 million", "third", "mantle", "expansions", "Necessity-based", "glaucophyte chloroplasts", "artisans and farmers", "inverted repeat regions", "pharmacists", "September 2007", "a declining state of mind", "G", "civil disobedience", "Sociologist", "World News Tonight", "carriage of their respective basic channels", "cytotoxic", "Liao, Jin, and Song", "Tyneside Classical", "nine", "18 million", "four", "Christian Whiton", "his mother", "Johann Gerhard", "Korean", "The Time of the Doctor", "7 January 1900", "90\u00b0", "bitterly disposed towards the French", "stolen", "Centrum", "$200,000", "4,686", "demographics and economic ties", "the Boeing 707", "Karl Marx", "President of the U.S.", "Disneyland", "the king", "South Africa", "fibre optics", "the Swiss Family Robinson", "a cone-shaped utensil", "Yasser Arafat", "the pigeon", "vodou", "a dowry", "Macduff", "Lori Rom", "Prince James, Duke of York and of Albany ( later King James II & VII )", "Charles Perrault"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7092013888888888}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9337", "mrqa_squad-validation-1714", "mrqa_squad-validation-6409", "mrqa_squad-validation-3958", "mrqa_squad-validation-6670", "mrqa_squad-validation-6884", "mrqa_squad-validation-7083", "mrqa_squad-validation-2406", "mrqa_squad-validation-10186", "mrqa_squad-validation-1509", "mrqa_searchqa-validation-6999", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-15841", "mrqa_naturalquestions-validation-495"], "SR": 0.671875, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 4, "before_eval_results": {"predictions": ["Mars", "education", "Kingdom of Prussia", "May 18, 1756", "odd prime", "economic growth", "quantum electrodynamics", "topographic gradients", "Hassan al Banna", "regional burden sharing", "Indianapolis Colts", "Pole Mokotowskie", "a school or other place of formal education", "Francis Blackburne", "black earth", "photolysis of ozone by light of short wavelength", "smart ticketing", "State Route 99", "F and \u2212F are equal in magnitude and opposite in direction", "free", "Air", "1,548", "whether the bill is within the legislative competence of the Parliament", "two", "Galileo Galilei", "Anglican Church, Uniting Church and Presbyterian Church", "patient care rounds drug product selection", "a vicious and destructive civil war", "greater scarcity", "The Eleventh Doctor", "86", "Arizona Cardinals", "not designed to fly through the Earth's atmosphere or return to Earth", "2015", "one hunting excursion", "a bishop", "destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church", "socialist realism", "July 23, 1963", "1162", "a method which pre- allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes", "Barbara Walters", "as soon as 2050", "Red River", "aircraft", "Buenos Aires", "Sub-Saharan Africa", "How I Met Your Mother", "the Louvre", "$1.5 million", "3 to 17", "Isabella", "Obama", "(Tara and Troy Livesay)", "Preah Vihear temple", "Ralph Lauren", "a futile effort\" if Noriko Savoie did flee to Japan", "T.I.", "(Zed)", "to work together to stabilize Somalia and cooperate in security and military operations", "battles, political intrigue, and the characters", "a cigarette", "Mulberry", "Shaft"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7318112707039337}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.34782608695652173, 0.25, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8634", "mrqa_squad-validation-1891", "mrqa_squad-validation-10333", "mrqa_squad-validation-7020", "mrqa_squad-validation-9665", "mrqa_squad-validation-3706", "mrqa_squad-validation-10068", "mrqa_squad-validation-2564", "mrqa_squad-validation-4746", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-2234", "mrqa_naturalquestions-validation-7134", "mrqa_triviaqa-validation-712", "mrqa_searchqa-validation-8929"], "SR": 0.65625, "CSR": 0.7, "EFR": 1.0, "Overall": 0.85}, {"timecode": 5, "before_eval_results": {"predictions": ["visitation of the Electorate of Saxony", "United States", "11", "May", "employ limited coercion", "1798", "3D printing technology", "Waterlogged", "Tim Allen", "wealth and income", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "40", "LeGrande", "filaments", "Joanna Lumley", "Energiprojekt AB in Sweden", "DuMont Television Network", "1913", "Egyptian Islamic Jihad organization", "consumer prices", "petroleum", "1870", "27.7 million tons", "the remainder of the British Isles", "Jean Auguste Dominique Ingres,", "Eliot Ness", "condemned the violence as the devil's work, and called for the nobles to put down the rebels like mad dogs", "Edgar", "experience and extra responsibilities", "chameleon circuit", "All-Channel Receiver Act", "secular powers", "at the opposite end from the mouth", "areas controlled by Russia", "kinescope", "University Athletic Association (UAA)", "three", "Chebyshev", "American reality television series", "Disco", "Christopher Lloyd Smalling", "Mary Harron", "Polk", "Minette Walters", "1983", "79 AD", "1993 to 2001", "Major League Soccer", "1669", "University of Vienna", "\"lo Stivale\" (the Boot)", "Richa Sharma", "Centennial Olympic Stadium", "Violet", "Vernier, Switzerland", "October 21, 2016", "former captain of the Indian cricket team", "FIFA World Cup, AFC Asian Cup and East Asian Football Championship", "The conversation", "9 February 2018", "Canada", "teenage", "an increase in dew point", "Madeline Reeves ( Donna Mills)"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8367311507936508}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-3345", "mrqa_squad-validation-5519", "mrqa_squad-validation-2322", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4614", "mrqa_triviaqa-validation-4029", "mrqa_newsqa-validation-2982", "mrqa_searchqa-validation-4118", "mrqa_naturalquestions-validation-10691"], "SR": 0.78125, "CSR": 0.7135416666666667, "EFR": 1.0, "Overall": 0.8567708333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["woodblocks", "General Hospital", "its circle logo", "Denver Broncos", "129 MSPs", "Lenin", "completed (or local) fields", "achievement-oriented", "alone", "stem cells", "John Pell, Lord of Pelham Manor", "quickly", "an induction motor", "Holy War", "pressure terms", "ten million", "Tommy Lee Jones", "nine", "13.34% (116.7 sq mi or 302 km2)", "kilopond", "water level", "1981", "rules that conflict with morality", "sixteenth century", "R\u00fcdesheim", "an epidemiological account", "time or space", "blacks", "Aristotle", "1724", "mid-Cambrian period", "Canada", "Stanford University", "Reuben Townroe", "small forward", "Alamo Bowl", "The King of Chutzpah", "Charles Russell", "German", "Minette Walters (born 26 September 1949)", "St. Patrick's Day in 1988", "Beaumont", "Michael He was nominated again for Best Production Design,", "Hungary", "ITV", "Ella Fitzgerald", "To Save a Life", "EBSCO Information Services", "Dutch", "Marc Bolan ( ; born Mark Feld; 30 September 1947)", "\"Frankenstein\"", "John Mills", "1992", "Saint-Domingue", "December 1978", "\"Kill Your Darlings\"", "University of Kansas", "The Land of Enchantment", "2001", "A.C.E.", "contraband", "Billy Bob Thornton", "Charles Martel", "Saturday Night Live"], "metric_results": {"EM": 0.625, "QA-F1": 0.6919270833333334}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5774", "mrqa_squad-validation-9430", "mrqa_squad-validation-7317", "mrqa_squad-validation-7614", "mrqa_squad-validation-2567", "mrqa_squad-validation-5303", "mrqa_squad-validation-7476", "mrqa_squad-validation-9098", "mrqa_squad-validation-7295", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-3503", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-4387", "mrqa_triviaqa-validation-2856", "mrqa_newsqa-validation-696", "mrqa_searchqa-validation-12796"], "SR": 0.625, "CSR": 0.7008928571428572, "EFR": 1.0, "Overall": 0.8504464285714286}, {"timecode": 7, "before_eval_results": {"predictions": ["Germany", "1985", "William Iron Arm", "Muhammad Abd al-Salaam Farag", "comedies", "lack of understanding", "British East Africa", "Atlantic", "Jingshi Dadian", "economic instability", "Jean- Marc Bosman", "Ismailiyah, Egypt", "Wiesner", "polynomial time", "quarterback", "ten times their own weight", "primes", "light", "successfully", "TGIF", "married outside their immediate French communities", "George Westinghouse", "force that acts between nucleons in atomic nuclei", "certification", "Alberto Calder\u00f3n", "Mercury", "Marconi successfully transmitted the letter S from England to Newfoundland", "private", "Cadeby stone", "Ten", "vice president", "Steven Gerrard", "Human Rights Watch", "Hine's school", "Obama", "Silvio Berlusconi", "London Heathrow's Terminal 5", "in the foyer", "football matches in knee-deep mud", "sharia law", "JBS Swift Beef Company", "Sonia Sotomayor", "Kurdish militant group", "pilot", "Wednesday", "$50", "back at work", "flooding", "1969", "composer", "50,000", "know what is important in life", "Bhola district", "SSM Cardinal Glennon Children's Medical Center", "pro-democracy activists", "he has no plans to fritter his cash away on fast cars", "Secretary of State Hillary Clinton", "7th century", "Wigan", "1974", "Salt Lake City", "Jewish law student", "The Little Foxes", "a cycle of twelve narrative poems"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6461616992291335}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.5, 0.0, 0.08, 0.7272727272727273, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.05263157894736842, 0.33333333333333337, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-6034", "mrqa_squad-validation-6925", "mrqa_squad-validation-8339", "mrqa_squad-validation-4289", "mrqa_squad-validation-4692", "mrqa_squad-validation-2160", "mrqa_squad-validation-8771", "mrqa_squad-validation-3069", "mrqa_squad-validation-10445", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_naturalquestions-validation-8664", "mrqa_searchqa-validation-4905", "mrqa_searchqa-validation-1341"], "SR": 0.5625, "CSR": 0.68359375, "EFR": 0.9285714285714286, "Overall": 0.8060825892857143}, {"timecode": 8, "before_eval_results": {"predictions": ["2 July 1505,", "beginning in early September and ending in mid-May", "high risk preparations and some other compounding functions", "Non Governmental and Intergovernmental Organizations", "chief electrician position", "1671", "on the sidelines", "12 January 1943", "Naimans (Naiman Mongols)", "Sonia Shankman Orthogenic School", "Innate immune systems", "the manufacturing sector", "Wardenclyffe Tower project", "horizontal compression", "Edgar Scherick", "Imperial", "the Moscone Center in San Francisco", "\"Manchu dynasty\"", "Since the 1980s", "oxygen-16", "Gerhard. Lessing", "one advanced lay servant course, and be interviewed by the District or Conference Committee on Lay Speaking.", "the roads between this and the A1's former alignment through the Tyne Tunnel", "engaging in the forbidden speech", "22,000\u201314,000 yr BP,", "two", "the Lek", "New England Patriots", "some work rule issues.", "28 of those now on hardcourt surfaces.", "July 4.", "the Catholic League", "more than two years,", "Newark's Liberty International Airport,", "We Found Love", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "\"Larry King Live\"", "to foster national reconciliation between religious and ethnic groups.", "Addis Ababa,", "five female pastors", "military trials for some Guant Bay detainees.", "to launch a group that will serve as an alternative to the Organization of American States.", "to pay him a monthly allowance,", "Tutsi and Hutu", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "\"Draquila -- Italy Trembles.\"", "hooked up with Mildred, a younger woman of about 80, in March.", "his former Boca Juniors teammate and national coach Diego Maradona,", "17 Again", "five", "75", "Kgalema Motlanthe,", "preventing our public-owned seas from turning into sprawling, watery versions of Houston, Texas, or Atlanta, Georgia.", "about 3,000 kilometers (1,900 miles)", "two courses", "NATO fighters", "in Austin, Texas,", "two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "Royals Baseball Academy", "Anah\u00ed", "a helicopter", "Nikkei 225 Stock Average", "Surrey", "David Bowie"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6345397999907993}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1818181818181818, 0.8, 0.0, 0.6666666666666666, 0.8, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4800000000000001, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.10256410256410256, 0.2608695652173913, 0.1818181818181818, 0.3333333333333333, 0.06451612903225806, 0.5, 0.0, 0.9523809523809523, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.888888888888889, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8526", "mrqa_squad-validation-1279", "mrqa_squad-validation-3113", "mrqa_squad-validation-589", "mrqa_squad-validation-1570", "mrqa_squad-validation-6128", "mrqa_squad-validation-7377", "mrqa_squad-validation-503", "mrqa_squad-validation-8084", "mrqa_squad-validation-2405", "mrqa_squad-validation-10083", "mrqa_squad-validation-5357", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-4000", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-93", "mrqa_searchqa-validation-8602", "mrqa_triviaqa-validation-1"], "SR": 0.453125, "CSR": 0.6579861111111112, "EFR": 1.0, "Overall": 0.8289930555555556}, {"timecode": 9, "before_eval_results": {"predictions": ["Private Education Student Financial Assistance", "philanthropy", "Mongolia", "Daily Mail", "heard her songs; he followed the fishermen and captured the mermaid.", "equal in magnitude and opposite in direction", "Metropolitan Statistical Areas", "orogenic wedges", "eleven", "Grissom, White, and Chaffee", "Amtrak San Joaquins", "two", "1", "Barbara Walters", "temperate", "girls", "by citizens", "by up to 3 pence in the pound", "Gerhard", "the courts of member states and the Court of Justice of the European Union", "1887", "new laws or amendments to existing laws as a bill", "487", "the Presiding Officer", "expansion", "3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "axial skeleton ( 28 in the skull and 52 in the torso ) and 126 bones in the appendicular skeleton ( 32 \u00d7 2 in the upper extremities", "Thebes", "moral tale", "The Maidstone Studios in Maidstone, Kent", "husky", "the National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1986", "Charles Darwin", "the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Poems : Series 1", "1956", "merengue and bachata music", "lamina dura", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "Proposition 103", "It entered the United States via California in 1997 and the Middle East in 2000", "the intersection of Mud Mountain Road and Highway 410, looking southeasterly.", "San Francisco Bay", "Tom Brady", "2017", "Duck", "three levels", "Sylvester Stallone", "every 23 hours", "never made", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year reported on the accompanying income statement", "1963", "a type of party", "altitude", "E \u00d7 12", "sow", "a bacteria", "John Joseph Travolta", "Allies of World War I, or Entente Powers,", "U.S. 93", "anyone wanting to harm them", "Danny Williams", "Utah"], "metric_results": {"EM": 0.5, "QA-F1": 0.6368713965644386}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9, 0.17391304347826084, 0.0, 1.0, 1.0, 0.0, 0.631578947368421, 1.0, 1.0, 0.33333333333333337, 0.24000000000000002, 0.0, 0.4, 1.0, 0.1904761904761905, 1.0, 0.14285714285714288, 0.125, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.7692307692307693, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-805", "mrqa_squad-validation-10333", "mrqa_squad-validation-3922", "mrqa_squad-validation-9641", "mrqa_squad-validation-2404", "mrqa_squad-validation-9452", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-893", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-1173", "mrqa_triviaqa-validation-2329", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2729", "mrqa_newsqa-validation-4179", "mrqa_searchqa-validation-10449"], "SR": 0.5, "CSR": 0.6421875, "EFR": 0.96875, "Overall": 0.80546875}, {"timecode": 10, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1735", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1877", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3503", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3994", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6984", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-4000", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-69", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16779", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2470", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-350", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-9187", "mrqa_squad-validation-10015", "mrqa_squad-validation-10052", "mrqa_squad-validation-10068", "mrqa_squad-validation-1008", "mrqa_squad-validation-10083", "mrqa_squad-validation-10103", "mrqa_squad-validation-10107", "mrqa_squad-validation-10116", "mrqa_squad-validation-10125", "mrqa_squad-validation-10186", "mrqa_squad-validation-10210", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-1030", "mrqa_squad-validation-10308", "mrqa_squad-validation-10333", "mrqa_squad-validation-10333", "mrqa_squad-validation-10344", "mrqa_squad-validation-10367", "mrqa_squad-validation-10374", "mrqa_squad-validation-104", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10466", "mrqa_squad-validation-10493", "mrqa_squad-validation-1051", "mrqa_squad-validation-1052", "mrqa_squad-validation-1068", "mrqa_squad-validation-1113", "mrqa_squad-validation-116", "mrqa_squad-validation-1165", "mrqa_squad-validation-1178", "mrqa_squad-validation-1188", "mrqa_squad-validation-1193", "mrqa_squad-validation-1200", "mrqa_squad-validation-1207", "mrqa_squad-validation-1211", "mrqa_squad-validation-1257", "mrqa_squad-validation-1269", "mrqa_squad-validation-1279", "mrqa_squad-validation-131", "mrqa_squad-validation-1330", "mrqa_squad-validation-1348", "mrqa_squad-validation-1368", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1509", "mrqa_squad-validation-1527", "mrqa_squad-validation-1536", "mrqa_squad-validation-1541", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1714", "mrqa_squad-validation-1769", "mrqa_squad-validation-1802", "mrqa_squad-validation-1891", "mrqa_squad-validation-1947", "mrqa_squad-validation-1967", "mrqa_squad-validation-2030", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2166", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-2297", "mrqa_squad-validation-2331", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2405", "mrqa_squad-validation-2409", "mrqa_squad-validation-2438", "mrqa_squad-validation-25", "mrqa_squad-validation-2554", "mrqa_squad-validation-2559", "mrqa_squad-validation-2564", "mrqa_squad-validation-2567", "mrqa_squad-validation-2576", "mrqa_squad-validation-2579", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2717", "mrqa_squad-validation-2778", "mrqa_squad-validation-2822", "mrqa_squad-validation-2827", "mrqa_squad-validation-2870", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-3050", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-313", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3261", "mrqa_squad-validation-3269", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3388", "mrqa_squad-validation-3445", "mrqa_squad-validation-3492", "mrqa_squad-validation-3603", "mrqa_squad-validation-3617", "mrqa_squad-validation-365", "mrqa_squad-validation-3699", "mrqa_squad-validation-3759", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3815", "mrqa_squad-validation-3833", "mrqa_squad-validation-3837", "mrqa_squad-validation-3844", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3922", "mrqa_squad-validation-3938", "mrqa_squad-validation-3958", "mrqa_squad-validation-3976", "mrqa_squad-validation-4030", "mrqa_squad-validation-4086", "mrqa_squad-validation-4191", "mrqa_squad-validation-4231", "mrqa_squad-validation-4232", "mrqa_squad-validation-4248", "mrqa_squad-validation-4269", "mrqa_squad-validation-43", "mrqa_squad-validation-4419", "mrqa_squad-validation-4480", "mrqa_squad-validation-4491", "mrqa_squad-validation-4560", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4746", "mrqa_squad-validation-475", "mrqa_squad-validation-4765", "mrqa_squad-validation-4836", "mrqa_squad-validation-4847", "mrqa_squad-validation-4896", "mrqa_squad-validation-4935", "mrqa_squad-validation-5009", "mrqa_squad-validation-5075", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5164", "mrqa_squad-validation-5180", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5221", "mrqa_squad-validation-5272", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5357", "mrqa_squad-validation-5363", "mrqa_squad-validation-5424", "mrqa_squad-validation-5451", "mrqa_squad-validation-5455", "mrqa_squad-validation-5471", "mrqa_squad-validation-5505", "mrqa_squad-validation-5519", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5541", "mrqa_squad-validation-5616", "mrqa_squad-validation-5651", "mrqa_squad-validation-5670", "mrqa_squad-validation-5774", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-583", "mrqa_squad-validation-5840", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-5877", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5908", "mrqa_squad-validation-5937", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-5971", "mrqa_squad-validation-5976", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6048", "mrqa_squad-validation-6083", "mrqa_squad-validation-6098", "mrqa_squad-validation-6098", "mrqa_squad-validation-6128", "mrqa_squad-validation-6158", "mrqa_squad-validation-618", "mrqa_squad-validation-6238", "mrqa_squad-validation-6294", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6381", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6506", "mrqa_squad-validation-6527", "mrqa_squad-validation-6530", "mrqa_squad-validation-6569", "mrqa_squad-validation-6580", "mrqa_squad-validation-6605", "mrqa_squad-validation-6670", "mrqa_squad-validation-6681", "mrqa_squad-validation-6707", "mrqa_squad-validation-6754", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-69", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-6996", "mrqa_squad-validation-7002", "mrqa_squad-validation-7020", "mrqa_squad-validation-7022", "mrqa_squad-validation-7034", "mrqa_squad-validation-7080", "mrqa_squad-validation-7083", "mrqa_squad-validation-7092", "mrqa_squad-validation-7094", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7303", "mrqa_squad-validation-7304", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7372", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7420", "mrqa_squad-validation-7476", "mrqa_squad-validation-7502", "mrqa_squad-validation-7614", "mrqa_squad-validation-7687", "mrqa_squad-validation-7690", "mrqa_squad-validation-7704", "mrqa_squad-validation-775", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-7886", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7981", "mrqa_squad-validation-805", "mrqa_squad-validation-8052", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8197", "mrqa_squad-validation-8247", "mrqa_squad-validation-829", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8364", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8416", "mrqa_squad-validation-8479", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8526", "mrqa_squad-validation-8546", "mrqa_squad-validation-8580", "mrqa_squad-validation-8600", "mrqa_squad-validation-863", "mrqa_squad-validation-8680", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8777", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8953", "mrqa_squad-validation-8957", "mrqa_squad-validation-8965", "mrqa_squad-validation-9002", "mrqa_squad-validation-9012", "mrqa_squad-validation-902", "mrqa_squad-validation-9023", "mrqa_squad-validation-9024", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9136", "mrqa_squad-validation-9141", "mrqa_squad-validation-9208", "mrqa_squad-validation-9254", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9337", "mrqa_squad-validation-9411", "mrqa_squad-validation-9430", "mrqa_squad-validation-9452", "mrqa_squad-validation-9457", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9527", "mrqa_squad-validation-9569", "mrqa_squad-validation-9597", "mrqa_squad-validation-9614", "mrqa_squad-validation-9615", "mrqa_squad-validation-9624", "mrqa_squad-validation-9635", "mrqa_squad-validation-9641", "mrqa_squad-validation-9665", "mrqa_squad-validation-9718", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_squad-validation-9845", "mrqa_squad-validation-985", "mrqa_squad-validation-9926", "mrqa_squad-validation-9940", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.916015625, "KG": 0.425, "before_eval_results": {"predictions": ["the Miller\u2013Urey experiment", "five", "Mughal emperors", "a series of strikes by coal miners and railroad workers over the winter of 1973\u201374 became a major factor in the change of government.", "informal", "HO", "increased settlement and deforestation", "Afranji", "Abercynon", "drinking water", "permafrost", "10 to 15 million people", "Cabot Science Library, Lamont Library, and Widener Library", "the Henry Cole wing", "the mayor (the President of Warsaw), who may sign them into law", "The time and space hierarchy theorems", "The innate immune system", "Cow Counties", "11", "Royal Institute of British Architects", "the 6th century", "The owner", "United Parcel Serivce", "Ernie", "Sapporo", "The Rosetta Stone", "m\u0101-jan", "The Tonight Ensemble", "the right hand side of the second line of letters", "n. pl. Japanese", "William Boyd", "Hilary Mantel's Wolf Hall", "the difference with Bagel set", "the gums", "The House That Hoban Built", "Richmond, Va.", "Wawrinka", "Humphrey Bogart", "Nigel Hawthorne", "\" animal", "Auric Goldfinger", "Hell Upside Down: The Making of The Poseidon Adventure", "five", "Mary Poppins", "Neil Armstrong", "Darts", "the Metropolitan Borough of Oldham, in Greater Manchester, England.", "British Defence Secretary", "Old Ironsides", "Bullnose", "brown", "the skull", "mhoiz", "gold", "Brainy", "New Zealand", "Paige O'Hara", "Chris Martin", "Ishtar Gate", "neo-Nazi", "several weeks", "an animal tranquilizer,", "Robin Givens", "the mouth"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6533347609048039}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6206896551724138, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3722", "mrqa_squad-validation-9808", "mrqa_squad-validation-6223", "mrqa_squad-validation-962", "mrqa_squad-validation-944", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5918", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-1905", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6822", "mrqa_triviaqa-validation-1579", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-515", "mrqa_searchqa-validation-2154"], "SR": 0.59375, "CSR": 0.6377840909090908, "EFR": 0.9615384615384616, "Overall": 0.7447082604895104}, {"timecode": 11, "before_eval_results": {"predictions": ["\"Old Briton\"", "Tiffany & Co.", "1985", "y. p. orientalis", "4k + 3", "Samuel Reshevsky", "mujahideen Muslim Afghanistan", "July 1977", "coal", "programmes", "Stanford Stadium", "friction", "his work", "monatomic", "MetroCentre", "SAP Center", "composite numbers (the Carmichael numbers)", "the courts of member states", "German-language publications", "27", "6", "summer", "Durham Cathedral", "Konakuppakatil Gopinathan Balakrishnan", "Warsaw Radio Mast", "March 14, 1942", "ceramics, metal, glass, and gardens", "Bart Howard", "on the microscope's stage", "hydrogen", "Britney Spears", "2005", "1612", "February 29", "in the differential", "216", "Dr. Addison Montgomery", "michael", "food", "Kansas City Chiefs", "the Indians", "Claudia Grace Wells", "in the fascia surrounding skeletal muscle", "volume ratio", "October 2004", "Babe Ruth", "grouiner", "Jaydev Shah", "Jodie Foster", "13,000 astronomical units", "1978", "wisdom", "Thomas Chisholm", "Rocinante", "before the first letter of an interrogative sentence", "Wikia", "The Daily Mirror", "an invoice", "the Cumberland Plain", "1,500", "Lee Myung-Bak", "Alexander Pushkin", "richmond", "The Time Machine"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6662946428571428}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.33333333333333337, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10142", "mrqa_squad-validation-4963", "mrqa_squad-validation-3450", "mrqa_squad-validation-3947", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-3602", "mrqa_triviaqa-validation-412", "mrqa_hotpotqa-validation-2913", "mrqa_newsqa-validation-3686", "mrqa_searchqa-validation-14088", "mrqa_searchqa-validation-2383"], "SR": 0.59375, "CSR": 0.6341145833333333, "EFR": 0.9615384615384616, "Overall": 0.7439743589743589}, {"timecode": 12, "before_eval_results": {"predictions": ["2014", "Muslim state", "Frederick William", "herbal remedies", "Mongolian patrimonial feudalism and the traditional Chinese autocratic-bureaucratic system", "96.26%", "late 19th century", "Louis Paul Cailletet", "more than $45,000", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "n < p < 2n \u2212 2", "illegal", "A", "The European Court of Justice", "phycobilin", "German-language publications", "1992", "25 percent", "in the stems and roots of certain vascular plants", "Richardson", "1976", "British Army soldiers shot and killed people while under attack by a mob", "Judiththia Aline Keppel", "Ted '' Levine", "Sebastian Lund", "noble gas", "a pole", "Jaydev Shah", "Polly Walker", "Raya Yarbrough", "Darlene Cates", "2014 Winter Olympics in Sochi", "northern Europe's seasonal less solar radiation", "Las Vegas", "to feel close to his son", "11 November 1918", "Jean F Kernel", "Haiti", "1949", "bearers", "Chandan Shetty", "Bill Henderson", "Liam Cunningham", "Nancy Jean Cartwright", "Max", "Walter", "a premalignant flat", "on the medulla oblongata", "Humpty Dumpty and Kitty Softpaws", "Ren\u00e9 Descartes", "Clarence Anglin", "Jason Lee", "October 1941", "2013", "Antarctica", "Bath and Wells", "John Nash", "Alistair Grant", "1991", "$2 billion", "Tutsis", "Inkerman", "Brownsville", "Ralph Lauren"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6077008928571428}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.11111111111111112, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.1111111111111111, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.25, 0.6, 0.4, 1.0, 0.5, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.8, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8206", "mrqa_squad-validation-8412", "mrqa_squad-validation-3296", "mrqa_squad-validation-3474", "mrqa_squad-validation-6655", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-5259", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3611", "mrqa_triviaqa-validation-6797", "mrqa_hotpotqa-validation-2319", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-3661", "mrqa_searchqa-validation-3744"], "SR": 0.484375, "CSR": 0.6225961538461539, "EFR": 1.0, "Overall": 0.7493629807692307}, {"timecode": 13, "before_eval_results": {"predictions": ["early 1990s", "the 17th century", "Four thousand", "P", "the seal of the Federal Communications Commission", "gentrification of older neighbourhoods", "a historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\"", "1206", "2011", "in his lab", "their bright colors sometimes override the chlorophyll green", "National Broadcasting Company", "up to three-fourths of the population of the Iranian Plateau", "The Five Doctors", "Open Door Policy", "1538", "Cricket", "minimum adequate diet", "Teri", "Herbert Hoover", "Saint Wenceslaus", "coal", "the Union Pacific & the Central Pacific", "joey", "the skulls of \"Peking Man,\" or Homo erectus", "Latin", "War of the Worlds", "Luxor", "to work hard", "the Osmonds", "cold air", "butterflies", "the ballerina", "The Real Apprentices", "Calypso", "Richard I", "A Million Little pieces", "the Peace Treaty of Paris", "\"Imagine\"", "the Billy Goats Gruff", "jedoublen/jeopardy", "bacon strips", "Saturn", "the Urals", "Amsterdam", "Etna", "Maui", "Richard Nixon", "\"Under The Sea\"", "the \"T\" Formation", "George Carlin", "Ernesto \"Che\" Guevara", "ukulele", "Kurdish", "Nicholas Sparks", "Lizzy Greene", "B\u00e9la Bart\u00f3k", "Ivan Owen", "English Electric Canberra", "the murder of 40-50 Karankawa people in Mexican Texas near present-day Matagorda by a party of White colonists in 1826", "Karthik Rajaram", "Lindsey oil refinery in eastern England.", "a month of training", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah, overloading that facility."], "metric_results": {"EM": 0.484375, "QA-F1": 0.569918735268}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0909090909090909, 0.5, 0.4444444444444444, 0.0, 0.1904761904761905]}}, "before_error_ids": ["mrqa_squad-validation-9575", "mrqa_squad-validation-8229", "mrqa_squad-validation-5605", "mrqa_squad-validation-6250", "mrqa_squad-validation-7792", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-6844", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-16654", "mrqa_searchqa-validation-5858", "mrqa_searchqa-validation-389", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-6353", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-8592", "mrqa_searchqa-validation-9885", "mrqa_triviaqa-validation-5761", "mrqa_hotpotqa-validation-1876", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-909"], "SR": 0.484375, "CSR": 0.6127232142857143, "EFR": 1.0, "Overall": 0.7473883928571429}, {"timecode": 14, "before_eval_results": {"predictions": ["Pittsburgh Steelers", "J. S. Bach", "Broncos", "from January 1964, until it achieved the first manned landing in July 1969, after which he returned to Air Force duty.", "unicellular organisms", "The Quasiturbine", "Dave Logan", "Roger NFL", "a cascade method", "December 2014", "Ladner", "the Ming dynasty", "Broncos", "15,100", "Ronnie Hillman", "Masha Skorobogatov", "Daniel A. Dailey", "on the urinary floor", "31 December 1600", "at an intersection with U.S. Route 340 ( US 340 ) near Front Royal", "1916", "Milira", "in vitro", "The Walking Dead", "Ceramic art", "government monopoly", "Tbilisi, Georgia", "a candidate state must be a free market democracy", "Yondu Udonta", "religious Hindu musical theatre styles", "Rocinante", "one", "John Roberts", "Ray Charles", "British and French Canadian fur traders", "routing information base ( RIB )", "deceased - donor ( formerly known as cadaveric ) or living - donor transplantation", "Bobby Darin", "1995 Mitsubishi Eclipse", "Lana Del Rey", "Kim Basinger", "photoelectric", "5,534", "Valmiki", "September 19", "President pro tempore", "February 16, 2016", "Wisconsin", "Merry Clayton", "very important", "Florida", "a violation of nature", "Nepal", "Austria - Hungary", "Word Options", "Australia", "June 2, 2008", "Reverend Lovejoy", "Arthur E. Morgan III,", "a Coptic family", "Australia", "Oxfam International", "a German immigrant whose father was the mayor of", "Oshkosh"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6294642857142857}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.2857142857142857, 0.8, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3854", "mrqa_squad-validation-6453", "mrqa_squad-validation-690", "mrqa_squad-validation-80", "mrqa_squad-validation-8062", "mrqa_squad-validation-362", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-5650", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2435", "mrqa_searchqa-validation-11788", "mrqa_searchqa-validation-913"], "SR": 0.53125, "CSR": 0.6072916666666667, "EFR": 1.0, "Overall": 0.7463020833333334}, {"timecode": 15, "before_eval_results": {"predictions": ["chest pains", "thermodynamic", "The Lone Ranger", "broken arm", "Arthur Woolf", "roughly equivalent to little Hugos, or those who want Hugo", "Miller", "Warren Buffett", "Malkin Athletic Center", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "miniature cydippids,", "Lucas Horenbout", "a 22-yard throw to receiver Andre Caldwell", "Spektor", "writ of certiorari", "the right of the dinner plate", "Joanne Wheatley", "flawed democracy", "Southwest Florida International Airport ( RSW )", "In 1987", "the pituitary gland", "Missi Hale", "their son", "Bokm\u00e5l", "Scarborough near Flamborough Head", "Television demonstrations are held", "19 days in 1998", "1937", "the chant was first adopted by the university's science club in 1886", "gastrocnemius", "biblical \u05de\u05b7\u05dc\u05b0\u05db\u05b8\u05bc\u05dd", "336", "1961", "1 atm pressure", "Firoz Shah Tughlaq", "Isaiah Amir Mustafa", "Southern California Timing Association ( SCTA )", "The photoelectric ( optical ) smoke detector", "New York University", "the mascot", "the ball is fed into the gap between the two forward packs", "on the microscope's stage", "9 February 2018", "31 October 1972", "Jesse McCartney", "4.25 inches ( 108 mm )", "Lady Gaga", "Director of National Intelligence", "April 1979", "the Isthmus of Corinth", "the shooter must be at least 18 or 21 years old ( or have a legal guardian present )", "the Luftwaffe", "three nucleotides spanning positions 507 and 508 of the CFTR gene", "a single peptide bond or one amino acid with two peptide bonds", "5,874", "Alberich", "Debbie Reynolds", "\"The Brothers\"", "about 100", "58", "the ulna", "\"to compare\"", "Monday night", "fight outside of an Atlanta strip club"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6077731547509508}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5263157894736842, 0.3076923076923077, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.30769230769230765]}}, "before_error_ids": ["mrqa_squad-validation-3190", "mrqa_squad-validation-845", "mrqa_squad-validation-9399", "mrqa_squad-validation-800", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-437", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-6481", "mrqa_triviaqa-validation-2131", "mrqa_hotpotqa-validation-1447", "mrqa_newsqa-validation-1789", "mrqa_searchqa-validation-7968", "mrqa_searchqa-validation-10092", "mrqa_newsqa-validation-85"], "SR": 0.484375, "CSR": 0.599609375, "EFR": 0.9393939393939394, "Overall": 0.7326444128787879}, {"timecode": 16, "before_eval_results": {"predictions": ["427", "Michael Mullett", "impact process effects", "since 2001", "double or triple non-French linguistic origins", "German", "a course of study, lesson plan, or a practical skill", "Super Bowl XXXIII", "a protest that blocked heavy traffic passing over the A13, Brenner Autobahn, en route to Italy.", "unequal", "charging their students tuition fees", "directly every four years", "Gatsby", "The Mission San Juan Capistrano", "a crawfish", "Meriwether Lewis", "parabhu", "The Age of Innocence", "New York", "a drink of water", "polders", "Canada", "an asylum.", "average", "Marlene Dietrich", "yod", "airplanes", "Kansas", "a par laurel", "John", "the opera", "a house", "carbon dioxide", "San Francisco", "the Venus", "the moon", "copper", "the forest", "Lake Baikal", "Brazil", "the L'Abime bridge", "Laos", "Chang Apana", "a British accent", "George", "Clinton", "Jio's HoF", "John the Baptist", "Iran", "a serve", "Touch of Evil", "Billy Idol", "Nightingale", "the Golden Age of murder", "Daryl Sabara", "commemorating fealty and filial piety", "William Shakespeare", "Simon Wicks", "a trio", "North Dakota", "Turkey", "grand champion", "more than 26,000", "Fat Man"], "metric_results": {"EM": 0.359375, "QA-F1": 0.45440423976608185}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5263157894736842, 1.0, 0.5714285714285715, 0.8571428571428571, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4267", "mrqa_squad-validation-4065", "mrqa_squad-validation-1844", "mrqa_squad-validation-4332", "mrqa_squad-validation-6983", "mrqa_squad-validation-964", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-3450", "mrqa_searchqa-validation-3445", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-16782", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-3214", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-9973", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-465", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-15522", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-15665", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-1397", "mrqa_searchqa-validation-519", "mrqa_triviaqa-validation-234", "mrqa_hotpotqa-validation-1701", "mrqa_newsqa-validation-1120", "mrqa_hotpotqa-validation-5388"], "SR": 0.359375, "CSR": 0.5854779411764706, "EFR": 1.0, "Overall": 0.7419393382352941}, {"timecode": 17, "before_eval_results": {"predictions": ["forceful taking of property", "Robert Iger", "Central business districts", "occupations", "prime number theorem", "a nonphotosynthetic eukaryote engulfed a chloroplast-containing alga", "Santa Clara", "Trevathan", "General Hospital", "being drafted into the Austro-Hungarian Army", "impetus", "a statue", "Titanic", "Queen Anne", "Heroes", "Prince Hamlet", "cheap", "Sir Anthony Eden", "Defending Your Life", "a person", "Jalisco state", "Santa Fe", "San Diego Comic-Con", "Muddy Waters", "\"What hath God wrought\"", "a ballot vote", "Manfred von Richthofen", "cowboys", "Michael Collins", "John J. Pershing", "a balloon", "La Crosse", "the Jesuit Institute", "Anton Chigurh", "An American Tragedy", "an anatomical animation", "New York City", "Amherst", "60 beats per minute", "a cushion", "Lignite", "a complete heartbeat", "a rabbit", "agriculture", "The Call of the Wild", "Vladimir Putin", "Hillary Clinton", "Nikola Tesla", "gingerbread", "Elza", "a transporter", "Hubble Space Telescope", "Eli Manning", "statistical estimation or statistical inference", "federal government", "the Caribbean", "Doncaster Rovers", "Christies Beach", "a 7th-century Anglo-Saxon tumulus (or \"barrow\")", "Monday night", "Eric Besson", "needle - like teeth", "3", "relieve families who had difficulty finding jobs during the Great Depression in the United States"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5884837962962963}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.962962962962963]}}, "before_error_ids": ["mrqa_squad-validation-3106", "mrqa_squad-validation-8638", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-7483", "mrqa_searchqa-validation-10653", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-8777", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-3910", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-222", "mrqa_triviaqa-validation-2095", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-3804", "mrqa_newsqa-validation-295", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-9856"], "SR": 0.484375, "CSR": 0.5798611111111112, "EFR": 1.0, "Overall": 0.7408159722222222}, {"timecode": 18, "before_eval_results": {"predictions": ["308", "Trajan's Column", "solution", "the Danube", "scrutinise legislation", "Ralph Woodward,", "major cities", "19th Century", "high density", "the Ten Commandments", "Richard Kuklinsk", "Jefferson Memorial", "Magic Johnson", "between 7,500 and 40,000", "The Soloist", "bronze", "an Indigenous American ethic group", "feats of exploration", "Abdul Razzak Yaqoob", "Norwegian", "1868", "James Fitz James, 1st Duke of Berwick", "Hopeless Records", "Lorne Michaels", "the National Basketball Association (NBA)", "Ang Lee", "near North Chicago, in Lake County, Illinois", "1971", "former Chelsea and Middlesbrough striker Jimmy Floyd Hasselbaink", "Arkansas", "2009", "The Ministry of Utmost Happiness", "Uzumaki", "Alabama", "March 30, 2025", "Miller Brewing", "Mike Greenwell", "Daimler-Benz", "1994", "John of Gaunt", "classical", "1,521", "a royal residence", "one", "fantasy role-playing game", "Leofric", "Australian Electoral Division", "Argentinian", "the International Hotel", "American black bear", "Mot\u00f6rhead", "Richa Sharma", "An impresario", "Lake Michigan", "Tulsa, Oklahoma", "homeless", "Jessica Smith", "Hungary", "five days a week", "Maryland", "OK", "Ulysses S. Grant", "The Maracot Deep", "kidnapping"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6697250666000665}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.18181818181818182, 1.0, 0.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9479", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-525", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-4297", "mrqa_naturalquestions-validation-10113", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-4001", "mrqa_newsqa-validation-4077", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-10375"], "SR": 0.59375, "CSR": 0.580592105263158, "EFR": 1.0, "Overall": 0.7409621710526315}, {"timecode": 19, "before_eval_results": {"predictions": ["toward the end of his life", "motivated students", "native tribes", "Cuba", "Ikh Yuan \u00dcls or Yekhe Yuan Ulus", "Parliamentary time", "Private Bill Committees", "photolysis of ozone", "Eliot Ness", "Joseph Stalin", "Bobby Eli", "Eurasian Plate", "Anglican", "10.5 %", "deceased - donor ( formerly known as cadaveric )", "the South Pacific Ocean", "silk floss", "Tom Waits", "Cherbourg in France", "Thomas Andrews", "T.J. Miller", "full '' sexual intercourse", "1963", "Paradise, Nevada", "Mike Nesmith", "Eagle Ridge Outdoor pool", "Office of Inspector General", "Skat", "1878", "Rockwell", "Scott Bakula as Dwayne `` King '' Cassius Pride", "October 2008", "the Two Brothers", "a theory of T\u0101\u1e47\u1e0dava dance ( Shiva )", "Heather Stebbins", "Spanish surname", "supervillains who pose catastrophic challenges to the world", "1999", "Geraldine Margaret Agnew - Somerville", "Bill Farmer", "October 2, 2017", "Stephen Curry of Davidson", "December 15, 2016", "a star", "heavy tank", "The Stanley Hotel", "Marty Robbins", "Albert Einstein", "early 2017", "Ed Sheeran", "centralized administrative planning", "HTTP / 1.1", "A substitute good", "Midsomer Murders", "Hawaii", "Hopeless Records", "death sentence", "Seasons of My Heart", "Terry Bollea", "The Call of the Wild", "a cricothyroidotomy", "the University of Pennsylvania", "Vancouver", "Hudson River"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5530535130718954}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.7000000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.2, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8084", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9275", "mrqa_hotpotqa-validation-4897", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-568"], "SR": 0.484375, "CSR": 0.57578125, "EFR": 0.9696969696969697, "Overall": 0.7339393939393939}, {"timecode": 20, "UKR": 0.740234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-2139", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2467", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10135", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2025", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-3628", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1789", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11788", "mrqa_searchqa-validation-12143", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-16723", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-3450", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6061", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6844", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-6969", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8063", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_squad-validation-10052", "mrqa_squad-validation-10107", "mrqa_squad-validation-10125", "mrqa_squad-validation-10149", "mrqa_squad-validation-10186", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10333", "mrqa_squad-validation-10341", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10445", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10493", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-116", "mrqa_squad-validation-1193", "mrqa_squad-validation-1257", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1527", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1684", "mrqa_squad-validation-1754", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2092", "mrqa_squad-validation-2166", "mrqa_squad-validation-2288", "mrqa_squad-validation-2302", "mrqa_squad-validation-232", "mrqa_squad-validation-2322", "mrqa_squad-validation-2324", "mrqa_squad-validation-2344", "mrqa_squad-validation-2406", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2559", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2737", "mrqa_squad-validation-2778", "mrqa_squad-validation-2827", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-32", "mrqa_squad-validation-3217", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3506", "mrqa_squad-validation-3617", "mrqa_squad-validation-362", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3923", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-401", "mrqa_squad-validation-4086", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4248", "mrqa_squad-validation-4287", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4836", "mrqa_squad-validation-4974", "mrqa_squad-validation-5012", "mrqa_squad-validation-5088", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5379", "mrqa_squad-validation-5451", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5950", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6069", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6250", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6605", "mrqa_squad-validation-6671", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6843", "mrqa_squad-validation-6846", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7476", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-789", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-800", "mrqa_squad-validation-805", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8193", "mrqa_squad-validation-8197", "mrqa_squad-validation-8307", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-845", "mrqa_squad-validation-852", "mrqa_squad-validation-8580", "mrqa_squad-validation-8696", "mrqa_squad-validation-8771", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8798", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8935", "mrqa_squad-validation-8953", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9141", "mrqa_squad-validation-9254", "mrqa_squad-validation-9270", "mrqa_squad-validation-929", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9457", "mrqa_squad-validation-9479", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9630", "mrqa_squad-validation-964", "mrqa_squad-validation-9718", "mrqa_squad-validation-9766", "mrqa_squad-validation-9768", "mrqa_squad-validation-985", "mrqa_squad-validation-9968", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1751", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2106", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.84765625, "KG": 0.4640625, "before_eval_results": {"predictions": ["Museum of the Moving Image in London", "Jean Fran\u00e7ois de Troy, Jean-Baptiste Pater", "the Ministry of Justice", "During the Second World War", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "an assembly center", "Leonardo da Vinci", "1350", "two Nobel Peace Prizes", "a Taylor series", "June 12, 2017", "Prussian", "Gatwick Airport", "27 November 1956", "Dan Conner", "supernatural psychological horror film", "Billy Joel", "The Times Higher Education Guide", "The conversation", "Al D'Amato", "nuclear weapons", "Lush Ltd.", "Port Macquarie", "non-alcoholic recipe", "Lonestar", "Brea, California", "World War II", "Brazilian Jiu-Jitsu", "Biola University", "Columbia Pictures", "February 22, 1968", "Erreway", "2013", "Kim Bauer", "the Joint Chiefs of Staff", "The Postal Service", "1955", "1993", "Morris County, Kansas", "John Duigan", "John Boyd Dunlop", "Iran", "The Cherokee\u2013American wars", "a creek", "Edward Trowbridge Collins Sr.", "Province of New York", "Archie Andrews", "January 28, 2016", "NBA Finals Most Valuable Player Award", "Steve Martin", "Israeli Declaration of Independence", "Sleepy Hollow", "Mumbai", "1996", "Scheria", "The National Council for the Unmarried Mother", "Doubting Castle", "Noida", "genocide", "Frank Hamer", "Howard Hughes", "Dairy Queen", "Australian rock band", "the Iberian Peninsula"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7286706349206349}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false], "QA-F1": [0.0, 0.2222222222222222, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7674", "mrqa_squad-validation-5489", "mrqa_squad-validation-8189", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2669", "mrqa_hotpotqa-validation-5727", "mrqa_triviaqa-validation-5418", "mrqa_newsqa-validation-3563", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-12943"], "SR": 0.671875, "CSR": 0.5803571428571428, "EFR": 1.0, "Overall": 0.7264620535714286}, {"timecode": 21, "before_eval_results": {"predictions": ["Several thousand", "Paris", "Islamic Republic", "different subject specialists each session during the week", "Sonia Shankman Orthogenic School", "inverse proportionality of acceleration", "alternating current", "1002", "Guardians of the Galaxy Vol. 2", "1987", "arts manager", "software programmer", "UHF channel 44 (or virtual channel 6 via PSIP)", "their unusual behavior", "bassline", "Ireland", "Valley Falls", "the U\u00ed \u00cdmair", "Ry\u016bkyuan", "Dallas", "a leg injury", "Hirsch index rating", "neo-Nazi", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "2008", "Michael Crawford", "Isobel", "Nicholas \" Nick\" Offerman", "American", "the Dominican Republic", "29,000", "Wes Unseld", "Mr. Church", "1990", "Ronald Wilson Reagan", "Raymond Albert Romano", "The Dressmaker", "Kennedy Road", "30.9%", "Peter Yarrow", "\"From Here to Eternity\"", "Kentucky Bats", "\"The Royal Family\"", "Backstreet Boys", "Lynn Minmei", "King James I of England", "Derek Jacobi", "August 14, 1848", "Catwoman", "The Hindu Group", "Detroit Lions and the Los Angeles Rams", "Audi", "1858", "Phillip Schofield and Christine Bleakley", "USS Chesapeake", "Jupiter", "Ric Pipino", "CNN", "Somali", "\"Do You Want Crying\"", "Joe Jackson", "Michael Arrington", "Krishna Rajaram", "at least 12 months"], "metric_results": {"EM": 0.5, "QA-F1": 0.599032738095238}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1894", "mrqa_squad-validation-10424", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-4843", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-4792", "mrqa_naturalquestions-validation-1786", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3190", "mrqa_newsqa-validation-3178", "mrqa_searchqa-validation-6538", "mrqa_searchqa-validation-939", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-274"], "SR": 0.5, "CSR": 0.5767045454545454, "EFR": 1.0, "Overall": 0.7257315340909091}, {"timecode": 22, "before_eval_results": {"predictions": ["1,230 kilometres (764 miles)", "anti-colonial movements", "The Middle and Modern Family", "An increase in imported cars", "immunomodulators", "inequality", "Duval County", "Univision", "Pieter van Musschenbroek", "\"Beauty and the Beast\"", "every aspect of public and private life", "Missouri Tigers", "Acela Express", "German", "Cartoon Network", "Aamir Khan", "trans-Pacific flight", "Kristina Ceyton and Kristian Moliere", "Ginger Rogers", "Dan Castellaneta", "1995 to 2012", "Saint Petersburg Conservatory", "Columbine", "one", "Colonel", "Ars Nova Theater", "Donna Paige Helmintoller", "Detroit, Michigan", "CBS News", "1838", "Assistant Director Neil J. Welch", "near Philip Billard Municipal Airport", "ZZ Top", "2", "The Handmaid's Tale", "Rick and Morty", "second largest", "Bamyan Province", "Melbourne", "before 1638", "247,597", "sexual activity", "Yoruba people", "Who's That Girl", "Telugu", "1800000 sqft", "Malayalam cinema", "New York State Route 907E", "Man Booker Prize", "child actor", "October 21, 2016", "Taeko Ikeda", "Claire Rhiannon Holt", "abbreviation", "Thomas Chisholm", "Brazil", "Superman", "Wales", "since 1983", "three", "threatening messages", "China", "\"Nothing is forever\"", "Khmer Rouge"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7625}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9302", "mrqa_hotpotqa-validation-3366", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-2446", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-1384", "mrqa_naturalquestions-validation-4308", "mrqa_triviaqa-validation-5792", "mrqa_newsqa-validation-377", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-6136"], "SR": 0.6875, "CSR": 0.5815217391304348, "EFR": 1.0, "Overall": 0.7266949728260869}, {"timecode": 23, "before_eval_results": {"predictions": ["28.5\u00b0E", "House of Hohenstaufen", "seven", "BBC Dead Ringers", "Mnemiopsis", "Dr. George E. Mueller", "salmon", "the Black Russian", "heating", "Gerald Robert Newhart", "constellations", "chile", "Australia", "Crime and Punishment", "The Plaza Hotel", "Stephen Hawking", "Rodeo", "Hawaii", "a forearm", "a bas relief", "Gerald Sharpe", "The M1 Abrams", "The Moon", "an egg", "Gerald IV", "Jean Foucault", "Gov. Schwarzenegger", "Gerald E. Neugebauer", "College of William and Mary", "Gerald Burton", "aluminum", "aujourd'hui", "Barnard College", "Caracalla", "Gerald Goodstein", "Gerald Jackson", "Delaware", "a stone", "George W. Bush", "a bat ray", "James Cook", "Bosom Buddies", "Alexander Calder", "Golden Retriever", "Walter Crawford Kelly, Jr.", "Edith Wharton", "Rapa Nui National Park", "Nike", "gravity", "an oblate spheroid", "chile", "Gerald Asscherick", "Bay of Montevideo", "The U.S. state of Georgia", "1820s", "chile", "A, O, E, and U", "a chock", "Las Vegas Boulevard", "\"Slaughterhouse-Five\"", "Food and Agriculture Organization", "further reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "a service agreement signed in April 2007 with the Kurdish Regional Government to build 180 kilometers of natural gas pipeline and two liquefied petroleum gas (LPG) plants, which are 80 percent complete.", "2050"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5198660714285714}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1045", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-4758", "mrqa_searchqa-validation-15407", "mrqa_searchqa-validation-12149", "mrqa_searchqa-validation-13231", "mrqa_searchqa-validation-9639", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-14893", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-2225", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-596", "mrqa_searchqa-validation-14432", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-4374", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-15964", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-3087", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-1639"], "SR": 0.4375, "CSR": 0.5755208333333333, "EFR": 0.9722222222222222, "Overall": 0.719939236111111}, {"timecode": 24, "before_eval_results": {"predictions": ["ca. 2 million", "Catholic", "The waxy cuticle of many leaves", "theta intermediary form", "long-run economic growth", "a legitimate forum for prosecution, while bringing them in line with the rule of law.", "two-state solution", "WFTV", "a Taliban member who had come for the talks about peace and reconciliation, and detonated the explosives as he entered the home.", "tuatara", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "eight", "Haiti", "American", "United States, NATO member states, Russia and India", "25", "18", "Sunday", "a rally", "Herman Cain", "humans", "Friday", "Saluhallen", "murder", "the Swat Valley", "two years", "228", "Keating Holland", "one of the many restaurants that offer visitors their products.\"", "Steve Williams", "iTunes", "Nazi Germany", "\"It's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "Iran", "five", "July", "A Biography", "100 percent", "whites", "Abu Sayyaf,", "The Palm", "There's no chance of it being open on time.", "Egypt", "Jeddah, Saudi Arabia", "poor families", "Russia", "2011", "use of torture and indefinite detention", "a skull", "Iran", "a review of state government practices completed in 100 days.", "stripper pole photos", "Michael Schumacher", "the governor of West Virginia", "Massachusetts", "phi", "the natural world", "The Lion King", "7pm", "1966", "Wendell Erdman Berry", "Twelfth Night", "Shabbat", "yellow"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6180305348423953}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.08695652173913043, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.3636363636363636, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6435", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2201", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1419", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-173", "mrqa_searchqa-validation-2076"], "SR": 0.53125, "CSR": 0.57375, "EFR": 0.9, "Overall": 0.705140625}, {"timecode": 25, "before_eval_results": {"predictions": ["primes", "UNESCO's World Heritage list", "lower incomes", "Wankel", "five or more seats", "the Bronx", "northwest Pakistan", "digging ditches.", "Oregon", "Democrats and Republicans", "Fullerton, California,", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "The minister later apologized, telling CNN his comments had been taken out of context.", "Briton Carl Froch", "10 below", "\"have no problems about the school, they are happy about everything.\"", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "a rifle and began firing.", "The pilot, whose name has not yet been released,", "The children have been living in an orphanage in Abeche", "The soldiers fired back at their attackers, killing at least four of them.", "air support", "Marie-Therese Walter", "three French journalists, a seven-member Spanish flight crew and one Belgian", "free enterprise in history", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "Swansea Crown Court,", "The Uighurs", "Michael Jackson", "Kurdistan Freedom Falcons,", "insurgent small arms fire,", "quality of teaching and learning in American schools", "a lump in Henry's nether regions", "on the 12th on the Blue Monster course at Doral", "glamour and hedonism", "\"I am sick of life\"", "a peace sign.", "the Southeast,", "Diego Milito", "a series of bear attacks in recent months in the United States.", "The FARC", "Florida", "his son, Isaac, and daughter, Rebecca.", "rebels", "three Ghanaians, two Liberians and a Togo national", "\"How I Met Your Mother,\"", "London", "Mafia", "Stratfor,", "The BBC", "CBS, CNN, Fox and The Associated Press.", "Ashley \"A.J.\" Jewell,", "stratum lucidum", "Gettysburg College", "Italian / Venetian John Cabot", "Celsius", "75", "supreme religious leader", "Charles Quinton Murphy", "Rio Gavin Ferdinand", "more than 110", "Athens", "Craig", "artesian"], "metric_results": {"EM": 0.390625, "QA-F1": 0.48596772151459655}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.4, 0.2666666666666667, 0.0, 0.0, 0.0, 0.2666666666666667, 0.0, 0.2222222222222222, 0.0, 0.08333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.8, 0.15384615384615385, 0.0, 0.0, 0.375, 0.28571428571428575, 1.0, 1.0, 0.5, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-3680", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-561", "mrqa_naturalquestions-validation-8585", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-1390", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-4004", "mrqa_searchqa-validation-1916", "mrqa_searchqa-validation-16961"], "SR": 0.390625, "CSR": 0.5667067307692308, "EFR": 1.0, "Overall": 0.7237319711538461}, {"timecode": 26, "before_eval_results": {"predictions": ["enhanced transit infrastructure, possible shuttles open to the public, and park space which will also be publicly accessible.", "Katy\u0144 Museum", "two", "a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship.", "Martin O'Neill", "1822", "Stephanie Plum", "Dominican", "Evgeni Arkadievich Platov", "Knoxville, Tennessee", "Enkare Nairobi", "Field Marshal Stapleton Cotton", "Shari Shattuck", "Si Da Ming Bu", "\"The Blue Album\"", "Apsley George Benet Cherry-Garrard", "A basilica", "steamy pictorials of celebrities", "Odawa", "Columbus Crew SC", "Yoo Seung-ho", "1989 until 1994", "\"To Know Him Is to Love Him,\"", "Wildhorn", "former pornographicstar", "Montreal", "Sam the Sham", "John Nicholas Galleher", "San Francisco 49ers", "Prince of Cambodia Norodom Sihanouk", "Durham, North Carolina", "Double Crossed", "19th", "Donald McNichol Sutherland", "provides its services in the Japanese market", "Security Management", "143,372", "musician", "Cersei", "Fort Worth", "Dutch", "shorthand writing", "North Atlantic Treaty Organisation (NATO)", "\"The Young ones\"", "Tabasco", "Sunday, November 2, 2003", "1853", "1.5 million households", "Neymar da Silva Santos J\u00fanior", "Plymouth Regional High School", "\"Tainted Love\"", "Dissection", "celebrity alumna Cecil Lockhart", "MercyMe", "in a thousand years", "a crash cymbal", "(Qatar)", "an English adaptation of Deutsch,", "38 feet", "identity documents", "Shenzhen", "Brigham Young", "Silly Putty", "a security issued by the Government National Mortgage Association"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5937596006144393}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8571428571428571, 0.0, 0.4, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6426", "mrqa_hotpotqa-validation-3749", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-111", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-34", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-3889", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-2131", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-5740", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-3954", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2495", "mrqa_searchqa-validation-10970"], "SR": 0.46875, "CSR": 0.5630787037037037, "EFR": 1.0, "Overall": 0.7230063657407407}, {"timecode": 27, "before_eval_results": {"predictions": ["In low-light conditions", "1550", "60 days", "$60,000", "four", "Steve Goodman", "Jonathan Goldstein", "Cristeta Comerford", "Secretary of Homeland Security", "1996", "Tristan Rogers", "Mason Alan Dinehart", "counter clockwise direction", "Paul Hogan", "tennis", "Matt Monro", "IIII", "Fa Ze Rug", "8ft", "Robert Irsay", "to manage the characteristics of the beer's head", "moral tale", "2019", "Scott Drever", "Arctic Ocean", "the earliest known official or large - scale celebration of Pi Day", "his guilt in killing the bird", "1", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "December 1, 2017", "James Watson and Francis Crick", "Sara Gilbert", "Tokyo / Helsinki", "the courts", "John Travolta", "Djokovic", "Rigg", "Ant & Dec", "Nepal", "Joanne Wheatley", "UNESCO / ILO", "September 24, 2012", "lowest air temperature record was set on 21 July 1983, with \u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F ) at Vostok Station", "summer", "between the Eastern Ghats and the Bay of Bengal", "Daniel Suarez", "1 - 2 spinal nerve segments above the point of entry", "16 August 1975", "useless, time - wasting", "Kyla Pratt", "Tim McGraw", "Beorn", "UN Supreme Commander Gen. Douglas MacArthur", "Robert Maxwell", "Scotland", "Forbes", "2006", "Fife", "April 28", "Russia and the United States", "one American diplomat to a \"prostitute\"", "unassisted triple play", "Ben Kingsley", "New Mexico"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6733845581501832}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.7692307692307692, 1.0, 1.0, 0.0, 0.125, 0.0, 0.0, 1.0, 0.09090909090909093, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 1.0, 1.0, 0.0, 0.9, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.4, 0.0, 0.4, 0.9090909090909091, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1313", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7158", "mrqa_triviaqa-validation-3649", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-506", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-2350", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-12174"], "SR": 0.546875, "CSR": 0.5625, "EFR": 0.896551724137931, "Overall": 0.7022009698275862}, {"timecode": 28, "before_eval_results": {"predictions": ["Elisabeth Sladen", "1206", "Genghis Khan", "Shenzhen in southern China.", "24", "Monday night where it left off in September with what Sedgwick called \"a fantastic five episodes.\"", "a delegation of American Muslim and Christian leaders", "September 21.", "1,500 but was overcrowded with about 2,000 people who were traveling from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Romney for his \"solid credentials,\" saying he was the most likely candidate to see through \"knee-jerk, ideological\" perspectives and \"bridge the political divide in Washington.\"", "269,000 copies", "Eden Park", "The Ski Train", "a sailboat matching the description of the missing 38-foot boat was found overturned about 5:15 p.m. Saturday,", "A large concrete block is next to his shoulder, with shattered pieces of it around him. Blood trickles down the road.", "to stop rocket fire on its southern cities and towns.", "dogs who walk on ice in Alaska.", "his father's parenting skills.", "the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "Haitians", "General Motors", "militant group declared an \"all-out war\" on the government", "more than 9,500 energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "it is currently home to 15 African and Asian elephants.", "to secure more funds from the region", "fire a missile toward Hawaii on July 4.", "He retired from the Army after nearly 40 years of service with the rank of lieutenant general on April 28 -- one day before he was sworn in as ambassador,", "defaulted on the mortgage and the house fell into foreclosure.", "A member of the group dubbed the \"Jena 6\"", "a growing number of state governments going after them.", "Anil Kapoor", "Illlinois.", "the South Korean military responded by raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond.", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Nazi Germany", "government-supported prevention efforts and aggressive public awareness campaigns, the so-called Brazilian response has been hailed as a model for developing countries.", "gun conviction", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "don't believe the U.S. assertion that the system is needed to guard against imminent threats from Iran or North Korea.", "David McKenzie", "citizenship because he was depriving his wife of the liberty to come and go with her face uncovered,", "a steep embankment in the Angeles National Forest", "through a facility in Salt Lake City, Utah,", "can't get along with her co-star Kristin Davis, while another would allege there were catfights on the set of the sequel,", "remains unknown,", "will they do when I got out of the game.", "Alwin Landry's supply vessel Damon Bankston", "on a volatile zone along the equator between South America and Africa.", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case is recovering well and wants to see the ocean and a pop concert,", "an empty tub, his face blue and purple and a chain around his neck,", "in a tenement in the Mumbai suburb of Chembur, with eight people living together in a single room.", "California, Texas and Florida, with the rest scattered through the South, Midwest and West.", "Rigg", "the efferent nerves that directly innervate muscles", "the colonization of the Americas began and the cocoa plant was discovered in regions of Mesoamerica, until the present", "The Duchess", "Bahrain", "Tokyo International Airport", "Danny Lebern Glover", "William Shakespeare", "British", "a full-automatic fire functionality", "American Sign Language", "Turtle Wax"], "metric_results": {"EM": 0.265625, "QA-F1": 0.4283026161595291}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.23529411764705882, 1.0, 0.0, 0.07692307692307693, 0.08, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.25, 0.6666666666666666, 0.06896551724137931, 1.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.8571428571428571, 0.6, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.2666666666666667, 0.4615384615384615, 1.0, 0.36363636363636365, 0.6666666666666666, 0.0909090909090909, 0.08333333333333333, 1.0, 0.1111111111111111, 0.923076923076923, 1.0, 0.45714285714285713, 1.0, 0.0, 0.5, 0.9473684210526316, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.5, 0.125, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-301", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-2338", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-7484", "mrqa_triviaqa-validation-1993", "mrqa_hotpotqa-validation-1922", "mrqa_searchqa-validation-9212", "mrqa_searchqa-validation-6288"], "SR": 0.265625, "CSR": 0.5522629310344828, "EFR": 1.0, "Overall": 0.7208432112068965}, {"timecode": 29, "before_eval_results": {"predictions": ["Disney\u2013ABC Domestic Television", "2011", "Soviet", "naltrexone (Vivitrol)", "sense of smell", "Ingrid Chambers", "Florence", "Wrigley's Spearmint", "Oprah Winfrey", "2004", "a Great Dane", "Director General of the Security Service", "Caracas", "Rock Follies of \u201977", "Do I Hear a Waltz?,", "The Nobel Prize in Literature", "Celtic", "Northwestern University", "the best value diamond for your money", "The Star Spangled Banner", "(a)(adj)", "Ibrox Stadium", "Boston", "Stirling Moss", "Micael Caine", "Llyn Padarn", "Little Dorrit", "Tacitus", "apples", "Chekhov", "Chris Evans", "Robert Burns", "Declaration of Independence", "Lome", "a condor", "Belgium", "Pilgrim's Progress", "Plato", "Fulham Football Club", "clay", "Australia", "Alaska", "God", "the Andaluc\u00eda region", "a symbol of the Michelin tyres company", "Tesco", "Z", "a bear suit", "Clio Awards", "Pygmalion", "Watford Football Club", "Trainspotting", "English author Rudyard Kipling", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Vicente Fox", "in Hungarian historical chronicles in the year 1206 as \"Galitsiya\"", "1919", "August 24, 1983", "ensuring that all prescription drugs on the market are FDA approved,", "International Polo Club Palm Beach in Florida.", "killed at least 63 people and wounded more than 200.", "Sappho", "the White House", "Steely Dan"], "metric_results": {"EM": 0.5, "QA-F1": 0.6171561482498983}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.4, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.9600000000000001, 1.0, 0.0, 1.0, 0.5, 0.07407407407407408, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7651", "mrqa_triviaqa-validation-2203", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-4186", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-566", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-1069", "mrqa_triviaqa-validation-874", "mrqa_triviaqa-validation-6429", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-855", "mrqa_naturalquestions-validation-3697", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-4547", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1612"], "SR": 0.5, "CSR": 0.5505208333333333, "EFR": 0.96875, "Overall": 0.7142447916666667}, {"timecode": 30, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2030", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-3249", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3655", "mrqa_hotpotqa-validation-3701", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-5047", "mrqa_hotpotqa-validation-505", "mrqa_hotpotqa-validation-5213", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5645", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-988", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4308", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-994", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2553", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-919", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-14268", "mrqa_searchqa-validation-14735", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-1664", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_squad-validation-10052", "mrqa_squad-validation-10125", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10308", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-1159", "mrqa_squad-validation-1193", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-1368", "mrqa_squad-validation-1503", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2166", "mrqa_squad-validation-2324", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2778", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-3259", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3831", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3916", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-4065", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4191", "mrqa_squad-validation-4248", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4746", "mrqa_squad-validation-4836", "mrqa_squad-validation-5009", "mrqa_squad-validation-5088", "mrqa_squad-validation-5108", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5180", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5521", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5964", "mrqa_squad-validation-6001", "mrqa_squad-validation-6069", "mrqa_squad-validation-6082", "mrqa_squad-validation-6158", "mrqa_squad-validation-6256", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6592", "mrqa_squad-validation-6605", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-709", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7476", "mrqa_squad-validation-7485", "mrqa_squad-validation-7502", "mrqa_squad-validation-7578", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-8159", "mrqa_squad-validation-8213", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8580", "mrqa_squad-validation-8681", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8935", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9141", "mrqa_squad-validation-9270", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9510", "mrqa_squad-validation-9569", "mrqa_squad-validation-964", "mrqa_squad-validation-9759", "mrqa_squad-validation-9766", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1329", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-576", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-855", "mrqa_triviaqa-validation-874", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-996"], "OKR": 0.8046875, "KG": 0.3953125, "before_eval_results": {"predictions": ["1947", "Madame de Pompadour", "$125 per month", "helen shapiro", "watcher", "llanabe", "cornet", "doubting man", "Lisieux", "the Astor family", "heddlu Dyfed-Powys", "canned Heat", "John Huston", "c.I.D.", "Christopher Lee", "cabot", "head of science", "Patrick Kielty", "Norfolk Island", "llansey", "Mexico", "Worcester Cathedral", "llanchenry", "Albert Finney", "Carrefour", "kenzie Russell", "in the garden of Gethsemane", "John Galliano", "swallow sidecar Company", "mitzi Gaynor", "Mickey Mouse", "UVB", "Plato", "Bugsy Malone", "1812", "llanar", "basil", "michael kryry", "cfs", "lyonesse", "fondue", "the AllStars", "kettle Falls, WA", "Copenhagen", "hokkaido", "mambo", "George W. Bush", "jockey", "comedian and writer David Mitchell", "llanow Weed", "constitution", "wrinkles", "Total Drama World Tour", "the king's army", "Real Madrid", "two", "Balvenie Castle", "January 18, 1977", "threatening messages", "London", "fight outside of an Atlanta strip club", "a lemur", "Communist Party", "doubting"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4996737637362637}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.30769230769230765, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5491", "mrqa_triviaqa-validation-6506", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2299", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5311", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-6367", "mrqa_triviaqa-validation-6074", "mrqa_triviaqa-validation-3052", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4169", "mrqa_triviaqa-validation-1108", "mrqa_triviaqa-validation-4499", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-7608", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-1468", "mrqa_triviaqa-validation-5684", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-3517", "mrqa_naturalquestions-validation-6353", "mrqa_hotpotqa-validation-1351", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-85", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-3674"], "SR": 0.421875, "CSR": 0.5463709677419355, "EFR": 1.0, "Overall": 0.6895085685483872}, {"timecode": 31, "before_eval_results": {"predictions": ["nine", "along the Lower Rhine", "Pet Shop Boys", "rugby", "homo-", "Budapest", "glennrad", "his shoe", "glenn tynan", "I'm Sorry, I'll Read That Again", "davina McCall", "javelin throw", "cymru", "cymru", "French", "the Vatican", "gluteus", "duke", "Turandot", "rymruite", "Pete Ham", "Pablo Picasso", "apples", "cymru", "the Spanish", "finland", "a karst", "Brazil", "the Soviet government", "Russia", "sense of taste", "chile", "a bandersnatch", "cymru", "wales", "cymru", "nick henderson", "Anna", "Paris", "ailing fish", "finado Tuerto, Argentina", "Charlie Chaplin", "Perfect Storm", "a small rubber ball", "pottery", "finisher", "newbury", "Abraham", "vice-admiral", "1936", "cymru", "joan crawford", "Rachel Kelly Tucker", "the BBC", "pilgrimages to Jerusalem", "a farmers' co-op", "Tim Allen", "Anatoly Lunacharsky", "an empty water bottle", "Japan's", "The Rev. Alberto Cutie", "Eurasian Economic Union", "glennatt", "al-Aziz"], "metric_results": {"EM": 0.296875, "QA-F1": 0.390625}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9344", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-4054", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-4765", "mrqa_triviaqa-validation-5237", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-1633", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-1261", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-5259", "mrqa_triviaqa-validation-7645", "mrqa_triviaqa-validation-5549", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-6730", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5728", "mrqa_newsqa-validation-103", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-12801", "mrqa_searchqa-validation-2441"], "SR": 0.296875, "CSR": 0.53857421875, "EFR": 1.0, "Overall": 0.68794921875}, {"timecode": 32, "before_eval_results": {"predictions": ["TeacherspayTeachers.com", "provisional elder/deacon", "wool", "prince", "Mujib", "b\u00e9la Bart\u00f3k", "Denver", "astronaut", "Salt Lake City", "four feet", "teacher", "china", "Canada", "Peter Nichols", "American Family Publishers", "28", "Microsoft", "brigit Forsyth", "Celsius", "curvature", "Pluto", "leicestershire", "Boris Johnson", "HMS Conqueror", "Tamar", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "driver", "inner ear", "Gloucestershire", "ned", "gymnastics", "george cukor", "Ishmael", "bluebell", "ned", "Prokofiev", "cyclones", "Dan Brown", "heavy horse", "Newcastle United", "Thank you", "magnetism", "second year", "William Neil Connor", "sweden", "charles chaplin", "horse-racing", "\"Slow\"", "femur", "dragon", "peregrines", "2008", "Siddharth Arora / Vibhav Roy", "Tim Rice", "five", "\"The Walking Dead\"", "1979", "1999", "capital murder and three counts of attempted murder", "Apple employees", "suicide", "marston moor", "how timing shapes and supports brain function", "daniel Brutus"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6380208333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-2608", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-1113", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-7671", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-2258", "mrqa_triviaqa-validation-6678", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-4171", "mrqa_triviaqa-validation-6726", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-5666", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-6815", "mrqa_naturalquestions-validation-10161", "mrqa_newsqa-validation-1889", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-1831"], "SR": 0.5625, "CSR": 0.5392992424242424, "EFR": 1.0, "Overall": 0.6880942234848485}, {"timecode": 33, "before_eval_results": {"predictions": ["1,100", "since 2001", "horseshoe", "barry Briggs", "table tennis", "archibald haddock", "bart\u00f3k", "14", "Harold Shipman", "the Undertones", "michael hordern", "rogers", "yeast", "michael hold", "(359-299 Ma)", "Thank you", "nipples", "queen Mary", "lolita", "muscle tissue", "surrealism", "shinto", "sewing machines", "Morgan Spurlock", "john Buchan", "seals", "workington", "state laws", "stenographer", "Altamont Speedway", "fourteen", "jack Sprat", "?Operation Dynamo?", "the Allstars", "john", "praseodymium", "pickled", "norway", "a thick layer of dough and then baked in a stone-bottom or hearth oven.", "chairman", "Wicked Witch", "Rita Hayworth", "the Observer", "Isaac Newton", "aleister Crowley", "bullfighting", "Arthur C. Clarke", "Marc Warren", "entropy", "Chad", "earring", "taggart", "on the microscope's stage", "( 3 gold, 5 silver, 1 bronze )", "Norway", "Virgin", "Philadelphia Naval Shipyard", "African descendants", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "cross-country skiers", "\"a smoking gun of confirmation of Brazil's effort to engage in operations to overthrow the government of Chile and a discussion of collusion with the United States.\"", "hook and ladder", "Hill Street Blues", "white"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6550065066512435}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5384615384615384, 1.0, 0.2105263157894737, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-1410", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-7142", "mrqa_triviaqa-validation-368", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-7043", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-3060", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-1369", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1538", "mrqa_hotpotqa-validation-1813", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-11071"], "SR": 0.5625, "CSR": 0.5399816176470589, "EFR": 0.9642857142857143, "Overall": 0.6810878413865546}, {"timecode": 34, "before_eval_results": {"predictions": ["second Gleichschaltung", "two of Tesla's uncles", "Renault", "kinks", "Massachusetts", "lyonesse", "6", "paul david hewson", "ink", "Toy Story", "house sparrow", "book of Revelation", "haggis", "Independence Day", "Charlie Brooker", "Oxford", "number 13", "Glasgow", "descarto", "Florence", "Wat Tyler", "Steve Davis", "black Wednesday", "vomiting", "ennio Morricone", "NBA", "devo", "Benjamin Franklin", "rhizome", "chile", "1066", "alan", "$1", "Glasgow", "daniel cuthbertson", "port", "checkers", "Norman Mailer", "Action Force", "jura", "a brother", "devonshire", "argos", "quant pole", "Scooby Doo", "Pennine Way", "peter Bowles", "lorne Greene", "descart\u00e9", "frans hals", "alpha Bravo Charlie", "descarto", "the University of Oxford", "Matt Monro", "William Chatterton Dix", "Belarus", "Big Friendly Giant", "Welterweight", "Harrison Ford", "The EU naval force", "Brooklyn, New York,", "2,000,000 years B.P.", "The 39 Steps", "Armageddon"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6112132352941176}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-1015", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-2232", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6896", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-4210", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-1891", "mrqa_newsqa-validation-1335", "mrqa_searchqa-validation-14545"], "SR": 0.578125, "CSR": 0.5410714285714286, "EFR": 0.9629629629629629, "Overall": 0.6810412533068784}, {"timecode": 35, "before_eval_results": {"predictions": ["Tracy Wolfson", "2013", "Russ Conway", "green", "Amsterdam", "York,", "london", "a poster", "john tommie Connor", "jons jons john Dalton", "king arthur", "legion", "soybeans", "annie leibovitz", "Pinot Noir", "seal", "london", "jack cade", "dennis whittington", "a cock", "Pisces", "ishmael", "smell", "Brad Pitt", "Eleanor Rigby", "The Simpsons", "One Direction", "yellows", "Cornell University", "bar", "vinegar joe", "london", "follicle-stimulating hormone", "paramitas", "step-by-step solution to", "normanandie", "france", "london", "barbara", "Costa Concordia", "london", "stephen taylor", "london", "false", "horseshoes", "prince andrew africa", "43rd", "oil capital of Europe", "oranges", "jays", "yasser Arafat", "the Black Sea", "Gary Grimes", "49 cents", "1", "Sim\u00f3n Jos\u00e9 Antonio de la Sant\u00edsima Trinidad de Bol\u00edvar y Palacios", "Wings of Desire", "actor and former fashion model", "Airbus A330-200", "an account of hiding from Jewish persecution in Nazi-occupied Amsterdam is one of the world's mostly widely-read books.", "an FAA-certified physician every year; those over 40, every six months.", "Roland Garros", "cyrano de Bergerac", "global market"], "metric_results": {"EM": 0.421875, "QA-F1": 0.45208333333333334}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-584", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-168", "mrqa_triviaqa-validation-1757", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-6664", "mrqa_triviaqa-validation-3593", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-5010", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-2706", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-5128", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-276", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-11091"], "SR": 0.421875, "CSR": 0.5377604166666667, "EFR": 1.0, "Overall": 0.6877864583333334}, {"timecode": 36, "before_eval_results": {"predictions": ["independent schools", "cymbals", "Djibouti and Yemen", "the Nile", "Barcelona", "salma hayek", "portaenau Ffestiniog", "richard nodel", "glaciers", "Delaware", "rabbit", "crow", "Flintstones", "the Great Chicago Fire", "gelatine", "Ecuador", "Cyprus", "an orphan", "homelessness", "Dublin", "london", "walker", "doesn't include additional costs such as insurance or business rates", "Google", "blue Peter", "Pembrokeshire Coast National Park", "Tripoli", "jim taylor", "Dreamgirls", "Opus Dei", "b\u0259\u02c8liz", "civil Law", "The Press Gang", "a Liberator", "jimmy gang", "a goat", "Dubai", "Sydney", "orange", "london & Bingley", "married their husbands", "Ordovices", "davy", "cuba chick", "mexico", "pascal", "John Galsworthy", "davy", "Dr. Julius No", "Amsterdam", "davy", "24", "Ford", "between 27 July and 7 August 2021", "94", "Kang and Kodos", "La Liga", "environmental", "five minutes before commandos descended from ropes that dangled from helicopters,", "st. Louis, Missouri,", "milk", "king jeconiah", "goodson", "city"], "metric_results": {"EM": 0.453125, "QA-F1": 0.491015625}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.625, 0.8, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-275", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-6236", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2281", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7186", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-7594", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-562", "mrqa_naturalquestions-validation-5647", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-621", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-12212"], "SR": 0.453125, "CSR": 0.535472972972973, "EFR": 0.9142857142857143, "Overall": 0.6701861124517375}, {"timecode": 37, "before_eval_results": {"predictions": ["uncivilized", "jerry zaks", "8", "m\u00e1laga del Sol", "hugh hefner", "Tiananmen Square", "noises off", "peter Firmin", "Till Death Us Do Part", "javier Bardem", "1720", "austria", "dysmenorrhea", "The Hague", "red", "the Circle line", "robert vtoroy", "Sally Ride", "G\u00e9rard Depardieu", "jim Carter", "lily", "herpes zoster", "small faces", "zoom", "Angela dothea Kasner", "mary", "chronic animal exposure", "henbert henry asquith", "spectator", "2 1/2 balls", "Aslan", "jethro", "Vancouver Island", "ken purdy", "The Blues Brothers", "six", "sash", "Christian Dior", "Stockholm", "salford", "beta", "jack robert", "Basil Fawlty", "blue", "mary west", "Zephryos", "Rumble in the Jungle", "violins and cellos", "Lady Gaga", "chardonnay", "colleen McCullough", "retinal ganglion cell axons and glial cells", "2026", "Brittany Paige Bouck", "National Basketball Association", "Cherokee Nation", "Basilileia t\u014dn Rh\u014dmai\u014dn", "1927", "Emma.", "the man facing up, with his arms out to the side.", "muskrat", "Santo Versace", "trisha yearwood", "Hal David and Burt Bacharach"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5088699494949496}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-5427", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-5616", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-4129", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-4819", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-3899", "mrqa_triviaqa-validation-6813", "mrqa_naturalquestions-validation-3316", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-1958", "mrqa_newsqa-validation-1855", "mrqa_searchqa-validation-12111", "mrqa_searchqa-validation-15107", "mrqa_naturalquestions-validation-6125"], "SR": 0.421875, "CSR": 0.532483552631579, "EFR": 1.0, "Overall": 0.6867310855263158}, {"timecode": 38, "before_eval_results": {"predictions": ["74", "(Cecil) Rhodes", "constant", "theology", "a triangle", "jedoublen", "howdy", "Anne", "root beer", "Honey Nut Cheerios", "Venus", "de Berry", "jimmy taylor", "secretary of state", "density", "greece", "Ocean's Eleven", "Barack Obama", "Stockholm", "Brave Little Toaster", "crescent rolls", "Macau", "light", "jaffa", "Dan Marino", "Munich", "viola", "duchy of Luxembourg", "Alice", "whisky", "Tower of London", "julikki Woods", "the Ten Commandments", "a corruption of \"God's wounds\"", "bix", "yellow", "the Crimean War", "opium", "Northanger Abbey", "concave", "the Caspian Sea", "a barbie doll", "Scooby-Doo", "Dean Cain", "hypnotic", "South Dakota", "greece", "tosto", "hautboy", "radio", "Skee", "subduction zone", "Erica Rivera", "Jonas Roberts", "whuthering Heights", "Craggy Island", "greece", "Idaho", "Belgian", "Esteban Ocon", "Roberto Micheletti", "green grump", "19", "Oahu"], "metric_results": {"EM": 0.5, "QA-F1": 0.5583333333333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-3544", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-15961", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-13762", "mrqa_searchqa-validation-16868", "mrqa_searchqa-validation-8389", "mrqa_searchqa-validation-7647", "mrqa_searchqa-validation-10062", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-844", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-5544", "mrqa_searchqa-validation-9662", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-11364", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-4749", "mrqa_hotpotqa-validation-4023", "mrqa_newsqa-validation-267", "mrqa_hotpotqa-validation-4625"], "SR": 0.5, "CSR": 0.531650641025641, "EFR": 1.0, "Overall": 0.6865645032051282}, {"timecode": 39, "before_eval_results": {"predictions": ["inequality in wealth and income", "Princess Muna al-Hussein", "Baugur Group", "Westchester County", "City Mazda Stadium", "841", "American", "Richie McDonald", "Kaep", "The WB supernatural drama series \"Charmed\"", "Southern Rhodesia", "Alonso L\u00f3pez", "a TV series for the BBC channel CBeebies by Brown Bag Films", "Buck Owens", "Galleria Vittorio Emanuele II", "coalwood", "Jane Hollander", "neuro-orthopaedic", "the City of Westminster, London", "British", "Ray", "6,241", "Dan Bilzerian", "STS-51-L.", "a valuation method detailed by Warren Buffett in 1986.", "Crackle", "Kristy Lee Cook", "Perth", "Love Streams", "1935", "5.3 million", "New York City", "Dara Torres", "dyers of Lincoln", "actress", "a fictional world", "May 5, 2015", "Red and Assiniboine Rivers", "Ephedrine", "Neymar", "Jefferson Memorial", "Strange Interlude", "Bothtec", "2,099", "a body of water", "35,402", "2004 Paris Motor Show", "1996", "33", "1999", "Axl Rose", "5", "John McConnell", "George Harrison", "Bruno Mars", "Wyre", "Italian national football team", "Heshmatollah Attarzadeh", "a one-shot victory in the Bob Hope Classic", "45-year-old", "Frank Sinatra & J. Edgar Hoover", "Monopoly", "a cookie jar", "a street in the Salt Lake City suburb of Sandy with Mitchell and his wife, Wanda Eileen Barzee."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6804315476190477}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.28571428571428575, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.28571428571428575, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.8, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3708", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-3495", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2707", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-943", "mrqa_triviaqa-validation-1097", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3344", "mrqa_searchqa-validation-16161", "mrqa_newsqa-validation-1230"], "SR": 0.59375, "CSR": 0.533203125, "EFR": 1.0, "Overall": 0.686875}, {"timecode": 40, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1616", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3708", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-612", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12704", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15107", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-2178", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7865", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9870", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10149", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1844", "mrqa_squad-validation-1967", "mrqa_squad-validation-2049", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3428", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3815", "mrqa_squad-validation-3836", "mrqa_squad-validation-3837", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4135", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-503", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5338", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-5859", "mrqa_squad-validation-5893", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6435", "mrqa_squad-validation-6506", "mrqa_squad-validation-6671", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7002", "mrqa_squad-validation-7193", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7704", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8084", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8935", "mrqa_squad-validation-902", "mrqa_squad-validation-9254", "mrqa_squad-validation-9300", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9479", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3640", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3874", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4239", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4527", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6152", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-657", "mrqa_triviaqa-validation-6762", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7400", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-996"], "OKR": 0.83203125, "KG": 0.47109375, "before_eval_results": {"predictions": ["2006", "Ghana", "Dublin", "Ribhu Dasgupta", "Lord Ruthven", "Peter Pan Live!", "Irish", "sulfur mustard", "12\u201318", "Mike Biden", "Jeffrey William Van Gundy", "People!", "Ballarat Bitter", "Bank of China Building", "1919", "26,000", "Comeng and Clyde Engineering", "Gillian Leigh Anderson", "Steve Carell", "The Second City", "Lauren Alaina", "the National Football League", "1943", "Fountains of Wayne", "Minnesota", "composer of both secular and sacred music,", "(91.1 FM)", "October 20, 2017", "Zimbabwe", "Iowa State", "Douglas Jackson", "Richard Strauss", "Berber", "Straits of Gibraltar", "American burlesque", "Russell T Davies", "Jay Schottenstein", "600", "April 30, 1982", "Martin \"Marty\" McCann", "Chinese Democracy", "Labour", "Orlando\u2013Kissimmee\u2013Sanford, Florida Metropolitan Statistical Area", "Prussia", "Boston, Massachusetts", "Bambi, a Life in the Woods", "from 1993 to 1996", "Watertown, New York", "green and yellow", "ice hockey", "1995 teen drama \"Kids\"", "3 September", "the Gaget, Gauthier & Co. workshop", "200 to 500 mg up to 7 mg", "Paul Gauguin", "Ynys M\u00f4n", "Saturday Night Live", "an antihistamine and an epinephrine auto-injector", "Ford Fairlane", "outside his house in Najaf's Adala neighborhood", "tim'rous beastie", "The Backstreet Boys", "momentum", "hemoglobin"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5860119047619048}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.5, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.2, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-5509", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-507", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-4639", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-1008", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-2482", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-8555", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-5417", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-1606", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-3357"], "SR": 0.46875, "CSR": 0.5316310975609756, "EFR": 1.0, "Overall": 0.7110918445121952}, {"timecode": 41, "before_eval_results": {"predictions": ["his sons and grandsons", "Memory Recognition and Recall in User Interfaces", "a Sump Pump", "a fisheye lens", "Bears", "Parris Island", "bagels", "Elizabeth Taylor", "She is also one of my favorite 80's actress.", "Mick Taylor", "White blood cells", "Al Capone", "Matsu", "Stardust", "a Book of Operas", "The Grasshopper and the Ants", "Quinn the Eskimo", "Medical Malpractice", "grizzly bear", "Kareem Abdul-Jabbar", "The Police", "Travel Channel", "Henry Clay Frick", "Mikhail Gorbachev", "a ghost", "Mayor of Los Angeles", "Amateur Radio", "(Three Storied Pagaoda)", "Godefroy", "Margaret Tobin", "Franklin D. Roosevelt", "Pisces", "the Golden Fleece", "Alzheimer's disease", "Chuck Yeager", "Penguin Suit up", "\"PANT\"s", "ajax", "Vermont", "Aleksandro Vladimirovich Popov", "Bazooka", "zenith", "Whig", "Vietnam", "Ectoplasm", "Satomi Kataoka", "Old North Church", "binocular", "Honey Bunch", "Legally Blonde", "Scorpio", "Rightly Guided Caliphs", "Kaley Christine Cuoco", "H ions", "Salix", "Pearl Slaghoople", "The Bible", "Crown Holdings Incorporated", "New Orleans Saints", "Fundamentalist Church of Jesus Christ of Latter Day Saints", "said the claims are unfounded,", "Aldgate East.", "because the Indians don't want to get involved in the armed struggle.", "crossword puzzle"], "metric_results": {"EM": 0.40625, "QA-F1": 0.48409090909090907}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-13266", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-9985", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-6579", "mrqa_searchqa-validation-9793", "mrqa_searchqa-validation-1329", "mrqa_searchqa-validation-1899", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-12679", "mrqa_searchqa-validation-16343", "mrqa_searchqa-validation-591", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-1035", "mrqa_triviaqa-validation-987"], "SR": 0.40625, "CSR": 0.5286458333333333, "EFR": 1.0, "Overall": 0.7104947916666666}, {"timecode": 42, "before_eval_results": {"predictions": ["to Westminster", "Ratatouille", "Ecclesiastes", "Catherine de' Medici", "Wenceslas", "Ecuador", "Microsoft", "Katharine Hepburn", "binocular", "the forest", "London", "Little Boy Blue", "cotton", "Macedonian Kingdom", "Seinfeld", "(John) Jones", "the Bell X-1", "Spider-Man", "(Queen) Guinevere", "Hudson Bay", "Hamlet", "axios", "St Patrick\\'s Cathedral", "a cereal", "King George III", "Saul", "Duckworth", "Novella", "Rastafari", "Beverly Cleary", "Neapolitan", "Heartbreak Hotel", "Spain", "a cherubim", "Pisa", "Joliet", "Bangkok", "Cuba Gooding", "Russia", "Burt Lancaster", "diagonals", "the Communist Party", "a sacristy", "Israel", "Othello", "Alabama", "Making the Band 3", "Martinique", "Deodorant", "the Leatherstocking series", "the Lion King", "Florida", "Norman", "after tentatively courting each other in `` Entropy ''", "dragonflies", "Bushism", "thorns", "alt-right", "Australian", "\"Bobby\" Bunda", "Negotiators for Zelaya and Roberto Micheletti,", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "Daytime Emmy Lifetime Achievement Award", "Australia"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5848958333333333}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.1, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9392", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-1830", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-11412", "mrqa_searchqa-validation-854", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-8318", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-5271", "mrqa_searchqa-validation-1179", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-314", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-8416", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-1452", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10320", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-4971", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-755", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-1351"], "SR": 0.484375, "CSR": 0.5276162790697674, "EFR": 1.0, "Overall": 0.7102888808139535}, {"timecode": 43, "before_eval_results": {"predictions": ["jellyfish", "enforcing racially separated educational facilities", "beef", "late January or early February", "Monk's Caf\u00e9", "Christopher Lloyd", "Morgan Freeman", "1910", "74", "John Vincent Calipari", "Floyd", "2019", "James Madison", "50 % of the total members of a house", "1917", "Icarus", "Mike Alstott", "Indian Ocean", "1982", "1956", "Harrys", "Monk's", "Fulton, Arkansas", "Pete Seeger", "depression", "Devastator", "second heart sound ( S )", "Texas A&M University", "Bonnie Lipton", "American drama film", "two parallel planes", "stomach of leg", "transmissions", "Coconut Cove", "most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor", "March 9, 2018", "from 1973 to 1988", "Jikji", "cadmium", "1989", "2 September 1990", "East Redbud", "Cephalopoda", "1939", "March 26, 1973", "Stephen Foster", "Charles", "Bee Gees", "Brazilian state of Mato Grosso", "The White House Executive Chef", "3.9 and 5.5 mmol / L ( 70 to 100 mg / dL )", "Mike Hammer", "conductor", "a game of bridge", "Manchester United", "Skipton Castle", "Duncan Kenworthy", "Elena Kagan", "a treadmill", "Egyptian State TV ran footage Thursday of the assassination of President Mohamed Anwar al-Sadat", "Che Guevara", "pinnipeds", "Heather Locklear", "Arizona Health Care Cost Containment System"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5242047882672882}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5000000000000001, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 0.30769230769230765, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.6666666666666666, 0.4444444444444445, 0.13333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-2630", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-946", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-912", "mrqa_searchqa-validation-8799", "mrqa_hotpotqa-validation-1803"], "SR": 0.390625, "CSR": 0.5245028409090908, "EFR": 0.9230769230769231, "Overall": 0.6942815777972029}, {"timecode": 44, "before_eval_results": {"predictions": ["Neo-Confucianism", "April 2011", "James Intveld", "frontal lobe", "Haliaeetus", "International Orange", "a low concentration in pigmentation", "Carol Ann Susi", "osseo - cartilaginous", "a premalignant flat ( or sessile ) lesion of the colon", "Arnold Schoenberg", "a jazz funeral without a body", "asexually", "mid November", "Deposition", "Andy Cole and Shearer", "George Strait", "boiling water reactor", "to solve its problem of lack of food self - sufficiency", "October 2012", "201", "Rafael Nadal", "the International Border", "Melissa Disney", "twice", "Gene MacLellan", "to signify cunnilingus", "infant, schoolboy, lover, soldier, justice, Pantalone and old age", "frozen carbon dioxide", "Luigi Fagioli", "on the microscope's stage", "deceased - donor ( formerly known as cadaveric )", "North Atlantic Ocean", "John Hancock", "silk floss tree", "around 100,000", "Triple threat", "Clarence Darrow", "alpha efferent neurons", "in teaching elocution", "the optic disc to the optic chiasma", "Butter Island off North Haven, Maine in the Penobscot Bay", "a combination of genetics and the male hormone dihydrotestosterone", "British Columbia, Canada", "the remaining surface of the enamel", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Frankie Valli", "908 mbar", "1940", "Pyeongchang County, Gangwon Province, South Korea", "Utah, Arizona, Wyoming, and Oroville, California", "Kuala Lampur", "Lidice", "Augustus Caesar", "Ars Nova Theater", "French", "1902", "Najaf.", "a monthly allowance,", "(3 degrees Fahrenheit),", "cutlery", "Caesar", "Possession", "last summer."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6481513278388278}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, true], "QA-F1": [0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.25, 1.0, 0.8, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.3333333333333333, 0.4615384615384615, 0.5714285714285715, 1.0, 1.0, 1.0, 0.8571428571428571, 0.4615384615384615, 1.0, 0.28571428571428575, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8160", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-9196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-1662", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-2863", "mrqa_newsqa-validation-1076", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-9372"], "SR": 0.515625, "CSR": 0.5243055555555556, "EFR": 0.9354838709677419, "Overall": 0.6967235103046595}, {"timecode": 45, "before_eval_results": {"predictions": ["adaptive immune system", "New Delhi", "Jacques Cousteau", "Mercedes -Benz G - Class", "Hermann M\u00fcller", "ratio of the length s of the arc by the radius r of the circle", "Lake Wales", "an unmasked and redeemed Anakin Skywalker", "Megan Park", "Chris Coppola", "Tulsa", "Broken Hill and Sydney", "John Goodman", "from the right side of the heart to the lungs", "during the period of rest ( day )", "California", "11 January 1923", "southwestern Colorado and northwestern New Mexico", "Ann Gillespie", "Pontic Mountains in Turkey", "Master Christopher Jones", "832 BCE", "Claudia Grace Wells", "Jerry Leiber and Mike Stoller", "1995 Dodge Stealth - Seen in The Turbo Charged Prelude for 2 Fast 2 Furious", "Natural - language processing", "Sir Alex Ferguson", "around 1872", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "2011", "September 1973", "Cairo, Illinois", "comic", "Abanindranath Tagore CIE", "Coldplay", "in the fovea centralis", "Empiricism", "735 feet ( 224 m )", "Lana Del Rey", "The Jewel of the Nile", "three degrees of freedom", "noli me tangere in", "Ludacris", "A costume", "approx. $2.50 ) in the Philippines or $20 abroad", "Frankie Muniz", "Freddie Highmore", "the somatic nervous system and the autonomic nervous system", "Andy Cole", "1966", "Scar's henchmen", "an abnormal visual condition that makes colorless objects appear tinged with color.", "Perth Racecourse", "a bramble fruit", "England", "two", "6,241", "Roy Foster", "share personal information.", "Stephen Tyrone Johns", "The Wild Thornberrys", "a spoon", "Sir Francis Drake", "Charles Swinburne"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6164565469252969}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.5714285714285715, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 0.8, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 0.375, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 0.375, 0.0, 0.0, 1.0, 1.0, 0.30769230769230765, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-3018", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5970", "mrqa_triviaqa-validation-5041", "mrqa_hotpotqa-validation-5438", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-2549", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-6979", "mrqa_searchqa-validation-2528"], "SR": 0.4375, "CSR": 0.5224184782608696, "EFR": 1.0, "Overall": 0.7092493206521739}, {"timecode": 46, "before_eval_results": {"predictions": ["New England Patriots", "1.5 to 11 times the amount of water in the oceans may be found hundreds of miles deep within the Earth's interior, although not in liquid form", "an ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "Cliff Richard", "McKim Marriott", "The British Indian Association", "foreign investors", "Redenbacher family", "the British - American boundary at the 49th parallel ( except Vancouver Island )", "a line of committed and effective Sultans", "Jules Shear", "from 13 to 22 June 2012", "Tandi, in Lahaul", "H.L.A. Hart", "Janie Crawford's `` ripening from a vibrant, but voiceless, teenage girl into a woman with her finger on the trigger of her own destiny", "West Norse sailors", "2005", "2012", "the region of the thorax between the neck and diaphragm in the front of the body", "Randy Goodrum", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169", "the early 1900s", "it failed to enforce its rule, and its vast territory was divided into several successor polities", "Buffalo Lookout", "Aristotle", "March 4, 1789", "Hemingway", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Cristeta Comerford 2005 -- present", "the settlers", "In FY 2015 and 2017 India's economy became the world's fastest growing major economy surpassing China", "Arnold Schoenberg", "Identification of alternative plans / policies", "The Outback", "sandstone", "Wisconsin", "85 %", "Long Island", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned, and to give his or her advice and opinion upon questions of law", "seven", "NFL owners", "The 133rd overall episode overall, it originally aired on Comedy Central in the United States on October 19, 2005", "Juliet compares Romeo to a rose saying that if he was not named Romeo he would still be handsome and be Juliet's love", "a certified question or proposition of law from one of the United States Courts of Appeals", "gathering money from the public, which circumvents traditional avenues of investment", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "2018", "the contestant makes a thirty - second call to one of a number of friends ( who provide their phone numbers in advance ) and reads them the question and answer choices, after which the friend provides input", "San Jose, California", "Nicklaus", "Indo - Pacific", "Shaft", "Denise van Outen", "West Virginia", "Syracuse", "Girls' Generation", "Manchester, England", "Authorities in Fayetteville, North Carolina,", "Nearly eight in 10 say things are going badly in the country,", "If a security officer were to pull a gun on an armed individual in a mall, it could result in \"airport style\" security measures, including bag checks and magnetometers.", "Jericho", "Jim Davis", "Katharine of Aragon", "Ashley \"A.J.\" Jewell,"], "metric_results": {"EM": 0.40625, "QA-F1": 0.536593624225366}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.0, 0.08333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.09090909090909091, 0.0, 1.0, 0.0, 0.5333333333333333, 0.0, 0.721311475409836, 0.0, 0.72, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7671232876712328, 1.0, 1.0, 0.1818181818181818, 0.08695652173913045, 0.11764705882352941, 0.8695652173913044, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.05714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.32, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-259", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-6448", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-171", "mrqa_naturalquestions-validation-5406", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-6804", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-2578", "mrqa_hotpotqa-validation-4117", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-982", "mrqa_searchqa-validation-5939"], "SR": 0.40625, "CSR": 0.5199468085106382, "EFR": 0.9473684210526315, "Overall": 0.698228670912654}, {"timecode": 47, "before_eval_results": {"predictions": ["Swelling of the salivary glands in the face (parotitis)", "The Last King of Scotland", "Kazakhstan", "Dickens", "the lung", "Knutsford", "Burma", "Ewan McGregor", "a falcon", "South Park", "Launcelot Gobbo", "Canada", "Phil Spector", "Champagne Cosmopolitan Cocktail", "Tiny Tim", "Surrealist", "Boston", "Roddy Doyle", "geography", "Operation Frequent Wind", "Berlin", "Charlie Chan", "Wanderers", "Pinwright's Progress", "a winter fur hat", "Lady Gaga", "a duck", "Christian Wulff", "the Kinks", "Lucille", "Debbie Rowe", "General Sir Herbert Kitchener", "a centaur", "iodine deficiency", "14", "Margaret Beckett", "James Hogg", "Welshpool", "George Bernard Shaw", "Table Tennis", "Woolton pie", "the Florida Current", "Boston Legal", "Brighton", "Gandalf", "1930", "Motown", "Quebec", "Pope Benedict XVI", "a dove", "John T. Cable", "chromosome 21 attached to another chromosome", "the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "$315,600", "Amy Poehler", "Eric Allan Kramer", "Koninklijke Ahold N.V.", "via YouTube days after 35 bodies were found in two trucks during rush hour in the city of Boca del Rio.", "the Dutch patent office", "debris", "Sweden", "Nancy Drew", "schizophrenia", "it is doing to your dog's paw pads"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6523809523809524}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0, 0.09523809523809523, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-6097", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-3098", "mrqa_triviaqa-validation-2077", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7274", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-961", "mrqa_triviaqa-validation-304", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-10537", "mrqa_hotpotqa-validation-4091", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-111", "mrqa_searchqa-validation-14439"], "SR": 0.59375, "CSR": 0.521484375, "EFR": 1.0, "Overall": 0.7090625}, {"timecode": 48, "before_eval_results": {"predictions": ["taxonomy", "eight", "April 1st", "June 1992", "won", "Jane Fonda", "Kimberlin Brown", "March 31, 2017", "Henri Fantin", "New York City", "American country music singer George Strait", "John Adams", "a major fall in stock prices", "On the west", "Charles Path\u00e9", "Phillip Paley", "statute or the Constitution itself", "18", "Game 1", "John Adams", "Abraham Gottlob Werner", "IMS", "the end of the 18th century", "American singer Lesley Gore", "sometime between 124 and 800 CE", "Ed, Edd n Eddy animated television series", "husky", "Teri Hatcher", "John Quincy Adams", "August 1991", "Uralic languages", "dromedary", "Bhupendranath Dutt", "2011", "a substance that fully activates the receptor that it binds to )", "Bill Russell", "Battle of Antietam", "sport utility vehicles", "Hunter Tylo", "Frank Theodore `` Ted '' Levine", "the 6th century AD", "James Rodr\u00edguez", "around 10 : 30am", "Jack Barry", "the White Sox", "45 %", "to condense the steam coming out of the cylinders or turbines", "Bill Russell", "1984", "the problems and / or goals", "the Sun ( elongation )", "the Northern line", "cilla black", "beims", "her grandmother", "Taoiseach of Ireland", "Los Angeles Dance Theater", "Kurdistan Workers' Party", "AbdulMutallab", "don Draper", "typewriter", "calico", "lute", "Gary Player"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6325599371693122}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.12500000000000003, 0.22222222222222224, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.29629629629629634, 0.6666666666666666, 0.4799999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-255", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-183", "mrqa_hotpotqa-validation-3700", "mrqa_newsqa-validation-1506", "mrqa_searchqa-validation-8622"], "SR": 0.5625, "CSR": 0.5223214285714286, "EFR": 1.0, "Overall": 0.7092299107142858}, {"timecode": 49, "before_eval_results": {"predictions": ["Napoleon", "lithium - ion batteries", "Patrick Warburton", "prisoners and guards", "pneumonoultramicroscopicsilicovolcanoconiosis", "Charles Crozat Converse", "Andrew Garfield", "July 4, 1776", "Keith Thibodeaux", "Jesus Christ", "Charles Path\u00e9", "eleven", "President alone, and the latter grants judicial power solely to the federal judiciary", "Johannes Gutenberg", "O'Meara", "a four - page pamphlet in 1876", "Owen Wilson and Jennifer Aniston", "fourth season", "four seasons", "The First Battle of Bull Run ( the name used by Union forces )", "5 % probability of incorrectly rejecting the null hypothesis", "slavery", "The Royalettes", "the Internal Revenue Service", "The Outback", "its vast territory was divided into several successor polities", "Louis XV", "2017", "genome", "Beorn", "power is defined as `` the ability to influence somebody to do something that he / she would not have done ''", "Confederate forces attacked Fort Sumter in South Carolina, shortly after U.S. President Abraham Lincoln was inaugurated", "in response to the Weimar Republic's failure to continue its reparation payments in the aftermath of World War I", "Indirect rule", "Zachary John Quinto", "the governor of West Virginia", "Wednesday, September 21, 2016", "ninth", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "milling process", "simulcast outside Japan by Crunchyroll", "1939", "1992", "Millerlite", "Felix Baumgartner", "Donald Fauntleroy Duck", "c. 3000 BC", "Bart Howard", "Paris", "1966", "the vascular cambium", "jocky Wilson", "Leeds", "Portugal", "Coleman Hawkins", "Zero Mostel", "Ellesmere Port, United Kingdom", "Genocide Prevention Task Force", "a grizzly bear", "Maersk Line Ltd.", "Prometheus", "Treasure Island", "Pablo Picasso", "speed sailing"], "metric_results": {"EM": 0.5, "QA-F1": 0.6423658619659613}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.5, 0.0, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.26666666666666666, 0.0, 0.5, 0.6666666666666666, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.07142857142857144, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.8837209302325582, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-1290", "mrqa_naturalquestions-validation-6888", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-6021", "mrqa_triviaqa-validation-3952", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2428", "mrqa_searchqa-validation-15480", "mrqa_newsqa-validation-1446"], "SR": 0.5, "CSR": 0.521875, "EFR": 0.96875, "Overall": 0.702890625}, {"timecode": 50, "UKR": 0.69921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1373", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10692", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5387", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8709", "mrqa_naturalquestions-validation-8819", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5744", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99"], "OKR": 0.8515625, "KG": 0.50703125, "before_eval_results": {"predictions": ["Jason", "Trainspotting", "Chess Records", "netherlands", "spark-ignition", "Concorde", "Al Jazeera", "French", "netherd", "netherlands", "2007", "Goldfinger", "Chicago", "Flower", "Gerald R. Ford", "Dengue fever", "Japan", "Ted Turner", "phobia", "netherlands", "Mount Everest", "Strangeways", "Carthage", "Wensum", "Robben", "United Kingdom", "Taekwondo", "toe", "apple", "sixth Wimbledon championship", "Nelson Mandela", "George Orwell", "Andrew Jackson", "Muriel Spark", "Table Tennis", "Entwistle Reservoir", "DeLorean", "six", "Perseus", "Yakutat", "football", "insurance", "muscle", "transuranic", "John Buchan", "Tesco", "Lolita", "Jeannie C. Riley", "Indus Valley", "waterfowl", "Pickwick", "Nancy Jean Cartwright", "Watson and Crick", "Authority", "his superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe and Johnny Storm / Human Torch in \"Fantastic Four\"", "German", "Che Guevara", "Robert Barnett", "five", "commercial airlines", "a netherlands", "John Lennon", "Murder by Death", "Republicans"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6993303571428571}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.3571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-139", "mrqa_triviaqa-validation-6499", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-6476", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-5363", "mrqa_triviaqa-validation-7267", "mrqa_triviaqa-validation-3982", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-112", "mrqa_triviaqa-validation-590", "mrqa_naturalquestions-validation-3468", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2506", "mrqa_searchqa-validation-5409"], "SR": 0.640625, "CSR": 0.524203431372549, "EFR": 0.9565217391304348, "Overall": 0.7077075341005967}, {"timecode": 51, "before_eval_results": {"predictions": ["Flatbush Zombies", "Australian", "\"Traumnovelle\" (\"Dream Story\")", "Denmark", "Bad Meets Evil", "Bellagio and The Mirage", "George Mikan", "more than 20 principal operations", "Guthred", "The New Yorker", "Jeffrey Jones", "St. Louis Cardinals", "NXT Tag Team Championship", "Lee Byung-hun", "as many as 16 universities in the eastern half of the United States from 1979 to 2013", "February 1", "capital crimes or capital offences", "Let Me Be the One (The Carpenters song)", "March", "Chuck Noll", "cate Blanchett", "Oregon", "Atlas ICBM", "Democratic", "Kim So-hyun", "Rolling Stones", "22,500", "Trey Parker", "Kew", "Albany", "twelfth", "Wembley Stadium", "Shameless", "Raden Panji Nugroho Notosusanto", "skiing and mountaineering", "Indian state of Gujarat", "Comedy Film Nerds", "cruiserweight", "five", "Leofric", "Bigfoot", "March 17, 2015", "yubin", "5249", "fourth", "three", "28 November 1973", "\"O\", \"La Nouba\", \"Myst\u00e8re\", \"Alegr\u00eda\", and \"Quidam\"", "Londonderry", "Santiago Herrera", "jewelry designer", "Steve Russell", "an old pronunciation of Gaultier or Walter, and similarly derived from the surname Watson ( `` Wat's son '' )", "while studying All My Sons by Arthur Miller, a play about a man whose choice to send out faulty airplane parts for the good of his business and family caused the death of twenty one pilots during World War II", "Pegasus", "sunday", "barildon-born Perry", "15-year-old's", "July 23.", "northern Baghdad", "Buena Park", "circumference", "T.S. Eliot", "althea Gibson"], "metric_results": {"EM": 0.34375, "QA-F1": 0.48355020852237607}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, true, false], "QA-F1": [0.22222222222222224, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.33333333333333337, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.3333333333333333, 0.25, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.9859154929577464, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-3270", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-1260", "mrqa_hotpotqa-validation-3712", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-3752", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-1886", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3651", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-7514", "mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-4557", "mrqa_newsqa-validation-1779", "mrqa_searchqa-validation-6192", "mrqa_searchqa-validation-834", "mrqa_triviaqa-validation-4216"], "SR": 0.34375, "CSR": 0.5207331730769231, "EFR": 1.0, "Overall": 0.7157091346153847}, {"timecode": 52, "before_eval_results": {"predictions": ["Frank Ocean", "Brookhaven", "2010", "Ryukyuan people", "Robert L. Stone", "Mexican", "The King of Hollywood", "five times", "in 1968 of killing two children when she herself was a child, and Franz Stangl, the commandant of the Treblinka extermination camp", "Charles Eug\u00e8ne Jules Marie Nungesser", "Kim Yoon-seok and Ha Jung-woo", "Jennifer Grey", "1978", "M2M", "Mark Neveldine and Brian Taylor", "Starship Planet", "Beauty and the Beast", "scratch & sniff cards", "The Leader In Me \u2014 How Schools and Parents Around the World Are Inspiring Greatness, One Child at a Time", "Larnelle Steward Harris", "Total Nonstop Action Wrestling", "Lambic", "Bit Instant", "Tom Jones", "James Abram Garfield", "Secrets and Lies", "Hard rock", "Ludwig van Beethoven", "Peter Kay\\'s Car Share", "Orfeo ed Euridice", "Dirt track racing", "Frederick I", "Karakalpaks", "Walldorf, Baden-W\u00fcrttemberg", "Harrison Ford", "Campbellsville", "Shinjuku", "1933", "Delphi Lawrence", "Philadelphia", "December 13, 2015", "The New York Stock Exchange (abbreviated as NYSE and nicknamed \"The Big Board\")", "Paradise, Nevada", "Russell T Davies", "four", "Meetinghouse Pond", "Mickey\\'s Christmas Carol", "2018\u201319 UEFA Europa League group stage", "Argentinian", "76,416", "Burning Man", "the president", "Ra\u00fal Eduardo Esparza", "October 30, 2017", "Live and Let Die", "Carrie", "on Mars", "Lucky Dube, one of South Africa's most famous musicians,", "his home was destroyed and his business is shattered,", "the Internet", "a puck amok", "Yes", "East Germany", "heart"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6864853896103896}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3636363636363636, 0.09999999999999999, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-3764", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-847", "mrqa_hotpotqa-validation-1263", "mrqa_naturalquestions-validation-321", "mrqa_triviaqa-validation-7133", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-2853", "mrqa_triviaqa-validation-3362"], "SR": 0.59375, "CSR": 0.5221108490566038, "EFR": 1.0, "Overall": 0.7159846698113208}, {"timecode": 53, "before_eval_results": {"predictions": ["doubles", "New York", "in the Outer Hebrides", "a muezzin", "nippon Sangyo", "a binder", "James Hogg", "Sarajevo", "Darby and Joan", "The Hurt Locker", "Stanley Kubrick's Full Metal jacket", "Blur", "Chicken Marengo", "Glenys Kinnock", "Sir herbert Kitchener", "white", "a bodice", "grizzly", "bukwus", "the Society of Jesus", "rowing", "his death in 1975", "Nowhere Boy", "Donald Trump", "Mikhail S. Gorbachev", "Popeye", "John Key", "Charlie Brooker", "Northwestern University", "Gulf of Mexico", "her lover,", "Achelois", "dynamite", "Oasis", "Jean Alexander", "Norman Brookes", "Kenneth Hockney", "La Toya Jackson", "George Washington", "Greek Home Management", "the United States", "Edinburgh", "Today", "hilo", "Bolton", "Norwegian Ibsen Company", "Super Bowl", "Stutter Rap (No Sleep til Bedtime)", "Vladimir Putin", "Donna Jo Napoli\u2019s Beast", "a bear (losing) market", "Augustus Waters", "1967", "alpaca fiber and mohair from Angora goats", "Ghana", "Peter Kay\\'s Car Share", "Miller Brewing", "The son of Gabon's former president", "more than two years,", "auction off one of the earliest versions of the Magna Carta later this year,", "the tartan of the Argyle clan", "a cause, principle, or system of beliefs", "a ringmaster", "Unseeded Frenchwoman Aravane Rezai"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5568350919913421}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.4, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-5000", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-1004", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2517", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-5848", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-2385", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5531", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-5344", "mrqa_searchqa-validation-4662", "mrqa_newsqa-validation-3285"], "SR": 0.453125, "CSR": 0.5208333333333333, "EFR": 0.9714285714285714, "Overall": 0.7100148809523809}, {"timecode": 54, "before_eval_results": {"predictions": ["honolulu", "The Great Gatsby", "ford", "atlantic bridge", "the First World War", "germany", "prince andrew", "sarah Ferguson", "Tallinn", "robert hartmanus", "hartman", "Skylab", "John Poulson", "germany hartman", "shoes", "the Great Depression", "corsets", "her Majesty", "nemesis", "dicken\\'s Dream", "Swansea City", "argon", "silurian", "meatloaf", "non-Orthodox synagogues", "j.M.W.", "the Lone Gunmen", "eggs", "hartman", "at the north-west corner of the central business district", "wonderwall", "basketball", "carburetor", "germany", "corsets", "michael hartman", "sigmund Freud", "winged horse", "Charlie Chaplin", "bat", "daniel peggotty", "hartman", "germany", "Arthur C. Clarke", "Buzz Aldrin", "power outage", "the Russian army", "index fingers", "Blenheim Palace", "rihanna", "cumbria", "28 July 1914 to 11 November 1918", "Fix You", "the spectroscopic notation for the associated atomic orbitals", "Copa Airlines", "his fifth", "Soha Ali Khan Khemu", "20-something woman at the tenteki 10 Caf\u00e9", "debris", "work for Grayback Forestry in Medford, Oregon,", "typhoid fever", "France", "the Edict of Nantes", "Austin and Pflugerville"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5081845238095238}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-1536", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-7740", "mrqa_triviaqa-validation-2312", "mrqa_triviaqa-validation-4349", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-5635", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-5896", "mrqa_triviaqa-validation-2243", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-4795", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-1212", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-2879", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-7712", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-303", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-4060", "mrqa_searchqa-validation-5331", "mrqa_naturalquestions-validation-3995"], "SR": 0.4375, "CSR": 0.5193181818181818, "EFR": 1.0, "Overall": 0.7154261363636364}, {"timecode": 55, "before_eval_results": {"predictions": ["aviva plc", "v Venezuela", "Mozart's", "scotland", "A Space Odyssey", "Catherine Cookson", "almonds", "for Vice President ( Sarah Palin, then-Governor of Alaska)", "Geneva", "jimmy t. Kirk", "peter Paul Rubens", "Persian Gulf", "durlach", "ascot", "seine", "12 Yard for BBC Two", "sheryl Crow", "winnie Mae", "spile", "come quietly", "children of Israel", "graphite", "Narragansett Bay", "Moby Dick", "The Scream", "gingerbread", "boddington bitter beer", "king jones", "wellbeing", "raspberries", "est Thomas", "surfer", "oakum", "blancmange", "rochdale", "penhaligon", "Black September", "7,926 miles", "georgia", "shoe", "gold", "car ferry", "professor Brian Cox", "Meow Mix", "9", "kidneys", "Bolivia", "jewish communities", "Jordan", "hans lippershey", "india", "Marlborough Churchill Blenheim Charlton", "statistical advantage for the casino that is built into the game", "the top of the cab can be crushed or sliced off as it swings round violently and tries to fold under the trailer", "Teriade", "November 23, 1996 in Japan and May 1997 in the rest of the world", "aeronautical engineer", "Lance Cpl. Maria Lauterbach", "Madeleine K. Albright", "three", "pinniped", "grotesque", "Liam Neeson", "an centaur"], "metric_results": {"EM": 0.5, "QA-F1": 0.5194791666666666}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.08000000000000002, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2690", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-4616", "mrqa_triviaqa-validation-5454", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1324", "mrqa_triviaqa-validation-3231", "mrqa_triviaqa-validation-6866", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3275", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-3897", "mrqa_triviaqa-validation-2442", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-4704", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-6964", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5510", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-2524", "mrqa_searchqa-validation-4686", "mrqa_searchqa-validation-2027"], "SR": 0.5, "CSR": 0.5189732142857143, "EFR": 1.0, "Overall": 0.7153571428571428}, {"timecode": 56, "before_eval_results": {"predictions": ["the Simms Twins", "j Judy gumm", "william hartnell", "rebecca", "will be the one to land him.", "jamaican", "magical mystery tour", "rio", "the Cyclopes", "purple", "1961", "antonyms", "florence", "tony meo", "arun kallarackal", "anton Lavoisier", "30th anniversary", "mccain", "tara", "russe", "albania", "john lenn", "sorbent", "Detroit", "united states", "sweeny tood the Demon Barber", "tarn", "the M8", "tidal Bay", "robbie coltrane", "alastair rocks", "sorawak", "cribbage", "1960s", "h Yorkshire", "lmfao", "Emma Chambers", "the kinks", "tufnell", "spain", "rebecca", "united states", "Pink Floyd", "robbie coltrane", "miles Morales", "anne earhart", "the American Revolutionary War", "touto", "hyphenated", "mono", "gaius Octavian", "the south coast of eastern New Guinea", "Lady Gaga", "Not wanting to feel broken - hearted, the female protagonist sings that she feels happy to have left her lover, who did not recognize the potential for a happy life with her", "\"Secrets and Lies\"", "October 3, 2017", "Morris Barney Dalitz", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "Unseeded", "White Hills, Arizona,", "the Chicago Bears", "the fairway", "the Provisional Irish Republican Army", "2018"], "metric_results": {"EM": 0.328125, "QA-F1": 0.3770833333333333}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true], "QA-F1": [0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-223", "mrqa_triviaqa-validation-454", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-4390", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-346", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-4270", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-3252", "mrqa_triviaqa-validation-188", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-4757", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6935", "mrqa_triviaqa-validation-2743", "mrqa_triviaqa-validation-4550", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-7131", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-9821", "mrqa_hotpotqa-validation-4161", "mrqa_newsqa-validation-3287", "mrqa_newsqa-validation-3032", "mrqa_searchqa-validation-16213", "mrqa_searchqa-validation-11582"], "SR": 0.328125, "CSR": 0.515625, "EFR": 0.9767441860465116, "Overall": 0.7100363372093023}, {"timecode": 57, "before_eval_results": {"predictions": ["bunch", "Illinois", "Edward Hopper", "robocop", "borgia", "Quentin Blake", "bazaar", "the day before the long fast for the Lenten fast", "new york", "Hamlet", "Chris Smalling", "007", "icadilly Circus", "hobbits", "Jordan", "Tangled", "Brothers in Arms", "afghanistan", "crossword puzzles", "sheree Murphy", "puff puff", "Robin Ellis", "tomato Basil Conchiglie Pasta", "davy crockett", "War and Peace", "paphos", "three", "east of Eden", "taylor de quincey", "zaragoza", "nick germany", "argentina", "king eddy", "british", "A Christmas Carol", "bridge", "elliptical", "trier", "boston", "blood", "zips", "isar", "Roman history", "mj\u00f6llnir", "admiral nixon", "florence", "woodstock", "birds", "nijinsky", "p Preston", "drogba", "Chairman of the Monetary Policy Committee", "B.J. Thomas", "65,535 bytes", "Prince Amedeo, Duke of Aosta", "San Antonio", "hulder", "The Obama administration on June 12 announced a task force devoted to federal ocean planning.", "Republicans", "some of the Awa", "Manhattan Island", "Patrick Henry", "sleep apnea", "the Eagles"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5943452380952381}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.0, 0.2666666666666667, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1493", "mrqa_triviaqa-validation-4243", "mrqa_triviaqa-validation-910", "mrqa_triviaqa-validation-4367", "mrqa_triviaqa-validation-6237", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-2935", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1563", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7378", "mrqa_triviaqa-validation-6841", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-3218", "mrqa_triviaqa-validation-6071", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-2177", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-1787", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-2399", "mrqa_newsqa-validation-4169", "mrqa_searchqa-validation-10934"], "SR": 0.515625, "CSR": 0.515625, "EFR": 1.0, "Overall": 0.7146874999999999}, {"timecode": 58, "before_eval_results": {"predictions": ["the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "10th Cavalry Regiment", "Claude Mak\u00e9l\u00e9l\u00e9", "John Robert Cocker", "Taylor Swift", "mountaineer, filmmaker, author, and motivational speaker", "\"Lonely\"", "Garrett Morris", "October 5, 1937", "1692", "Dizzy Dean", "Target Corporation", "British Labour Party", "Bandai", "Bill Ponsford", "Ward Bond", "Code#02PrettyPrettyPretty", "Every Rose Has its Thorn", "Cleveland Browns", "Jacking", "27 December 1901", "My Beautiful Dark Twisted Fantasy", "Broadcasting House in London", "20", "Amway", "Congo River", "Minneapolis", "Alemannic", "illnesses", "XVideos", "1967", "1967", "Spider-Man is the alter-ego of Peter Parker, a talented young freelance photographer and aspiring scientist, and Miles Morales, a high school student,", "Lawrence of Arabia", "The Fault in Our Stars", "Gareth Jones", "Prime Minister of Denmark", "J35-A-23", "Scotty Grainger", "Balloon Street, Manchester", "Somerset County, Pennsylvania", "Italy", "Psych", "Gateways", "Iran", "Veneto", "Empire Falls", "Fitzroya cupressoides", "Vernon L. Smith", "Dan Rowan", "Oakdale", "March 18, 2005", "1981", "Austria", "Switzerland", "The Treaty of Waitangi", "1930-1939", "a U.S. military helicopter made a hard landing in eastern Afghanistan,", "African National Congress Deputy President Kgalema Motlanthe,", "new DNA evidence", "blue whale", "Marcia Clark", "Dirty Little Book Publishing Secrets", "bullfight"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6332837301587302}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-2551", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-243", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-2338", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-10135", "mrqa_triviaqa-validation-5517", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1382", "mrqa_searchqa-validation-1481", "mrqa_triviaqa-validation-6175"], "SR": 0.53125, "CSR": 0.5158898305084746, "EFR": 1.0, "Overall": 0.714740466101695}, {"timecode": 59, "before_eval_results": {"predictions": ["the first integrated circuit", "Oracle Corporation", "Levittown", "the Teenage Mutant Ninja Turtles", "seven", "Ashanti Region", "1934", "New Hampshire", "1980", "cricket fighting", "Dachshund", "red, fallow and roe deer", "Duncan Kenworthy", "Stern-Plaza", "the Netherlands", "Continental Army", "Hl\u00edn (Old Norse \"protectress\")", "Henry Lau", "1", "Russian Empire", "The Catholic Church in Ireland", "people working in film and the performing arts", "Lykan HyperSport", "1911", "Gareth Barry", "1999", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "Margarine Unie", "John Landis", "A123 Systems, LLC", "Ian Fleming", "Minnesota", "14", "an anvil", "50 Greatest Players in National Basketball Association History", "Viola Larsen", "Dizzy Dean", "Magnus Carlsen", "BBC Focus", "Towards the Sun", "1958", "World War II", "Jenn Brown", "\"Glee\"", "Purdue University", "Indianapolis", "A hard rock/blues rock band,", "Bury, Greater Manchester, England", "\"Agent Vinod\"", "Leninist", "Johan Leysen", "Laura Jane Haddock", "when the cell is undergoing the metaphase of cell division ( where all chromosomes are aligned in the center of the cell in their condensed form )", "1986", "giraffe", "Sesli S\u00f6zl\u00fck", "Wagner", "to step up.\"", "\"Lashkar-e-Jhangvi,", "Christina Romete,", "Home on the Range", "the metre", "Donnie Wahlberg", "1918-1919."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6597537878787878}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true], "QA-F1": [0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0909090909090909, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-4420", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-1148", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-5966", "mrqa_triviaqa-validation-4253", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-3068", "mrqa_searchqa-validation-12933"], "SR": 0.59375, "CSR": 0.5171875, "EFR": 1.0, "Overall": 0.7150000000000001}, {"timecode": 60, "UKR": 0.6796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4534", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-4728", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-637", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-768", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-921", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-10651", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4312", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2415", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1403", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1768", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2243", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3432", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5228", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6098", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6536", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7606", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-997"], "OKR": 0.818359375, "KG": 0.50546875, "before_eval_results": {"predictions": ["September,", "five minutes before commandos descended", "Arsene Wenger", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "They are co-chair of the Genocide Prevention Task Force.", "The two-year probe, dubbed Operation Swiper, involved physical surveillance, intelligence gathering and court-authorized electronic eavesdropping on dozens of telephones in which thousands of conversations were intercepted,", "4.6 million", "Ferraris, a Lamborghini and an Acura NSX", "Vicente Carrillo Leyva,", "discovery\" for the museum \"of the last 90 years.\"", "Communist Party of Nepal (Maoist)", "581 points", "Molotov cocktails, rocks and glass.", "1994,", "10 years", "the Gulf", "25 percent", "Orbiting Carbon Observatory,", "then-Sen. Obama", "Claude Monet", "4,000", "apartment building", "an older generation", "Former Mobile County Circuit Judge Herman Thomas,", "Daytime Emmy Lifetime Achievement Award", "Johannesburg.", "Barack Obama,", "dual nationality", "Knox's parents.", "Cash for Clunkers program", "would not answer questions.", "Jennifer Arnold and husband Bill Klein,", "prostate cancer,", "Zimbabwe", "Britain.", "a lightning strike", "10 percent", "eight or nine young girls, some younger then 18, who were returned to their families.\"", "Jaipur", "cancer", "Alan Graham", "\"A salute to the martyrs of the massacre, and our condolences to their families.\"", "forgery and flying without a valid license,", "georgeWashington", "Lavau's son, Sean,", "three out of four questioned say that things are going well for them personally.", "poems", "37", "your environmental efforts make even more impact than Harrison Ford's chest.", "South African captain Graeme Smith", "2009", "William Jennings Bryan", "The neck", "Barry Humphries", "Mozambique Channel", "Ede & Ravenscroft", "Adelaide", "punk rock", "Great Northern Railway", "Otis Elevator", "Iberian peninsula", "George Balanchine", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.6382529288240495}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.3, 0.0, 0.1379310344827586, 1.0, 0.0, 1.0, 0.15384615384615383, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.923076923076923, 1.0, 0.0, 0.5, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.05714285714285715, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.13333333333333333, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-3297", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-4092", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-56", "mrqa_triviaqa-validation-2468", "mrqa_hotpotqa-validation-3984", "mrqa_searchqa-validation-15121"], "SR": 0.515625, "CSR": 0.5171618852459017, "EFR": 1.0, "Overall": 0.7041355020491803}, {"timecode": 61, "before_eval_results": {"predictions": ["Donald Duck", "Iran's parliament speaker", "Department of Homeland Security Secretary Janet Napolitano", "18", "china", "World leaders", "Casalesi Camorra clan", "managing his time.", "his club", "we seek a new way forward, based on mutual interest and mutual respect.", "$50", "collaborating with the Colombian government,", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "1,500", "Jada,", "200", "Karen Floyd", "Space shuttle Discovery,", "Brazil", "EU naval force", "to help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "Harrison Ford", "Sunday", "28", "The Falklands, known as Las Malvinas in Argentina,", "New York City Mayor Michael Bloomberg", "Department of Homeland Security Secretary Janet Napolitano", "two", "Too many glass shards left by beer drinkers in the city center,", "30-minute", "338", "UNICEF", "eight", "Daniel Radcliffe", "Department of Homeland Security Secretary Janet Napolitano", "Obama girls", "tanker", "lightning strikes", "if he did cheat on you (and you didn't cheat back),", "Afghan lawmakers", "New York Philharmonic Orchestra", "Colombia.", "2-0", "nearly 100", "1616", "to sniff out cell phones.", "Casey Anthony, 22,", "people look at the content of the speech, not just the delivery.", "2005", "root out terrorists within its borders.", "a point for Bayern Munich as the German Bundesliga leaders were held to a 1-1 draw by Cologne on Saturday.", "March 31 to April 8, 2018", "Veterans Committee", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "St Paul's Cathedral", "22", "Rome", "University of Vienna", "Dutch", "Naomi Campbell", "Earhart", "cricket", "Mad Men", "the courts"], "metric_results": {"EM": 0.53125, "QA-F1": 0.637022017182379}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 0.5, 0.4, 0.0, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.25, 0.2857142857142857, 0.4444444444444445, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.1111111111111111, 0.5, 0.3076923076923077, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-2145", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4915", "mrqa_triviaqa-validation-1058", "mrqa_hotpotqa-validation-3500", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-7617"], "SR": 0.53125, "CSR": 0.5173891129032258, "EFR": 0.9333333333333333, "Overall": 0.6908476142473118}, {"timecode": 62, "before_eval_results": {"predictions": ["Golden Valley, Minnesota,", "Emmy and four", "small forward", "Southern hard rock band", "Araminta Ross", "the Mach number (M or Ma)", "eight", "August 17, 2017", "Al Capone", "Atomic", "St Augustine's Abbey", "Vilyam \"Willie\" Genrikhovich Fisher", "minister and biographer", "Carl Michael Edwards II", "\"the most influential private citizen in the America of his day\"", "rhythm and blues", "Standard Oil", "over 1.6 million passengers", "British Labour Party", "September 8, 2017", "Obafemi Akinwunmi Martins", "Charles Edward Stuart", "Jeremy Hammond", "Steve Carell", "Saint Motel", "Melissa Ivy Rauch", "Flyweight", "Levon Helm", "Jean Acker", "the attack on Pearl Harbor", "Fountains of Wayne", "Nick Offerman", "Sam Raimi", "SAS Fr\u00f6sundavik", "Double Crossed", "Edmonton, Alberta", "8,211", "KXII", "the Wikimedia Foundation", "Greek-American", "Mika H\u00e4kkinen", "Debbie Isitt", "Los Angeles", "1999", "Outside is an American magazine focused on the outdoors.", "Food and Agriculture Organization", "Wojtek", "Edward Longshanks and the Hammer of the Scots", "Los Angeles", "West Point", "New York City", "Speaker of the House of Representatives", "13", "the sea witch character who appears in the fairy tale `` The Little Mermaid '' by Hans Christian Andersen", "an arrowhead", "Corin Redgrave", "a son of Amram and Jochebed, of the tribe of Levi (", "UNICEF", "Bob Bogle,", "she was humiliated by last month's incident, in which she was forced to painful remove the piercings behind a curtain as she heard snickers from male TSA officers nearby.", "They Call Me Mr. Tibbs", "fermented", "David", "sixth"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7230282738095237}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-5444", "mrqa_hotpotqa-validation-4237", "mrqa_hotpotqa-validation-4866", "mrqa_hotpotqa-validation-4539", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-821", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-2552", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-298", "mrqa_triviaqa-validation-6039", "mrqa_triviaqa-validation-5231", "mrqa_newsqa-validation-390", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-16523", "mrqa_naturalquestions-validation-5292"], "SR": 0.609375, "CSR": 0.5188492063492063, "EFR": 1.0, "Overall": 0.7044729662698412}, {"timecode": 63, "before_eval_results": {"predictions": ["African National Congress", "Ronald Cummings", "five", "Bob Bogle", "Bob Bogle", "the creation of an Islamic emirate in Gaza,", "suppress the memories and to live as normal a life as possible;", "Caster Semenya", "Kandi Burruss,", "the BBC's central London offices", "Kgalema Motlanthe,", "as he tried to throw a petrol bomb at the officers,", "Karl Eikenberry", "1959", "South Korea", "Elena Kagan", "Harrison Ford", "Christmas parade", "Salt Lake City, Utah,", "eight in 10", "your ex's loved ones ask why", "2-1", "racial intolerance.", "a spurned suitor.", "23 million square meters (248 million square feet)", "part of the proceeds", "nine newly-purchased bicycles at the scene, and think they were used to carry the explosives.", "refused Wednesday to soften the Vatican's ban on condom use as he arrived in Africa for his first visit to the continent as pope.", "Akshay Kumar", "August 19, 2007.", "a strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television.", "should have met with the Dalai Lama.", "can use their sick leave to take care of domestic partners and children and same-sex partners of Foreign Service employees will be included in medical evacuations and housing allocations,", "the test results by the medical examiner's office,", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.\"", "\"Dancing With the Stars.\"", "Brown-Waite", "will prevent some vehicles from being \"totaled,\"", "Hundreds", "strife in Somalia,", "protest child trafficking and shout anti-French slogans", "colonel in the Rwandan army,", "around 1918 or 1919.", "in the mouth.", "cancer", "Susan Atkins,", "dozens more children and young women", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "for pulling on the top-knot of an opponent, and was restricted from leaving his house in Tokyo,", "at the University of Alabama in Huntsville", "a secretary of state under President Bill Clinton.(CNN)", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "the temperature at which it becomes semi solid and loses its flow characteristics", "a paint consisting of pigment and glue size commonly used in the United States as poster paint", "touchstone", "Quentin Tarantino", "uric Goldfinger", "Thomas Mawson", "Al D'Amato", "Peel Holdings", "Java", "trist", "the ecliptic", "Harriet M. Welsch"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5146883084524871}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.14285714285714288, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.06896551724137931, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3157894736842105, 0.0, 0.0, 0.5, 0.7894736842105263, 0.0, 0.13333333333333333, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2724", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-10403", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-6865", "mrqa_searchqa-validation-2650", "mrqa_searchqa-validation-15537", "mrqa_searchqa-validation-5471"], "SR": 0.40625, "CSR": 0.51708984375, "EFR": 1.0, "Overall": 0.70412109375}, {"timecode": 64, "before_eval_results": {"predictions": ["Abbot and Costello", "Great British Bake Off", "Gary Havelock", "butcher", "Fiji", "Natty Bumppo", "Derek Jacobi", "Mali", "Aleister Crowley", "Jon Stewart", "\"Barefoot Bandit\"", "Ytterby", "fox-like", "lithium", "Boston Braves", "kitsunes", "gilda Shedstecker", "1825", "argentina", "Lisieux, Normandy", "Ascot", "jenny wales", "Charlie Cairoli", "gavaskar", "zEITGEISTER", "florida", "William Caxton", "\" Buzz\" Aldrin", "highball", "coal", "Dutch", "Reform Club", "unite", "a specific group of people or of a nation", "the Netherlands", "The Wizard", "pistil", "philosopher", "Thomas Cranmer", "the Mad Hatter", "Nick Clegg", "Virginia", "the Vulcan salute", "the largest buttock muscle", "Nikola Tesla", "adrian Edmondson", "Persian Empire", "the innermost digit of the forelimb", "Boyle\u2019s law", "Antonio Vivaldi", "Bachelor of Science", "lamina dura", "July 2014", "the French CYCLADES project directed by Louis Pouzin", "Massachusetts", "Princess Jessica", "supply chain management", "Dr. Jennifer Arnold and husband Bill Klein,", "2nd Lt. John Auer,", "that Birnbaum had resigned \"on her own terms and own volition.\"", "the Arctic", "Schtze Benjamin", "the Rhine & the Main", "Amber Laura Heard"], "metric_results": {"EM": 0.453125, "QA-F1": 0.47135416666666663}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-6566", "mrqa_triviaqa-validation-5536", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-84", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-195", "mrqa_triviaqa-validation-4258", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-1949", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-5009", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-1171", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-1001", "mrqa_newsqa-validation-1862", "mrqa_searchqa-validation-7466", "mrqa_searchqa-validation-4544", "mrqa_hotpotqa-validation-652"], "SR": 0.453125, "CSR": 0.5161057692307692, "EFR": 1.0, "Overall": 0.7039242788461538}, {"timecode": 65, "before_eval_results": {"predictions": ["hemlock", "paul johnson", "robert", "eyes", "spain", "hockney", "sierra leone", "Preston", "Walmer Castle", "zodiacal equinox", "spain", "Coalbrookdale", "robert borgia", "Periodic System", "Ashe", "bread", "jones", "j Jakarta", "spike milligan", "mitsubishi", "the Panama Canal", "1960", "fused of two or more digits of the feet", "apples", "lug", "paul rudd", "Hamelin", "Harold Godwinson", "Kuwait", "leicestershire county", "sprint", "green", "The Grapes of Wrath", "Coldplay", "pamphlets, posters, ballads", "Rugrats", "president", "lawn", "fat", "Austria", "george gently", "fool", "paul boyle", "Markus Aemilius Lepidus", "business", "tall", "stanley da ponte", "thrushchev", "blue ivy", "High-F fructose Corn Syrup (HFCS)", "DNA's structure", "Schwarzenegger", "Wakanda", "lamina dura", "Bill Cosby", "Robert L. Stone", "Haitian Revolution", "South Africa", "to launch a group that will serve as an alternative to the Organization of American States.", "64", "kolkata", "tuna tune-up Casserole", "rich girl", "Old English pyrige ( pear tree )"], "metric_results": {"EM": 0.453125, "QA-F1": 0.531253234989648}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 0.34782608695652173, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-4758", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-5514", "mrqa_triviaqa-validation-3686", "mrqa_triviaqa-validation-990", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-5504", "mrqa_triviaqa-validation-4521", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3962", "mrqa_triviaqa-validation-4255", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6015", "mrqa_hotpotqa-validation-4757", "mrqa_newsqa-validation-3226", "mrqa_newsqa-validation-2224", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-16126", "mrqa_searchqa-validation-884"], "SR": 0.453125, "CSR": 0.5151515151515151, "EFR": 1.0, "Overall": 0.703733428030303}, {"timecode": 66, "before_eval_results": {"predictions": ["a return ticket", "Sheffield Wednesday", "Buddhism", "Andrew Jackson", "The Bad Beginning", "Rooney Mara", "the Red sea", "red", "sesame seeds", "grizzly bear", "jennifer antony", "Swiss", "Poem Hunter", "Wars of the Roses", "terence Edward \" Terry\" Hall", "acetone", "San Francisco", "Paris", "sewing machine", "Atlas", "Flanagan and Allen", "anophthalmia", "bernard simon", "nymphet", "peter Principle", "Video", "Frank McCourt", "Little Jack Horner", "mark keeler", "blancmange", "dennis simon", "Louis- Stanislas-Xavier", "stand-up", "\"Good Morning to All\"", "1948", "Pride & Prejudice", "william golding", "bographical sketch", "\"bad\"", "Mr. Brainwash", "calypso", "one-eyed", "phrenology", "catherine of aragon", "\"Penry\" Pooch", "apple", "wedges", "Joan Rivers", "Mr. Humphries", "katherine mansfield", "heineken", "1987", "1996", "Ella Mitchell", "\"Wicked Twister\"", "Lerotholi Polytechnic Football Club", "7\u00b056'", "mild to moderate depression", "Saturday", "\"green-card warriors\"", "the Bering Sea", "Charlottetown", "Agatha Christie", "Larry King"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6169270833333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.5, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-3268", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6164", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-1067", "mrqa_newsqa-validation-150", "mrqa_searchqa-validation-140", "mrqa_newsqa-validation-2128"], "SR": 0.5625, "CSR": 0.5158582089552239, "EFR": 1.0, "Overall": 0.7038747667910448}, {"timecode": 67, "before_eval_results": {"predictions": ["Yuri Andropov", "kate winslet", "joseph marlowe", "Camino Franc\u00e9s", "fox", "linda coren Mitchell", "\"Son et lumi\u00e8re\"", "latte", "tomatoes", "fred", "wrought iron", "saint Columba", "d\u00fcrer", "1215", "1937", "12", "michelle foot", "king george IV", "nahuatl", "Venice", "adnams", "Massachusetts", "nikkei", "Nutbush", "robert schumann", "joseph evans", "NASCAR", "Jordan", "linda evans", "llanberis", "The Battle Marengo", "darshaan", "ocular", "n Nicaraguan", "The Live Read of Space Jam", "par-5", "state of oklahoma", "Jason Bourne", "budgantes", "argon", "carbohydrates", "antelope", "Nevada", "SW19", "Jews of Germany", "n Eva craig", "reclaim Our Streets: Soccer Ace And Vc Hero Join Our Crusade", "naypyidaw", "swansea", "Manchester", "archenemy black jack", "Justin Bieber", "2017 season", "eleven", "Club Deportivo Castell\u00f3n, S.A.D.", "East Kn Boyle", "John II Casimir Vasa", "2.5 million copies", "Vivek Wadhwa,", "Wednesday", "parody", "Animal House", "Lake Victoria", "the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5431547619047619}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-565", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-6399", "mrqa_triviaqa-validation-1929", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-2021", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-7545", "mrqa_triviaqa-validation-1943", "mrqa_triviaqa-validation-506", "mrqa_triviaqa-validation-7710", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-3632", "mrqa_newsqa-validation-1443"], "SR": 0.453125, "CSR": 0.5149356617647058, "EFR": 0.9714285714285714, "Overall": 0.6979759716386554}, {"timecode": 68, "before_eval_results": {"predictions": ["Sydney Harbour", "Richard Seddon", "16", "archers", "st james cephalonia", "Top Cat", "Fotheringhay", "tungsten", "New Zealand", "fenn street", "Kristiania", "South Pacific", "kurt", "mozart", "thalia", "paddy mcinness", "woodstock", "st james", "Chicago", "jack", "obedience", "outdoor", "Sarajevo", "h Hokkaido", "Norman Mailer", "david boyard", "florence", "crabapple", "braille", "PC", "Stockholm", "Washington", "Switzerland", "fort boyard", "pressure", "fort boyard", "st james", "peter taylor", "dr ichak adizes", "1936", "honda", "ravi shankar", "Dunfermline Athletic", "cribbage", "midtown", "the Library of Congress", "quarter", "copper", "pear", "cunard", "elton john", "peptide bond", "William the Conqueror", "Aslan", "Gregory Carlton \"Greg\" Anthony", "\"Pete and Gladys\"", "Lowe's Companies, Inc.", "India", "Cash for Clunkers", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Nassau", "Maurice Jarre", "degaussing", "10 Years"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6114583333333333}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-4762", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-928", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-3851", "mrqa_triviaqa-validation-5077", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-3414", "mrqa_triviaqa-validation-566", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-1862", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5611", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-5498", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-536", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-7518", "mrqa_naturalquestions-validation-3016", "mrqa_hotpotqa-validation-3119", "mrqa_hotpotqa-validation-1129", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-3611"], "SR": 0.546875, "CSR": 0.5153985507246377, "EFR": 1.0, "Overall": 0.7037828351449276}, {"timecode": 69, "before_eval_results": {"predictions": ["victoria plum Brit", "Ronald Searle", "dennis taylor", "loki", "The Avengers", "snakes", "insulin", "lilac", "Ryan Stone", "laryngeal prominence", "Andes", "leprechaun", "Hawaii", "steels", "heraldry", "tom good", "japan", "british", "Sherlock Holmes", "Ida noddack", "The Rocky and Bullwinkle", "vindaloo", "South Africa", "bror bror", "mark Twain", "holly johnson", "beef", "khaki uniforms", "seattle", "joseph w", "Dunfermline Athletic", "4", "joseph caiaphas", "penrhyn", "australia", "African violet", "ourselves alone", "James Dean", "Eva Herzigov\u00e1", "drizzle", "chiropractic", "The Wicker Man", "stieg Larsson", "james devlin", "orecchiette", "british university", "first web page", "Croatia", "cete", "greyfriars", "kevin Bridges", "John Locke", "2017 season", "Matt Monro", "comic", "Disha Patani", "USS \"Enterprise\"", "Ben Freeth", "john Stewart", "kryptonite", "solar eclipses", "Sonny & Cher", "the Aegean", "Crank Yankers"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5989583333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2531", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-1801", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-2824", "mrqa_triviaqa-validation-4485", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1322", "mrqa_triviaqa-validation-6999", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-335", "mrqa_triviaqa-validation-4829", "mrqa_triviaqa-validation-6456", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-7614", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-110", "mrqa_searchqa-validation-9522", "mrqa_searchqa-validation-1393"], "SR": 0.546875, "CSR": 0.5158482142857144, "EFR": 1.0, "Overall": 0.7038727678571429}, {"timecode": 70, "UKR": 0.708984375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3054", "mrqa_hotpotqa-validation-306", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-242", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3680", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-975", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1461", "mrqa_squad-validation-147", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2564", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3473", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3923", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5884", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6670", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-6981", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7083", "mrqa_squad-validation-7094", "mrqa_squad-validation-7339", "mrqa_squad-validation-78", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-9002", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9344", "mrqa_squad-validation-9411", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1514", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1801", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4615", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5117", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6285", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.80078125, "KG": 0.47734375, "before_eval_results": {"predictions": ["the Philippines", "silurian", "nell gervais", "big eggo", "tequila", "french", "perry pear", "ireland", "gold", "Tina Turner", "Sparks", "nissan", "washing", "mexico", "Benjamin Britten", "Eric Coates", "st Pancras", "beer", "Toronto", "the Cevennes", "lady Gaga", "john simm", "carbon copy", "1979", "Donald Trump", "furlong", "Tomorrow Never Dies", "to heat and boil water for tea.", "grover Washington Jr.", "Melbourne", "bullfighting", "Autobahn", "Kiss Me Kate", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "hindenburg", "Steff Graff", "sandra", "Tangled", "spain", "nell Raggett", "stockings", "cooperative", "smallpox", "sandra b Barker", "Leicester City", "violin", "nipples", "king george v", "the Temple of Artemis", "acetic acid", "Achille Lauro", "Frank Langella", "anion", "Valene Kane", "Hilo", "Objectivism", "16,116", "mayor", "\u00a320 million ($41.1 million)", "Eleven people", "Anna Eleanor Roosevelt", "a small needle", "Smilla", "France"], "metric_results": {"EM": 0.5, "QA-F1": 0.57421875}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-4689", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-6736", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-5521", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-3327", "mrqa_triviaqa-validation-2633", "mrqa_triviaqa-validation-7099", "mrqa_triviaqa-validation-2648", "mrqa_triviaqa-validation-1591", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-2372", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-2351", "mrqa_hotpotqa-validation-4382", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-335", "mrqa_searchqa-validation-15744", "mrqa_searchqa-validation-6116"], "SR": 0.5, "CSR": 0.515625, "EFR": 0.96875, "Overall": 0.694296875}, {"timecode": 71, "before_eval_results": {"predictions": ["31536000 seconds", "Suez Canal", "robert boyer", "london", "Paris", "john poulson", "sapodilla", "1963", "h Hofmannsthal", "Leonard Nimoy", "sandi Toksvig", "Louis Le Vau", "Rustle My Davies", "nee davis", "Edinburgh", "germany", "Arabah", "james gumm", "Ut\u00f8ya island", "Lesley Garrett", "Ty Hardin", "b\u00e4umer", "112 pounds", "limestone", "Bristol Aeroplane Company", "charliesheen", "anita Brookner", "bitter", "endometriosis", "James Stewart", "typewriter", "eight", "Pizza Express", "Lilo & Stitch", "Hugh Quarshie", "billie holiday", "kokbarok", "duke of York", "Eric Morley", "sandstone trail", "The Babysitter Murders", "james chastain", "a barge", "Yemen", "antelope", "relativistic mass", "james", "muskets", "bajan", "malted barley", "hoagland", "Jonny Buckland", "Stanley Tucci", "Janie Crawford", "teen brains go!", "Milk Barn Animation", "nursery rhyme", "the Russian air force,", "President Obama and Britain's Prince Charles", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "Captains Courageous", "16", "kayak", "Wisconsin"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49947916666666664}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.26666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-4932", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3511", "mrqa_triviaqa-validation-7290", "mrqa_triviaqa-validation-3506", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-4979", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7629", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-5018", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-564", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5346", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1051", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-3830"], "SR": 0.40625, "CSR": 0.5141059027777778, "EFR": 1.0, "Overall": 0.7002430555555555}, {"timecode": 72, "before_eval_results": {"predictions": ["Cambridge", "wichterle and Drahoslav Lim", "Poland", "Washington", "apples", "high jump", "a horizontal desire", "Hungary", "port Talbot", "pantagruel", "activewear", "halloween", "Sydney", "Charlie Chaplin", "olfactory nerves", "russell", "judy holliday", "cedars", "michael connelly", "geyser", "blue ivy", "Prince Edward, Earl of Wessex", "Israel", "blackburn rovers", "1943", "Elizabeth Taylor", "daimler", "davies", "Bosnia and Herzegovina", "james hargreaves", "antonia pinter", "peter stuyvesant", "South Africa", "Dirk Bikembergs", "kurf\u00fcrstendamm", "Mark Twain", "surfer", "ever decreasing circles", "quito", "Sensurround", "roland", "sandown", "goat", "lady", "utan", "bb", "phoenician", "ten", "Kajagoogoo", "Carly Simon", "robbie holliday airport", "March 12, 2013", "The Ecology", "intermembrane space", "Ronnie Schell", "La Familia Michoacana", "Starlite", "Djibouti,", "The cervical cancer vaccine,", "South America and Africa.", "Warsaw", "City Slickers", "ex-wife", "Richa Sharma"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6263020833333334}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-1196", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-7446", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-2203", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-4025", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-4148", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4215", "mrqa_triviaqa-validation-249", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-4498", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-5471", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-180", "mrqa_hotpotqa-validation-1782", "mrqa_newsqa-validation-2784", "mrqa_searchqa-validation-1212"], "SR": 0.59375, "CSR": 0.5151969178082192, "EFR": 0.9230769230769231, "Overall": 0.6850766431770285}, {"timecode": 73, "before_eval_results": {"predictions": ["agreta lavisa Gustafsson", "c\u00e9vennes", "lilo anditch", "agricola", "George Best", "lilo", "Bagram Collection Point", "pink", "charlie", "ostrich", "ireland", "Louren\u00e7o Marques", "silk warp", "swaziland", "cartoonist, author, art critic and stage designer", "jack the Ripper", "Shooter McGavin", "dodo", "imola", "albus", "brazil", "Thailand", "america", "worcester cathedral", "poincar\u00e9 conjecture", "Superman", "wales", "rudolph", "mary square garden", "The Equals", "baffin island", "Woodstock", "molybdenum", "pangea", "Hungary", "apollon", "Matterhorn", "gold hallmarks", "tide of the Mississippi", "genesis", "trumpet", "South Carolina", "self", "james chadwick", "coffee house", "Vietnam War", "pilgrimage", "althea Gibson", "althorp", "Pyrenees", "Noah", "Richmond, BC", "October 1, 2015", "near Flamborough Head", "25 November 2015", "Jesper Myrfors", "Vancouver", "Rod Blagojevich,", "Mary Procidano,", "opium", "the Spanish Succession", "Minnesota", "quid", "Ugly Betty"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6494791666666666}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-80", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-5646", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-1967", "mrqa_triviaqa-validation-2741", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-606", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-2810", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-4216", "mrqa_naturalquestions-validation-4092", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-5018", "mrqa_newsqa-validation-3632"], "SR": 0.578125, "CSR": 0.5160472972972974, "EFR": 0.9629629629629629, "Overall": 0.693223927052052}, {"timecode": 74, "before_eval_results": {"predictions": ["jamaica", "get well soon", "jordan christie carleson", "london", "halloween", "Compundyne", "Samson", "copenhagen", "selenium", "western Caribbean Sea", "bathtub curve", "john napier", "silvergrass", "macbeth", "Eton College", "philip ltd", "jaws", "keeper of the Longstone (Fame Islands) lighthouse", "Tommy Allsup", "robert boyle", "jaws of Saturn", "sphinx", "jaws", "william morris", "pennsylvania state university", "Father John O'Connor", "Henry Ford", "a jet", "dihydrogen monoxide", "dennis Wilson", "sir russell", "Alison Krauss", "apricots", "four", "neurons", "Poland", "banjo", "cricket", "time bandits", "The Hague", "one foot in the Grave", "russell", "copper", "russell tchaikovsky", "speed camera", "food, water, sleep, and warmth", "blue", "passport", "florence", "costume shop", "j jaws", "in the middle of the 15th century", "1937", "turkey", "business", "boxer", "The Merchant's Tale", "Stanford University", "Sgt. Barbara Jones of the Orlando Police Department.", "because the Indians were gathering information about the rebels to give to the Colombian military.", "George F. Babbitt", "Emanuel Swedenborg", "nod", "Thorleif Haug"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5224702380952382}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 0.5, 0.5, 0.6666666666666666, 0.6, 1.0, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-4919", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-1912", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-4433", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-3070", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-2981", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-4603", "mrqa_triviaqa-validation-53", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-5806", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-957", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-2830", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-5156", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3332", "mrqa_searchqa-validation-13441", "mrqa_naturalquestions-validation-2509"], "SR": 0.390625, "CSR": 0.514375, "EFR": 1.0, "Overall": 0.700296875}, {"timecode": 75, "before_eval_results": {"predictions": ["rugby", "hyperbole", "North by Northwest", "danelaw", "mahatma Gandhi", "for Gallantry", "filibustering", "colette", "willow", "eurozone", "Separate Tables", "harry Spencer", "Ulysses S. Grant", "1929", "aviva", "Antarctica", "The Hurt Locker", "Supreme Commander Gen. Douglas MacArthur", "chief Inspector Japp", "zager and evans", "c\u00e9vennes", "Genesis", "Jimmy Hoffa", "tennis", "jeSuisCharlie", "france ethel gumm", "dark blood", "lowestoft", "Washington state", "teaching evolution in violation of a Tennessee state law.", "lulu", "einyes", "faggots", "k2", "Angus Deayton", "david bowie", "Chester Racecourse", "tchaikovsky", "faversham", "Jimmy Knapp", "antoine", "new Zealand", "antoine", "butcher", "edward Woodward", "priests", "violins", "(Charles) Taylor", "eucalyptus", "1883", "herald of free enterprise", "3000 BC", "lacteal", "Renishaw Hall", "The Jefferson Memorial", "96", "New Orleans, Louisiana", "JBS Swift Beef Company, of Greeley, Colorado,", "Silicon Valley.", "10-person", "Leon Trotsky", "beta", "(A)", "ancients"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7214556277056277}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.36363636363636365, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-7341", "mrqa_triviaqa-validation-3649", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-378", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-1786", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-4661", "mrqa_naturalquestions-validation-10408", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-4936", "mrqa_searchqa-validation-8292"], "SR": 0.640625, "CSR": 0.5160361842105263, "EFR": 1.0, "Overall": 0.7006291118421053}, {"timecode": 76, "before_eval_results": {"predictions": ["Ginsburg", "propeller", "Joe Louis", "George Clooney", "Wyeth", "the Louvre", "feminism", "potatoes", "Wallace & Gromit", "arc", "Mozambique", "the Blue Nile", "troy", "\"Timber!\"", "reptiles", "auto Leasing", "coconut", "Imaginext", "the Tsardom of Russia", "Lord Bill Astor", "swedish", "Making the Band", "pennies", "the Colorado", "lighting", "onomatopoeia", "Library of Congress", "Hawaii", "Sigurd Jorsalfar", "Georgetown University", "Insufficient", "difference", "Kellen Winslow", "Madison County", "Kennebunkport", "a room with a View", "a leg", "Africa", "Ingenue", "Notre-Dame de Paris", "scientific research", "Peppermint Patty", "paul mccartney", "Iberian", "bionic", "trip", "baccarat", "Treasury Sec.", "Wallis Warfield Simpson", "grapevine", "corset", "December 14, 2017", "Virginia Dare", "man", "caribbean", "the Panama Canal", "Will Smith", "1950", "\"Loch Lomond\"", "Aircraft maneuvering", "Britain and France", "Alwin Landry's supply vessel Damon Bankston", "The Obama administration", "funchal"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6255208333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16515", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-5110", "mrqa_searchqa-validation-8417", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-16759", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12916", "mrqa_searchqa-validation-737", "mrqa_searchqa-validation-9899", "mrqa_searchqa-validation-4056", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-13006", "mrqa_searchqa-validation-12225", "mrqa_searchqa-validation-10756", "mrqa_searchqa-validation-5273", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-11797", "mrqa_naturalquestions-validation-9781", "mrqa_triviaqa-validation-5219", "mrqa_hotpotqa-validation-2065", "mrqa_hotpotqa-validation-5638", "mrqa_newsqa-validation-660"], "SR": 0.53125, "CSR": 0.5162337662337662, "EFR": 1.0, "Overall": 0.7006686282467532}, {"timecode": 77, "before_eval_results": {"predictions": ["August 16, 2016", "Bob Fosse", "(Guglielmo) Marconi", "Mexico", "Wynton Marsalis", "Volleyball", "Havana", "Edwin Hubble", "Einstein", "Lhasa", "the U.S. Census Bureau", "New Kids on the Block", "Manila Bay", "Lady Chatterley", "molasses", "Hard Knock Life", "a crumpet", "(Douglas) MacArthur", "(Al) Gore", "Sappho", "the Netherlands", "Texas", "Heart of Darkness", "The Hippocratic Oath", "a French-based Afghan resistance group", "Solidarity", "Kookaburra", "the Battle of Hastings", "(Tom) Tom", "Craftsman", "a key", "W.H. Auden", "Chuck Berry", "Lou Gehrig", "a diaphragm", "the Marquis of Sade", "Louis Comfort Tiffany", "a tornado", "the joker", "New Zealand", "a glove", "Jutland", "Kindergarten", "(Cephalopoda)", "Titanic", "San Francisco", "Gulliver's Travels", "a carriage", "Billy Bathgate", "Richmond", "steel", "1988", "Cheryl Campbell", "from an Ohio newspaper on 8 February 1925", "Joe Brown", "Funchel", "Virgil", "Timothy Dowling", "WANH (91.5 FM)", "near Bear Creek", "Gov. Rod Blagojevich", "staff sergeant", "\"procedure on her heart,\"", "55th district"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6333333333333333}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14018", "mrqa_searchqa-validation-7461", "mrqa_searchqa-validation-2395", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-16708", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-1687", "mrqa_searchqa-validation-1963", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14160", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-11728", "mrqa_searchqa-validation-3821", "mrqa_searchqa-validation-388", "mrqa_searchqa-validation-14662", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-12470", "mrqa_searchqa-validation-10670", "mrqa_naturalquestions-validation-6665", "mrqa_triviaqa-validation-1610", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-5500", "mrqa_newsqa-validation-563", "mrqa_newsqa-validation-2547", "mrqa_hotpotqa-validation-5006"], "SR": 0.5625, "CSR": 0.5168269230769231, "EFR": 0.9285714285714286, "Overall": 0.6865015453296703}, {"timecode": 78, "before_eval_results": {"predictions": ["Flint, Michigan.", "a president who understands the world today, the future we seek and the change we need.", "5:20 p.m.", "Former Mobile County Circuit Judge Herman Thomas", "Top Gun", "Daniel Radcliffe", "Alan Graham", "Tennessee", "Intensifying", "of \"v Vigilante justice,\" Prince George's County Police Cpl. Richard Findley,", "Dubai", "gun", "Larry King", "Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "airlines around the world shut down every year.", "repression and dire economic circumstances.", "African National Congress", "204,000", "Haiti.", "police", "Zuma", "Drinking, partying, and resisting monogamy,", "US Airways Flight 1549", "the estate with its 18th-century sights, sounds, and scents.", "injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "an auxiliary lock", "The Rosie Show", "Between 1,000 and 2,000", "Tuesday,", "St Petersburg and Moscow,", "6-2 6-1", "breast cancer.", "Diego Milito's", "\"Three Little Beers,\"", "romantic", "Sen. Evan Bayh", "\"a striking blow to due process and the rule of law.\"", "Friday.", "a judge to order the pop star's estate to pay him a monthly allowance,", "consumer confidence", "Afghanistan and India", "Hugo Chavez", "Expedia", "coastal development destroys 20,000 acres of estuaries and near-coast fish habitat.", "bronze", "nine", "JBS Swift Beef Company, of Greeley, Colorado,", "250,000", "state senators", "Friday,", "Jaime Andrade", "hijab", "chief lawyer of the United States government", "the Sons of Liberty in Boston, Massachusetts", "Arkansas", "Adam Smith", "the ozone layer", "Manchester Victoria station in air rights space", "communist", "1896", "the ceiling", "Jonathan Swift", "Molson", "sir Adrian Boult"], "metric_results": {"EM": 0.5, "QA-F1": 0.6135354459848881}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false], "QA-F1": [0.0, 0.9565217391304348, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6399999999999999, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.45454545454545453, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7058823529411764, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3870967741935484, 0.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-368", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-642", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-7615", "mrqa_hotpotqa-validation-1531", "mrqa_hotpotqa-validation-375", "mrqa_searchqa-validation-3681", "mrqa_triviaqa-validation-5099"], "SR": 0.5, "CSR": 0.5166139240506329, "EFR": 0.96875, "Overall": 0.6944946598101266}, {"timecode": 79, "before_eval_results": {"predictions": ["an Italian and six Africans", "Daniel Radcliffe", "the remaining rebel strongholds in the north of Sri Lanka,", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Samoa", "BET", "in the Iraq's autonomous region of Kurdistan.", "Adam Yahiye Gadahn,", "mental health and recovery.", "75.", "co-wrote", "2005.", "12", "Lana Clarkson", "Iran", "attempted robbery stemming from a fatal encounter with police officer Daniel Enchautegui.", "70,000 in Sri Lanka's war zone.", "severe flooding", "56", "frozen world located in the Gaslight Theater.", "Umar Farouk AbdulMutallab", "anesthetic and sedative.", "Saturday's Hungarian Grand Prix.", "Sub-Saharan Africa", "Manny Pacquiao", "Aung San Suu Kyi", "Jeanne Tripplehorn", "modern and classic designs", "seven-time Formula One world champion Michael Schumacher", "Austin Wuennenberg,", "1983", "the U.S. Holocaust Memorial Museum,", "the college campus.", "an African-American woman for the job.", "U.S. program to assassinate terrorists in Iraq.", "misdemeanor", "Thursday", "the man facing up, with his arms out to the side.", "golf", "Israel handed the United Nations a report justifying its actions", "prostate cancer,", "Iraqi and U.S. soldiers were attacked by small-arms, machine-gun and RPG fire", "$1.45 billion", "people look at the content of the speech, not just the delivery.", "\"Wax on, wax off\"", "for strategy, plans and policy on the Army staff.", "walk on ice in Alaska.", "a city of romance, of incredible architecture and history.", "regulators in the agency's Colorado office", "Chancellor Angela Merkel", "Nothing But Love", "number of times a pitcher pitches in a season", "Audrey II", "access to US courts", "troposphere", "arthur ashe", "Hippo", "2004", "Tim \"Ripper\" Owens", "Brad Silberling", "Mother Vineyard", "Mars", "the Capitol", "Out - With"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6929991018146389}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 0.9523809523809523, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.08695652173913045, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714288, 0.33333333333333337, 0.9411764705882353, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2835", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-1706", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7987", "mrqa_triviaqa-validation-4441", "mrqa_searchqa-validation-15009", "mrqa_naturalquestions-validation-582"], "SR": 0.578125, "CSR": 0.5173828125, "EFR": 1.0, "Overall": 0.7008984375}, {"timecode": 80, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-512", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11854", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5646", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7035", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-984"], "OKR": 0.814453125, "KG": 0.4984375, "before_eval_results": {"predictions": ["Tyler \"Ty\" Mendoza", "A Little Princess", "four", "footballer", "five", "Julie Taymor", "Greg Anthony", "Drifting", "Logar", "Rebirth", "a personalized certificate, an official pin, medallion, and/or a congratulatory letter", "the Harpe brothers", "Bedknobs and Broomsticks", "Martin \"Marty\" McCann", "Yubin, Yeeun", "Herbert Ross", "Elena Verdugo", "melodic hard rock", "three", "Christian Kern", "Taylor Alison Swift", "SARS", "the son of writer William F. Buckley Jr.", "1345 to 1377", "Polish Army", "Noel", "India Today", "North Dakota", "2006", "\"Histoires ou contes du temps pass\u00e9\"", "\"ali\u02bb i\" (noble) of the Royal Family of the Kingdom of Hawaii", "mixed martial arts", "Yarrow and Stookey", "Mathieu Kassovitz", "Eileen Atkins", "Summerlin, Nevada", "Jean-Marc Vall\u00e9e", "Klasky Csupo", "1950s", "Prussia", "Newfoundland and Labrador", "Tom Kartsotis", "Knowlton Hall", "shock cavalry", "Manhattan", "Professor Frederick Lindemann, Baron Cherwell", "dementia", "5 foot 9 inch tall twins", "seven", "July 11, 2016", "\"Nina\"", "5", "40 %", "March 1, 2018", "piano", "Pink Panther", "G\u00e9rard Depardieu", "is a businessman, team owner, radio-show host and author.", "137", "14", "Rhizo", "Emperor Maximillian", "Lake Michigan", "Hot Chocolate"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6511904761904761}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.25, 0.8571428571428571, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.28571428571428575, 1.0, 0.25, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-1975", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-3898", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-399", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-4131", "mrqa_naturalquestions-validation-8301", "mrqa_naturalquestions-validation-190", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-1295", "mrqa_searchqa-validation-1349", "mrqa_searchqa-validation-6921", "mrqa_triviaqa-validation-85"], "SR": 0.53125, "CSR": 0.517554012345679, "EFR": 1.0, "Overall": 0.7063233024691358}, {"timecode": 81, "before_eval_results": {"predictions": ["the cornet", "Wimbledon", "Hungary", "the HIV/AIDS", "Nepal", "singluar", "Sanjaya", "fauves", "Dresden", "Turkish", "the Shirley Temple", "flavor Flav", "Michael Nichols", "backcountry", "blue blood", "Acetylene", "32", "Harriet", "Illinois", "a cosmopolitan", "Amsterdam", "Grover Cleveland", "Clyde", "James Naismith", "Harold Godwinson", "North Carolina", "Job", "1969", "Take Me", "pickles", "Stand by Me", "Lead", "Nokia", "Bernard Malamud", "Cyprus", "the first run", "Neil Diamond", "Munich", "Babe Ruth", "wildebeest", "Sicilian pizza", "Pirates", "the Atlantic bluefin tuna", "Arts and Crafts", "Loam", "Subclue 2", "Uvula", "Biloxi", "Treasure Island", "Valiant", "a hope chest", "specific brain regions", "Robber baron", "NFL owners", "Jane Seymour", "Guy", "george bernard shaw", "Michael Greif", "Tom Ewell", "Wolfgang Amadeus Mozart", "Senate Democrats", "seven", "16", "cancerous tumor."], "metric_results": {"EM": 0.640625, "QA-F1": 0.6875}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5160", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-6682", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-12400", "mrqa_searchqa-validation-7539", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-14093", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-1314", "mrqa_searchqa-validation-7545", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-15450", "mrqa_searchqa-validation-8240", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-16773", "mrqa_searchqa-validation-7174", "mrqa_naturalquestions-validation-3840", "mrqa_hotpotqa-validation-2337", "mrqa_newsqa-validation-1546"], "SR": 0.640625, "CSR": 0.5190548780487805, "EFR": 1.0, "Overall": 0.7066234756097561}, {"timecode": 82, "before_eval_results": {"predictions": ["Jaws 2", "the femur", "Ovid", "Explosives", "a squire", "Tudor", "Australia", "wine", "sheep", "Washington, D.C.", "lily", "H", "the Isle of Wight", "gung ho", "Dale Earnhardt", "Mary Elizabeth Garrett", "Jordan", "Tiger Woods", "North Africa", "Arapahoe Philharmonic", "Stephen Hawking", "James Madison", "X-Ray", "Disturbia", "Michael Moore", "The Indianapolis 500", "I, Daniel Blake", "tapping", "an anchor", "Johannesburg", "carbon", "the Philistines", "tremor", "Louis Chevy", "Morocco", "M&M\\'s", "Hieronymus Bosch", "Neil Diamond", "Molire", "Malaysia", "bionic", "Hamlet", "Lance Armstrong", "Steak fingers", "Edith Wharton", "the Berlin Wall", "Uranus", "George Costanza", "telephone operator", "a bonnet", "Henry Moore", "summer", "the Great Crash", "Elena Anaya", "Brisbane Road", "Vinegar Joe", "austria", "Girl Meets World", "three", "\"Pour le M\u00e9rite\"", "The meter reader", "Zimbabwe's main opposition party", "in '07,", "Don Draper"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6458333333333333}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-734", "mrqa_searchqa-validation-7247", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-802", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-15533", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-14165", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-13445", "mrqa_searchqa-validation-14350", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-319", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8837", "mrqa_triviaqa-validation-5807", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2964"], "SR": 0.609375, "CSR": 0.5201430722891567, "EFR": 0.96, "Overall": 0.6988411144578314}, {"timecode": 83, "before_eval_results": {"predictions": ["Atlanta", "Dmitri Mendeleev", "calligraphy", "James Edward Ellington", "Maria Sharapova", "Chile", "glow", "John Waters", "Aristophanes", "the Americans with Disabilities Act", "freelance", "y Yahoo", "Thurman Munson", "a barrel", "Chippewa", "Rooster Cogburn", "15", "Richard Burton", "gears", "meringue", "The Dying Swan", "the Big Bang", "winter", "Alyssa Milano", "Tahiti", "Herbert Hoover", "Keith Urban", "an isosceles triangle", "The Jinx", "Neil Armstrong", "the Netherlands", "Kelly Clarkson", "Michael Douglas", "aquiline", "Troy weight", "Neil Simon", "A Raisin in the Sun", "trespass", "Ronald Reagan", "Patrick Henry", "the light bulb", "the war", "the viola", "ostrich", "I love rock and roll", "American culture", "the New Indiana State Motto", "Ziploc", "Hannibal", "Anne Wiggins Brown", "Beethoven", "Gene Barry", "Tachycardia", "American comedy - drama", "the American Civil War", "mexico", "Denise Richards", "India Today", "Distinguished Service Cross", "non-binary", "raping and murdering a woman in Missouri.", "Jeanne Tripplehorn's", "Zelaya and Roberto Micheletti,", "New York Islanders"], "metric_results": {"EM": 0.6875, "QA-F1": 0.753422619047619}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8000000000000002, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4451", "mrqa_searchqa-validation-11935", "mrqa_searchqa-validation-9367", "mrqa_searchqa-validation-7651", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-9041", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-16490", "mrqa_searchqa-validation-3237", "mrqa_searchqa-validation-14205", "mrqa_searchqa-validation-3697", "mrqa_searchqa-validation-6308", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-4354", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-435"], "SR": 0.6875, "CSR": 0.5221354166666667, "EFR": 1.0, "Overall": 0.7072395833333334}, {"timecode": 84, "before_eval_results": {"predictions": ["Arnold Schoenberg", "Ford", "urease", "Grover\\'s Corner", "President Lincoln", "topaz", "Universal City", "surrender", "Norman Mailer", "subtraction", "Harpy", "Macon, Georgia", "capital is Annapolis", "fur", "Titan", "the crossword clue", "quick picks", "Batista", "the Inside Passage", "makrama", "Toy Story", "fight", "the Ark of the Covenant", "the Grand Cross", "Granite", "emperor", "Klondike", "a dove", "the Jet Propulsion Laboratory", "Francis Scott Key", "8 Mile", "Tarzan", "Diebold", "cheese", "New Guinea", "Queen Latifah", "the Liberty Bell", "anchovy", "Saint Teresa of Calcutta", "Clarence Thomas", "the day of Mars", "naqur", "whimper", "Prison Break", "Iberian Peninsula", "the ceiling", "a kart", "Kilimanjaro", "the Koala", "Circus", "Extradition", "1979", "Rudy Kangaroo", "Van Halen", "pasta carbonara", "finger", "Robert Schumann", "Pacific Place", "1941", "yellow fever is transmitted by mosquitoes", "help the convicts find calmness in a prison culture fertile with violence and chaos.", "Polo", "Friday,", "2009"], "metric_results": {"EM": 0.5, "QA-F1": 0.5651041666666666}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-10313", "mrqa_searchqa-validation-9264", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-3217", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-14803", "mrqa_searchqa-validation-12869", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-12518", "mrqa_searchqa-validation-9830", "mrqa_searchqa-validation-4630", "mrqa_searchqa-validation-8767", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-12943", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-7275", "mrqa_searchqa-validation-14709", "mrqa_searchqa-validation-10541", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-5264", "mrqa_triviaqa-validation-7214", "mrqa_hotpotqa-validation-3149", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-1008", "mrqa_naturalquestions-validation-1856"], "SR": 0.5, "CSR": 0.521875, "EFR": 0.96875, "Overall": 0.7009375}, {"timecode": 85, "before_eval_results": {"predictions": ["11", "cancer", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "Picasso's muse and mistress, Marie-Therese Walter.", "a motor scooter", "Pixar's \"Toy Story\"", "supermodel", "South Africa", "Missouri.", "AbdulMutallab", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Hong Kong's Victoria Harbor", "three", "detainees of Immigration and Customs Enforcement accuse the agency in a lawsuit of forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "8 p.m.", "acid attack by a spurned suitor.", "Bowie", "resigned as leader of the ruling political party Monday following a poor showing in Sunday's elections,", "a number of calls,", "summer", "Tuesday afternoon.", "Peppermint oil, soluble fiber, and antispasmodic drugs", "Molotov cocktails, rocks and glass.", "Sen. Evan Bayh of Indiana and Virginia Gov. Tim Kaine", "last April,", "2008.", "\"fusion teams,\"", "three", "the L'Aquila earthquake, which killed nearly 300 people and devastated the city when it struck last year,", "88", "Cash for Clunkers", "$199", "cancerous tumor.", "American", "CNN's \"Piers Morgan Tonight\"", "several weeks,", "$1.4 million,", "next year", "the death of", "the 11th year in a row.", "will look at how the universe formed by analyzing particle collisions.", "Saturn owners", "dead", "11 healthy eggs", "Alfredo Astiz,", "drug cartels", "some of the Awa", "two remaining crew members", "Sabina Guzzanti", "more than 4,000 commercial farmers off their land, destroying Zimbabwe's once prosperous agricultural sector.", "Robert Gates", "British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Bj\u00f8rn Floberg", "to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "\"IRL\"", "Gaston Leroux", "Volkswagen", "\u00c6thelred I", "Robert \"Bobby\" Germaine, Sr.", "white and orange", "an epiphyte", "Re- Animator", "a walk", "Girls' Generation"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6441498468636626}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.07692307692307691, 1.0, 1.0, 1.0, 0.6363636363636364, 1.0, 0.9523809523809523, 0.6666666666666666, 1.0, 0.0, 1.0, 0.19047619047619044, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.9473684210526316, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.9696969696969697, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3186", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-164", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-3537", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9476"], "SR": 0.46875, "CSR": 0.5212572674418605, "EFR": 1.0, "Overall": 0.7070639534883721}, {"timecode": 86, "before_eval_results": {"predictions": ["cement (or concrete)", "Virginia", "orsche", "coax", "haiku", "waive", "Malaysia", "loverly", "economics", "the Elvis Presley Automobile Museum", "funnel", "Beverly Hills", "Irish Coffee", "a live young chicken", "gasoline cars", "Isaac Newton", "Billy Budd", "John Brown", "Communist", "Gene Krupa", "a skull", "13 Cain", "Smashing Pumpkins", "cruller", "I", "Ma Barker", "Northanger Abbey", "Wyatt Earp", "\"Star Trek\"", "Mensa", "febreze", "Begin (Miniature)", "a lamb", "Philip Seymour Hoffman", "a belief", "Wayne Gretzky", "amu", "Michael Irvin", "Gap", "salt", "Tower Hill", "Thanksgiving", "Westinghouse", "a salad Dressing", "The Fugitive", "Sisyphus", "Java", "Bimini", "bioluminescence", "Rococo", "the First Barbary War", "Pakistan", "961", "Reba McEntire and Linda Davis", "dodo", "Northumberland", "Louis XVI", "July 16, 1971", "sitters", "12", "monarchy's", "Africa", "Donald Trump", "Johnny cage"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6284598214285714}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15136", "mrqa_searchqa-validation-16965", "mrqa_searchqa-validation-8424", "mrqa_searchqa-validation-1398", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-5358", "mrqa_searchqa-validation-12385", "mrqa_searchqa-validation-6460", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-12090", "mrqa_searchqa-validation-14681", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-6245", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-8644", "mrqa_searchqa-validation-2161", "mrqa_searchqa-validation-5579", "mrqa_searchqa-validation-5867", "mrqa_naturalquestions-validation-3672", "mrqa_hotpotqa-validation-680", "mrqa_hotpotqa-validation-5667", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1586", "mrqa_hotpotqa-validation-4514"], "SR": 0.5625, "CSR": 0.5217313218390804, "EFR": 1.0, "Overall": 0.7071587643678161}, {"timecode": 87, "before_eval_results": {"predictions": ["John Adams, Benjamin Franklin, Alexander Hamilton, John Jay, Thomas Jefferson, James Madison, and George Washington", "no more than 4.25 inches ( 108 mm )", "uterus", "China", "eleven", "the third ventricle", "Cody Fern", "five", "2007 and 2008", "New York City", "Schadenfreude", "longitude", "a Christmas Tree", "Johannes Gutenberg", "The Mecca", "Rocky Dzidzornu", "Jennifer Grey", "neuropsychology", "Yosemite National Park", "April 3, 1973", "Jane Lynch", "The Supreme Court of the Philippines", "1997", "Emma Watson", "near Flamborough Head", "Big Boi", "the President of India", "Pedro Espada", "nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "the Senate", "Session Initiation Protocol", "bowel obstruction, short bowel syndrome, gastroschisis, prolonged diarrhea regardless of its cause, high - output fistula, very severe Crohn's disease or ulcerative colitis, and certain pediatric GI disorders", "presbyters", "Tessa Peake - Jones", "Katherine Allentuck", "September 19 - 22, 2017", "the 2001 -- 2002 season", "1773", "Randy VanWarmer", "senators", "Teri Garr", "the producers, businesses, and workers of the import - competing sector in the country from foreign competitors", "13", "10.5 %", "Gene MacLellan", "from shore to shore", "Brad Dourif", "1995", "Sanchez Navarro", "2008", "23 September 1889", "My Fair Lady", "Armageddon", "Thailand", "\u00c6thelstan", "Eugene", "\"Highwayman\"", "Hezbollah.", "Former Mobile County Circuit Judge Herman Thomas", "the abduction of minors.", "Dag Hammarskjld", "Erin Go Bragh", "Art Garfunkel", "three"], "metric_results": {"EM": 0.625, "QA-F1": 0.694912575874747}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.9189189189189189, 0.0, 1.0, 0.2631578947368421, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962962962962963, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-6157", "mrqa_naturalquestions-validation-8061", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-7074", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8439", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3080", "mrqa_newsqa-validation-1402", "mrqa_searchqa-validation-3012"], "SR": 0.625, "CSR": 0.5229048295454546, "EFR": 0.8333333333333334, "Overall": 0.6740601325757576}, {"timecode": 88, "before_eval_results": {"predictions": ["2013", "Triple Alliance of Germany, Austria - Hungary, and Italy", "John von Neumann", "Joanne Wheatley", "Alicia Vikander", "list of island countries", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "Mickey Rourke", "4.37 light - years ( 1.34 pc )", "eight hours ( UTC \u2212 08 : 00 )", "John Joseph Patrick Ryan", "12 to 36 months old", "when boy meets girl", "Malayalam", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "the Old Testament", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "Kryptonite", "the Fly Girls", "a crust of mashed potato", "in Christian eschatology", "a hydrolysis reaction", "the employer", "an end - user or audience", "1973", "fresh nuclear fuel", "Nancy Jean Cartwright", "Germany", "total cost", "Justin Timberlake", "1978", "geologist Charles Lyell", "1956", "Symphony No. 40 in G minor", "Thomas Middleditch", "Taittiriya Samhita", "Fix You", "the second team ( the first AFL / AFC team ) to win back - to - back championships", "during initial entry training", "New England ( Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "Andy Kim", "2003", "Kirsten Simone Vangsness", "Ludacris", "vehicles inspired by theJeep that are suitable for use on rough terrain", "tissues in the vicinity of the nose", "Yuzuru Hanyu", "Felicity Huffman", "air superiority", "1974", "National Industrial Recovery Act", "Mercury", "Bob Marley & the Wailers", "varnish", "model", "between the 8th and 16th centuries", "Blue Origin", "2,700-acre", "to provide security as needed.\"", "forcibly injecting them with psychotropic drugs", "Summit", "words", "Atchison", "The elections are slated for Saturday."], "metric_results": {"EM": 0.484375, "QA-F1": 0.6084567432579198}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.2, 1.0, 1.0, 0.5714285714285715, 0.0, 0.48275862068965514, 1.0, 0.7499999999999999, 0.6, 1.0, 0.5714285714285715, 0.0, 1.0, 0.5245901639344263, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-8528", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2272", "mrqa_naturalquestions-validation-954", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-5026", "mrqa_newsqa-validation-2756", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-11445", "mrqa_newsqa-validation-1133"], "SR": 0.484375, "CSR": 0.5224719101123596, "EFR": 0.9696969696969697, "Overall": 0.7012462759618658}, {"timecode": 89, "before_eval_results": {"predictions": ["Paul Lynde", "the naos", "1902", "420", "the fourth ventricle", "Emmett Lathrop `` Doc '' Brown", "Steve Russell", "if the occurrence of one does not affect the probability of occurrence of the other", "full '' sexual intercourse", "the Archies", "the government - owned Panama Canal Authority", "to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "a Native American nation from the Great Plains", "Jay Baruchel", "Plank", "the 1890s", "in 1651", "the red bone marrow of large bones", "Sarah Silverman", "Janie Crawford", "in 1958", "3", "supervillains who pose catastrophic challenges to the world", "in 1932", "March 15, 1945", "reproductive", "the NFL", "Biotic -- Biotic resources are obtained from the biosphere ( living and organic material ), such as forests and animals, and the materials that can be obtained from them", "1975", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "March 1995", "Austin, Texas", "Muhammad", "July 21, 1861", "the Deathly Hallows", "1984 Summer Olympics in Los Angeles", "the `` 0 '' trunk code", "David Joseph Madden", "December 12, 2017", "8 December 1985", "British Indian Association", "Internal epithelia", "on the microscope's stage", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "uvea", "he was unable to wrest", "1971", "Moton Field, the Tuskegee Army Air Field", "1995 Dodge Stealth", "Beijing", "pickup trucks", "Nowhere Boy", "The Cavern Club", "kidney", "Luigi Segre", "Ronald Lyle \" Ron\" Goldman", "Adrian Lyne", "543", "from Geraldine Ferraro to Bill Clinton.", "stay on track and get me through prison,\"", "tanks", "the Potomac", "Horn", "Basketball Page 30"], "metric_results": {"EM": 0.640625, "QA-F1": 0.756677731990232}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.923076923076923, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.07999999999999999, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.4444444444444444, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-699", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-2282", "mrqa_triviaqa-validation-519", "mrqa_hotpotqa-validation-2410", "mrqa_newsqa-validation-2027", "mrqa_searchqa-validation-1282", "mrqa_searchqa-validation-2590", "mrqa_triviaqa-validation-7302"], "SR": 0.640625, "CSR": 0.5237847222222223, "EFR": 0.9565217391304348, "Overall": 0.6988737922705315}, {"timecode": 90, "UKR": 0.673828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3812", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8093", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.837890625, "KG": 0.52265625, "before_eval_results": {"predictions": ["Viking", "Tiananmen", "Colonel Sebastian Moran", "December 7, 1941", "frauds", "Chrysler", "the Buddha", "Real Madrid", "York", "Norman Hartnell", "A Beautiful Mind", "UK Butterflies", "The Entertainment Capital of the World", "\u221223.5\u00b0", "Verona", "Ishmael", "Christmas in Hollis", "macbeth", "throw Darts", "physics", "Poland", "Rapa Nui", "Peter Sellers", "fever", "Milton Keynes", "Kuiper belt", "1954", "China", "wimpole", "fishes", "Independence Day", "English", "Keane", "Nicolas Sarkozy", "Harry Potter", "mercury", "Jack Ruby", "fat", "website", "Helen Gurley Brown", "New Zealand", "Groucho Marx", "Exile", "1664", "Shanghai", "Stieg Larsson", "five", "Manitoba", "Priam", "Denise van Outen", "argument form", "775", "nasal septum", "Puerto Rico", "Dealey Plaza", "four months in jail", "December 25, 2009", "L'Aquila earthquake,", "Janet and La Toya", "he and Armento, 51, were drinking at a strip club when they decided to go hunt for valium.", "2C", "rain", "I Will Remember You", "the Bavarian Alps"], "metric_results": {"EM": 0.59375, "QA-F1": 0.653125}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.8, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2675", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-3167", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-2755", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-5187", "mrqa_triviaqa-validation-7317", "mrqa_triviaqa-validation-2576", "mrqa_triviaqa-validation-7092", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-4760", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-3004", "mrqa_naturalquestions-validation-9574", "mrqa_hotpotqa-validation-4221", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-2839", "mrqa_searchqa-validation-14485"], "SR": 0.59375, "CSR": 0.5245535714285714, "EFR": 0.9615384615384616, "Overall": 0.7040934065934066}, {"timecode": 91, "before_eval_results": {"predictions": ["diddle", "lusitania", "Bild Newspaper", "japan", "blind side", "germany", "imola", "fifty-six", "Herald of Free Enterprise", "bridge", "norway", "Leicester", "joe moran", "gilt bronze", "yellow", "(Burkina) Faso", "mortadella", "Wembley", "phil archer", "The Telegraph", "palladium", "leander", "militia", "lite", "dorset", "1825", "Mussolini", "Paris", "wildeve", "ophthalmologist", "Donald Trump", "ruritania", "cardinal", "lord nelson", "lily Allen", "germany", "Blofeld", "mozart", "head", "Cardiff", "one hundred", "alan", "de goya", "brawn", "zipporah", "carousel", "Michael Hordern", "mary poppins", "quatermass experiment", "ben Disraeli", "a cappella", "M\u00f6ssbauer spectrometer", "in the case of disputes between two or more states", "2014", "Cielos del Sur S.A.", "tragedy", "Eastern College Athletic Conference", "in a CT scan", "\"The Cycle of Life,\"", "July 23.", "tuna", "december", "mercury", "The 2010 eruptions of Eyjafjallaj\u00f6kull"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5774305555555554}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-4425", "mrqa_triviaqa-validation-6622", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-3194", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-4458", "mrqa_naturalquestions-validation-3828", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-2585", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2013", "mrqa_searchqa-validation-4498", "mrqa_searchqa-validation-6831", "mrqa_naturalquestions-validation-5300"], "SR": 0.546875, "CSR": 0.5247961956521738, "EFR": 1.0, "Overall": 0.7118342391304348}, {"timecode": 92, "before_eval_results": {"predictions": ["maarten tromp", "indonesia", "bulgaria", "Arizona Diamondbacks", "pilot of the future", "rudolph", "fat", "Singapore", "bird", "stanley mary cadden", "Hebrew", "heisenberg", "carlsberg", "cumberland", "Billy Connolly", "spanish", "kiel Canal", "australia", "faggots", "Madison Square Garden", "Jeffery deaver", "John Flamsteed", "woven silk pyjamas", "croquet", "kinks", "spearchucker", "reservoirs", "Botticelli", "a giraffe", "arthur", "Donatello", "south-West Africa", "wales", "Patsy Cline", "pet sounds", "stanley", "haute", "archers", "seine", "Parsley the Lion", "kiki", "sir John Major", "sheep", "Siberia", "oscar", "poirot", "three", "ciolis effect", "Harley", "barra Streisand", "kipps: The Story of a Simple Soul", "in the 2001 -- 2002 season", "the NFL", "Identity Theory", "Melbourne Storm", "A Rush of Blood to the Head", "1993", "1994", "Derek Mears", "11", "a span", "Lake Titicaca", "12Ft", "Austin, Texas,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6248511904761904}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4506", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-3842", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-6743", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-525", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-4934", "mrqa_triviaqa-validation-1601", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-1507", "mrqa_newsqa-validation-406", "mrqa_searchqa-validation-4717", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-6744", "mrqa_newsqa-validation-3021"], "SR": 0.546875, "CSR": 0.5250336021505376, "EFR": 0.896551724137931, "Overall": 0.6911920652576937}, {"timecode": 93, "before_eval_results": {"predictions": ["star", "h Hercules", "spain", "japan", "james gernner", "philadelphia houlihan", "flower", "kerry kitten", "dillie lisette", "sinus node", "red", "170", "Dutch", "indonesia", "spain", "giambologna", "jocelyn houlihan", "gluteal region", "majorca", "12", "carry on leo", "Alexander Borodin", "hector bERLIOZ", "king arthur", "spain", "st aidan", "john virgo", "richard seddon", "moles", "stiefbeen", "Prince Edward", "duke of parma", "cryonics", "human", "takifugu rubripes", "sodor", "the Porteous Riots", "willie nelson", "giambologna", "giambologna", "giambologna", "jupiter", "vinnie Barbarino", "germany", "dame judi dench", "austria", "Essex Eagles", "kiser Chiefs", "philistine", "nicolas cage", "ralph Vaughan Williams", "an active supporter of the League of Nations", "April 1917", "Saint Alphonsa", "Takura Tendayi", "Whitesnake", "310", "Robert Barnett,", "voice-assistant software", "The switch had been scheduled for February 17,", "an orchid", "Aswan", "lira", "Joanna Moskawa"], "metric_results": {"EM": 0.5, "QA-F1": 0.5731534090909091}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2634", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-6873", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4135", "mrqa_triviaqa-validation-5975", "mrqa_triviaqa-validation-4323", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-6771", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-389", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-3276", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-1424", "mrqa_searchqa-validation-14194"], "SR": 0.5, "CSR": 0.5247672872340425, "EFR": 0.96875, "Overall": 0.7055784574468085}, {"timecode": 94, "before_eval_results": {"predictions": ["a score", "Frida Khalo", "the Kite Runner", "Pope John Paul II", "Louisa May Alcott", "Rock Island", "Turandot", "the Bolsheviks", "cloning", "Signs", "Edward", "forgery", "the Police", "a carrots", "Manhattan", "Rehab", "a ballpoint pen", "tap", "Ernie Banks", "Christopher Columbus", "Olivia Newton-John", "the Great White Way", "shrewd", "Virginia Andrews", "Peter Shaffer", "the Ubangi River", "(William) Harvey", "reptile", "a gizzard", "Bangkok", "the Reform Party", "Catwoman", "bats", "(Giacomo) Puccini", "Omaha", "the Monitor", "magnesium oxide", "silver", "the second pilot episode of the science fiction television series Star Trek", "Takana", "the Silk Road", "dreams", "Google", "Jack Ruby", "Hairspray", "James VI", "a palace", "a blood type O", "Italy", "green", "Engelbert Humperdinck", "the 1930s", "three", "April 16, 2014", "Velvet Revolution", "taka", "Hampton Court Palace", "Marvel Comics", "three or more separate periods", "Donald Wayne Johnson", "Marcell Jansen", "At least 88 people had been hurt,", "African-Americans", "the Atlas ICBM"], "metric_results": {"EM": 0.703125, "QA-F1": 0.74296875}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9550", "mrqa_searchqa-validation-10265", "mrqa_searchqa-validation-5390", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-14368", "mrqa_searchqa-validation-12939", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-5868", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-4594", "mrqa_hotpotqa-validation-4578", "mrqa_newsqa-validation-2068", "mrqa_hotpotqa-validation-1350"], "SR": 0.703125, "CSR": 0.5266447368421052, "EFR": 1.0, "Overall": 0.7122039473684211}, {"timecode": 95, "before_eval_results": {"predictions": ["Clifford Roberts", "Rikki-Tavi", "Brooklyn", "MEXICO", "Peter Paul Rubens", "Tom Parker", "Belgium", "Zen", "LDL", "a biblia", "the Miami Dolphins", "the MIM-104 Patriot", "a knife", "Maria Cross", "Northern Exposure", "Pocahontas", "Easy Rider", "the East River", "milk", "Ned Kelly", "Jakarta", "Cherokee", "Jim Bunning", "brood", "Kennedy", "Arby\\'s", "Albert Einstein", "milk", "VICTor HUGO", "fudge", "a", "a cattle prod", "Henry Bessemer", "stimulation", "egg", "Ken Russell", "\"The Crucible", "the United Healthcare Workers East", "the zenith", "apogee", "Calais", "semaphore", "a reverse", "Coors Field", "Edgar Rice", "\"The Silver State\"", "the Pygmies of central Africa", "a philosopher", "2W", "\"The Postman Always Rings Twice\"", "Kansas City", "1979", "Mockingjay -- Part 2", "Massachusetts", "Springfield", "Uruguay", "Vader", "George Adamski", "five", "Marine Corps", "two", "to add a \"black box\" label warning", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "black"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5984375}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3709", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-4090", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16295", "mrqa_searchqa-validation-10226", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-6979", "mrqa_searchqa-validation-664", "mrqa_searchqa-validation-4716", "mrqa_searchqa-validation-8919", "mrqa_searchqa-validation-7412", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-9198", "mrqa_searchqa-validation-3964", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-7194", "mrqa_naturalquestions-validation-1427", "mrqa_triviaqa-validation-3765", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-3778", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3198"], "SR": 0.515625, "CSR": 0.5265299479166667, "EFR": 1.0, "Overall": 0.7121809895833333}, {"timecode": 96, "before_eval_results": {"predictions": ["Flickr", "Eric Angat", "air", "Leontyne Price", "a dragonfly", "King Charles I", "Casey Kasem", "Puerto Rico", "sheep", "The witches of Eastwick", "(Buffalo) Bill Cody", "(Shorn) Connery", "May", "aOreo", "Cyrano de Bergerac", "Alaska", "birds", "the European Union", "Verdi", "Matt Lauer", "the Kremlin", "\"How the firebrand Shi'ite cleric became a major power broker in the new Iraq\"", "Frogs", "heracles", "a Clerk", "Wales", "\"Austin City limits\"", "the Sacred Cod", "Agatha Christie", "get your house back", "Esther", "cat scratch fever", "New Kids on the Block", "the Tigris river", "country", "the Salem witch trials", "Lincoln", "center of gravity", "morocco", "Simon Cowell", "fiber", "Lenin", "a fruitcake", "nests", "the Firebird", "Kansas", "a radical", "Air France", "Louis Brandeis", "a plaque", "David", "the 1979 -- 80 season", "Edward Keeling", "a quarterback", "Dick Whittington", "leeds", "gloster", "1828", "Elijah Wood", "Dizzy Dean", "The Ski Train", "Israel", "a rapist", "skirts"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6760416666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-3073", "mrqa_searchqa-validation-14494", "mrqa_searchqa-validation-13498", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-862", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4540", "mrqa_searchqa-validation-79", "mrqa_searchqa-validation-5250", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9745", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-6912", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-3093", "mrqa_hotpotqa-validation-4621", "mrqa_newsqa-validation-1176"], "SR": 0.609375, "CSR": 0.5273840206185567, "EFR": 1.0, "Overall": 0.7123518041237114}, {"timecode": 97, "before_eval_results": {"predictions": ["The All-New Blue Ribbon Cookbook", "a pig", "Fear of Flying", "War Admiral", "Abraham Lincoln", "the A horizon", "McDuck", "Czechoslovakia", "Roussimoff", "coloratura", "Buddhism", "Roosevelt", "a flash", "Cold Mountain", "horror", "a push", "A Night at the Roxbury", "King Henry II", "the Claddagh Ring", "Keith Richards", "the Hydra", "(Margaret) King", "the Bronx Zoo", "Marcia Clark", "the Lincoln Tunnel", "anbatross", "Bob Fosse", "an assertion", "Georgia", "Nixon", "Madame Tussaud", "\"Cloverfield\"", "Shakespeare", "Mother Jones", "(Norman) Fell", "Hatfield and McCoy", "Walter Scott", "Pig Latin", "the Nile", "air traffic", "a mutton", "Latin", "new wave", "Patrick Ewing", "Vienna", "Darwin", "New Orleans", "Parody", "broccoli", "arteries", "Carol Burnett", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "opinion in a legal case in certain legal systems", "9 February 2018", "18", "Jeremy Thorpe", "blue", "Cersei Lannister", "British Labour Party", "Balvenie Castle", "Animal Planet", "St. Louis, Missouri.", "The strawberry,\"", "ITV"], "metric_results": {"EM": 0.5, "QA-F1": 0.6152777777777778}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.16666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-15628", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-16083", "mrqa_searchqa-validation-5057", "mrqa_searchqa-validation-3194", "mrqa_searchqa-validation-13412", "mrqa_searchqa-validation-15943", "mrqa_searchqa-validation-16528", "mrqa_searchqa-validation-11879", "mrqa_searchqa-validation-8337", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6243", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-4255", "mrqa_searchqa-validation-16590", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-8347", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-2490", "mrqa_hotpotqa-validation-3634"], "SR": 0.5, "CSR": 0.5271045918367347, "EFR": 1.0, "Overall": 0.712295918367347}, {"timecode": 98, "before_eval_results": {"predictions": ["Franklin, Indiana", "Nelson County", "47", "Sir Hiram Stevens Maxim", "15 October 1988", "5 February 1976", "Boston Celtics", "Hermione Youlanda Ruby Clinton-Baddeley", "45,698", "Andries Jonker", "Ashanti Region of Ghana", "Groupe PSA", "Texas Tech", "brigadier general", "Omega SA", "South Australia", "Apatosaurus", "Resorts World Genting", "the Beatles", "British", "1950", "six", "Future", "London", "Cuyler Reynolds", "The original News Corporation or News Corp.", "Toxics Release Inventory", "Figaro", "South African", "\"Apprendi v. New Jersey\" (2000)", "Bambi: Eine Lebensgeschichte aus dem Walde", "close to 50 million", "Whoopi Goldberg", "Transporter 3", "Disco", "Duke Frederick", "Afghanistan", "Thriller", "Antonio Lippi", "March 30, 2025", "English", "Azeroth", "Isabella II", "McG", "Vitor Belfort", "11 November 1783", "villanelle", "Emilia-Romagna", "Who\\'s That Girl: Original Motion Picture Soundtrack", "Alan Young", "Cristiano Ronaldo", "Virginia Dare", "David Tennant", "inefficient", "Simeon Williamson", "baffin", "Moffitt", "Alwin Landry's", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Lashkar-e-Jhangvi,", "a hoist", "bananas", "Tallahassee", "Brooke Wexler"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7128472222222222}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 0.6, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-4963", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1669", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-3578", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2364", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6842", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2498"], "SR": 0.59375, "CSR": 0.5277777777777778, "EFR": 1.0, "Overall": 0.7124305555555555}, {"timecode": 99, "UKR": 0.6796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5687", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-1037", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-1440", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15285", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15680", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3057", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4454", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1734", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2791", "mrqa_triviaqa-validation-2844", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.818359375, "KG": 0.5046875, "before_eval_results": {"predictions": ["Bob Mould", "1926", "Antonio Lippi", "the 1993 election", "The Allies of World War I", "Logan International Airport", "1979", "an Academy Award in the category Best Sound", "The Suite Life of Zack & Cody", "Switzerland", "2017", "The 107 m hall containing Tropical Islands and the 161 m steam generator at Schwarze Pumpe power station", "Pakistan", "science fiction drama", "Marigold Newey", "Darkroom", "1972", "Royce da 5'9\" (Bad) and Eminem (Evil)", "evangelical Christian periodical", "Lionel Eugene Hollins", "water", "Harlem neighborhood", "Bardot", "Love Actually", "the Commanding General of the United States Army", "George Orwell", "five", "20 March to 1 May 2003", "1993", "imp My Ride", "1886", "Switzerland\u2013European Union relations", "Syracuse", "Godspell", "1755", "The Big Bang Theory", "1966", "Johnny Herbert", "Annales de chimie et de physique", "British fantasy and science fiction", "punk rock", "the Atlantic Ocean", "Adelaide Laetitia \" Addie\" Miethke", "Red", "Theodore Robert Bundy", "Matt Kemp", "Epic Records", "Roslyn Castle", "2.1 million members", "Rothschild banking dynasty", "Corendon Airlines", "Claudia Grace Wells", "Hugo Weaving", "South Africa", "baffles", "henry hudson", "calculus", "Adidas", "whether to close some entrances, bring in additional officers, and make security more visible.", "the 3rd Platoon, A Company, 2nd Light Tactical Reconnaissance Battalion,", "Syria", "cream cheese", "the New Year's Day", "bullnose"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6991319444444444}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true], "QA-F1": [0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-5238", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-2940", "mrqa_hotpotqa-validation-1711", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-3154", "mrqa_hotpotqa-validation-1822", "mrqa_hotpotqa-validation-735", "mrqa_triviaqa-validation-1641", "mrqa_triviaqa-validation-5283", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1859", "mrqa_searchqa-validation-14467", "mrqa_searchqa-validation-14664"], "SR": 0.578125, "CSR": 0.52828125, "EFR": 1.0, "Overall": 0.706203125}]}