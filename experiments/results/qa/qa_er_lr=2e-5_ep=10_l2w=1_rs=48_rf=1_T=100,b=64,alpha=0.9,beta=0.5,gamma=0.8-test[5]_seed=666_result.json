{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=2e-5_ep=10_l2w=1_rs=48_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=666', diff_loss_weight=1.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=2e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=2e-5_ep=10_l2w=1_rs=48_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=666/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=1, replay_size=48, save_ckpt_freq=25, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=8, result_file='experiments/results/qa/qa_er_lr=2e-5_ep=10_l2w=1_rs=48_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=666_result.json', stream_id=5, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 9650, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a plug valve", "1550", "French Louisiana west of the Mississippi River", "2012", "carbon dioxide", "the Lisbon Treaty", "all colors", "in the chloroplasts of C4 plants", "An attorney", "democracy", "The Greens", "third", "Enthusiastic teachers", "expositions", "no French regular army troops were stationed in North America", "estates of the Holy Roman Empire", "Stromatoveris", "2011", "Louis Pasteur", "the owner", "Time Lord", "mosaic floors", "economic", "1893", "environmental factors like light color and intensity", "Gandhi", "deforestation", "Middle Rhine Valley", "pump this into the mesoglea", "low-light conditions", "No Child Left Behind", "one way streets", "\u20ac25,000 per year", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "unbalanced torque", "Ulaanbaatar", "power", "very weak", "Judith Merril", "Gender pay gap", "the Ilkhanate", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "University of Chicago campus", "3D printing technology", "1957", "2000", "a certain number of teacher's salaries are paid by the State", "the Dutch Republic", "San Jose Marriott", "April 20", "the Commission", "evacuate the cylinder", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "Hurricane Beryl", "temperature and light", "terra nullius", "growth", "human", "the \u2018combs\u2019", "1978", "non-Catholics", "Sanders", "vice president", "700 employees"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8576388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10143", "mrqa_squad-validation-8841", "mrqa_squad-validation-2145", "mrqa_squad-validation-739", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-7449", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-9764", "mrqa_squad-validation-7051"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["1986", "Peridinin", "standardized", "50% to 60%", "Stromatoveris", "lower incomes", "Fort Duquesne", "Katharina von Bora", "Miller", "women", "The Love Boat", "Frank Marx", "architect or engineer", "$2 million", "superintendent of New York City schools", "Santa Clara, California", "Kingdom of Prussia", "the same league as the Asian Economic Tigers", "Palestine", "Aristotle and Archimedes", "in the chloroplasts of C4 plants", "Outlaws", "increased blood flow into tissue", "Edgar Scherick", "14th to the 19th century", "Gibraltar and the \u00c5land islands", "Evangelical Lutheran Church", "oxygen", "the BBC National Orchestra of Wales", "Thanksgiving", "the founding of new Protestant churches", "it is impossible to determine what the acceleration of the rope will be", "Venus", "those who proceed to secondary school or vocational training", "zoning and building code requirements", "Ikh Zasag", "Central Bridge", "the Holy Roman Empire, the Duchy of Prussia, the Channel Islands, and Ireland", "William Tyndale", "1935", "seven", "Grumman", "1191", "Maciot de Bethencourt", "Euclid", "case law by the Court of Justice", "long, slender tentacles", "mesoglea", "1970s", "white", "misguided", "2014", "Reconstruction and the Gilded Age", "European Parliament and the Council of the European Union", "members", "Manakin Episcopal Church", "Nicholas Stone", "ongoing tectonic subsidence", "the release of her eponymous debut album the following year", "a form of business network", "The Senate ( north ) wing was completed in 1800", "The euro is the result of the European Union's project for economic and monetary union", "Djokovic went on to win his fifth Australian Open title", "\"colorful\" mercenary group fought for Padua, Florence & other Italian city-states"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8236599338161839}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2, 0.15384615384615385, 0.18181818181818182, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7332", "mrqa_squad-validation-6031", "mrqa_squad-validation-8459", "mrqa_squad-validation-10339", "mrqa_squad-validation-10321", "mrqa_squad-validation-3021", "mrqa_squad-validation-3946", "mrqa_squad-validation-5588", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-7062", "mrqa_searchqa-validation-2579"], "SR": 0.796875, "CSR": 0.8125, "retrieved_ids": ["mrqa_squad-train-27406", "mrqa_squad-train-28265", "mrqa_squad-train-26566", "mrqa_squad-train-38806", "mrqa_squad-train-66939", "mrqa_squad-train-32793", "mrqa_squad-train-86581", "mrqa_squad-train-80109", "mrqa_squad-train-40542", "mrqa_squad-train-69205", "mrqa_squad-train-64524", "mrqa_squad-train-35510", "mrqa_squad-train-84812", "mrqa_squad-train-71879", "mrqa_squad-train-2369", "mrqa_squad-train-82413", "mrqa_squad-train-50472", "mrqa_squad-train-75947", "mrqa_squad-train-39312", "mrqa_squad-train-29506", "mrqa_squad-train-53848", "mrqa_squad-train-38670", "mrqa_squad-train-61581", "mrqa_squad-train-12541", "mrqa_squad-validation-9764", "mrqa_squad-validation-7449", "mrqa_squad-validation-4452", "mrqa_squad-validation-739", "mrqa_squad-validation-8841", "mrqa_squad-validation-7364", "mrqa_squad-validation-2145", "mrqa_squad-validation-7051", "mrqa_squad-validation-10143", "mrqa_squad-validation-3019", "mrqa_squad-validation-9173"], "EFR": 1.0, "Overall": 0.90625}, {"timecode": 2, "before_eval_results": {"predictions": ["Lek", "prohibited emigration", "The Private Education Student Financial Assistance", "highly-paid", "Labor", "mathematical models of computation", "special efforts", "rhetoric", "the British", "a year", "Genghis Khan", "supervisory church body", "77", "a cubic interpolation formula", "King Sigismund III Vasa", "1835", "the exploitation of the valuable assets and supplies of the nation that was conquered", "poor management, internal divisions, and effective Canadian scouts", "five", "liquid oxygen", "Gosforth Park", "Metropolitan Police Authority", "18 February 1546", "2005", "1.7 billion years ago", "Mike Carey", "coal", "18 February", "Stanford University", "1976", "LOVE Radio", "ambiguity", "Khasar", "Sky Digital", "99.4", "about a third", "the issue of laity having a voice and vote in the administration of the church", "1995", "Endosymbiotic gene transfer", "avionics, telecommunications, and computers", "linebacker", "feed water", "Sir Edward Poynter", "oxygen", "August 1967", "Velamen parallelum", "terrorist organisation", "three", "Lowry Digital", "worst-case time complexity", "2010", "33", "Buffalo Lookout", "the term originated in Missouri, during the Kirtland period of Latter Day Saint history, circa 1834", "The User State Migration Tool", "1773", "Onsan illness", "October 6, 2017", "11 p.m. to 3 a.m", "Haliaeetus", "Cetshwayo", "Calpurnia's son", "weekend", "Tom Hanks"], "metric_results": {"EM": 0.875, "QA-F1": 0.8924632352941176}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7357", "mrqa_squad-validation-7612", "mrqa_squad-validation-6171", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-430", "mrqa_newsqa-validation-1080"], "SR": 0.875, "CSR": 0.8333333333333334, "retrieved_ids": ["mrqa_squad-train-18429", "mrqa_squad-train-26657", "mrqa_squad-train-9638", "mrqa_squad-train-47462", "mrqa_squad-train-42639", "mrqa_squad-train-25326", "mrqa_squad-train-62762", "mrqa_squad-train-50759", "mrqa_squad-train-69709", "mrqa_squad-train-60118", "mrqa_squad-train-59982", "mrqa_squad-train-14690", "mrqa_squad-train-9314", "mrqa_squad-train-79338", "mrqa_squad-train-68141", "mrqa_squad-train-53028", "mrqa_squad-train-83272", "mrqa_squad-train-5322", "mrqa_squad-train-67103", "mrqa_squad-train-34496", "mrqa_squad-train-56488", "mrqa_squad-train-66255", "mrqa_squad-train-17465", "mrqa_squad-train-38859", "mrqa_squad-validation-3019", "mrqa_squad-validation-3021", "mrqa_squad-validation-7364", "mrqa_squad-validation-8841", "mrqa_naturalquestions-validation-1912", "mrqa_squad-validation-10143", "mrqa_naturalquestions-validation-7062", "mrqa_squad-validation-10321", "mrqa_squad-validation-5588", "mrqa_squad-validation-7332", "mrqa_squad-validation-9764", "mrqa_squad-validation-3946", "mrqa_squad-validation-4452", "mrqa_squad-validation-8459", "mrqa_searchqa-validation-2579", "mrqa_squad-validation-9173", "mrqa_squad-validation-10339", "mrqa_squad-validation-739", "mrqa_naturalquestions-validation-191", "mrqa_squad-validation-2145", "mrqa_squad-validation-7051", "mrqa_squad-validation-6031", "mrqa_naturalquestions-validation-1187", "mrqa_squad-validation-7449"], "EFR": 1.0, "Overall": 0.9166666666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["female", "1884", "Sayyid Abul Ala Maududi", "a family member", "James E. Webb", "Lutheran and Reformed", "the phycoerytherin pigment", "can also concentrate wealth, pass environmental costs on to society, and abuse workers and consumers", "swimming-plates", "10 July 1856", "130 million cubic foot", "teleforce", "Heinrich Himmler", "34\u201319", "Baptism", "Decision problems", "customs", "1953", "The Day of the Doctor", "Muhammad Khan", "Sun Life Stadium", "the Council", "February 9, 1953", "March", "sea gooseberry", "1961", "Trio Tribe", "Dai Setsen", "the Late Medieval Catholic Church", "January 1979", "phagocytic", "Rankine cycle", "$2.2 billion", "Seine", "theory of general relativity", "15 February 1546", "Marquis de la Jonqui\u00e8re", "BBC Dead Ringers", "Kenyans for Kenya", "Fresno", "Saudi", "the Presiding Officer", "an intuitive understanding", "default emission factors", "Inherited wealth", "Michael P. Millardi", "Goldman Sachs", "the BRAAVOO website", "the world's catalog of ideas", "Great faces, great places", "praying", "the children were nestled all snug in their beds", "the U.N. organization raised the temples of Abu Simbel up out of the way of flooding caused when the Aswan High Dam was built", "the Leyden jar", "\"socialism\"", "the last two of these had libretti by Gaetano Rossi,", "Droll Stories: Collected from the Abbeys of Touraine", "70%", "the first Macy's Thanksgiving Day Parade", "making* this First Annual Clearing Sale one that will long be.... Bed Spreads, very large size, fine", "the British", "early 1960s", "April 1917", "due to the close quarters and poor hygiene exhibited at that time Athens became a breeding ground for disease"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6774147727272728}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.4, 1.0, 0.45454545454545453]}}, "before_error_ids": ["mrqa_squad-validation-8595", "mrqa_squad-validation-7525", "mrqa_squad-validation-2595", "mrqa_squad-validation-6072", "mrqa_squad-validation-5860", "mrqa_squad-validation-5262", "mrqa_squad-validation-3863", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-10156"], "SR": 0.640625, "CSR": 0.78515625, "retrieved_ids": ["mrqa_squad-train-81122", "mrqa_squad-train-68011", "mrqa_squad-train-86147", "mrqa_squad-train-50322", "mrqa_squad-train-850", "mrqa_squad-train-47383", "mrqa_squad-train-32793", "mrqa_squad-train-31482", "mrqa_squad-train-76272", "mrqa_squad-train-14179", "mrqa_squad-train-63405", "mrqa_squad-train-48895", "mrqa_squad-train-37191", "mrqa_squad-train-54628", "mrqa_squad-train-16595", "mrqa_squad-train-75886", "mrqa_squad-train-56542", "mrqa_squad-train-33682", "mrqa_squad-train-55942", "mrqa_squad-train-64576", "mrqa_squad-train-9123", "mrqa_squad-train-7221", "mrqa_squad-train-15341", "mrqa_squad-train-78856", "mrqa_naturalquestions-validation-7062", "mrqa_squad-validation-2145", "mrqa_squad-validation-7357", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-7051", "mrqa_naturalquestions-validation-9453", "mrqa_squad-validation-8459", "mrqa_squad-validation-10143", "mrqa_squad-validation-9764", "mrqa_squad-validation-6031", "mrqa_squad-validation-6171", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-1912", "mrqa_squad-validation-739", "mrqa_squad-validation-9173", "mrqa_squad-validation-10339", "mrqa_squad-validation-7449", "mrqa_squad-validation-8841", "mrqa_naturalquestions-validation-430", "mrqa_squad-validation-3946", "mrqa_naturalquestions-validation-1187", "mrqa_squad-validation-7332", "mrqa_squad-validation-7612", "mrqa_searchqa-validation-2579"], "EFR": 0.8695652173913043, "Overall": 0.8273607336956521}, {"timecode": 4, "before_eval_results": {"predictions": ["boom-and-bust cycles", "The Prince of P\u0142ock", "hormones", "1840", "occupational stress", "in the parts of the internal canal network under the comb rows", "separating faith and reason", "Tesla Electric Company", "African-American", "Thomson", "1905", "Nun komm, der Heiden Heiland", "John Fox", "all health care settings", "cut in half", "the study of rocks", "home ports", "lower wages", "geophysical surveys", "Huguenots", "their actual social power and wealth", "2,900 kilometres", "Elie Metchnikoff", "an algorithm", "Immediately after Decision Time", "Confucian propriety and ancestor veneration", "between 25-minute episodes", "eight", "elude host immune responses", "Pusey Library", "inequality", "designs", "cytokines", "requiring his arrest", "wide sidewalks", "other members", "Air Force", "an occupancy permit", "reactive allotrope of oxygen", "Nederrijn at Angeren", "multi-cultural", "pump", "Zeebo", "Australia", "a judicial officer", "computer based models", "Henry Purcell", "Ram Nath Kovind", "embryo", "Cheap trick", "Sandy Knox and Billy Stritch", "Hudson Bay", "Etienne de Mestre", "bow bridge", "1991", "Nicole Gale Anderson", "1", "sedimentary", "Mrs. Wolowitz", "plate tectonics", "Medellin", "Yolande of Brienne", "Kris Allen", "Fernando Gaitn"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7276041666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2463", "mrqa_squad-validation-9928", "mrqa_squad-validation-7338", "mrqa_squad-validation-2943", "mrqa_squad-validation-8093", "mrqa_squad-validation-7708", "mrqa_squad-validation-6957", "mrqa_squad-validation-3497", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-4002", "mrqa_hotpotqa-validation-4815", "mrqa_searchqa-validation-172"], "SR": 0.6875, "CSR": 0.765625, "retrieved_ids": ["mrqa_squad-train-8994", "mrqa_squad-train-7850", "mrqa_squad-train-41311", "mrqa_squad-train-19802", "mrqa_squad-train-46026", "mrqa_squad-train-52467", "mrqa_squad-train-13932", "mrqa_squad-train-2399", "mrqa_squad-train-67147", "mrqa_squad-train-17230", "mrqa_squad-train-70009", "mrqa_squad-train-76719", "mrqa_squad-train-4895", "mrqa_squad-train-75655", "mrqa_squad-train-11895", "mrqa_squad-train-34266", "mrqa_squad-train-86545", "mrqa_squad-train-4082", "mrqa_squad-train-36123", "mrqa_squad-train-32079", "mrqa_squad-train-60827", "mrqa_squad-train-79047", "mrqa_squad-train-26638", "mrqa_squad-train-2070", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-4367", "mrqa_squad-validation-10143", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-2579", "mrqa_squad-validation-7364", "mrqa_squad-validation-6031", "mrqa_squad-validation-5860", "mrqa_squad-validation-7051", "mrqa_squad-validation-5588", "mrqa_squad-validation-4452", "mrqa_squad-validation-2595", "mrqa_squad-validation-6171", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-3021", "mrqa_squad-validation-10339", "mrqa_searchqa-validation-2568", "mrqa_squad-validation-9764", "mrqa_squad-validation-9173", "mrqa_searchqa-validation-15194", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-10156", "mrqa_searchqa-validation-8711"], "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 5, "before_eval_results": {"predictions": ["former flooded terraces", "beginning of the 20th century", "1974", "ABC", "dictatorial", "Ben Johnston", "quantum", "Book of Exodus", "Synthetic aperture radar", "Battlestar Galactica and Bionic Woman", "patients' prescriptions and patient safety issues", "\"No, that's no good\"", "1697", "3 January 1521", "magma", "a \"principal hostile country\"", "Jan Hus", "Newton", "Croatia", "2011", "Swynnerton Plan", "the machine gun", "Theatre Museum", "August 10, 1948", "they are distinct or equal classes", "the 2004 Treaty establishing a Constitution for Europe", "Serge Chermayeff", "Thomas Edison", "Mnemiopsis", "the flail of God", "Woodward Park", "The Melbourne Cricket Ground", "Wednesdays", "most common", "up to a thousand times as many", "tears and urine", "six", "plants and algae", "the Constitution of India", "1913", "Yuzuru Hanyu", "Konakuppakatil Gopinathan Balakrishnan", "1942", "March 2016", "Texas, Oklahoma, and the surrounding Great Plains", "a balance sheet", "Mayor Hudnut", "1995", "William the Conqueror", "1922", "an anembryonic gestation", "Bemis Heights", "9pm ET ( UTC - 5 )", "twice", "S\u00e9rgio Mendes", "Alaska", "Abraham", "a network connection device", "The euro", "\"beyond violet\"", "2000", "Rodong Sinmun", "a papermaking process that precedes the rolling of the paper", "Sierra Repertory Theatre"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6855779184583533}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3076923076923077, 0.5714285714285715, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5818", "mrqa_squad-validation-1827", "mrqa_squad-validation-1566", "mrqa_squad-validation-10388", "mrqa_squad-validation-7613", "mrqa_squad-validation-3770", "mrqa_squad-validation-1780", "mrqa_squad-validation-3985", "mrqa_squad-validation-4572", "mrqa_squad-validation-6439", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-2749", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15169"], "SR": 0.609375, "CSR": 0.7395833333333333, "retrieved_ids": ["mrqa_squad-train-67666", "mrqa_squad-train-50973", "mrqa_squad-train-23886", "mrqa_squad-train-56195", "mrqa_squad-train-71431", "mrqa_squad-train-25850", "mrqa_squad-train-67750", "mrqa_squad-train-74575", "mrqa_squad-train-83297", "mrqa_squad-train-82820", "mrqa_squad-train-48430", "mrqa_squad-train-76396", "mrqa_squad-train-80132", "mrqa_squad-train-54396", "mrqa_squad-train-40210", "mrqa_squad-train-59644", "mrqa_squad-train-41125", "mrqa_squad-train-33649", "mrqa_squad-train-48550", "mrqa_squad-train-10656", "mrqa_squad-train-51955", "mrqa_squad-train-12910", "mrqa_squad-train-7447", "mrqa_squad-train-11927", "mrqa_naturalquestions-validation-5780", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-8711", "mrqa_naturalquestions-validation-9453", "mrqa_squad-validation-7332", "mrqa_squad-validation-6072", "mrqa_naturalquestions-validation-1987", "mrqa_squad-validation-9764", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8116", "mrqa_squad-validation-5588", "mrqa_squad-validation-3019", "mrqa_searchqa-validation-2579", "mrqa_naturalquestions-validation-430", "mrqa_squad-validation-3863", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-2466", "mrqa_squad-validation-7051", "mrqa_squad-validation-5860", "mrqa_squad-validation-7449", "mrqa_squad-validation-739", "mrqa_squad-validation-7708", "mrqa_squad-validation-8595"], "EFR": 0.92, "Overall": 0.8297916666666667}, {"timecode": 6, "before_eval_results": {"predictions": ["four", "2 million", "53%", "Pliocene", "relationship between teachers and children", "LeGrande", "sixth", "Jason Bourne", "11.1%", "6000", "University of Chicago College Bowl Team", "decline of organized labor", "Santa Clara Marriott", "oxygen chambers", "two", "two catechisms", "Cologne, Germany", "1991", "Silk Road", "Surveyor 3", "145", "growth and investment", "In 1965, at the instigation of Warner Sinback, a data network based on this voice-phone network was designed to connect GE's four computer sales and service centers", "Vampire bats", "antiforms", "still be standing", "weight", "as \"Genghis Khan's Mongolia\"", "oil was priced in dollars, oil producers' real income decreased", "Beyonc\u00e9 and Bruno Mars", "a university or college", "More than 1 million", "pseudorandom number generators", "Japan", "Coriolis force", "Mickey Rourke", "May 2016", "Nicklaus", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Panama Canal Authority", "silk, hair / fur ( including wool ) and feathers", "France's Legislative Assembly", "two", "Sebastian Vettel", "April 10, 2018", "Gorakhpur", "How I Met Your Mother", "elected", "December 15, 2016", "Abraham Gottlob Werner", "Jourdan Miller", "1991", "Samantha Jo '' Mandy '' Moore", "Danish", "Broken Hill and Sydney", "159", "China (formerly the Republic of China ), Russia ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "Judiththia Aline Keppel", "Medellin", "Crown Holdings Incorporated", "Expedia", "the Large Orbiting Telescope or Large Space Telescope", "the mammoth's skull", "to step up"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7685641125179168}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19047619047619052, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.07692307692307693, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.782608695652174, 0.25, 0.25, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-6965", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_squad-validation-6228", "mrqa_squad-validation-3718", "mrqa_squad-validation-9161", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-6106", "mrqa_searchqa-validation-10372", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-429"], "SR": 0.671875, "CSR": 0.7299107142857143, "retrieved_ids": ["mrqa_squad-train-85883", "mrqa_squad-train-32498", "mrqa_squad-train-76000", "mrqa_squad-train-64071", "mrqa_squad-train-20493", "mrqa_squad-train-62283", "mrqa_squad-train-58099", "mrqa_squad-train-54523", "mrqa_squad-train-38632", "mrqa_squad-train-52733", "mrqa_squad-train-84395", "mrqa_squad-train-2450", "mrqa_squad-train-3239", "mrqa_squad-train-18539", "mrqa_squad-train-55899", "mrqa_squad-train-17593", "mrqa_squad-train-42933", "mrqa_squad-train-65662", "mrqa_squad-train-418", "mrqa_squad-train-31689", "mrqa_squad-train-2657", "mrqa_squad-train-9712", "mrqa_squad-train-83002", "mrqa_squad-train-47434", "mrqa_naturalquestions-validation-1912", "mrqa_squad-validation-7332", "mrqa_squad-validation-2463", "mrqa_squad-validation-2595", "mrqa_searchqa-validation-4367", "mrqa_squad-validation-3770", "mrqa_squad-validation-7457", "mrqa_squad-validation-7338", "mrqa_naturalquestions-validation-844", "mrqa_squad-validation-6031", "mrqa_squad-validation-739", "mrqa_naturalquestions-validation-7080", "mrqa_squad-validation-6439", "mrqa_naturalquestions-validation-6461", "mrqa_squad-validation-1780", "mrqa_squad-validation-3946", "mrqa_naturalquestions-validation-9240", "mrqa_squad-validation-7357", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-7449", "mrqa_squad-validation-8595", "mrqa_squad-validation-5860", "mrqa_naturalquestions-validation-1911"], "EFR": 0.9523809523809523, "Overall": 0.8411458333333333}, {"timecode": 7, "before_eval_results": {"predictions": ["Director", "travel literature, cartography, geography, and scientific education", "oxygen chambers", "Graham Gano", "Two", "In 1066", "2008", "Mojave Desert", "Operating System Principles", "St. Lawrence and Mississippi watersheds", "27", "4000", "Rhine Gorge", "helicoid stromal thylakoids", "highest", "impact process effects", "schools in some Asian, African and Caribbean countries", "Warner Bros. Presents", "pharmacists", "high-voltage", "4:51", "Kabaty Forest", "the seal of the Federal Communications Commission", "strong sedimentation", "The European Commission", "SAP Center", "respiration", "352", "eliminate the accusing law", "October 6, 2004", "\"The Day of the Doctor\"", "Pakistan", "November 1999", "September 6, 2019", "English", "the fourth season", "three", "Nick Kroll", "The Ranch", "Billy Gibbons", "an apprentice", "in the brain", "31", "1970s", "the U.S. State Department", "Art Carney", "accomplish the objectives of the organization", "the female spends extra time basking to keep her eggs warm", "December 1922", "Category 4", "September 2017", "3 September", "the southern USA", "Terrell Owens", "since 3, 1, and 4", "five", "Dolph Lundgren", "Hampton Court Palace", "Sela Ward", "her decades-long portrayal of Alice Horton", "the Universal Licensing System", "Benjamin Britten", "an isosceles triangle", "the NOW Magazine"], "metric_results": {"EM": 0.640625, "QA-F1": 0.674393315018315}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4629", "mrqa_squad-validation-1938", "mrqa_squad-validation-6409", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16130", "mrqa_triviaqa-validation-6548"], "SR": 0.640625, "CSR": 0.71875, "retrieved_ids": ["mrqa_squad-train-48429", "mrqa_squad-train-47323", "mrqa_squad-train-65186", "mrqa_squad-train-83134", "mrqa_squad-train-75250", "mrqa_squad-train-62886", "mrqa_squad-train-78707", "mrqa_squad-train-75630", "mrqa_squad-train-68391", "mrqa_squad-train-83867", "mrqa_squad-train-38427", "mrqa_squad-train-44802", "mrqa_squad-train-65529", "mrqa_squad-train-58569", "mrqa_squad-train-591", "mrqa_squad-train-82577", "mrqa_squad-train-2649", "mrqa_squad-train-86530", "mrqa_squad-train-22330", "mrqa_squad-train-58611", "mrqa_squad-train-21969", "mrqa_squad-train-61527", "mrqa_squad-train-65573", "mrqa_squad-train-67799", "mrqa_squad-validation-2943", "mrqa_squad-validation-4838", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-9753", "mrqa_triviaqa-validation-2749", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-9453", "mrqa_squad-validation-3985", "mrqa_squad-validation-1566", "mrqa_squad-validation-6228", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-7080", "mrqa_squad-validation-3021", "mrqa_squad-validation-9764", "mrqa_naturalquestions-validation-1987", "mrqa_searchqa-validation-15194", "mrqa_squad-validation-5860", "mrqa_naturalquestions-validation-1863", "mrqa_squad-validation-3998", "mrqa_squad-validation-7449", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-4674", "mrqa_squad-validation-6171"], "EFR": 0.9565217391304348, "Overall": 0.8376358695652174}, {"timecode": 8, "before_eval_results": {"predictions": ["cytokines", "William Pitt", "North Carolina and New Mexico", "p-adic norm", "Hassan al Banna", "Gottfried Fritschel", "the Third Doctor", "singular plastoglobulus", "pound-force", "the Ming dynasty", "Dorothy and Michael Hintze", "The Small Catechism", "36%", "Giovanni Battista Foggini", "April 20", "biomass", "their belief in the validity of the social contract", "K MJ-TV", "the Foreign Protestants Naturalization Act", "southern and central parts of France", "10%", "not designed to fly through the Earth's atmosphere or return to Earth", "Metro Trains Melbourne", "BBC 1", "$2 million", "Vince Lombardi Trophy", "Galileo", "in linked groups or chains", "meaning", "The Tiber", "1885", "James Madison", "Ryan Pinkston", "federal republic", "lacteal", "21 June 2007", "$6.2 trillion or approximately 45 % of the debt held by the public was owned by foreign investors", "N\u0289m\u0289n\u0289\u0289", "8ft", "Bartolomeu Dias", "William Wyler", "1961", "March 1930", "Julie Adams", "Thomas Jefferson", "February 2017", "October 29, 2015", "United States customary units", "Millerlite", "Sunday night", "Billy Hill", "Mara Jade", "Malina Weissman", "September 6, 2019", "1773", "A lacteal", "April 26, 2005", "Jennifer Eccles", "Albert", "a coma", "Croatia", "Drew Kesse", "teen", "they were part of a group of 20 similar cars making an annual road trip"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6651041666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.125, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375]}}, "before_error_ids": ["mrqa_squad-validation-8958", "mrqa_squad-validation-7876", "mrqa_squad-validation-8786", "mrqa_squad-validation-5724", "mrqa_squad-validation-6706", "mrqa_squad-validation-4715", "mrqa_squad-validation-8769", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-5739", "mrqa_hotpotqa-validation-2800", "mrqa_newsqa-validation-3043", "mrqa_searchqa-validation-2141", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-3476"], "SR": 0.640625, "CSR": 0.7100694444444444, "retrieved_ids": ["mrqa_squad-train-86141", "mrqa_squad-train-43885", "mrqa_squad-train-44561", "mrqa_squad-train-8543", "mrqa_squad-train-23631", "mrqa_squad-train-67933", "mrqa_squad-train-3247", "mrqa_squad-train-54586", "mrqa_squad-train-553", "mrqa_squad-train-81391", "mrqa_squad-train-69308", "mrqa_squad-train-79491", "mrqa_squad-train-24908", "mrqa_squad-train-65753", "mrqa_squad-train-79472", "mrqa_squad-train-2295", "mrqa_squad-train-36120", "mrqa_squad-train-31349", "mrqa_squad-train-52295", "mrqa_squad-train-68039", "mrqa_squad-train-25368", "mrqa_squad-train-54063", "mrqa_squad-train-57474", "mrqa_squad-train-23491", "mrqa_squad-validation-6439", "mrqa_squad-validation-6171", "mrqa_squad-validation-3863", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-2466", "mrqa_squad-validation-6409", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-6500", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-2463", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-7062", "mrqa_squad-validation-5860", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-1863", "mrqa_searchqa-validation-4319"], "EFR": 1.0, "Overall": 0.8550347222222222}, {"timecode": 9, "before_eval_results": {"predictions": ["to emphasize academics over athletics", "3,600", "nine", "individual states and territories", "30%\u201350% O2", "one of his wife's ladies-in-waiting", "liquid", "their greatest common divisor is one", "Europe", "cell membrane", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe", "Laverne & Shirley", "carbohydrates", "his butchery", "Jean Ribault", "March 2011", "Continental Edison Company in France", "2010", "more equality in the income distribution", "X reduces to Y", "38", "1887", "1469", "a \"world classic of epoch-making oratory.\"", "up to half", "lowest pitch to highest", "WD-40", "a barbie doll type", "Georgie Porgie", "a grain spirit or it may be made from other plants", "William Shaksper", "The Fray", "Venus", "Helen Hayes MacArthur", "Canberra", "The Awl", "Alexander Graham Bell", "Anna Pavlova", "an actuary computes premium rates, dividends, risks, etc.", "Lasorda", "a boy & Cecil", "Chicago Cubs", "Outer Mongolia", "a goat", "a father of seven,", "Resentment over someone's good fortune without wanting it", "a professional tennis player Jaime Sommers", "How romantic", "Beverly Mantle", "the chimney flue", "Andrew Jackson", "Madonna", "Fauves", "a sailfish", "a decorative clip or bar that is used to hold a girl's or woman's hair", "Matilda Fitzwater", "Egypt", "James Hutton", "Morocco", "Stuart Neame", "Total Nonstop Action Wrestling", "an American chemist and professor of chemistry at Harvard University", "China and Japan", "The United Nations is calling on NATO to do more to stop the Afghan opium trade"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6458085317460318}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.25, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3687", "mrqa_squad-validation-7700", "mrqa_squad-validation-1240", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-6421", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-2179"], "SR": 0.546875, "CSR": 0.69375, "retrieved_ids": ["mrqa_squad-train-78605", "mrqa_squad-train-20150", "mrqa_squad-train-83728", "mrqa_squad-train-82937", "mrqa_squad-train-52155", "mrqa_squad-train-36393", "mrqa_squad-train-32997", "mrqa_squad-train-78336", "mrqa_squad-train-14320", "mrqa_squad-train-58731", "mrqa_squad-train-67331", "mrqa_squad-train-22289", "mrqa_squad-train-8870", "mrqa_squad-train-9322", "mrqa_squad-train-30602", "mrqa_squad-train-58312", "mrqa_squad-train-11769", "mrqa_squad-train-38049", "mrqa_squad-train-65646", "mrqa_squad-train-76276", "mrqa_squad-train-6122", "mrqa_squad-train-40935", "mrqa_squad-train-77984", "mrqa_squad-train-55761", "mrqa_squad-validation-6439", "mrqa_naturalquestions-validation-1912", "mrqa_searchqa-validation-10372", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-7390", "mrqa_squad-validation-7876", "mrqa_squad-validation-8595", "mrqa_newsqa-validation-1080", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-9715", "mrqa_squad-validation-6171", "mrqa_searchqa-validation-2568", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2969", "mrqa_squad-validation-1780", "mrqa_squad-validation-3946", "mrqa_squad-validation-8459", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-6998", "mrqa_searchqa-validation-14194", "mrqa_squad-validation-7332", "mrqa_hotpotqa-validation-4815"], "EFR": 1.0, "Overall": 0.846875}, {"timecode": 10, "UKR": 0.837890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8454", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-3245", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-5035", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-10000", "mrqa_squad-validation-10115", "mrqa_squad-validation-10136", "mrqa_squad-validation-1017", "mrqa_squad-validation-10181", "mrqa_squad-validation-10184", "mrqa_squad-validation-10217", "mrqa_squad-validation-10263", "mrqa_squad-validation-10281", "mrqa_squad-validation-10290", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-10361", "mrqa_squad-validation-10369", "mrqa_squad-validation-1038", "mrqa_squad-validation-10410", "mrqa_squad-validation-10454", "mrqa_squad-validation-10496", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1181", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1226", "mrqa_squad-validation-1240", "mrqa_squad-validation-1254", "mrqa_squad-validation-1269", "mrqa_squad-validation-1371", "mrqa_squad-validation-1499", "mrqa_squad-validation-1521", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1651", "mrqa_squad-validation-166", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1748", "mrqa_squad-validation-1780", "mrqa_squad-validation-1787", "mrqa_squad-validation-1848", "mrqa_squad-validation-1863", "mrqa_squad-validation-1892", "mrqa_squad-validation-1924", "mrqa_squad-validation-1938", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-1998", "mrqa_squad-validation-2019", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-2108", "mrqa_squad-validation-2145", "mrqa_squad-validation-2209", "mrqa_squad-validation-2233", "mrqa_squad-validation-2241", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-247", "mrqa_squad-validation-2521", "mrqa_squad-validation-2545", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2642", "mrqa_squad-validation-27", "mrqa_squad-validation-2751", "mrqa_squad-validation-2820", "mrqa_squad-validation-2885", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-3039", "mrqa_squad-validation-305", "mrqa_squad-validation-3076", "mrqa_squad-validation-3144", "mrqa_squad-validation-3164", "mrqa_squad-validation-317", "mrqa_squad-validation-3184", "mrqa_squad-validation-322", "mrqa_squad-validation-3230", "mrqa_squad-validation-3270", "mrqa_squad-validation-334", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3376", "mrqa_squad-validation-3380", "mrqa_squad-validation-3392", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3497", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3687", "mrqa_squad-validation-3703", "mrqa_squad-validation-3718", "mrqa_squad-validation-374", "mrqa_squad-validation-3769", "mrqa_squad-validation-3770", "mrqa_squad-validation-381", "mrqa_squad-validation-3824", "mrqa_squad-validation-3829", "mrqa_squad-validation-3842", "mrqa_squad-validation-3848", "mrqa_squad-validation-3852", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3917", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3985", "mrqa_squad-validation-3986", "mrqa_squad-validation-3998", "mrqa_squad-validation-4000", "mrqa_squad-validation-4009", "mrqa_squad-validation-402", "mrqa_squad-validation-4031", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4181", "mrqa_squad-validation-4187", "mrqa_squad-validation-4213", "mrqa_squad-validation-4291", "mrqa_squad-validation-4312", "mrqa_squad-validation-4348", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4452", "mrqa_squad-validation-4467", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-451", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4539", "mrqa_squad-validation-4557", "mrqa_squad-validation-4557", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4838", "mrqa_squad-validation-491", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5019", "mrqa_squad-validation-5064", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-516", "mrqa_squad-validation-5262", "mrqa_squad-validation-5396", "mrqa_squad-validation-5436", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5493", "mrqa_squad-validation-5527", "mrqa_squad-validation-5546", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5602", "mrqa_squad-validation-5631", "mrqa_squad-validation-5664", "mrqa_squad-validation-5677", "mrqa_squad-validation-57", "mrqa_squad-validation-5726", "mrqa_squad-validation-5750", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5806", "mrqa_squad-validation-5818", "mrqa_squad-validation-5852", "mrqa_squad-validation-5860", "mrqa_squad-validation-5865", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6031", "mrqa_squad-validation-6066", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6176", "mrqa_squad-validation-6206", "mrqa_squad-validation-6222", "mrqa_squad-validation-6229", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6319", "mrqa_squad-validation-6330", "mrqa_squad-validation-6347", "mrqa_squad-validation-6353", "mrqa_squad-validation-6355", "mrqa_squad-validation-6409", "mrqa_squad-validation-6439", "mrqa_squad-validation-6502", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6611", "mrqa_squad-validation-6649", "mrqa_squad-validation-6664", "mrqa_squad-validation-6694", "mrqa_squad-validation-6790", "mrqa_squad-validation-6815", "mrqa_squad-validation-6838", "mrqa_squad-validation-6875", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6898", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7036", "mrqa_squad-validation-7039", "mrqa_squad-validation-7064", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7228", "mrqa_squad-validation-7260", "mrqa_squad-validation-7261", "mrqa_squad-validation-7297", "mrqa_squad-validation-7332", "mrqa_squad-validation-7338", "mrqa_squad-validation-7357", "mrqa_squad-validation-7364", "mrqa_squad-validation-7368", "mrqa_squad-validation-7380", "mrqa_squad-validation-739", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7445", "mrqa_squad-validation-7457", "mrqa_squad-validation-7470", "mrqa_squad-validation-7492", "mrqa_squad-validation-7503", "mrqa_squad-validation-7525", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-762", "mrqa_squad-validation-7693", "mrqa_squad-validation-7700", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7775", "mrqa_squad-validation-7781", "mrqa_squad-validation-7785", "mrqa_squad-validation-779", "mrqa_squad-validation-7863", "mrqa_squad-validation-7871", "mrqa_squad-validation-7917", "mrqa_squad-validation-7943", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8016", "mrqa_squad-validation-8043", "mrqa_squad-validation-8093", "mrqa_squad-validation-8125", "mrqa_squad-validation-8154", "mrqa_squad-validation-8177", "mrqa_squad-validation-8184", "mrqa_squad-validation-8192", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8365", "mrqa_squad-validation-8414", "mrqa_squad-validation-8449", "mrqa_squad-validation-8459", "mrqa_squad-validation-8471", "mrqa_squad-validation-8484", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8568", "mrqa_squad-validation-8585", "mrqa_squad-validation-8661", "mrqa_squad-validation-8670", "mrqa_squad-validation-8670", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-8841", "mrqa_squad-validation-888", "mrqa_squad-validation-8904", "mrqa_squad-validation-8925", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8958", "mrqa_squad-validation-8985", "mrqa_squad-validation-908", "mrqa_squad-validation-9095", "mrqa_squad-validation-9161", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9403", "mrqa_squad-validation-9405", "mrqa_squad-validation-9446", "mrqa_squad-validation-9464", "mrqa_squad-validation-9556", "mrqa_squad-validation-957", "mrqa_squad-validation-9594", "mrqa_squad-validation-9615", "mrqa_squad-validation-9669", "mrqa_squad-validation-9716", "mrqa_squad-validation-9717", "mrqa_squad-validation-9764", "mrqa_squad-validation-9814", "mrqa_squad-validation-9816", "mrqa_squad-validation-9876", "mrqa_squad-validation-9907", "mrqa_squad-validation-9928", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7463"], "OKR": 0.951171875, "KG": 0.49375, "before_eval_results": {"predictions": ["Northern Europe and the Mid-Atlantic", "$2 million", "fish stocks to collapse", "Chris Keates", "its many castles and vineyards", "Cinerama Productions/Palomar theatrical library", "Antigone", "3.5 million", "Denver Broncos", "1997", "A \u2192 G deamination", "since 2001", "A", "1784", "Narrow alleys", "another problem", "economic", "John and Benjamin Green", "1530", "installed electrical arc light based illumination systems", "two", "the poor", "Irish Sweepstakes", "Pearl Jam", "Grey's Anatomy", "silk", "Bruce Springsteen", "Wounded Knee", "Maria Callas", "Henry Moore", "Red Sox", "Charlotte, North Carolina", "an eagle", "Narcissus", "Fred Williamson", "the Orange River", "tapestry", "the Holy Grail", "the Smashing Pumpkins", "Fran", "Ludwig Van Beethoven", "Lake Victoria", "tidal streams", "Dr Pepper", "FRAM", "Sarah Orne Jewett", "Guns N' Roses", "London", "jet stream", "You Bet Your Life", "China", "the Gulf of St. Lawrence", "Kenny G", "lion", "Franklin Pierce", "an inland place", "Michael Schumacher", "a four - page pamphlet", "pool", "Glasgow", "Paul W. S. Anderson", "Hugh Dowding", "NATO fighters", "Congress"], "metric_results": {"EM": 0.625, "QA-F1": 0.6891493055555555}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.888888888888889, 1.0, 0.7499999999999999, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8990", "mrqa_squad-validation-5887", "mrqa_squad-validation-6655", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3530", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-8760", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-12552", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-6181", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6896"], "SR": 0.625, "CSR": 0.6875, "retrieved_ids": ["mrqa_squad-train-4507", "mrqa_squad-train-41642", "mrqa_squad-train-39078", "mrqa_squad-train-76183", "mrqa_squad-train-84447", "mrqa_squad-train-56956", "mrqa_squad-train-38561", "mrqa_squad-train-28772", "mrqa_squad-train-44922", "mrqa_squad-train-71180", "mrqa_squad-train-17558", "mrqa_squad-train-70255", "mrqa_squad-train-27686", "mrqa_squad-train-56516", "mrqa_squad-train-52981", "mrqa_squad-train-10926", "mrqa_squad-train-71487", "mrqa_squad-train-40958", "mrqa_squad-train-11782", "mrqa_squad-train-10898", "mrqa_squad-train-59664", "mrqa_squad-train-9981", "mrqa_squad-train-17865", "mrqa_squad-train-41744", "mrqa_naturalquestions-validation-7694", "mrqa_searchqa-validation-14480", "mrqa_newsqa-validation-3331", "mrqa_naturalquestions-validation-3392", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-2579", "mrqa_squad-validation-7051", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-2141", "mrqa_naturalquestions-validation-10460", "mrqa_searchqa-validation-686", "mrqa_naturalquestions-validation-6106", "mrqa_searchqa-validation-5631", "mrqa_naturalquestions-validation-2969", "mrqa_searchqa-validation-2568", "mrqa_squad-validation-1827", "mrqa_squad-validation-5818", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-4348", "mrqa_squad-validation-3946", "mrqa_squad-validation-2943", "mrqa_squad-validation-6031", "mrqa_squad-validation-7332"], "EFR": 0.9583333333333334, "Overall": 0.7857291666666667}, {"timecode": 11, "before_eval_results": {"predictions": ["North Africa", "Grumman", "submit cheerfully to the highest penalty that can be inflicted upon me for what in law is a deliberate crime", "1689", "St. Johns River", "the AS-205 mission was canceled", "The President of the Council and a Commissioner can sit in on ECB meetings, but don't have voting rights", "Ismailiyah, Egypt", "phycobilisomes", "lupus erythematosus", "December 1878", "bars, caf\u00e9s and clubs", "both PNU and ODM camps", "O(n2)", "Bill Clinton", "qu", "International Crops Research Institute for the Semi-Arid Tropics", "straight line", "Germany", "autoimmune", "January 26, 1996", "his advocacy", "Seoul", "2005", "2005", "May 21, 2000", "the 100 metres", "January 2016", "seven", "Samuel Beckett's", "a small remote hamlet, lying on Nostie Bay, an inlet at the northeastern end of the sea loch,", "Sonic Mania", "Homeland", "Carson City", "League of the Three Emperors", "Barack Obama", "Brian A. Miller", "Washington, D.C.", "December 13, 2015", "Front Row", "1590", "Vixen", "Revolution Studios", "Mach number", "1990", "Michael A. Cremo", "Gangsta's Paradise", "The A41", "Bette Davis, Olivia de Havilland, Joseph Cotten, Agnes Moorehead and Mary Astor", "five", "Indiana", "Esteban Ocon", "ABC", "the British military", "National Lottery", "2018", "Dan Stevens", "Milton Friedman", "Billy Wilder", "twice", "a full garden and pool, a tennis court, or several heli-pads", "Chad", "a pillar", "President Yahya Khan"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6386519909688013}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.06896551724137931, 0.0, 1.0, 0.4, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9912", "mrqa_squad-validation-6759", "mrqa_squad-validation-3113", "mrqa_squad-validation-3954", "mrqa_squad-validation-4150", "mrqa_squad-validation-1713", "mrqa_squad-validation-4883", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-5604", "mrqa_newsqa-validation-3227", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-971"], "SR": 0.578125, "CSR": 0.6783854166666667, "retrieved_ids": ["mrqa_squad-train-42661", "mrqa_squad-train-166", "mrqa_squad-train-80226", "mrqa_squad-train-23503", "mrqa_squad-train-67303", "mrqa_squad-train-61062", "mrqa_squad-train-23353", "mrqa_squad-train-51603", "mrqa_squad-train-28380", "mrqa_squad-train-37737", "mrqa_squad-train-47639", "mrqa_squad-train-59839", "mrqa_squad-train-84787", "mrqa_squad-train-82946", "mrqa_squad-train-72190", "mrqa_squad-train-76578", "mrqa_squad-train-62003", "mrqa_squad-train-83172", "mrqa_squad-train-67915", "mrqa_squad-train-24685", "mrqa_squad-train-77480", "mrqa_squad-train-43949", "mrqa_squad-train-69561", "mrqa_squad-train-36757", "mrqa_naturalquestions-validation-473", "mrqa_searchqa-validation-16908", "mrqa_naturalquestions-validation-1378", "mrqa_squad-validation-3985", "mrqa_squad-validation-1240", "mrqa_searchqa-validation-6463", "mrqa_naturalquestions-validation-8239", "mrqa_squad-validation-8958", "mrqa_searchqa-validation-4169", "mrqa_squad-validation-9173", "mrqa_squad-validation-5887", "mrqa_squad-validation-2943", "mrqa_triviaqa-validation-6548", "mrqa_squad-validation-8786", "mrqa_naturalquestions-validation-3686", "mrqa_searchqa-validation-4394", "mrqa_squad-validation-7338", "mrqa_squad-validation-6957", "mrqa_squad-validation-5588", "mrqa_naturalquestions-validation-8277", "mrqa_squad-validation-1827", "mrqa_squad-validation-10321", "mrqa_naturalquestions-validation-10460", "mrqa_searchqa-validation-1053"], "EFR": 0.9629629629629629, "Overall": 0.7848321759259259}, {"timecode": 12, "before_eval_results": {"predictions": ["Sky Q Silver set top boxes with a Wi-Fi or Power-line connection", "water pump", "Tesla coil", "1946", "21 to 11", "Parliamentary Bureau", "Japan and Latin America", "sent missionaries", "Arizona Cardinals", "842 pounds", "1540s", "John Fox", "American Indians in the colony of Georgia", "orbit the Moon", "poison", "quickly", "a system of many biological structures and processes within an organism that protects against disease", "March 1896", "The Lightning thief", "James `` Jamie '' Dornan", "W. Edwards Deming", "usually in May", "physiology", "$657.4 million", "During the last Ice Age", "Accounting Standards Board ( ASB )", "Ole Einar Bj\u00f8rndalen", "General George Washington", "following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Djokovic", "Longliners", "1961", "dome", "1997", "Procol Harum", "Sheev Palpatine", "Bobb McKittrick", "blues rock", "septum", "The White House Executive chef", "vaskania", "the church at Philippi", "10 May 1940", "Brenda", "bohrium", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "cartilage", "a couple broken apart by the Iraq War", "Spanish American wars of independence", "Tristan Rogers", "Owen Vaccaro", "Walter Brennan", "around 1872", "Brad Johnson", "1992 to 2013", "King Richard II of England", "between Austria and Switzerland", "(Le Divorce)", "Gillian Leigh Anderson", "the Kooyong Classic in Melbourne", "Monday", "a historic American Negro spiritual", "blinking his left eye", "Carr Inlet"], "metric_results": {"EM": 0.546875, "QA-F1": 0.640890522875817}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false], "QA-F1": [0.7058823529411764, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.13333333333333336, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.20000000000000004, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-978", "mrqa_squad-validation-3130", "mrqa_squad-validation-3811", "mrqa_squad-validation-9863", "mrqa_squad-validation-8164", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-10620", "mrqa_triviaqa-validation-4886", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-5292", "mrqa_newsqa-validation-1360", "mrqa_searchqa-validation-13332", "mrqa_searchqa-validation-9822", "mrqa_searchqa-validation-10098"], "SR": 0.546875, "CSR": 0.6682692307692308, "retrieved_ids": ["mrqa_squad-train-55313", "mrqa_squad-train-66429", "mrqa_squad-train-17547", "mrqa_squad-train-39009", "mrqa_squad-train-54150", "mrqa_squad-train-72354", "mrqa_squad-train-77820", "mrqa_squad-train-68293", "mrqa_squad-train-71039", "mrqa_squad-train-61541", "mrqa_squad-train-60515", "mrqa_squad-train-63738", "mrqa_squad-train-24166", "mrqa_squad-train-17471", "mrqa_squad-train-23389", "mrqa_squad-train-72312", "mrqa_squad-train-8678", "mrqa_squad-train-1050", "mrqa_squad-train-25276", "mrqa_squad-train-9931", "mrqa_squad-train-54454", "mrqa_squad-train-82232", "mrqa_squad-train-82555", "mrqa_squad-train-2959", "mrqa_searchqa-validation-3451", "mrqa_squad-validation-7525", "mrqa_squad-validation-4572", "mrqa_squad-validation-2463", "mrqa_squad-validation-8958", "mrqa_squad-validation-9764", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-6463", "mrqa_newsqa-validation-429", "mrqa_squad-validation-4715", "mrqa_naturalquestions-validation-9453", "mrqa_hotpotqa-validation-1747", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-2568", "mrqa_squad-validation-1780", "mrqa_naturalquestions-validation-9737", "mrqa_searchqa-validation-2579", "mrqa_squad-validation-3718", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-7095", "mrqa_hotpotqa-validation-1404", "mrqa_naturalquestions-validation-3686", "mrqa_hotpotqa-validation-1650"], "EFR": 0.9655172413793104, "Overall": 0.7833197944297082}, {"timecode": 13, "before_eval_results": {"predictions": ["Protestantism", "Extension", "Riverside", "interactions", "Hamburg merchants and traders", "the Department of Justice", "water flow through the body cavity", "67.9", "Fort Duquesne", "Sports Programs, Inc.", "quality rental units", "Pittsburgh Steelers", "Edward Teller", "the geographical area it covers as well as the frequency of meeting", "stay", "Andrew Lortie", "vertebrates and invertebrates", "Thirty years after the Galactic Civil War", "a series of structural rearrangements in the RTK that lead to its enzymatic activation", "eight years", "1968 New York Times interview", "Longline fishing", "various locations in Redford's adopted home state of Utah", "Stephen A. Douglas", "April 2011", "1940", "Paradise, Nevada", "Herman Hollerith", "Dr. Rajendra Prasad", "Ron Harper", "hairpin corner", "over two days", "the IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "metaphase of cell division", "it activates a relay which will handle the higher current load", "Donald Trump", "Liam Cunningham", "the spectroscopic notation for the associated atomic orbitals", "Veronica", "moral", "A rotation", "their bearers", "Gustav Bauer", "2002", "Mohammad Reza Pahlavi", "Virginia Beach", "a convergent plate boundary", "Jourdan Miller", "10,605", "84", "1773", "Jesse McCartney", "79", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1920", "Catherine Zeta-Jones", "Michael Crawford", "247,597", "10,000", "those missing", "the Mormon Tabernacle Choir", "carbon dioxide", "Pickwick", "Sunshine State"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6008413461538462}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.5, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10475", "mrqa_squad-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-1166"], "SR": 0.53125, "CSR": 0.6584821428571428, "retrieved_ids": ["mrqa_squad-train-34211", "mrqa_squad-train-58541", "mrqa_squad-train-34084", "mrqa_squad-train-83426", "mrqa_squad-train-81028", "mrqa_squad-train-19740", "mrqa_squad-train-77525", "mrqa_squad-train-80308", "mrqa_squad-train-63875", "mrqa_squad-train-26813", "mrqa_squad-train-72552", "mrqa_squad-train-74393", "mrqa_squad-train-6381", "mrqa_squad-train-8317", "mrqa_squad-train-6645", "mrqa_squad-train-62222", "mrqa_squad-train-21468", "mrqa_squad-train-5317", "mrqa_squad-train-51591", "mrqa_squad-train-41141", "mrqa_squad-train-79927", "mrqa_squad-train-71001", "mrqa_squad-train-52558", "mrqa_squad-train-15565", "mrqa_searchqa-validation-11532", "mrqa_squad-validation-7338", "mrqa_searchqa-validation-11367", "mrqa_squad-validation-7613", "mrqa_squad-validation-4452", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-1279", "mrqa_naturalquestions-validation-9235", "mrqa_searchqa-validation-4393", "mrqa_squad-validation-8164", "mrqa_searchqa-validation-6445", "mrqa_naturalquestions-validation-712", "mrqa_squad-validation-8841", "mrqa_hotpotqa-validation-1534", "mrqa_squad-validation-1827", "mrqa_newsqa-validation-3227", "mrqa_naturalquestions-validation-5739", "mrqa_hotpotqa-validation-558", "mrqa_squad-validation-6072", "mrqa_squad-validation-3863", "mrqa_newsqa-validation-1510", "mrqa_searchqa-validation-6234", "mrqa_naturalquestions-validation-3332", "mrqa_squad-validation-5588"], "EFR": 1.0, "Overall": 0.7882589285714285}, {"timecode": 14, "before_eval_results": {"predictions": ["a Tulku", "the Quaternary", "the Treaty of Aix-la-Chapelle", "Brad Nortman", "Behind the Sofa", "BBC Dead Ringers", "1206", "Louis Pasteur", "The Brain of Morbius", "the depths of the oceans and seas", "118", "a mainline Protestant Methodist denomination", "Albert Einstein", "Vince Lombardi Trophy", "death in body and soul", "Candice Susan Swanepoel", "AT&T", "Australian", "German", "Chris Anderson", "the northwest tip of Canisteo Peninsula in Amundsen Sea", "1949", "Red", "Australian", "DI Humphrey Goodman", "Jena Malone", "John M. Dowd", "twelfth", "Hawaii Republican", "New York", "Molly Hatchet", "tragedy", "Cricket fighting", "14th Street", "bass", "Brad Wilk", "2012", "New Orleans, Louisiana", "Robert \"Bobby\" Germaine, Sr.", "November 22, 1993", "Australian", "1966", "2012", "1926", "Texas's 27th congressional district", "\"Santa Barbara\"", "mother goddess", "VAQ-135", "1892", "Ludwig van Beethoven", "Sun Records founder Sam Phillips", "Manchester United", "Turkmenistan", "1942", "October 6, 2017", "at a given temperature", "wolf", "Ganges", "January 24, 2006", "repression and dire economic circumstances", "a pager", "Baltimore", "In 1917", "Shenandoah National Park in the Blue Ridge Mountains of Virginia"], "metric_results": {"EM": 0.625, "QA-F1": 0.7034451659451659}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.09090909090909093]}}, "before_error_ids": ["mrqa_squad-validation-10168", "mrqa_squad-validation-8229", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-10613", "mrqa_newsqa-validation-1879", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-1813"], "SR": 0.625, "CSR": 0.65625, "retrieved_ids": ["mrqa_squad-train-54484", "mrqa_squad-train-48118", "mrqa_squad-train-21704", "mrqa_squad-train-3661", "mrqa_squad-train-84934", "mrqa_squad-train-14801", "mrqa_squad-train-19142", "mrqa_squad-train-77595", "mrqa_squad-train-18422", "mrqa_squad-train-36822", "mrqa_squad-train-47708", "mrqa_squad-train-47956", "mrqa_squad-train-73946", "mrqa_squad-train-23716", "mrqa_squad-train-80062", "mrqa_squad-train-32843", "mrqa_squad-train-57061", "mrqa_squad-train-15468", "mrqa_squad-train-26402", "mrqa_squad-train-66152", "mrqa_squad-train-54070", "mrqa_squad-train-70964", "mrqa_squad-train-23931", "mrqa_squad-train-27194", "mrqa_squad-validation-7457", "mrqa_triviaqa-validation-4886", "mrqa_naturalquestions-validation-5531", "mrqa_squad-validation-6171", "mrqa_squad-validation-7700", "mrqa_squad-validation-4838", "mrqa_searchqa-validation-172", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8277", "mrqa_newsqa-validation-3227", "mrqa_searchqa-validation-971", "mrqa_squad-validation-1240", "mrqa_naturalquestions-validation-5780", "mrqa_squad-validation-5860", "mrqa_squad-validation-5262", "mrqa_squad-validation-4326", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-7310", "mrqa_squad-validation-1827", "mrqa_squad-validation-7332", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-6524", "mrqa_squad-validation-7708"], "EFR": 1.0, "Overall": 0.7878125}, {"timecode": 15, "before_eval_results": {"predictions": ["1937", "trade unions", "June 4, 2014", "Journey's End", "tech-oriented", "John Houghton", "heterokontophyte", "NP-complete", "13th century", "128,843", "by a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections", "56.2%", "11 points", "Chaplain to the Forces", "KlingStubbins", "Edward Albert", "in honour of Louis Mountbatten", "Alcorn State", "The Guest", "The Light in the Piazza", "Philadelphia", "12", "The A41", "Royce da 5'9\" (Bad) and Eminem (Evil)", "\"Pimp My Ride\"", "1998", "casting, job opportunities, and career advice.", "Mary Harron", "Flashback", "Eenasul Fateh", "Chicago", "Australia", "2014", "the Second World War", "Lismore", "rural", "teenage actor or teen actor", "Summerlin, Clark County, Nevada", "Lester Ben \"Benny\" Binion", "YG Entertainment", "water", "Noel", "\"Pour le M\u00e9rite\"", "Trey Parker and Matt Stone", "Riot Act", "Aqua", "various registries.", "four", "Christy Walton", "Commanding General", "Hechingen", "Black Sabbath", "manager", "8,211", "Kristin Beth Baxter", "in the cell nucleus", "a Bristol Box Kite", "1961", "a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "South Dakota State Penitentiary", "Douglas Fir", "wine or kiss a fool", "a striking blow to due process and the rule of law", "Philip Markoff"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6568452380952381}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.5, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.4, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6279", "mrqa_squad-validation-4298", "mrqa_squad-validation-257", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-5320", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2178", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-289", "mrqa_triviaqa-validation-7461", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-13595", "mrqa_searchqa-validation-1757"], "SR": 0.53125, "CSR": 0.6484375, "retrieved_ids": ["mrqa_squad-train-44638", "mrqa_squad-train-21018", "mrqa_squad-train-37013", "mrqa_squad-train-77546", "mrqa_squad-train-21604", "mrqa_squad-train-22798", "mrqa_squad-train-63123", "mrqa_squad-train-78842", "mrqa_squad-train-63470", "mrqa_squad-train-62666", "mrqa_squad-train-63908", "mrqa_squad-train-7268", "mrqa_squad-train-14990", "mrqa_squad-train-41", "mrqa_squad-train-44874", "mrqa_squad-train-28329", "mrqa_squad-train-33226", "mrqa_squad-train-3189", "mrqa_squad-train-76334", "mrqa_squad-train-71383", "mrqa_squad-train-86023", "mrqa_squad-train-76314", "mrqa_squad-train-36401", "mrqa_squad-train-9588", "mrqa_hotpotqa-validation-2058", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7062", "mrqa_newsqa-validation-1360", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-8239", "mrqa_squad-validation-1827", "mrqa_naturalquestions-validation-9737", "mrqa_squad-validation-3497", "mrqa_searchqa-validation-172", "mrqa_squad-validation-1240", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-9822", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-7390", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3469", "mrqa_squad-validation-3130", "mrqa_squad-validation-3687", "mrqa_squad-validation-4326", "mrqa_squad-validation-6228", "mrqa_searchqa-validation-3613"], "EFR": 1.0, "Overall": 0.78625}, {"timecode": 16, "before_eval_results": {"predictions": ["Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Puritan", "James Wolfe", "March 1974", "2003", "Frederick II the Great", "Lower taxes, increased economic development, unification of the community, better public spending and effective administration by a more central authority", "the Armenians vassal-states of Sassoun and Taron", "redistributive taxation", "Seattle Seahawks", "paid professionals", "squaring", "revelry", "Krishna Rajaram", "an internationally known Catholic priest", "British charities for aid to Gaza", "1-0", "Choi", "second-degree aggravated battery", "Romney", "one day", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "be silent", "200", "2,000", "several weeks", "this will be the first time any version of the Magna Carta has ever gone up for auction", "a piece of gauze, and they wipe off all the dead skin", "Michael Jackson", "Caylee Anthony", "10 below", "women", "Manmohan Singh", "jazz", "1983", "cancer", "Kenyan forces", "Casalesi Camorra", "Some 200 potential jurors were called, according to CNN affiliate KVBC.", "Appathurai", "Eintracht Frankfurt", "opium poppies", "Bronx County District Attorneys Office", "1,073", "Arthur E. Morgan III", "Arturo Gonzalez Rodriguez", "Chinese nationals", "Pakistan", "the FBI", "Akio Toyoda", "18", "Draquila -- Italy Trembles", "India", "Mumbai", "Miami Heat", "a combination of genetics and the male hormone dihydrotestosterone", "Senegal", "Windermere", "Field of Dreams", "Bill Paxton", "a star", "KLM", "Sex Pistols", "an ambitious Jewish boy growing up in a poor neighborhood in Montreal"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6231643570889895}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, true, true, false], "QA-F1": [0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.32, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8294", "mrqa_squad-validation-7296", "mrqa_squad-validation-1136", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-2900", "mrqa_triviaqa-validation-4966", "mrqa_hotpotqa-validation-5742", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-4356"], "SR": 0.5625, "CSR": 0.6433823529411764, "retrieved_ids": ["mrqa_squad-train-47857", "mrqa_squad-train-12493", "mrqa_squad-train-4402", "mrqa_squad-train-65132", "mrqa_squad-train-36972", "mrqa_squad-train-42707", "mrqa_squad-train-44986", "mrqa_squad-train-668", "mrqa_squad-train-28962", "mrqa_squad-train-72471", "mrqa_squad-train-75085", "mrqa_squad-train-37755", "mrqa_squad-train-85141", "mrqa_squad-train-76275", "mrqa_squad-train-71917", "mrqa_squad-train-75370", "mrqa_squad-train-52717", "mrqa_squad-train-38706", "mrqa_squad-train-55852", "mrqa_squad-train-59478", "mrqa_squad-train-36957", "mrqa_squad-train-11588", "mrqa_squad-train-64486", "mrqa_squad-train-19794", "mrqa_searchqa-validation-13232", "mrqa_hotpotqa-validation-2213", "mrqa_searchqa-validation-16130", "mrqa_newsqa-validation-3476", "mrqa_searchqa-validation-3530", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-6234", "mrqa_squad-validation-4572", "mrqa_searchqa-validation-14655", "mrqa_triviaqa-validation-7461", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-3332", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-9536", "mrqa_naturalquestions-validation-1187", "mrqa_squad-validation-10388", "mrqa_naturalquestions-validation-10156", "mrqa_squad-validation-7332", "mrqa_squad-validation-5887", "mrqa_hotpotqa-validation-516", "mrqa_searchqa-validation-14371", "mrqa_squad-validation-9161", "mrqa_newsqa-validation-1080", "mrqa_searchqa-validation-11367"], "EFR": 1.0, "Overall": 0.7852389705882353}, {"timecode": 17, "before_eval_results": {"predictions": ["higher economic inequality", "Sonderungsverbot", "high schools", "a glass case suspended from the lid", "phagocytic", "2000", "five", "weight", "Leukocytes", "3D printing technology", "Ong Khan", "colonel", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "The oceans are growing crowded, and governments are increasingly trying to plan their use", "Wigan Athletic", "Russian air company Vertikal-T", "Graeme Smith", "228", "the Bush administration's controversial system of military trials for some Guant Bay detainees", "her father's", "St. Francis De Sales Catholic Church", "The Tinkler", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "air support", "seven deaths", "African National Congress", "Somali", "The oldest documented bikinis", "Tom Baer", "Adam Yahiye Gadahn,", "Mark Sanford", "150", "anti- strike", "the equator", "Chinese President Hu Jintao", "183", "warn doctors", "Too many glass shards left by beer drinkers", "Cirque du Soleil", "\"Goldstone Report\"", "11th year in a row", "two paintings by Pablo Picasso,", "fastest circumnavigation of the globe in a powerboat", "Guinea, Myanmar, Sudan and Venezuela", "Austin Wuennenberg", "Diversity", "forcibly drugging deportees", "Oaxacan countryside of southern Mexico", "buckling under pressure from the ruling party", "a polo match", "more than 100", "match-fixing allegations", "Alfredo Astiz,", "MacFarlane", "convert single - stranded genomic RNA into double - stranded cDNA", "Harrison Ford", "Andes", "\"The Snowman\"", "2009", "Candid Microphone", "Marilyn Monroe", "The Who", "\"The Band Concert\"", "salsa Bell"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5774725848327822}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 0.25, 0.2666666666666667, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8421052631578948, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.7272727272727272, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4864864864864865, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-7136", "mrqa_squad-validation-2000", "mrqa_squad-validation-7845", "mrqa_squad-validation-6477", "mrqa_newsqa-validation-412", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1123", "mrqa_naturalquestions-validation-1974", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1649"], "SR": 0.484375, "CSR": 0.6345486111111112, "retrieved_ids": ["mrqa_squad-train-4264", "mrqa_squad-train-78239", "mrqa_squad-train-58702", "mrqa_squad-train-37652", "mrqa_squad-train-46994", "mrqa_squad-train-44060", "mrqa_squad-train-2606", "mrqa_squad-train-77162", "mrqa_squad-train-38723", "mrqa_squad-train-40745", "mrqa_squad-train-30038", "mrqa_squad-train-11028", "mrqa_squad-train-45633", "mrqa_squad-train-28256", "mrqa_squad-train-18876", "mrqa_squad-train-55113", "mrqa_squad-train-79459", "mrqa_squad-train-19333", "mrqa_squad-train-7932", "mrqa_squad-train-25796", "mrqa_squad-train-7895", "mrqa_squad-train-54508", "mrqa_squad-train-60494", "mrqa_squad-train-84061", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-7461", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-788", "mrqa_squad-validation-2943", "mrqa_naturalquestions-validation-7095", "mrqa_searchqa-validation-16130", "mrqa_newsqa-validation-2184", "mrqa_hotpotqa-validation-132", "mrqa_squad-validation-8841", "mrqa_naturalquestions-validation-9453", "mrqa_searchqa-validation-3451", "mrqa_naturalquestions-validation-6524", "mrqa_squad-validation-7876", "mrqa_newsqa-validation-2112", "mrqa_hotpotqa-validation-16", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-13151", "mrqa_triviaqa-validation-6548", "mrqa_hotpotqa-validation-1650"], "EFR": 0.9696969696969697, "Overall": 0.7774116161616161}, {"timecode": 18, "before_eval_results": {"predictions": ["10 Cloverfield Lane", "in the condenser", "1999", "mesoglea", "a body of treaties and legislation,", "liquid", "socially", "Mark Twain's", "amylopectin starch granules", "Tower District", "\"a violent and brutal extremist group with a number of individuals affiliated with al Qaeda.", "pro-democracy activists clashed Friday with Egyptian security forces in central Cairo,", "a construction site in the heart of Los Angeles", "overthrow the socialist government of Salvador Allende in Chile,", "The Pilgrims ran aground in modern-day Haiti on Christmas Day 1492", "a rally at the State House next week", "2,000", "Michael Schumacher", "Ventures", "seven", "\"The noose incident occurred two weeks after Black History Month was mocked in an off-campus party", "resigned", "\"I'm just getting started.\"", "21,", "diplomatic relations", "hand-painted Swedish wooden clogs", "Daniel Radcliffe", "Muslim", "five", "mother", "$10 billion", "a seven-member Spanish flight crew and one Belgian", "a high school on the coastal island,", "9-week-old", "a form of liquid morphine used by terminally ill patients will remain on the market even though it is an \"unapproved drug,\"", "Lucky Dube,", "children's books", "James Newell Osterberg", "At least 40", "Afghan security forces", "Lindsey Vonn", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "a grizzly bear", "International Polo Club Palm Beach in Florida", "celebrity-inspired names", "HPV", "poor families", "At least 88", "creation of an Islamic emirate in Gaza", "an \"unnamed international terror group\"", "Nicole", "Manchester City", "Taher Nunu", "On 26 March 2015, the sixth series was confirmed to be the final series", "January 2017", "cubed", "a cashmere sweater", "Waylon Albright \"Shooter\" Jennings", "people working in film and the performing arts", "Coppertone", "Your pristine room", "Marlborough, New Hampshire", "1968", "The Daily Stormer"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5298069067370539}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.13333333333333333, 0.8, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.2564102564102564, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-606", "mrqa_squad-validation-1235", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2471", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-5209", "mrqa_hotpotqa-validation-2986", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-2393"], "SR": 0.421875, "CSR": 0.6233552631578947, "retrieved_ids": ["mrqa_squad-train-2470", "mrqa_squad-train-27699", "mrqa_squad-train-72638", "mrqa_squad-train-63473", "mrqa_squad-train-1265", "mrqa_squad-train-9557", "mrqa_squad-train-47391", "mrqa_squad-train-84362", "mrqa_squad-train-69985", "mrqa_squad-train-3271", "mrqa_squad-train-34681", "mrqa_squad-train-13770", "mrqa_squad-train-59871", "mrqa_squad-train-48320", "mrqa_squad-train-33778", "mrqa_squad-train-39513", "mrqa_squad-train-25462", "mrqa_squad-train-12127", "mrqa_squad-train-55224", "mrqa_squad-train-38055", "mrqa_squad-train-59309", "mrqa_squad-train-73847", "mrqa_squad-train-81535", "mrqa_squad-train-35826", "mrqa_newsqa-validation-429", "mrqa_hotpotqa-validation-16", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-3331", "mrqa_squad-validation-6706", "mrqa_newsqa-validation-3463", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-5149", "mrqa_newsqa-validation-4086", "mrqa_searchqa-validation-3613", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-2466", "mrqa_squad-validation-4326", "mrqa_hotpotqa-validation-2205", "mrqa_naturalquestions-validation-2143", "mrqa_searchqa-validation-11367", "mrqa_squad-validation-7613", "mrqa_squad-validation-5588", "mrqa_squad-validation-3687", "mrqa_hotpotqa-validation-3780", "mrqa_squad-validation-3811", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-801"], "EFR": 1.0, "Overall": 0.7812335526315789}, {"timecode": 19, "before_eval_results": {"predictions": ["blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "an Executive Committee", "New Orleans", "the death of Elisabeth Sladen", "NFL Experience", "English and Swahili", "61%", "plastoglobule(s)", "three", "Turkey", "Wombat", "Vakatakas", "a critical period", "Peyton Place", "a 45.52-carat rare blue diamond", "gin", "Pilate", "enamel", "Wagyu", "Tagline", "Battle of Hastings", "the Caspian Sea", "a work journal", "Arabian Nights", "Gannett Company", "\"Won't Get Fooled Again\"", "Don Juan De Marco", "Fes", "FIFA World Cup Final", "Interlaken", "Mystic Pizza", "Princeton", "Mandy", "Marter Health Sciences Library", "Malay Peninsula", "Herman Wouk", "Frederick IV,", "Richard Saunders", "poetry", "Napoleon Bonaparte", "sauropods", "second", "thermodynamics", "Derek Smalls", "Untouchables", "Harry Houdini", "\"Randy\"", "double Vision", "Sporcle", "a biographical novel", "Camembert", "James Ross Clemens", "aces", "1982", "not being pushed around by big labels, managers, and agents and being told what to do, and being true to yourself creatively", "Hoyo de Monterrey Epicure Especial", "\"Thrilla in Manila\"", "North America, Australia, and India", "841", "Ike", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "East River", "state system", "near the city of Cairo, Illinois"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5454049422799423}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.9777777777777777, 0.0, 0.5, 0.0, 1.0, 1.0, 0.9090909090909091, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8786", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-16558", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-2769", "mrqa_newsqa-validation-3214", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-3559"], "SR": 0.4375, "CSR": 0.6140625, "retrieved_ids": ["mrqa_squad-train-3377", "mrqa_squad-train-6836", "mrqa_squad-train-48856", "mrqa_squad-train-60913", "mrqa_squad-train-56152", "mrqa_squad-train-52813", "mrqa_squad-train-3172", "mrqa_squad-train-71516", "mrqa_squad-train-28538", "mrqa_squad-train-69482", "mrqa_squad-train-41277", "mrqa_squad-train-45188", "mrqa_squad-train-17509", "mrqa_squad-train-39360", "mrqa_squad-train-72388", "mrqa_squad-train-71245", "mrqa_squad-train-57618", "mrqa_squad-train-58226", "mrqa_squad-train-28387", "mrqa_squad-train-22068", "mrqa_squad-train-54221", "mrqa_squad-train-10594", "mrqa_squad-train-9150", "mrqa_squad-train-38328", "mrqa_naturalquestions-validation-9088", "mrqa_squad-validation-10168", "mrqa_newsqa-validation-3679", "mrqa_triviaqa-validation-2749", "mrqa_hotpotqa-validation-5667", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-16130", "mrqa_squad-validation-7051", "mrqa_searchqa-validation-6463", "mrqa_naturalquestions-validation-9239", "mrqa_squad-validation-7525", "mrqa_hotpotqa-validation-5154", "mrqa_searchqa-validation-12363", "mrqa_squad-validation-3863", "mrqa_naturalquestions-validation-1911", "mrqa_newsqa-validation-412", "mrqa_naturalquestions-validation-9235", "mrqa_hotpotqa-validation-5320", "mrqa_naturalquestions-validation-430", "mrqa_hotpotqa-validation-5644", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-1021", "mrqa_hotpotqa-validation-5808", "mrqa_squad-validation-9173"], "EFR": 0.9722222222222222, "Overall": 0.7738194444444444}, {"timecode": 20, "UKR": 0.828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-1252", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1967", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-3519", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-782", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-920", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11270", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2607", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4469", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-971", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10143", "mrqa_squad-validation-10168", "mrqa_squad-validation-10241", "mrqa_squad-validation-10266", "mrqa_squad-validation-10370", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1141", "mrqa_squad-validation-115", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1254", "mrqa_squad-validation-127", "mrqa_squad-validation-1288", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1747", "mrqa_squad-validation-1765", "mrqa_squad-validation-1827", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-218", "mrqa_squad-validation-22", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2379", "mrqa_squad-validation-2383", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-2538", "mrqa_squad-validation-2545", "mrqa_squad-validation-257", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2886", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-305", "mrqa_squad-validation-3052", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3567", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3864", "mrqa_squad-validation-3917", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3975", "mrqa_squad-validation-3986", "mrqa_squad-validation-3994", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4187", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4883", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5097", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-5237", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5604", "mrqa_squad-validation-5677", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5859", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6347", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6594", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7034", "mrqa_squad-validation-7039", "mrqa_squad-validation-7051", "mrqa_squad-validation-71", "mrqa_squad-validation-7125", "mrqa_squad-validation-7136", "mrqa_squad-validation-7192", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7449", "mrqa_squad-validation-7521", "mrqa_squad-validation-7576", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7708", "mrqa_squad-validation-7751", "mrqa_squad-validation-7814", "mrqa_squad-validation-7863", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7881", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8471", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8670", "mrqa_squad-validation-8710", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9095", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9367", "mrqa_squad-validation-9405", "mrqa_squad-validation-942", "mrqa_squad-validation-9594", "mrqa_squad-validation-9614", "mrqa_squad-validation-9669", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-4881", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-7496"], "OKR": 0.935546875, "KG": 0.5125, "before_eval_results": {"predictions": ["the main contractor", "widespread education", "300 men", "an attack on New France's capital, Quebec", "two-thirds", "Decompression sickness", "1979", "Parliament Square, High Street and George IV Bridge", "1959", "the Superdome", "grizzly bear", "Dracula", "Sid Vicious", "Nitrous oxide", "the Tchaikovsky 1812 Overture", "Frederic Remington", "lowlands", "Arkansas", "an object oriented programming", "5 BC", "the Uniform Code of Military Justice", "Whig", "ER", "a playful", "food Network", "The Princess Diaries", "Arkansas", "Mao Zedong", "Doner", "a bell clapper", "Deutsche Bank", "Sundance", "a den", "amber", "Holly Golightly", "Umbria", "a Roth IRA", "Quentin Tarantino", "Palatine Hill", "Kentucky", "an axiom", "Daylight Saving Time", "Miniskirt", "Airplane", "Scooter Libby", "overwhelm", "Katniss Everdeen", "Equatorial Guinea", "a fat prisoner", "the one in the bottom,", "Cecil Rhodes", "Aerobic", "Anaheim", "Steve Hale", "Epidemiology", "Belgium", "Jack Frost", "137th", "Merck & Co.", "semiconductors", "the Arctic north of Murmansk down to the southern climes of Sochi by way of St Petersburg and Moscow,", "France", "Hagrid", "Phil Mickelson"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6532407407407407}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.7407407407407407, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4918", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-16826", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-12996", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-3811", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-3514", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-4992", "mrqa_hotpotqa-validation-3157", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-4118", "mrqa_triviaqa-validation-436"], "SR": 0.546875, "CSR": 0.6108630952380952, "retrieved_ids": ["mrqa_squad-train-35519", "mrqa_squad-train-5829", "mrqa_squad-train-8420", "mrqa_squad-train-61061", "mrqa_squad-train-35042", "mrqa_squad-train-39038", "mrqa_squad-train-54503", "mrqa_squad-train-47735", "mrqa_squad-train-59212", "mrqa_squad-train-25582", "mrqa_squad-train-69573", "mrqa_squad-train-5615", "mrqa_squad-train-60252", "mrqa_squad-train-22034", "mrqa_squad-train-74770", "mrqa_squad-train-46343", "mrqa_squad-train-84338", "mrqa_squad-train-34890", "mrqa_squad-train-27271", "mrqa_squad-train-25363", "mrqa_squad-train-36243", "mrqa_squad-train-31999", "mrqa_squad-train-32411", "mrqa_squad-train-53804", "mrqa_hotpotqa-validation-5328", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-7554", "mrqa_searchqa-validation-1156", "mrqa_hotpotqa-validation-788", "mrqa_squad-validation-6494", "mrqa_searchqa-validation-13900", "mrqa_naturalquestions-validation-3332", "mrqa_hotpotqa-validation-3951", "mrqa_searchqa-validation-13600", "mrqa_squad-validation-5860", "mrqa_squad-validation-7051", "mrqa_squad-validation-7700", "mrqa_searchqa-validation-5172", "mrqa_squad-validation-4452", "mrqa_naturalquestions-validation-2102", "mrqa_newsqa-validation-1032", "mrqa_squad-validation-2000", "mrqa_newsqa-validation-4122", "mrqa_squad-validation-6409", "mrqa_naturalquestions-validation-10156", "mrqa_triviaqa-validation-2441", "mrqa_squad-validation-8294", "mrqa_hotpotqa-validation-4418"], "EFR": 0.9655172413793104, "Overall": 0.7705104423234811}, {"timecode": 21, "before_eval_results": {"predictions": ["a lesson plan", "laws of physics", "1893", "Welsh", "people themselves", "criminal", "a monthly subscription", "15,000 BC", "novella", "the President of the United States", "above the light source and under the sample in an upright microscope", "November 3, 2007", "1939", "April 1917", "1959", "orphanage", "September 19 - 22, 2017", "tolled ( quota ) highways", "an evaluation by an individual", "Bobby Eli", "Arunachal Pradesh", "Dick Rutan and Jeana Yeager", "Paracelsus", "January 2004", "members of the gay ( LGBT ) community", "late January or early February", "they believed that it violated their rights as Englishmen to `` No taxation without representation ''", "Jerry Lee Lewis", "push the food down the esophagus", "Splodgenessabounds", "Triple threat", "Edd Kimber, Joanne Wheatley, John Whaite, Frances Quinn, Nancy Birtwhistle, Nadiya Hussain, Candice Brown and Sophia Faldo", "the type of hazard ahead", "diastema", "Eddie Murphy", "comic book", "Secretaries of State and Defense and the National Security advisor", "flour and water", "National Football League ( NFL )", "Gupta Empire", "card verification code ( CVC )", "T - Bone Walker", "Ray Charles", "Francis Hutcheson", "1937", "Cairo, Illinois", "Barbara Windsor", "British", "Norman Whitfield and Barrett Strong", "Executive Residence of the White House Complex", "the eighth episode of Arrow's second season", "the courts", "Kanawha River", "athletics", "isosceles", "1898", "James I", "WFTV", "tennis", "Las Vegas", "Austria", "different women coping with breast cancer", "Harry Nicolaides", "he reached under the counter, grabbed his gun and told the robber to drop the bat and get down on his knees."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6743055022370038}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6428571428571429, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.05405405405405406, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5000000000000001, 0.5490196078431372, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 0.5, 1.0, 0.6666666666666666, 0.5, 0.35294117647058826, 0.0, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.07692307692307693]}}, "before_error_ids": ["mrqa_squad-validation-2336", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-9330", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-4715", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1985"], "SR": 0.578125, "CSR": 0.609375, "retrieved_ids": ["mrqa_squad-train-48916", "mrqa_squad-train-31488", "mrqa_squad-train-64659", "mrqa_squad-train-60955", "mrqa_squad-train-12797", "mrqa_squad-train-35572", "mrqa_squad-train-25340", "mrqa_squad-train-69903", "mrqa_squad-train-48523", "mrqa_squad-train-35997", "mrqa_squad-train-20760", "mrqa_squad-train-77132", "mrqa_squad-train-50001", "mrqa_squad-train-67432", "mrqa_squad-train-58153", "mrqa_squad-train-1933", "mrqa_squad-train-25843", "mrqa_squad-train-63585", "mrqa_squad-train-73465", "mrqa_squad-train-69997", "mrqa_squad-train-73532", "mrqa_squad-train-47375", "mrqa_squad-train-72080", "mrqa_squad-train-8975", "mrqa_squad-validation-2943", "mrqa_hotpotqa-validation-230", "mrqa_squad-validation-3687", "mrqa_squad-validation-8229", "mrqa_triviaqa-validation-7461", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-16546", "mrqa_squad-validation-7613", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-2178", "mrqa_hotpotqa-validation-3944", "mrqa_naturalquestions-validation-4036", "mrqa_searchqa-validation-2579", "mrqa_newsqa-validation-2471", "mrqa_squad-validation-7525", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-2184", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-11406", "mrqa_naturalquestions-validation-6484", "mrqa_hotpotqa-validation-2378", "mrqa_naturalquestions-validation-5348", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1279"], "EFR": 0.8518518518518519, "Overall": 0.7474797453703704}, {"timecode": 22, "before_eval_results": {"predictions": ["literacy and numeracy, craftsmanship or vocational training, the arts, religion, civics, community roles, or life skills", "bark of mulberry trees", "drama series", "1806", "distributive efficiency", "on issues related to the substance of the statement", "23 cities", "Continental drift", "Frank Oz", "1975", "775 rooms", "Kimberlin Brown", "AD 95 -- 110", "the status line", "the disk, about 26,000 light - years from the galaxy Center, on the inner edge of the Nebula Arm, one of the spiral - shaped concentrations of gas and dust", "absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "handheld subscriber equipment", "Weston - super-Mare, which stood in for Clevedon", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Javier Fern\u00e1ndez", "a lightning strike", "a landmark decision by the United States Supreme Court on US labor law and constitutional law", "Coton in the Elms", "crossbar", "Wakanda and the Savage Land", "1992", "ATP", "shortwave radio", "a place of trade, entertainment, and education", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively", "Robert Hooke", "interstellar space", "Alicia Vikander as Lara Croft,", "gay", "the name announcement of Kylie Jenner's first child", "approximately 5 liters", "somatic cell nuclear transfer", "Betty", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "June 8, 2009", "head - up display", "a presidential representative democratic republic, whereby the President of El Salvador is both head of state and head of government, and of an Executive power is exercised by the government", "Ferm\u00edn Francisco de Lasu\u00e9n", "a moral tale", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "1994 season", "Spanish / Basque origin", "Laura Jane Haddock", "Atlanta", "2002", "the nasal septum", "a coffee house", "Chief Inspector of Prisons", "Cheshire", "#364", "24800 mi", "liberal revolutions of 1848", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Matamoros, Mexico,", "a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "X-Files", "authentication", "inseparable"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7076935381332634}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true], "QA-F1": [0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6315789473684211, 0.9166666666666666, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.7999999999999999, 0.4, 1.0, 0.14285714285714288, 0.9090909090909091, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1802", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-9141", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-3633"], "SR": 0.578125, "CSR": 0.6080163043478262, "retrieved_ids": ["mrqa_squad-train-43706", "mrqa_squad-train-25657", "mrqa_squad-train-73447", "mrqa_squad-train-49624", "mrqa_squad-train-48208", "mrqa_squad-train-3886", "mrqa_squad-train-71536", "mrqa_squad-train-17908", "mrqa_squad-train-38060", "mrqa_squad-train-11815", "mrqa_squad-train-79873", "mrqa_squad-train-23159", "mrqa_squad-train-42856", "mrqa_squad-train-72773", "mrqa_squad-train-54457", "mrqa_squad-train-34634", "mrqa_squad-train-73378", "mrqa_squad-train-16180", "mrqa_squad-train-73438", "mrqa_squad-train-45991", "mrqa_squad-train-76657", "mrqa_squad-train-69223", "mrqa_squad-train-80479", "mrqa_squad-train-52586", "mrqa_searchqa-validation-971", "mrqa_newsqa-validation-1012", "mrqa_naturalquestions-validation-10554", "mrqa_searchqa-validation-2714", "mrqa_naturalquestions-validation-7967", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-5328", "mrqa_searchqa-validation-4367", "mrqa_naturalquestions-validation-3558", "mrqa_squad-validation-1938", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-6524", "mrqa_newsqa-validation-4059", "mrqa_hotpotqa-validation-2582", "mrqa_searchqa-validation-16546", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-3028", "mrqa_squad-validation-3811", "mrqa_newsqa-validation-3043", "mrqa_squad-validation-3021", "mrqa_searchqa-validation-2141", "mrqa_squad-validation-7332", "mrqa_squad-validation-257", "mrqa_triviaqa-validation-4886"], "EFR": 0.9259259259259259, "Overall": 0.7620228210547504}, {"timecode": 23, "before_eval_results": {"predictions": ["100,000", "an extensive neoclassical centre referred to as Tyneside Classical", "algebraic", "his birthtown, Smiljan", "Persia", "ABC-DuMont", "bathtub curve", "First World War", "John Constable", "Charlie Harper", "Salve", "Duncan", "Everton", "September", "cogito ergo sum", "Bull Moose Party", "the Nelson Bridge", "Demi Moore", "the College of Cardinals", "Cornell", "Robert Stroud", "Alice in Wonderland", "caffeine", "\"The Blind Side\"", "11", "17", "Achille Lauro", "Quentin Tarantino", "Bert Jones", "New York", "Wyatt", "Chuck Hagel", "Haiti", "Bangladesh", "argument form", "Claire Goose", "one king, one queen, two bishops, two knights, and eight pawns", "Bristol", "Vert", "Andy Murray", "Independence Day", "a phantom 8-ender", "Hanseatic League", "Crusades", "Geoffrey", "Thundercats", "Nursery Comics", "The European Council", "Volkswagen", "George IV", "Kalavinka", "United States", "Edward Seton", "Thomas Jefferson", "the central plains", "William Adelin", "Barbary pirates", "Sir William Collins", "to hold onto his land", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf, police said.", "computer security expert Tadayoshi Kohno of the University of Washington.", "Port In A Storm", "the Lone Star", "Bahadur Shah Zafar II"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6689583333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.08, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5180", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-5816", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7212", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2325", "mrqa_naturalquestions-validation-1782", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2371", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-13686"], "SR": 0.609375, "CSR": 0.6080729166666667, "retrieved_ids": ["mrqa_squad-train-58975", "mrqa_squad-train-24960", "mrqa_squad-train-9882", "mrqa_squad-train-76242", "mrqa_squad-train-40835", "mrqa_squad-train-19046", "mrqa_squad-train-30816", "mrqa_squad-train-55171", "mrqa_squad-train-21552", "mrqa_squad-train-38752", "mrqa_squad-train-78970", "mrqa_squad-train-67969", "mrqa_squad-train-18032", "mrqa_squad-train-5722", "mrqa_squad-train-60653", "mrqa_squad-train-36033", "mrqa_squad-train-85857", "mrqa_squad-train-57202", "mrqa_squad-train-58013", "mrqa_squad-train-74096", "mrqa_squad-train-65780", "mrqa_squad-train-22726", "mrqa_squad-train-1186", "mrqa_squad-train-69349", "mrqa_squad-validation-2000", "mrqa_naturalquestions-validation-3028", "mrqa_squad-validation-6957", "mrqa_searchqa-validation-12440", "mrqa_naturalquestions-validation-2309", "mrqa_newsqa-validation-412", "mrqa_squad-validation-8595", "mrqa_naturalquestions-validation-9130", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-2761", "mrqa_naturalquestions-validation-6166", "mrqa_newsqa-validation-2233", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-2476", "mrqa_searchqa-validation-16378", "mrqa_naturalquestions-validation-289", "mrqa_squad-validation-6228", "mrqa_searchqa-validation-11991", "mrqa_hotpotqa-validation-516", "mrqa_naturalquestions-validation-2102", "mrqa_newsqa-validation-3214", "mrqa_searchqa-validation-13900", "mrqa_newsqa-validation-1514"], "EFR": 1.0, "Overall": 0.7768489583333335}, {"timecode": 24, "before_eval_results": {"predictions": ["Napoleon", "Victorian Alps in the northeast", "skin damage", "three", "European Parliament and the Council of the European Union", "Steve McQueen", "\u00c9dith Piaf", "guitar", "Midtown", "bogge", "the finest luxury shoes and boots from the finest of Spanish cordovan leather", "boxer", "Geneva", "Call for the Dead", "Woodrow Wilson", "Menorca", "Wales", "meadowbank Thistle", "Bulldog Drummond", "distance selling", "Edward VI", "raw linseed oil", "Mercury", "trumpet", "architecture", "james bond", "Iain Banks", "Andalusia", "gluten", "Jan Van Eyck", "Thames", "dalton\u00b7ism", "Peter Davison", "Yasmin Aga Khan", "World War II", "King Leonidas", "June Brae", "Yosemite", "the Sandstone Trail", "king duncan", "8 minutes", "uranium", "Old Betsy", "a bra", "West Point", "Cecilia", "a secularization of ritualistic", "ichak", "Whittle", "The Watsons", "Loose ends", "Chester", "the Brazilian state of Mato Grosso to its confluence with the Paran\u00e1 River north of Corrientes and Resistencia", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "2005", "Wes Craven", "1698", "Bill Clinton", "an Airbus A320-214", "a broken pelvis,", "246", "the carnival season", "The Treasure of the Sierra Madre", "smallpox"], "metric_results": {"EM": 0.5, "QA-F1": 0.5882378472222223}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2911", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-3769", "mrqa_triviaqa-validation-5981", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-4073", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-2151", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-2426", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-1857"], "SR": 0.5, "CSR": 0.60375, "retrieved_ids": ["mrqa_squad-train-46531", "mrqa_squad-train-84409", "mrqa_squad-train-15050", "mrqa_squad-train-17831", "mrqa_squad-train-68882", "mrqa_squad-train-67015", "mrqa_squad-train-39669", "mrqa_squad-train-74573", "mrqa_squad-train-510", "mrqa_squad-train-43328", "mrqa_squad-train-43215", "mrqa_squad-train-44411", "mrqa_squad-train-8713", "mrqa_squad-train-46640", "mrqa_squad-train-39114", "mrqa_squad-train-75529", "mrqa_squad-train-46285", "mrqa_squad-train-30981", "mrqa_squad-train-30415", "mrqa_squad-train-81013", "mrqa_squad-train-21893", "mrqa_squad-train-41211", "mrqa_squad-train-2944", "mrqa_squad-train-49068", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-12611", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-8934", "mrqa_squad-validation-6655", "mrqa_naturalquestions-validation-3329", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-989", "mrqa_newsqa-validation-216", "mrqa_searchqa-validation-6234", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6500", "mrqa_newsqa-validation-2233", "mrqa_hotpotqa-validation-3020", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-6328", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-2141", "mrqa_naturalquestions-validation-6453", "mrqa_squad-validation-3985", "mrqa_squad-validation-3113", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-430"], "EFR": 0.96875, "Overall": 0.769734375}, {"timecode": 25, "before_eval_results": {"predictions": ["Thoreau", "Thomas Piketty", "1,548", "zoning and building code requirements", "Science and Discovery", "Lake Placid", "manhattan", "Vietnam", "a non-speaking character", "bluebird", "wherry", "300", "1939", "mania", "Piers Morgan", "mansions", "Billie Holiday", "the National Council for the Unmarried Mother", "Mickelson", "Jean-Paul Sartre", "Len Deighton", "tartan", "Alex Garland", "L. Pasteur", "Dionysus", "Benjamin Disraeli", "Johannesburg", "Martin Luther King", "Bridgeport", "Aidensfield", "a point in the first set", "blue tang", "David Cameron", "Newfoundland and Labrador", "Eddie Cochran", "Alessandro Giuseppe Antonio Anastasio Volta", "OutKast", "Wanderers", "sunshine", "Biafra secession", "Tina Turner", "Flint", "Cuba", "dove", "Heston Blumenthal", "Harold Godwinson", "John Arthur Johnson", "Ritchie Valens", "posh", "Toyota", "Bristol", "Gargantua", "Krypton", "Cyanea capillata", "if the concentration of a compound exceeds its solubility", "The Bob Edwards Show", "Franz Ferdinand", "Paul John Manafort Jr.", "the underprivileged", "Demi Moore and Alicia Keys", "80", "Maldives", "Matt Leinart", "Max Planck"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6221172057109557}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.923076923076923, 0.0, 0.0, 0.25, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-3418", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-5293", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2965", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4832", "mrqa_hotpotqa-validation-3714", "mrqa_newsqa-validation-439", "mrqa_searchqa-validation-13257"], "SR": 0.53125, "CSR": 0.6009615384615384, "retrieved_ids": ["mrqa_squad-train-13373", "mrqa_squad-train-63020", "mrqa_squad-train-8153", "mrqa_squad-train-19661", "mrqa_squad-train-30813", "mrqa_squad-train-67488", "mrqa_squad-train-63306", "mrqa_squad-train-16312", "mrqa_squad-train-17879", "mrqa_squad-train-82919", "mrqa_squad-train-33370", "mrqa_squad-train-8627", "mrqa_squad-train-5634", "mrqa_squad-train-79790", "mrqa_squad-train-22303", "mrqa_squad-train-3989", "mrqa_squad-train-14273", "mrqa_squad-train-9316", "mrqa_squad-train-64413", "mrqa_squad-train-41482", "mrqa_squad-train-84061", "mrqa_squad-train-73475", "mrqa_squad-train-58572", "mrqa_squad-train-4000", "mrqa_searchqa-validation-7724", "mrqa_squad-validation-6957", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-2606", "mrqa_squad-validation-4918", "mrqa_newsqa-validation-3721", "mrqa_squad-validation-978", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-2143", "mrqa_newsqa-validation-1360", "mrqa_hotpotqa-validation-4418", "mrqa_squad-validation-4883", "mrqa_naturalquestions-validation-7554", "mrqa_newsqa-validation-2735", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-2462", "mrqa_searchqa-validation-792", "mrqa_naturalquestions-validation-8356", "mrqa_newsqa-validation-469", "mrqa_triviaqa-validation-7401", "mrqa_newsqa-validation-3404", "mrqa_searchqa-validation-4032", "mrqa_hotpotqa-validation-2009"], "EFR": 1.0, "Overall": 0.7754266826923077}, {"timecode": 26, "before_eval_results": {"predictions": ["paid professionals", "Basel", "\"we want to practice Christian love toward them and pray that they convert,\"", "Informal rule", "animals", "raven", "argon", "John Logie Baird", "london", "Pickwick", "the Titanic", "Benjamin Britten", "taekwondo", "Gibraltar", "Rome", "florita", "bone", "Len Hutton", "bury", "oxygen", "magnesium", "Roy Keane", "Venus", "Ben Watson", "French", "Jupiter", "\u201cIf\u2013\u201d", "The Union Gap", "johnson johnson", "heaven", "Australia", "cerebrospinal fluid", "Looney Tunes", "Ely", "Netherlands", "Vladivostok", "Boiling", "beetles", "phoenicia", "Norwegian", "Gulf of Suez", "beard", "lichfield Cathedral", "Lithium", "salema", "America", "(ch.1, p. 49-50)", "tempera", "Rio", "peacock", "china", "china", "6ft 1in", "Judith Cynthia Aline Keppel", "94 by 50 feet", "Elisha Nelson Manning", "photographs, film and television", "March 17, 2015", "AbdulMutallab", "an eye for an eye", "three", "Prince Edward Island", "Dennis Miller", "May"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6891927083333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.375, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2368", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-1605", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-6046", "mrqa_hotpotqa-validation-3563"], "SR": 0.65625, "CSR": 0.6030092592592593, "retrieved_ids": ["mrqa_squad-train-40345", "mrqa_squad-train-44716", "mrqa_squad-train-27817", "mrqa_squad-train-52184", "mrqa_squad-train-58062", "mrqa_squad-train-41389", "mrqa_squad-train-218", "mrqa_squad-train-65364", "mrqa_squad-train-14078", "mrqa_squad-train-45731", "mrqa_squad-train-27457", "mrqa_squad-train-48878", "mrqa_squad-train-33585", "mrqa_squad-train-33812", "mrqa_squad-train-79373", "mrqa_squad-train-21860", "mrqa_squad-train-30785", "mrqa_squad-train-60434", "mrqa_squad-train-33218", "mrqa_squad-train-37769", "mrqa_squad-train-61902", "mrqa_squad-train-39724", "mrqa_squad-train-29131", "mrqa_squad-train-4440", "mrqa_naturalquestions-validation-4036", "mrqa_newsqa-validation-3785", "mrqa_squad-validation-3998", "mrqa_searchqa-validation-13332", "mrqa_hotpotqa-validation-511", "mrqa_squad-validation-3687", "mrqa_hotpotqa-validation-3714", "mrqa_triviaqa-validation-7056", "mrqa_squad-validation-7136", "mrqa_hotpotqa-validation-558", "mrqa_triviaqa-validation-2361", "mrqa_naturalquestions-validation-5739", "mrqa_searchqa-validation-3479", "mrqa_squad-validation-9161", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-6524", "mrqa_hotpotqa-validation-4102", "mrqa_naturalquestions-validation-9715", "mrqa_hotpotqa-validation-1534", "mrqa_searchqa-validation-4715", "mrqa_newsqa-validation-3214", "mrqa_hotpotqa-validation-2213", "mrqa_newsqa-validation-2900", "mrqa_naturalquestions-validation-2196"], "EFR": 0.9545454545454546, "Overall": 0.7667453177609428}, {"timecode": 27, "before_eval_results": {"predictions": ["the sidelines", "15th", "60%", "Xbox One", "two", "It will be the golfer's first public appearance since his November 27 car crash outside his home near Orlando, Florida.", "the National Restaurant Association", "\"wipe out\" the United States", "Two suspects are in custody.", "an older generation", "project work", "the 1962 \"Tete de Cheval\" (\"Horse's Head\") and the 1944 \"Verre et Pichet\" (\"Glass and Pitcher\") by Picasso", "that they are angry and scared, even though they're content with their own personal circumstances.", "Elena Kagan", "Mandi Hamlin", "750", "launch a long-range missile", "Clarkson", "Expedia", "Juliet", "allergic reaction to peanuts,", "jobs", "between government soldiers and Taliban militants", "Rawalpindi", "the county jail in Spanishfork,", "Sunday", "four", "a residential area in East Java", "Johannesburg", "nearly $2 billion", "Six members of Zoe's Ark", "2002", "the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "\"It has never been the policy of this president or this administration to torture.\"", "Former Mobile County Circuit Judge Herman Thomas", "Cairo", "As mayor of Seoul from 2002 to 2004,", "Kerstin and two of her brothers,", "\"the evidence and investigatory effort has minimized the likelihood that Haleigh's disappearance is the work of a strangers.\"", "made out of either heavy flannel or wool -- fabrics that would not be transparent when wet -- and covered the entire body from neck to toe", "Melbourne", "into the Southeast,", "Sunday", "$273 million", "through a facility in Salt Lake City, Utah,", "millionaire's surtax", "an animal tranquilizer,", "Section 60", "NATO's International Security Assistance Force", "Jaime Andrade", "1994", "Dance Your Ass Off", "Santiago Ram\u00f3n y Cajal", "Los Angeles", "a fortified complex at the heart of Moscow", "magician", "on the Tyne", "A New Generation", "Lin-Manuel Miranda", "15,024", "novelist and poet", "mantle", "lab-Tested, Muppet-Vetted Formulas for Smartifying Your Life", "marshmallows"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5166231903663969}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.1739130434782609, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.2857142857142857, 0.9, 1.0, 0.6, 0.0, 0.14285714285714288, 0.25, 0.0, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-1534", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-5756", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-4411", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1864", "mrqa_searchqa-validation-13251"], "SR": 0.390625, "CSR": 0.5954241071428572, "retrieved_ids": ["mrqa_squad-train-70761", "mrqa_squad-train-66247", "mrqa_squad-train-65197", "mrqa_squad-train-68891", "mrqa_squad-train-66255", "mrqa_squad-train-36886", "mrqa_squad-train-49261", "mrqa_squad-train-58298", "mrqa_squad-train-41667", "mrqa_squad-train-49993", "mrqa_squad-train-51616", "mrqa_squad-train-46742", "mrqa_squad-train-60245", "mrqa_squad-train-15411", "mrqa_squad-train-48164", "mrqa_squad-train-13927", "mrqa_squad-train-64201", "mrqa_squad-train-28555", "mrqa_squad-train-41790", "mrqa_squad-train-21289", "mrqa_squad-train-53823", "mrqa_squad-train-20797", "mrqa_squad-train-34149", "mrqa_squad-train-82742", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-6046", "mrqa_searchqa-validation-5149", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-2965", "mrqa_newsqa-validation-1855", "mrqa_searchqa-validation-11406", "mrqa_squad-validation-3946", "mrqa_squad-validation-8164", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-10536", "mrqa_squad-validation-6228", "mrqa_hotpotqa-validation-3714", "mrqa_triviaqa-validation-5209", "mrqa_newsqa-validation-3915", "mrqa_searchqa-validation-9390", "mrqa_triviaqa-validation-4317", "mrqa_squad-validation-8459", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-1372", "mrqa_searchqa-validation-4169", "mrqa_squad-validation-3770", "mrqa_newsqa-validation-250"], "EFR": 0.9230769230769231, "Overall": 0.7589345810439561}, {"timecode": 28, "before_eval_results": {"predictions": ["Denver's Executive Vice President of Football Operations and General Manager", "illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins", "Pittsburgh", "Cress", "molecular clouds in interstellar space", "Stefanie Scott", "the highway between the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "Ram Nath Kovind", "Senator Joseph McCarthy", "100 lakh", "members of the gay ( LGBT ) community", "Copper ( Cu ), silver ( Ag ), and gold ( Au )", "Wembley Stadium", "more than a million", "1775", "Continental drift", "Julie Adams", "a combination of genetics and the male hormone dihydrotestosterone", "Jonathan Cheban", "Norman Greenbaum", "De Waynene Warren", "Thirty years after the Galactic Civil War", "restoring someone's faith in love and family relationships", "an unknown species", "April 15, 2018", "April 17, 1982", "the Speaker of the House of Representatives", "London, United Kingdom", "a minority report", "as an extension to this procedure", "its population", "Club Bijou on Chapel Street", "pre-Columbian times", "central plains", "five states", "costume party", "directly into the bloodstream", "Kenny Anderson", "beneath the liver", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "Nathan Hale", "Jesse Frederick James Conaway", "the naos", "defense against rain rather than sun", "the colonization of the Americas began and the cocoa plant was discovered in regions of Mesoamerica, until the present", "September 19, 2017", "West Egg on prosperous Long Island in the summer of 1922", "it was first published on November 12, 1976 by Ballantine Books.", "Butter Island off North Haven, Maine in the Penobscot Bay", "wintertime", "Tony Rydinger", "singers Laura Williams and Sally Dworsky", "Flanagan", "islands of Miquelon and Saint Pierre", "Mediterranean Sea", "Manor of More", "Jeffrey Perry", "the Democratic Unionist Party (DUP)", "military personnel", "1964", "\"The Orchid thief\"", "sacrificed CLEAN animals to YHWH", "Balfour Declaration", "Gordon"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5494297781413202}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.823529411764706, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.125, 1.0, 0.7368421052631577, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-386", "mrqa_squad-validation-6848", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-6349", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-3145", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-2150", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1570", "mrqa_searchqa-validation-4495"], "SR": 0.421875, "CSR": 0.5894396551724138, "retrieved_ids": ["mrqa_squad-train-43608", "mrqa_squad-train-85675", "mrqa_squad-train-2795", "mrqa_squad-train-51954", "mrqa_squad-train-77213", "mrqa_squad-train-20627", "mrqa_squad-train-1870", "mrqa_squad-train-53490", "mrqa_squad-train-8412", "mrqa_squad-train-45480", "mrqa_squad-train-63788", "mrqa_squad-train-61126", "mrqa_squad-train-47144", "mrqa_squad-train-18162", "mrqa_squad-train-50024", "mrqa_squad-train-45252", "mrqa_squad-train-80857", "mrqa_squad-train-44360", "mrqa_squad-train-65403", "mrqa_squad-train-52152", "mrqa_squad-train-74536", "mrqa_squad-train-85455", "mrqa_squad-train-77312", "mrqa_squad-train-26401", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-5860", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-16558", "mrqa_naturalquestions-validation-7080", "mrqa_searchqa-validation-9822", "mrqa_squad-validation-3998", "mrqa_searchqa-validation-3479", "mrqa_squad-validation-739", "mrqa_naturalquestions-validation-4544", "mrqa_searchqa-validation-16378", "mrqa_triviaqa-validation-1917", "mrqa_newsqa-validation-1012", "mrqa_naturalquestions-validation-1187", "mrqa_hotpotqa-validation-3157", "mrqa_triviaqa-validation-5756", "mrqa_naturalquestions-validation-4348", "mrqa_searchqa-validation-13554", "mrqa_newsqa-validation-2184", "mrqa_searchqa-validation-2761", "mrqa_triviaqa-validation-4668", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-6484"], "EFR": 0.8918918918918919, "Overall": 0.7515006844128612}, {"timecode": 29, "before_eval_results": {"predictions": ["Antigone", "the Meuse", "1806", "New Delhi", "MacFarlane", "13 -- 3 Eagles soared past the Minnesota Vikings and the Atlanta Falcons in the playoffs, earning a trip to Super Bowl XXXIX in Jacksonville against the defending champion New England Patriots", "Hon July Moyo and the deputy minister is Sesel Zvidzai", "many forested parts of the world", "Narendra Modi", "living and organic material", "Aaron Harrison", "The White House Executive chef", "Michael Crawford", "9 February 2018", "the red bone marrow of large bones", "Drew Barrymore", "Pangaea", "Jonathan Breck", "dermis", "Joe Pizzulo and Leeza Miller", "Ming", "201", "Chuck Noland", "Montreal Canadiens", "Britney Spears", "Waylon Jennings", "Nancy Jean Cartwright", "Coldplay", "Ephesus", "New York Yankees", "1996", "when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "United States customary units", "Joe Spano", "Michael Moriarty", "Rock Island, Illinois", "2002", "September 1959", "Louis Hynes", "Bonnie Lipton", "denigrating incumbent Democrat Martin Van Buren", "the probability of rejecting the null hypothesis given that it is true", "Poems : Series 1", "Kol", "the kitchen", "Tagalog or English", "Ernest Rutherford", "Napoleon Bonaparte", "the 12th century", "Yosemite National Park", "Norman Pritchard", "2014", "Mustelidae", "stars and galaxies,", "King Henry VI", "October 13, 1980", "250cc world championship", "Polihale State Park", "Justice Department motion filed last week in support of the Defense of Marriage Act", "Bronx.", "almost 9 million", "\"Tennessee Waltz\"", "swan pan", "Cyrus"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6795924077602505}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.5, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.24000000000000002, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.375, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-3760", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-5106", "mrqa_hotpotqa-validation-1489", "mrqa_newsqa-validation-1426", "mrqa_searchqa-validation-7895"], "SR": 0.5625, "CSR": 0.5885416666666667, "retrieved_ids": ["mrqa_squad-train-85723", "mrqa_squad-train-29681", "mrqa_squad-train-8619", "mrqa_squad-train-41871", "mrqa_squad-train-22458", "mrqa_squad-train-20542", "mrqa_squad-train-55013", "mrqa_squad-train-3440", "mrqa_squad-train-76824", "mrqa_squad-train-40839", "mrqa_squad-train-56043", "mrqa_squad-train-1461", "mrqa_squad-train-46210", "mrqa_squad-train-44616", "mrqa_squad-train-71932", "mrqa_squad-train-33122", "mrqa_squad-train-49240", "mrqa_squad-train-21416", "mrqa_squad-train-81293", "mrqa_squad-train-79855", "mrqa_squad-train-22208", "mrqa_squad-train-78740", "mrqa_squad-train-68508", "mrqa_squad-train-47255", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-13332", "mrqa_newsqa-validation-1300", "mrqa_triviaqa-validation-1605", "mrqa_naturalquestions-validation-3066", "mrqa_triviaqa-validation-2000", "mrqa_newsqa-validation-3500", "mrqa_hotpotqa-validation-132", "mrqa_triviaqa-validation-77", "mrqa_newsqa-validation-4064", "mrqa_naturalquestions-validation-8227", "mrqa_squad-validation-3770", "mrqa_searchqa-validation-10856", "mrqa_triviaqa-validation-1463", "mrqa_squad-validation-4298", "mrqa_squad-validation-8786", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-3558", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-3028", "mrqa_newsqa-validation-1496"], "EFR": 0.9642857142857143, "Overall": 0.7657998511904761}, {"timecode": 30, "UKR": 0.806640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-392", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-974", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-13874", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15740", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-1649", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9269", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10181", "mrqa_squad-validation-10268", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1765", "mrqa_squad-validation-1791", "mrqa_squad-validation-1848", "mrqa_squad-validation-1890", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-2019", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2243", "mrqa_squad-validation-2411", "mrqa_squad-validation-2456", "mrqa_squad-validation-247", "mrqa_squad-validation-2545", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2742", "mrqa_squad-validation-305", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3507", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3718", "mrqa_squad-validation-3770", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-386", "mrqa_squad-validation-3863", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3986", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-4046", "mrqa_squad-validation-4054", "mrqa_squad-validation-4175", "mrqa_squad-validation-4213", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4629", "mrqa_squad-validation-4883", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5097", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5588", "mrqa_squad-validation-5692", "mrqa_squad-validation-5724", "mrqa_squad-validation-5781", "mrqa_squad-validation-5818", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-6019", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6353", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6543", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6965", "mrqa_squad-validation-6973", "mrqa_squad-validation-6999", "mrqa_squad-validation-7039", "mrqa_squad-validation-71", "mrqa_squad-validation-7192", "mrqa_squad-validation-7368", "mrqa_squad-validation-7426", "mrqa_squad-validation-7521", "mrqa_squad-validation-7612", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7814", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-829", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9176", "mrqa_squad-validation-9196", "mrqa_squad-validation-942", "mrqa_squad-validation-9445", "mrqa_squad-validation-957", "mrqa_squad-validation-9614", "mrqa_squad-validation-9764", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4465", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7461"], "OKR": 0.869140625, "KG": 0.49140625, "before_eval_results": {"predictions": ["flammable cabin and space suit materials", "1992", "at least four", "Genesis", "real estate investment trust", "Louisiana's Bayou", "the carat", "Mission: Impossible", "deacon", "Edinburgh", "Teha'amana", "Galpagos", "Bill Murray", "Battle of Chancellorsville", "boxing", "Wii", "the Suez Canal", "Dave Matthews Band", "a nightingale", "dental bridges", "The Hatched Guide to Friday the 13th", "copy center chain", "jeopardy/2109_Qs.txt at master", "a photon", "skull", "Cherokee", "nekropolis", "Eleanor Roosevelt", "Grand Central Oyster Bar", "fortune", "Saturday Night Live", "bamboos", "Sir Isaac Newton", "'Black Dahlia'", "Narnia", "Freud", "Burma Ruby stone", "librettos", "Jonathan Swift", "Who's Afraid of Virginia Woolf", "Medium", "bison", "an American sitcom and a spin-off of Sanford and Son that aired on NBC from December 4, 1975 to March 11, 1976", "Botswana", "Susan B. Anthony dollar", "Mattel", "Little Red Riding Hood", "spoon splint", "milk", "Slavic", "nerves", "jelly doughnut holes", "infection, irritation, or allergies", "Castleford is a town in the metropolitan borough of Wakefield, West Yorkshire, England", "annually ( usually in May ) at the Palais des Festivals et des Congr\u00e8s", "Hillsborough", "Gobi desert", "Sherlock Holmes", "You're Next", "Headless Body in Topless Bar", "political correctness", "Hanin Zoabi", "Princess Diana,", "homicide"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6221564440993789}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2608695652173913, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 0.16666666666666669, 0.42857142857142855, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3542", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-12684", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-7004", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-12355", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-16480", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-382", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-2855", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-6237"], "SR": 0.53125, "CSR": 0.5866935483870968, "retrieved_ids": ["mrqa_squad-train-57504", "mrqa_squad-train-74306", "mrqa_squad-train-49175", "mrqa_squad-train-43642", "mrqa_squad-train-60054", "mrqa_squad-train-60014", "mrqa_squad-train-2458", "mrqa_squad-train-65686", "mrqa_squad-train-41111", "mrqa_squad-train-39884", "mrqa_squad-train-54647", "mrqa_squad-train-29364", "mrqa_squad-train-44676", "mrqa_squad-train-38027", "mrqa_squad-train-46013", "mrqa_squad-train-35100", "mrqa_squad-train-62137", "mrqa_squad-train-26230", "mrqa_squad-train-53574", "mrqa_squad-train-16002", "mrqa_squad-train-83374", "mrqa_squad-train-12233", "mrqa_squad-train-39347", "mrqa_squad-train-59791", "mrqa_naturalquestions-validation-6759", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2178", "mrqa_searchqa-validation-15508", "mrqa_squad-validation-5724", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-2605", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-339", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-6328", "mrqa_newsqa-validation-2886", "mrqa_hotpotqa-validation-2009", "mrqa_naturalquestions-validation-5579", "mrqa_newsqa-validation-1300", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-10321", "mrqa_searchqa-validation-8665", "mrqa_newsqa-validation-1985", "mrqa_naturalquestions-validation-1974", "mrqa_newsqa-validation-1456", "mrqa_triviaqa-validation-3004", "mrqa_squad-validation-10168"], "EFR": 1.0, "Overall": 0.7507762096774193}, {"timecode": 31, "before_eval_results": {"predictions": ["the West", "Begter", "beginning in 2016", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor )", "the University of Oxford", "July 1, 1923", "a pH indicator, a color marker, and a dye", "winter", "Freedom Day 27 April 2000", "Meghalaya ( formerly Assam )", "either in front or on top of the brainstem", "Janie Crawford", "Jim Capaldi, Paul Carrack, and Peter Vale", "displacement is zero as start and end points coincide", "786", "Department of Health and Human Services", "Blind carbon copy to tertiary recipients who receive the message", "the eye ( of ) round, bottom round, and top round, with or without the `` round '' bone ( femur )", "1957", "Martin Lawrence", "An error does not count as a hit but still counts as an at bat for the batter", "Andreas Vesalius", "Moscazzano", "Kristy Swanson", "detritus", "Asuka", "Jay Baruchel", "Oceania", "revolution or orbital revolution", "Houston Astros", "Six Degrees of Separation", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "the retina", "fascia surrounding skeletal muscle", "Pangaea", "2012", "near the inner rim of the Orion Arm, within the Local Fluff of the Local Bubble, and in the Gould Belt", "Ricky Nelson", "a `` dabber '' or `` dauber ''", "Debbie Gibson", "Hagrid", "the Mishnah", "a column - like or oval ( egg - shaped ) symbol of Shiva", "the first week of April", "Algeria", "the King James Bible", "Harlem River", "1998", "R.E.M.", "332", "above the light sources and under the sample in an upright microscope, and above the stage and below the light source in an inverted microscope", "Auburn Tigers football team", "Illinois", "Northumberland", "Northern Ireland", "Travis County", "Boston Bruins", "Adam Dawes", "Republicans", "Zed", "a tenement in the Mumbai suburb of Chembur,", "a dummy", "Aristotle", "nothing gained"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7202123077882576}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.5, 0.5, 1.0, 0.0, 0.13953488372093023, 1.0, 1.0, 0.0, 0.19999999999999998, 0.6666666666666666, 1.0, 0.5, 0.11111111111111112, 1.0, 1.0, 0.2978723404255319, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.6956521739130436, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 0.375, 1.0, 1.0, 1.0, 0.6666666666666666, 0.9473684210526315, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-5599", "mrqa_triviaqa-validation-3940", "mrqa_hotpotqa-validation-5831", "mrqa_newsqa-validation-1512"], "SR": 0.578125, "CSR": 0.58642578125, "retrieved_ids": ["mrqa_squad-train-32381", "mrqa_squad-train-53112", "mrqa_squad-train-42625", "mrqa_squad-train-34719", "mrqa_squad-train-42255", "mrqa_squad-train-44544", "mrqa_squad-train-53600", "mrqa_squad-train-9086", "mrqa_squad-train-81118", "mrqa_squad-train-25940", "mrqa_squad-train-4960", "mrqa_squad-train-33427", "mrqa_squad-train-54421", "mrqa_squad-train-34321", "mrqa_squad-train-44497", "mrqa_squad-train-13547", "mrqa_squad-train-42705", "mrqa_squad-train-6901", "mrqa_squad-train-33944", "mrqa_squad-train-48755", "mrqa_squad-train-38572", "mrqa_squad-train-45508", "mrqa_squad-train-60776", "mrqa_squad-train-4656", "mrqa_naturalquestions-validation-10460", "mrqa_newsqa-validation-4041", "mrqa_triviaqa-validation-7212", "mrqa_naturalquestions-validation-10614", "mrqa_searchqa-validation-5172", "mrqa_newsqa-validation-140", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3332", "mrqa_squad-validation-6439", "mrqa_searchqa-validation-971", "mrqa_naturalquestions-validation-2930", "mrqa_newsqa-validation-3503", "mrqa_naturalquestions-validation-9979", "mrqa_hotpotqa-validation-5808", "mrqa_squad-validation-5860", "mrqa_newsqa-validation-3679", "mrqa_squad-validation-2000", "mrqa_searchqa-validation-4394", "mrqa_hotpotqa-validation-5644", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-11532", "mrqa_squad-validation-7708"], "EFR": 0.7777777777777778, "Overall": 0.7062782118055555}, {"timecode": 32, "before_eval_results": {"predictions": ["magnitude and direction", "2003", "1982", "National Aviation Hall of Fame class of 2001", "Giotto", "1985", "more than 26,000", "Lakshmibai", "Premier League", "French", "2009", "a homebrew campaign setting", "paniscus", "the title character", "Greg Gorman and Helmut Newton", "Shameless", "stolperstein", "1901", "Carl Zeiss AG", "YouTube", "Bambi, a Life in the Woods", "Robert \"Bobby\" Germaine", "2004", "IndyCar", "one", "\"Twice in a Lifetime\"", "the Sun", "Greg Hertz", "Kolkata", "\"The Walking Dead\"", "Ted Nugent", "jewelry designer", "Gust Avrakotos", "Charles Perrault's", "Coll\u00e8ge de France", "Hard Rock Stadium", "Marty Ingels", "1945", "Edward R. Murrow", "Conservatorio Verdi", "Mindy Kaling", "June 10, 1982", "beer and soft drinks", "Liga MX", "Donald Duck", "The School Boys", "Lord Chancellor of England", "Taoiseach", "The English Electric Canberra", "Richa Sharma", "48,982", "The Sound of Music", "53", "Michigan State Spartans", "Frank Langella", "an elephant", "a cuckoo", "chloronium", "Stansted", "homicide", "maintain an \"aesthetic environment\" and ensure public safety", "Marshal Ptain", "tanks", "Hannah Montana"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6993189102564104}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-1030", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4655", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3726", "mrqa_searchqa-validation-4641", "mrqa_searchqa-validation-4628"], "SR": 0.640625, "CSR": 0.5880681818181819, "retrieved_ids": ["mrqa_squad-train-26005", "mrqa_squad-train-71476", "mrqa_squad-train-41608", "mrqa_squad-train-43977", "mrqa_squad-train-82923", "mrqa_squad-train-25716", "mrqa_squad-train-36455", "mrqa_squad-train-69313", "mrqa_squad-train-66925", "mrqa_squad-train-42846", "mrqa_squad-train-46668", "mrqa_squad-train-39340", "mrqa_squad-train-64979", "mrqa_squad-train-46513", "mrqa_squad-train-13081", "mrqa_squad-train-52354", "mrqa_squad-train-6617", "mrqa_squad-train-9169", "mrqa_squad-train-43460", "mrqa_squad-train-64978", "mrqa_squad-train-70048", "mrqa_squad-train-38853", "mrqa_squad-train-18005", "mrqa_squad-train-79690", "mrqa_hotpotqa-validation-2887", "mrqa_naturalquestions-validation-7212", "mrqa_searchqa-validation-1053", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-6610", "mrqa_searchqa-validation-16076", "mrqa_newsqa-validation-1534", "mrqa_hotpotqa-validation-3020", "mrqa_triviaqa-validation-1046", "mrqa_squad-validation-6031", "mrqa_hotpotqa-validation-3157", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-3772", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-4411", "mrqa_squad-validation-7612", "mrqa_naturalquestions-validation-7767", "mrqa_searchqa-validation-3633", "mrqa_newsqa-validation-140", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-6821", "mrqa_newsqa-validation-1538"], "EFR": 1.0, "Overall": 0.7510511363636363}, {"timecode": 33, "before_eval_results": {"predictions": ["CEPR", "special university classes, called Lehramtstudien", "CTV", "13\u20133", "a 31-year-old man who has written a popular show tune and who is having marriage troubles", "July 25 to August 4", "1958", "Norway", "twenty-three", "Crips", "The Crowned Prince of the Philadelphia Mob", "Kentucky Derby", "Charles Edward Stuart", "historic buildings, arts, and published works", "August 9, 2017", "Movie Masters", "Franklin", "G\u00e9rard Depardieu", "books, films and other media", "King Duncan", "Europop", "1835", "Mayor Ed Lee", "Ghana", "Norwegian", "Dutch", "1976", "January 23, 1898", "Motorised quadricycle", "30.9%", "Charlyn Marie \" Chan\" Marshall", "1968", "76,416", "Father Dougal McGuire", "June 17, 2007", "Deputy F\u00fchrer", "The United States of America", "Ryukyuan people", "coaxial", "November 15, 1903", "international producers and directed by filmmakers from around the world", "1961", "1951", "Indian", "one child, Lisa Brennan-Jobs", "Pablo Escobar", "ZZ Top", "Larry Gatlin & the Gatlin Brothers Band", "Russian Empire", "Flex-fuel", "Blue Ridge Parkway", "King of Cool", "a subduction zone", "Barry Bonds", "Owen Vaccaro", "Machu Picchu", "Exile", "a downtown caf\u00e9 late at night", "normal maritime traffic", "U Win Tin,", "$1.45 billion", "onomatopoeia", "Singapore", "the femur"], "metric_results": {"EM": 0.625, "QA-F1": 0.7093048878205128}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.7499999999999999, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2043", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-4274", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-3703", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-742", "mrqa_searchqa-validation-15477"], "SR": 0.625, "CSR": 0.5891544117647058, "retrieved_ids": ["mrqa_squad-train-34034", "mrqa_squad-train-14458", "mrqa_squad-train-73816", "mrqa_squad-train-51438", "mrqa_squad-train-75434", "mrqa_squad-train-45543", "mrqa_squad-train-39758", "mrqa_squad-train-9654", "mrqa_squad-train-33932", "mrqa_squad-train-58918", "mrqa_squad-train-14299", "mrqa_squad-train-64392", "mrqa_squad-train-74177", "mrqa_squad-train-74323", "mrqa_squad-train-11323", "mrqa_squad-train-29302", "mrqa_squad-train-10927", "mrqa_squad-train-53696", "mrqa_squad-train-25572", "mrqa_squad-train-22893", "mrqa_squad-train-38388", "mrqa_squad-train-20409", "mrqa_squad-train-81967", "mrqa_squad-train-28765", "mrqa_naturalquestions-validation-7242", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-511", "mrqa_searchqa-validation-12552", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-765", "mrqa_naturalquestions-validation-2006", "mrqa_searchqa-validation-6181", "mrqa_triviaqa-validation-1463", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-8228", "mrqa_hotpotqa-validation-957", "mrqa_squad-validation-1802", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16546", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-11406", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-469", "mrqa_squad-validation-10168", "mrqa_hotpotqa-validation-5464", "mrqa_naturalquestions-validation-4924"], "EFR": 0.9583333333333334, "Overall": 0.7429350490196078}, {"timecode": 34, "before_eval_results": {"predictions": ["Manning", "an upper limit", "25 million", "A simple iron boar crest", "Vienna", "that Fama and French's research is period dependent", "the Harpe brothers", "Bill Clinton", "Dirk Werner Nowitzki", "Detroit, Michigan,", "It is based in Bury St Edmunds, Suffolk, England", "novelty songs, comedy, and strange or unusual recordings", "Mahoning County", "16 November 1973", "\"There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "Bohemia", "New York", "The Washington Post", "400 MW", "Mauritian", "Household Words", "Gatwick Airport", "Kagoshima Airport", "Minette Walters", "CTV", "\"Supergirl\"", "2013", "Les Miles", "40 Days and 40 Nights", "James Tinling", "2014", "Louis King", "gull-wing doors", "Terry Malloy", "Operation Neptune", "Attack the Block", "House of Commons", "Hessians", "Battle of Chester", "Wayne County, Michigan", "Samoa", "mistress of the Robes", "Eleanor of Aquitaine", "Barack Obama's", "August 17, 2017", "Guardians of the Galaxy Vol. 2", "the 41st President of the United States", "1963", "Bologna Process", "Paris", "Nebraska Cornhuskers women's basketball team", "Salman Rushdie", "the Internal Revenue Service", "Mongol Yuan Dynasty", "commemorating fealty and filial piety", "Chihuahua", "Arkansas", "throat", "1979", "his father", "$8.8 million", "Red Heat", "Miriam Makeba", "a mesio-occlusal cavity"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7146081349206349}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-384", "mrqa_squad-validation-10316", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-4143", "mrqa_triviaqa-validation-2316", "mrqa_newsqa-validation-501", "mrqa_searchqa-validation-9394"], "SR": 0.640625, "CSR": 0.590625, "retrieved_ids": ["mrqa_squad-train-691", "mrqa_squad-train-58046", "mrqa_squad-train-19276", "mrqa_squad-train-19022", "mrqa_squad-train-9458", "mrqa_squad-train-68505", "mrqa_squad-train-79444", "mrqa_squad-train-19828", "mrqa_squad-train-15025", "mrqa_squad-train-27946", "mrqa_squad-train-6208", "mrqa_squad-train-55506", "mrqa_squad-train-51115", "mrqa_squad-train-28754", "mrqa_squad-train-70510", "mrqa_squad-train-58078", "mrqa_squad-train-68108", "mrqa_squad-train-25181", "mrqa_squad-train-13213", "mrqa_squad-train-15641", "mrqa_squad-train-66666", "mrqa_squad-train-27890", "mrqa_squad-train-3733", "mrqa_squad-train-15807", "mrqa_hotpotqa-validation-989", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6555", "mrqa_newsqa-validation-3463", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6916", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1021", "mrqa_naturalquestions-validation-3390", "mrqa_searchqa-validation-2495", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-3452", "mrqa_searchqa-validation-4169", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-4309", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_searchqa-validation-11406"], "EFR": 1.0, "Overall": 0.7515625}, {"timecode": 35, "before_eval_results": {"predictions": ["DuMont Television Network", "Mount Kenya", "Albany, New York", "1908", "3 May 1958", "1986 to 2013", "Ronald Wilson Reagan", "Ashridge", "Ted", "Bay of Fundy", "CD Castell\u00f3n", "2001", "Sean Yseult", "country", "Suzanne N.J. 'Susie' Chun Oakland is a Democratic member of the Hawaii Senate, representing the 13th District since 1996", "Guadalcanal Campaign", "Paul W. S. Anderson", "15 February 1970", "Talib Kweli", "Shooter Jennings", "Cincinnati", "Bad Moon Rising", "Rikki Farr", "381.6 days", "Atomic Kitten", "Trey Parker and Matt Stone", "Matthew Edward Gonzalez", "The Gold Coast", "1898", "\u00c6thelred the Unready", "PlayStation 4", "Malta", "1966", "Key West,", "Europe", "Black Mountain College", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies.", "Fabio Cannavaro", "American actor, comedian, and writer", "Prince George's County", "EQT Plaza in Pittsburgh, Pennsylvania", "1891", "L\u00edneas A\u00e9reas", "Gainsborough Trinity", "Los Angeles", "October 13, 1980", "water sprite", "Afghanistan", "Syracuse University", "FIFA Women's World Cup", "Orange County", "76,416", "found in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Quantitative psychological research", "Will", "Deep Blue", "Albert Reynolds", "George Washington", "U.S. senators", "California-based Current TV", "two", "baht", "The Lost Boys", "Succotash"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7241061354425715}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9787234042553191, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4089", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-257", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-10259", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-2595"], "SR": 0.609375, "CSR": 0.5911458333333333, "retrieved_ids": ["mrqa_squad-train-13661", "mrqa_squad-train-35801", "mrqa_squad-train-46546", "mrqa_squad-train-13164", "mrqa_squad-train-39814", "mrqa_squad-train-54763", "mrqa_squad-train-24803", "mrqa_squad-train-29628", "mrqa_squad-train-85989", "mrqa_squad-train-36370", "mrqa_squad-train-47490", "mrqa_squad-train-69443", "mrqa_squad-train-2614", "mrqa_squad-train-34059", "mrqa_squad-train-73683", "mrqa_squad-train-76455", "mrqa_squad-train-61984", "mrqa_squad-train-23242", "mrqa_squad-train-11845", "mrqa_squad-train-33385", "mrqa_squad-train-58583", "mrqa_squad-train-82517", "mrqa_squad-train-19597", "mrqa_squad-train-19731", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-5579", "mrqa_hotpotqa-validation-5644", "mrqa_searchqa-validation-13332", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1007", "mrqa_hotpotqa-validation-4676", "mrqa_squad-validation-7876", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-7009", "mrqa_squad-validation-4326", "mrqa_squad-validation-6957", "mrqa_squad-validation-4715", "mrqa_squad-validation-3998", "mrqa_hotpotqa-validation-3790", "mrqa_squad-validation-7357", "mrqa_naturalquestions-validation-3442", "mrqa_squad-validation-259", "mrqa_searchqa-validation-11271", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-2102", "mrqa_hotpotqa-validation-5228", "mrqa_newsqa-validation-1805"], "EFR": 0.96, "Overall": 0.7436666666666666}, {"timecode": 36, "before_eval_results": {"predictions": ["between June and September", "emergency plans", "\"We Found Love\" in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "a bank", "July for A Country Christmas", "The Casalesi Camorra clan", "Tulsa, Oklahoma.", "41,280", "Old Trafford", "\"release\" civilians", "Number Ones", "Zac Efron", "the Indian embassy in Kabul", "stabbed Tate, who was 8\u00bd months pregnant, and wrote the word \"pig\" in blood on the door of the home", "Fourth time lucky in Atlanta in 1996.", "Annie Duke", "it was legitimate to kill Jewish children anywhere in the world was utterly chilling and beyond any kind of civilised, humanitarian norm.", "Arizona's bill orders immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally.", "producing rock music with a country influence", "The Kirchners", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "root out terrorists within its borders.", "violent separatist campaign", "at the ancient Greek site of Olympia", "3,000", "closing these racial gaps", "Behar", "22", "3-0", "150", "the two remaining crew members from the helicopter", "U.N. agencies", "more than 30", "the man was dead", "only one", "23 million square meters (248 million square feet)", "south of Kabul in the eastern Afghan province of Logar", "Virgin America", "fuel economy and safety while boosting", "American Civil Liberties Union", "summer", "75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "Jason Chaffetz", "mental health and recovery.", "56", "a pool of blood beneath his head", "Frank Ricci", "the Ku Klux Klan", "90", "Cash for Clunkers", "Argentina", "1997", "As of July 2017, there were 103 national parks encompassing an area of 40,500 km ( 15,600 sq mi ), comprising 1.23 % of India's total surface area", "Carolyn Sue Jones", "vanilla", "Hercules", "a guide to the general climate of the regions of the planet", "Gian Carlo Menotti", "50th anniversary of the founding of the National Basketball Association", "Gararish", "v. suzanne valadon", "Daisy Miller", "Zugtelephonie A. G.", "Apollo"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6486965811965812}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.8, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.24, 0.0, 1.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.4, 1.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.0, 1.0, 0.4, 0.0, 0.15384615384615385, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.375, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-978", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-533", "mrqa_naturalquestions-validation-1028", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4389", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-2623"], "SR": 0.5625, "CSR": 0.5903716216216216, "retrieved_ids": ["mrqa_squad-train-52199", "mrqa_squad-train-15933", "mrqa_squad-train-55634", "mrqa_squad-train-11597", "mrqa_squad-train-17304", "mrqa_squad-train-67662", "mrqa_squad-train-82864", "mrqa_squad-train-15400", "mrqa_squad-train-65354", "mrqa_squad-train-55409", "mrqa_squad-train-41792", "mrqa_squad-train-13700", "mrqa_squad-train-82094", "mrqa_squad-train-18454", "mrqa_squad-train-80342", "mrqa_squad-train-63276", "mrqa_squad-train-23169", "mrqa_squad-train-28973", "mrqa_squad-train-68345", "mrqa_squad-train-85345", "mrqa_squad-train-79405", "mrqa_squad-train-4157", "mrqa_squad-train-70272", "mrqa_squad-train-56821", "mrqa_hotpotqa-validation-2205", "mrqa_naturalquestions-validation-4619", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-1011", "mrqa_naturalquestions-validation-9239", "mrqa_squad-validation-3863", "mrqa_searchqa-validation-10372", "mrqa_triviaqa-validation-4307", "mrqa_searchqa-validation-13232", "mrqa_squad-validation-7457", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2769", "mrqa_squad-validation-1780", "mrqa_searchqa-validation-16558", "mrqa_newsqa-validation-469", "mrqa_triviaqa-validation-3004", "mrqa_searchqa-validation-1053", "mrqa_newsqa-validation-3214", "mrqa_searchqa-validation-9394", "mrqa_triviaqa-validation-4143", "mrqa_newsqa-validation-2735", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-2792"], "EFR": 1.0, "Overall": 0.7515118243243243}, {"timecode": 37, "before_eval_results": {"predictions": ["five", "state's attorney", "Abdullah Gul", "Ed McMahon,", "they are co-chair of the Genocide Prevention Task Force.", "off Somalia's coast.", "clogs", "upper respiratory infection", "two", "tells stories of different women coping with breast cancer in five vignettes.", "CNN's Larry King", "Dr. Jennifer Arnold and husband Bill Klein,", "Twitter", "Alwin Landry", "American third seed Venus Williams", "both Won Sei Hoon, who heads South Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim", "Robert Mugabe", "2nd Lt. Holley Wimunc,", "surrender", "J. Crew.", "$17,000", "Hillary Clinton,", "Al-Aqsa mosque", "\"momentous discovery\"", "downtown Nairobi.", "Robert Barnett", "Asia", "Matthew Fisher", "Zimbabwean government", "Ben Roethlisberger", "Five of us", "Pew Research Center", "Jason Bendett", "Brazil", "of his private information", "$24.1 million", "Some 200 potential jurors", "Salt Lake City, Utah,", "on Sunday.", "Robert Mugabe", "13", "One of Osama bin Laden's sons", "for security reasons and not because of their faith.", "\"We tortured (Mohammed al+) Qahtani,\"", "autonomy.", "the Arctic north of Murmansk down to the southern climes of Sochi", "Long Island", "Ma Khin Khin Leh,", "several months", "Kerstin Fritzl,", "plutonium production.", "from the breast or lower chest of beef or veal", "winter", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Elberta", "bullfight", "bachus-Polka", "1887", "Atlantic Coast Conference", "uncle", "Pennsylvania", "the Rabbit", "Brunswick", "Labrador retrievevers"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5699204441391942}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4444444444444445, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 0.75, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.6153846153846153, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-2297", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2448", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-9856", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-5703", "mrqa_searchqa-validation-12609", "mrqa_triviaqa-validation-3505"], "SR": 0.421875, "CSR": 0.5859375, "retrieved_ids": ["mrqa_squad-train-38639", "mrqa_squad-train-23870", "mrqa_squad-train-77976", "mrqa_squad-train-51136", "mrqa_squad-train-15441", "mrqa_squad-train-4119", "mrqa_squad-train-18642", "mrqa_squad-train-49855", "mrqa_squad-train-19272", "mrqa_squad-train-6569", "mrqa_squad-train-41621", "mrqa_squad-train-68897", "mrqa_squad-train-12318", "mrqa_squad-train-16732", "mrqa_squad-train-40640", "mrqa_squad-train-69751", "mrqa_squad-train-62649", "mrqa_squad-train-56747", "mrqa_squad-train-73835", "mrqa_squad-train-63445", "mrqa_squad-train-65139", "mrqa_squad-train-43501", "mrqa_squad-train-86584", "mrqa_squad-train-6936", "mrqa_newsqa-validation-1512", "mrqa_naturalquestions-validation-129", "mrqa_squad-validation-10339", "mrqa_searchqa-validation-5755", "mrqa_squad-validation-1240", "mrqa_newsqa-validation-3686", "mrqa_hotpotqa-validation-2582", "mrqa_naturalquestions-validation-2666", "mrqa_searchqa-validation-15169", "mrqa_naturalquestions-validation-5531", "mrqa_hotpotqa-validation-182", "mrqa_newsqa-validation-2184", "mrqa_triviaqa-validation-1463", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1534", "mrqa_squad-validation-2595", "mrqa_hotpotqa-validation-5237", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-9239", "mrqa_hotpotqa-validation-5790", "mrqa_naturalquestions-validation-7967"], "EFR": 0.972972972972973, "Overall": 0.7452195945945945}, {"timecode": 38, "before_eval_results": {"predictions": ["vector quantities", "Transport Workers Union leaders", "March 24,", "Eleven", "Mexican military", "Pakistani officials,", "$7.8 million", "Stratfor", "Madeleine K. Albright", "Hillary Clinton", "Polo", "German Foreign Ministry,", "10,000 refugees,", "IV cafe.", "Red Lines", "body bags", "40", "European choir chairs", "Islamic", "at a construction site in the heart of Los Angeles.", "last week", "Sunni Arab and Shiite tribal leaders", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "antihistamine and an epinephrine auto-injector", "North Korea", "Hong Kong from other parts of Asia, such as India and mainland China,", "Lebanese prisoners", "the release of the four men", "ties,", "President Sheikh Sharif Sheikh Ahmed", "Saturday's Hungarian Grand Prix.", "power-sharing talks", "the matron swore and scream at the girls and assaulted them,", "in an artificial coma", "12-1", "Africa", "fear of losing their licenses to fly.", "Zimbabwe's main opposition party", "FBI", "his first grand Slam,", "will not support the Stop Online Piracy Act,", "CNN", "Obama and McCain", "strength of its brand name and the diversity of its product portfolio", "U.S. State Department and British Foreign Office", "Monday", "Fiona MacKeown", "sculptures", "Pakistan's High Commission in India", "A member of the group dubbed the \"Jena 6\"", "pain-relief drugs.", "1871", "Kenneth Kaunda", "experimental psychology", "all animals", "Treaty of Utrecht", "Massachusetts", "James Harrison", "Ford Island", "Tomorrowland", "Tiger Woods", "Wall Street", "Martin Luther King", "Sesame Street"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6527964784398608}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.2857142857142857, 0.0, 1.0, 0.4, 1.0, 1.0, 0.8333333333333333, 0.4, 1.0, 0.11764705882352941, 0.8, 0.0, 1.0, 0.33333333333333337, 0.5714285714285715, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-1060", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-10040", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-6409", "mrqa_searchqa-validation-2728", "mrqa_searchqa-validation-13907"], "SR": 0.515625, "CSR": 0.5841346153846154, "retrieved_ids": ["mrqa_squad-train-25205", "mrqa_squad-train-14141", "mrqa_squad-train-26825", "mrqa_squad-train-10763", "mrqa_squad-train-46955", "mrqa_squad-train-32973", "mrqa_squad-train-24917", "mrqa_squad-train-81636", "mrqa_squad-train-22638", "mrqa_squad-train-75066", "mrqa_squad-train-50537", "mrqa_squad-train-3777", "mrqa_squad-train-46018", "mrqa_squad-train-64565", "mrqa_squad-train-85387", "mrqa_squad-train-54546", "mrqa_squad-train-57655", "mrqa_squad-train-43050", "mrqa_squad-train-12341", "mrqa_squad-train-54775", "mrqa_squad-train-73103", "mrqa_squad-train-41137", "mrqa_squad-train-50477", "mrqa_squad-train-22784", "mrqa_newsqa-validation-62", "mrqa_searchqa-validation-4394", "mrqa_triviaqa-validation-7056", "mrqa_naturalquestions-validation-8934", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-2971", "mrqa_naturalquestions-validation-1987", "mrqa_searchqa-validation-1053", "mrqa_squad-validation-978", "mrqa_triviaqa-validation-4210", "mrqa_squad-validation-1235", "mrqa_searchqa-validation-10856", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-1906", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7212", "mrqa_triviaqa-validation-4411", "mrqa_triviaqa-validation-3223", "mrqa_naturalquestions-validation-289", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-3020", "mrqa_naturalquestions-validation-2196", "mrqa_newsqa-validation-3714", "mrqa_hotpotqa-validation-2205"], "EFR": 1.0, "Overall": 0.7502644230769231}, {"timecode": 39, "before_eval_results": {"predictions": ["Andrew Lortie", "conditions have been made more tolerable", "coniferales", "high cooking", "Silver Hatch", "peripheral nerves", "Ethiopia", "red Admiral", "two teen-age gangs", "the Harrier", "a person trained for travelling in space", "SAR", "Alastair Cook", "Alamo", "The Three Pigs", "Asia", "hip joint", "comedian", "meninges", "The Official Languages Act", "Guildford Dudley", "Munich", "Henry Mancini", "Fred Astaire", "Capitoline Hills", "Tuesday, September 21, 2010", "Sudan", "Low Countries", "Police Procedural", "The Book of Proverbs", "TV personality", "florida", "Tornado", "police drama", "s\u00e3o (Villa) island of Cabo Verde,", "pancreas", "puff", "football", "Antoine Lavoisier", "florida", "geologist", "societies or amalgamations of persons", "Pet Shop Boys", "Chris Salmon", "Algiers", "Miss Helene Hanff", "Aabaptists", "Crispin", "Hebrew", "John Virgo", "herpes virus,", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods )", "Garfield Sobers", "in the mountains outside City 17", "Kodos", "Johannes Vermeer", "O.T. Genasis", "Climatecare,", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Schalke", "Lost in America", "autumnal", "Soviet Union", "Senate Democrats"], "metric_results": {"EM": 0.4375, "QA-F1": 0.51035579004329}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 0.21428571428571427, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-970", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-86", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3132", "mrqa_searchqa-validation-6304", "mrqa_newsqa-validation-1550"], "SR": 0.4375, "CSR": 0.58046875, "retrieved_ids": ["mrqa_squad-train-82007", "mrqa_squad-train-47605", "mrqa_squad-train-10430", "mrqa_squad-train-48568", "mrqa_squad-train-69806", "mrqa_squad-train-49180", "mrqa_squad-train-17596", "mrqa_squad-train-64314", "mrqa_squad-train-50982", "mrqa_squad-train-53226", "mrqa_squad-train-50276", "mrqa_squad-train-41464", "mrqa_squad-train-32305", "mrqa_squad-train-24742", "mrqa_squad-train-47226", "mrqa_squad-train-79251", "mrqa_squad-train-31809", "mrqa_squad-train-68256", "mrqa_squad-train-46635", "mrqa_squad-train-74566", "mrqa_squad-train-66444", "mrqa_squad-train-28577", "mrqa_squad-train-80233", "mrqa_squad-train-18162", "mrqa_squad-validation-606", "mrqa_newsqa-validation-2884", "mrqa_triviaqa-validation-77", "mrqa_searchqa-validation-8582", "mrqa_triviaqa-validation-1094", "mrqa_newsqa-validation-1392", "mrqa_searchqa-validation-1279", "mrqa_squad-validation-10168", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-3542", "mrqa_triviaqa-validation-2361", "mrqa_hotpotqa-validation-5305", "mrqa_squad-validation-3985", "mrqa_triviaqa-validation-2199", "mrqa_newsqa-validation-1517", "mrqa_squad-validation-2932", "mrqa_newsqa-validation-895", "mrqa_naturalquestions-validation-3922", "mrqa_hotpotqa-validation-3790", "mrqa_naturalquestions-validation-8116", "mrqa_squad-validation-2336", "mrqa_newsqa-validation-872", "mrqa_naturalquestions-validation-2143"], "EFR": 0.9722222222222222, "Overall": 0.7439756944444443}, {"timecode": 40, "UKR": 0.826171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1389", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10606", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-1264", "mrqa_searchqa-validation-12829", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16453", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10263", "mrqa_squad-validation-10317", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10369", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-127", "mrqa_squad-validation-1408", "mrqa_squad-validation-1430", "mrqa_squad-validation-1453", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2094", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3124", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3444", "mrqa_squad-validation-3497", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3946", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-4715", "mrqa_squad-validation-491", "mrqa_squad-validation-4918", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5664", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6706", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7147", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7296", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7751", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8154", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-8841", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-568", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-899"], "OKR": 0.8828125, "KG": 0.49609375, "before_eval_results": {"predictions": ["chameleon circuit", "Jake La Motta", "danish", "Joshua", "pangram", "Let Die", "Joan Crawford", "lancaster", "Brazil", "Robert Hooke", "Hadrian", "John", "Sony Interactive Entertainment", "king Henry I of England", "green", "1215", "buttercups", "Robinson Crusoe", "Charles Dickens", "Brussels", "Egypt", "a neutron", "earache", "New York Yankees", "Four Tops", "hudd", "July 20, 1969", "9 gallons", "bali", "lilac", "Hilary Swank", "cardinal", "a dove", "a toad", "John McCarthy", "Hitler", "three", "George I", "Shaft", "hindfoot", "The Daily Mirror", "zak Starkey", "horse", "Pakistan", "belgian", "Machu Picchu", "Paul McCartney", "Madness", "Jane Eyre", "Kansas", "jindabyne", "solemniser", "Ricky Nelson", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "Port Moresby, Papua New Guinea", "A bass", "Security Management", "eight", "Russia", "was like a class to help women \" learn how to dance and feel sexy,\"", "Malacca", "Hank Aaron", "livin' on a prayer", "Octopus"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6396205357142857}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.7000000000000001, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-571", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-1746", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-56", "mrqa_triviaqa-validation-2632", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6915", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-774", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-11621", "mrqa_naturalquestions-validation-6903"], "SR": 0.53125, "CSR": 0.5792682926829269, "retrieved_ids": ["mrqa_squad-train-64505", "mrqa_squad-train-27315", "mrqa_squad-train-41708", "mrqa_squad-train-50899", "mrqa_squad-train-36965", "mrqa_squad-train-686", "mrqa_squad-train-83935", "mrqa_squad-train-37286", "mrqa_squad-train-15644", "mrqa_squad-train-41524", "mrqa_squad-train-22174", "mrqa_squad-train-49279", "mrqa_squad-train-54796", "mrqa_squad-train-70464", "mrqa_squad-train-1631", "mrqa_squad-train-45059", "mrqa_squad-train-65543", "mrqa_squad-train-7868", "mrqa_squad-train-9767", "mrqa_squad-train-79078", "mrqa_squad-train-49116", "mrqa_squad-train-18026", "mrqa_squad-train-54586", "mrqa_squad-train-38777", "mrqa_searchqa-validation-15075", "mrqa_naturalquestions-validation-4029", "mrqa_searchqa-validation-2866", "mrqa_triviaqa-validation-7461", "mrqa_squad-validation-1235", "mrqa_naturalquestions-validation-8441", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-6463", "mrqa_triviaqa-validation-7083", "mrqa_newsqa-validation-442", "mrqa_squad-validation-8229", "mrqa_triviaqa-validation-2036", "mrqa_naturalquestions-validation-4762", "mrqa_searchqa-validation-2141", "mrqa_naturalquestions-validation-922", "mrqa_searchqa-validation-9196", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-8063", "mrqa_newsqa-validation-1538", "mrqa_hotpotqa-validation-3714", "mrqa_newsqa-validation-3503", "mrqa_hotpotqa-validation-2262", "mrqa_triviaqa-validation-7508", "mrqa_hotpotqa-validation-4864"], "EFR": 1.0, "Overall": 0.7568692835365853}, {"timecode": 41, "before_eval_results": {"predictions": ["Arthur H. Compton", "food, music, culture and language of Latin America", "Los Angeles", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Tim Clark, Matt Kuchar and Bubba Watson", "Philip Markoff,", "Haeftling,", "forgery and flying without a valid license", "Sea World in San Antonio", "Mafia", "OneLegacy,", "1800s", "convicts caught with phones", "16", "cancer", "$40 and a loaf of bread.", "McDonald's' plans", "she's in love, thinks maybe it's a good thing she thought Rounds was straight.", "France", "President Obama and Britain's Prince Charles", "eight", "South Africa's", "Madeleine K. Albright", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "back at work", "will be inducted into the Baseball Hall of Fame in July.", "collapsed ConAgra Foods plant", "five", "hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Ronald Cummings", "Elisabeth's father,", "his club", "they", "$60 billion on America's infrastructure.", "$199", "J.G. Ballard,", "Republicans", "discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "Airbus A330-200", "United States, NATO member states, Russia and India", "fatally shooting a limo driver on February 14, 2002.", "ties,", "walk on ice in Alaska.", "all three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "China", "Steve Wozniak", "\"Rin Tin Tin: The Life and the Legend\"", "Sri Lanka's", "The station", "state's attorney", "First Lieutenant Israel Greene", "126", "Brevet Colonel Robert E. Lee", "The Telegraph", "prime minister Yitzhak Rabin", "la Marseillaise", "Kristy Lee Cook", "John Samuel Waters Jr.", "Prada", "anglo-antilles", "Douglas MacArthur", "rice", "at least 18 or 21 years old"], "metric_results": {"EM": 0.5, "QA-F1": 0.6322912358698649}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.07142857142857142, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.12121212121212123, 1.0, 0.26666666666666666, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 0.0, 0.4, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.12903225806451615, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.33333333333333337, 0.4444444444444445, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7880", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-371", "mrqa_triviaqa-validation-4759", "mrqa_hotpotqa-validation-66", "mrqa_searchqa-validation-1621", "mrqa_searchqa-validation-149", "mrqa_naturalquestions-validation-8617"], "SR": 0.5, "CSR": 0.5773809523809523, "retrieved_ids": ["mrqa_squad-train-85124", "mrqa_squad-train-22787", "mrqa_squad-train-17579", "mrqa_squad-train-50771", "mrqa_squad-train-70313", "mrqa_squad-train-66353", "mrqa_squad-train-11890", "mrqa_squad-train-36252", "mrqa_squad-train-85482", "mrqa_squad-train-20311", "mrqa_squad-train-71328", "mrqa_squad-train-54921", "mrqa_squad-train-79792", "mrqa_squad-train-43798", "mrqa_squad-train-4345", "mrqa_squad-train-77078", "mrqa_squad-train-41206", "mrqa_squad-train-40365", "mrqa_squad-train-12282", "mrqa_squad-train-8478", "mrqa_squad-train-77641", "mrqa_squad-train-64233", "mrqa_squad-train-8102", "mrqa_squad-train-962", "mrqa_naturalquestions-validation-6015", "mrqa_newsqa-validation-3327", "mrqa_triviaqa-validation-6421", "mrqa_hotpotqa-validation-3090", "mrqa_newsqa-validation-1512", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-6500", "mrqa_hotpotqa-validation-2009", "mrqa_newsqa-validation-140", "mrqa_squad-validation-1566", "mrqa_triviaqa-validation-2151", "mrqa_newsqa-validation-1673", "mrqa_squad-validation-4150", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-5358", "mrqa_naturalquestions-validation-86", "mrqa_squad-validation-3021", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-15770", "mrqa_squad-validation-3497", "mrqa_naturalquestions-validation-3", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-12363"], "EFR": 0.96875, "Overall": 0.7502418154761905}, {"timecode": 42, "before_eval_results": {"predictions": ["ITT", "246", "masterful 1998 best-seller", "The cause of the child's death will be listed as homicide by undetermined means,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Iran's", "20", "Oxygen Channel's \"Dance Your Ass Off\"", "the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "15-year-old", "AbdulMutallab,", "London and Buenos Aires", "Democratic National Convention", "his native Philippines", "Kitty Kelley, biographer of the rich and famous,", "Les Bleus", "The local Republican Party", "Amanda Knox's aunt", "Michael Krane,", "15,000", "about 12 million", "the Gulf", "May 4", "3-2", "Robert Mugabe", "three men with suicide vests who were plotting to carry out the attacks,", "kill then-Sen. Obama", "10 percent", "165-room", "when the economy turns unfriendly,", "Ignazio La Russa", "Amir Zaki", "$40 and a loaf of bread.", "about 30 miles southwest of Nashville,", "Tulsa, Oklahoma.", "3-3", "nearly $2 billion", "Russian concerns that the defensive shield could be used for offensive aims.", "1981", "Knox's parents, Curt Knox and Edda Mellas,", "London's 20,000-capacity O2 Arena.", "\"Golden City,\"", "more than 100", "Microsoft", "Michael Partain,", "Mitt Romney", "Islamic", "prisoners at the South Dakota State Penitentiary", "part", "does not grant full health-care coverage,", "season five", "1960s", "1973", "Afghanistan", "yankees", "fochabers", "\"Shake It Off\"", "North West England", "Selden", "a pre-operative transsexual", "a novel that shows the culture of the United States at a specific time", "The Moonstone", "16"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6008323273948274}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 0.26666666666666666, 1.0, 0.0, 0.0, 0.2857142857142857, 0.5, 0.0, 1.0, 0.4, 0.5, 0.5, 0.25, 0.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.5, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1429", "mrqa_naturalquestions-validation-7239", "mrqa_triviaqa-validation-1380", "mrqa_triviaqa-validation-6580", "mrqa_hotpotqa-validation-5848", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-709", "mrqa_naturalquestions-validation-1640"], "SR": 0.484375, "CSR": 0.575218023255814, "retrieved_ids": ["mrqa_squad-train-82601", "mrqa_squad-train-79659", "mrqa_squad-train-83004", "mrqa_squad-train-62337", "mrqa_squad-train-3384", "mrqa_squad-train-32030", "mrqa_squad-train-40449", "mrqa_squad-train-31860", "mrqa_squad-train-66778", "mrqa_squad-train-54896", "mrqa_squad-train-42609", "mrqa_squad-train-28444", "mrqa_squad-train-55329", "mrqa_squad-train-28524", "mrqa_squad-train-15662", "mrqa_squad-train-27502", "mrqa_squad-train-42906", "mrqa_squad-train-1402", "mrqa_squad-train-13686", "mrqa_squad-train-82830", "mrqa_squad-train-80432", "mrqa_squad-train-26059", "mrqa_squad-train-78042", "mrqa_squad-train-16244", "mrqa_hotpotqa-validation-516", "mrqa_naturalquestions-validation-7242", "mrqa_newsqa-validation-203", "mrqa_searchqa-validation-16076", "mrqa_newsqa-validation-1064", "mrqa_hotpotqa-validation-4879", "mrqa_newsqa-validation-4122", "mrqa_searchqa-validation-14194", "mrqa_squad-validation-1566", "mrqa_squad-validation-7136", "mrqa_hotpotqa-validation-3020", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2074", "mrqa_naturalquestions-validation-3686", "mrqa_triviaqa-validation-1463", "mrqa_newsqa-validation-3803", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-5583", "mrqa_hotpotqa-validation-1576", "mrqa_searchqa-validation-13247", "mrqa_squad-validation-4326", "mrqa_hotpotqa-validation-5228", "mrqa_triviaqa-validation-2456", "mrqa_newsqa-validation-1080"], "EFR": 0.9696969696969697, "Overall": 0.7499986235905567}, {"timecode": 43, "before_eval_results": {"predictions": ["400", "14", "3-0", "how health care can affect families.", "open heart surgery,", "Oaxacan countryside of southern", "the punishment for the player", "wings", "Vernon Forrest,", "Mandi Hamlin", "U.S. State Department and British Foreign Office", "five female pastors", "Phoenix, Arizona, police", "\"We tortured (Mohammed al+) Qahtani,\"", "Twenty-four elephants,", "Six", "Wednesday.", "along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "Russia", "a nurse", "Sheikh Abu al-Nour al-Maqdessi,", "a senior at Stetson University studying computer science.", "his health and about a comeback.", "1,500", "through the weekend,", "three", "\"The e-mails\"", "Aniston, Demi Moore and Alicia Keys", "January", "land issues in Zimbabwe and approaching international courts has thus far not worked either.", "Miguel Cotto", "Two pages -- usually high school juniors who serve Congress as messengers", "A family friend of a U.S. soldier", "in a stream in shark River Park in Monmouth County", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "all buses, subways and trolleys in Philadelphia and on the Frontier line in Bucks and Montgomery counties stopped running at 3 a.m.", "five", "Long troop deployments", "St. Louis, Missouri.", "know what's important in life,", "a number of calls, and those calls were intriguing, and we're chasing those down now.", "Clifford Harris,", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Republican Party,", "almost 9 million", "Asashoryu", "an upper respiratory infection", "Adriano", "\"Zed,\" a Columbian mammoth", "the prime minister's handling of the L'Aquila earthquake, which killed nearly 300 people and devastated the city when it struck last year,", "death squad killings", "1940", "American Horror Story : Roanoke", "Scarlett Johansson", "skull", "Red Sea", "Narragansett Bay", "Scotty Grainger Jr.", "Robert A. Iger", "Salgaocar", "mansfield park", "Abercrombie & Fitch", "a soap opera", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5890844353015405}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473682, 1.0, 0.3076923076923077, 0.5714285714285715, 1.0, 0.14285714285714288, 0.32, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4799999999999999, 0.36363636363636365, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.21052631578947367]}}, "before_error_ids": ["mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1086", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-143", "mrqa_triviaqa-validation-3275", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-802", "mrqa_searchqa-validation-6252", "mrqa_naturalquestions-validation-4915"], "SR": 0.484375, "CSR": 0.5731534090909092, "retrieved_ids": ["mrqa_squad-train-18603", "mrqa_squad-train-20956", "mrqa_squad-train-69534", "mrqa_squad-train-72460", "mrqa_squad-train-40207", "mrqa_squad-train-7526", "mrqa_squad-train-5592", "mrqa_squad-train-902", "mrqa_squad-train-51121", "mrqa_squad-train-18903", "mrqa_squad-train-58097", "mrqa_squad-train-59396", "mrqa_squad-train-69695", "mrqa_squad-train-48395", "mrqa_squad-train-60197", "mrqa_squad-train-75619", "mrqa_squad-train-31110", "mrqa_squad-train-44191", "mrqa_squad-train-15349", "mrqa_squad-train-30800", "mrqa_squad-train-14547", "mrqa_squad-train-18722", "mrqa_squad-train-26738", "mrqa_squad-train-68960", "mrqa_newsqa-validation-2582", "mrqa_hotpotqa-validation-2986", "mrqa_newsqa-validation-1415", "mrqa_hotpotqa-validation-5292", "mrqa_squad-validation-2911", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-650", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2884", "mrqa_hotpotqa-validation-1182", "mrqa_squad-validation-1780", "mrqa_hotpotqa-validation-1576", "mrqa_naturalquestions-validation-844", "mrqa_newsqa-validation-47", "mrqa_squad-validation-6031", "mrqa_newsqa-validation-2384", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-2395", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-2178", "mrqa_naturalquestions-validation-4074"], "EFR": 0.9696969696969697, "Overall": 0.7495857007575758}, {"timecode": 44, "before_eval_results": {"predictions": ["help transfer and dissipate excess energy", "Chesapeake Bay, south of Annapolis in Maryland", "the Isthmus of Corinth", "Alex Ryan", "Wimpy\\'s", "2001", "fall of 2015", "Betty", "devised by Leonard Nimoy, who portrayed the half - Vulcan character Mr. Spock on the original Star Trek television series", "Rodney Crowell", "Jason Momoa", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "a donor molecule", "Shawn Wayans", "Iowa ( 36.6 % )", "1996", "The uvea", "the Director of National Intelligence", "Eukarya", "Zeebo", "1977", "the Department of Health and Human Services, Office of Inspector General,", "France", "development of electronic computers", "a contemporary drama in a rural setting", "1939", "Kristy Swanson", "Jyotirindra Basu", "roughly five hundred", "2018", "the Naturalization Act of 1790", "The Jamestown settlement in the Colony of Virginia", "Arkansas", "December 24, 1836", "at slightly different times", "during the American Civil War", "the Executive Residence of the White House Complex", "200 to 500 mg", "Timothy B. Schmit", "March 2, 2016", "Thirty years after the Galactic Civil War", "Woody Paige", "Anna Faris", "$75,000", "four", "`` Blood is the New Black ''", "The small intestine or small bowel", "USS Chesapeake", "18 - season", "to offer the hope that a happy day being marked would recur many more times", "President Lyndon Johnson", "Hillary Clinton", "Al Pacino", "passion fruit", "Kinnairdy Castle", "Mako", "Kona", "his business dealings for possible securities violations", "40", "16", "John Deere", "snowboarding", "dollop", "Algiers"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7149181547619048}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9445", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-824", "mrqa_triviaqa-validation-3099", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-880", "mrqa_searchqa-validation-2656"], "SR": 0.640625, "CSR": 0.5746527777777778, "retrieved_ids": ["mrqa_squad-train-69997", "mrqa_squad-train-17645", "mrqa_squad-train-30100", "mrqa_squad-train-15049", "mrqa_squad-train-22745", "mrqa_squad-train-7658", "mrqa_squad-train-15372", "mrqa_squad-train-15233", "mrqa_squad-train-53224", "mrqa_squad-train-53949", "mrqa_squad-train-14559", "mrqa_squad-train-30976", "mrqa_squad-train-72127", "mrqa_squad-train-48922", "mrqa_squad-train-67483", "mrqa_squad-train-7543", "mrqa_squad-train-5570", "mrqa_squad-train-57228", "mrqa_squad-train-12733", "mrqa_squad-train-16750", "mrqa_squad-train-57066", "mrqa_squad-train-65729", "mrqa_squad-train-18780", "mrqa_squad-train-10650", "mrqa_newsqa-validation-631", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-3717", "mrqa_newsqa-validation-3463", "mrqa_hotpotqa-validation-1996", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-3500", "mrqa_squad-validation-384", "mrqa_newsqa-validation-2497", "mrqa_naturalquestions-validation-6787", "mrqa_squad-validation-2000", "mrqa_naturalquestions-validation-5113", "mrqa_newsqa-validation-3181", "mrqa_hotpotqa-validation-1489", "mrqa_naturalquestions-validation-6207", "mrqa_squad-validation-1136", "mrqa_squad-validation-6171", "mrqa_naturalquestions-validation-129", "mrqa_newsqa-validation-386", "mrqa_naturalquestions-validation-1285", "mrqa_hotpotqa-validation-5448", "mrqa_newsqa-validation-11", "mrqa_triviaqa-validation-3359"], "EFR": 0.9565217391304348, "Overall": 0.7472505283816425}, {"timecode": 45, "before_eval_results": {"predictions": ["locomotion", "The first likely description of the disease was in 1841 by Charles Oscar Waters", "Cheryl Campbell", "The Satavahanas", "Michael Moriarty", "Canada", "111", "Virginia", "random - access memory ( RAM )", "1940", "Doug Diemoz", "Charlene Holt", "Kari Wahlgren", "Fred Ott", "Mad - Eye Moody", "O'Meara", "Washingtonizards", "Missi Hale", "2001", "Eddie Murphy", "Portugal. The Man", "Theodore Roosevelt", "1940", "parthenogenic", "Lyle Waggoner", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Coconut Cove", "the third season", "the biblical name of a Canaanite god associated with child sacrifice", "a mid-size four - wheel drive luxury Volvo", "10 June 1940", "Bill Pullman", "Little G minor symphony", "using a baby as bait, allowing a child to go through a torturous treatment to gain information, and allowing Dean to become a Vampire", "August 2, 1990", "786 -- 802", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Franklin and Wake counties in the U.S. state of North Carolina", "Pepsi", "In the U.S., tomato pur\u00e9e is a processed food product, usually consisting of only tomatoes, but can also be found in the seasoned form.", "Bartemius Crouch Jr", "Martin Lawrence", "Effy", "Numa Pompilius", "Jurchen Aisin Gioro clan", "Muhammad", "Americans", "August 22, 1980", "Professor Kantorek", "Yondu Udonta", "the next episode, `` Seeing Red ''", "luster", "bushfires", "Adrian Edmondson", "Figaro", "Big 12 Conference", "Debbie Reynolds", "Dubai", "legitimacy of that race.", "Salt Lake City, Utah,", "Java, Dutch East", "OPEC", "an American jazz saxophonist and composer", "Current TV"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6278862898224311}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.9600000000000001, 0.4347826086956522, 0.0, 0.16216216216216214, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.75, 0.4, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-5296", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4457", "mrqa_newsqa-validation-2590"], "SR": 0.53125, "CSR": 0.5737092391304348, "retrieved_ids": ["mrqa_squad-train-41700", "mrqa_squad-train-18282", "mrqa_squad-train-75696", "mrqa_squad-train-26358", "mrqa_squad-train-17504", "mrqa_squad-train-13065", "mrqa_squad-train-70994", "mrqa_squad-train-85818", "mrqa_squad-train-32354", "mrqa_squad-train-78353", "mrqa_squad-train-75917", "mrqa_squad-train-77946", "mrqa_squad-train-49065", "mrqa_squad-train-56038", "mrqa_squad-train-31978", "mrqa_squad-train-36291", "mrqa_squad-train-27931", "mrqa_squad-train-36651", "mrqa_squad-train-58092", "mrqa_squad-train-36499", "mrqa_squad-train-26331", "mrqa_squad-train-42238", "mrqa_squad-train-974", "mrqa_squad-train-49430", "mrqa_naturalquestions-validation-5348", "mrqa_searchqa-validation-2568", "mrqa_newsqa-validation-2244", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-2000", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-10461", "mrqa_hotpotqa-validation-2769", "mrqa_naturalquestions-validation-844", "mrqa_searchqa-validation-709", "mrqa_naturalquestions-validation-2309", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-2582", "mrqa_naturalquestions-validation-10613", "mrqa_squad-validation-7449", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-6759", "mrqa_squad-validation-7845", "mrqa_naturalquestions-validation-4762", "mrqa_triviaqa-validation-6915", "mrqa_searchqa-validation-4393", "mrqa_newsqa-validation-1191", "mrqa_searchqa-validation-6876"], "EFR": 0.9, "Overall": 0.735757472826087}, {"timecode": 46, "before_eval_results": {"predictions": ["the main porch", "Pastoral farming", "The Nitty Gritty Dirt Band", "code insignia", "Luther Ingram", "Taron Egerton", "Lucius Verus", "Siddharth Arora / Vibhav Roy", "Ray Harroun", "scrolls", "Clarence Anglin", "to establish an electrochemical gradient ( often a proton gradient ) across a membrane", "Ptolemy", "a single, implicitly structured data item in a table", "President pro tempore", "capillary action", "electron donors", "T.J. Miller", "Ren\u00e9 Descartes", "2006 -- 04", "typically the player to the dealer's right", "1955", "the town of Acolman, just north of Mexico City", "23 September 1889", "indigenous to many forested parts of the world", "by January 2018", "Colon Street", "The higher the vapor pressure of a liquid at a given temperature", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "1923", "Hugh S. Johnson", "alpha efferent neurons", "Lord Banquo", "harm - joy", "lead", "al - khimar", "291", "Anthony Hopkins", "the middle of the 15th century", "Ingrid Bergman", "a political ideology", "c. 1000 AD", "Definition of the problems and / or goals", "Missouri River", "Venezuela and the remainder in Colombia", "Anakin Luke", "privatized", "concerned with all legal affairs, and is the chief lawyer of the United States government", "2018", "C\u03bc and C\u03b4", "September 19, 2017", "Beaujolais", "Edward Woodward", "Black Swan", "Lake Wallace", "Paul W. S. Anderson", "1804\u20131844", "43", "the world's tallest building,", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law.", "V", "Vedas", "Gertrude Stein", "Ilkley"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6946193015683148}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7272727272727272, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2, 1.0, 0.25, 0.5, 0.0, 0.6842105263157895, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.10526315789473685, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-538", "mrqa_triviaqa-validation-2399", "mrqa_hotpotqa-validation-1605", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-4201"], "SR": 0.578125, "CSR": 0.5738031914893618, "retrieved_ids": ["mrqa_squad-train-17825", "mrqa_squad-train-48766", "mrqa_squad-train-81354", "mrqa_squad-train-30290", "mrqa_squad-train-6151", "mrqa_squad-train-74667", "mrqa_squad-train-32931", "mrqa_squad-train-83927", "mrqa_squad-train-56922", "mrqa_squad-train-39162", "mrqa_squad-train-76759", "mrqa_squad-train-76125", "mrqa_squad-train-68129", "mrqa_squad-train-29579", "mrqa_squad-train-48181", "mrqa_squad-train-286", "mrqa_squad-train-254", "mrqa_squad-train-5589", "mrqa_squad-train-34131", "mrqa_squad-train-41321", "mrqa_squad-train-53579", "mrqa_squad-train-35326", "mrqa_squad-train-30994", "mrqa_squad-train-85349", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3741", "mrqa_searchqa-validation-2568", "mrqa_naturalquestions-validation-7239", "mrqa_searchqa-validation-8760", "mrqa_triviaqa-validation-1683", "mrqa_searchqa-validation-123", "mrqa_hotpotqa-validation-62", "mrqa_naturalquestions-validation-3559", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-5588", "mrqa_searchqa-validation-7004", "mrqa_naturalquestions-validation-6234", "mrqa_searchqa-validation-16659", "mrqa_newsqa-validation-1514", "mrqa_naturalquestions-validation-4074", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-9853", "mrqa_newsqa-validation-3806", "mrqa_triviaqa-validation-5756", "mrqa_newsqa-validation-4064", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-3020"], "EFR": 0.8888888888888888, "Overall": 0.73355404107565}, {"timecode": 47, "before_eval_results": {"predictions": ["2014", "Tim Russert", "Sebastian Lund ( Rob Kerkovich )", "Celtic", "1970", "two", "September 6, 2019", "Russell Huxtable", "September 19, 2017", "the Anglo - Norman French waleis", "31 October 1972", "23 September 1889", "Speaker of the House of Representatives", "Pittsburgh in 2008", "frontal lobe", "1940s", "increased productivity, trade, and secular economic trends", "2 %", "approximately 5 liters", "G -- Games", "the 17th episode in the third season", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time", "New York Yankees have played in 40 World Series and won 27", "94 by 50 feet", "Sam Waterston", "Valinor", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak )", "bohrium", "Ravi Shastri", "the main road through the gated community of Pebble Beach", "Mansa Musa's", "electrons", "November 2014", "Alice", "Janis Joplin", "Australia", "T'Pau", "European powers", "in florida it is illegal to sell alcohol before 1 pm on any sunday", "in South America", "Edgar Lungu", "Melanie Martinez", "A status line", "House", "Brian Steele", "Munich, Bavaria", "Jennifer O'Neill", "1998", "Thomas Chisholm", "Tommy James", "travel sickness", "dip", "\"The best is yet to come.\"", "Chattahoochee", "Patterns of Sexual Behavior", "\"Futurama\"", "Argentine", "ice jam", "Facebook and Google,", "decaffeinated", "One Flew Over the Cuckoo's Nest", "Stephen Hawking", "20%"], "metric_results": {"EM": 0.5, "QA-F1": 0.5959398674242424}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 0.4, 0.18181818181818182, 0.8, 1.0, 1.0, 0.125, 0.0, 0.0, 0.4, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-485", "mrqa_hotpotqa-validation-114", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14104", "mrqa_newsqa-validation-2608"], "SR": 0.5, "CSR": 0.572265625, "retrieved_ids": ["mrqa_squad-train-83562", "mrqa_squad-train-61352", "mrqa_squad-train-61107", "mrqa_squad-train-50831", "mrqa_squad-train-56487", "mrqa_squad-train-19879", "mrqa_squad-train-67636", "mrqa_squad-train-14506", "mrqa_squad-train-69710", "mrqa_squad-train-22269", "mrqa_squad-train-15642", "mrqa_squad-train-45736", "mrqa_squad-train-45575", "mrqa_squad-train-14552", "mrqa_squad-train-26976", "mrqa_squad-train-43086", "mrqa_squad-train-18015", "mrqa_squad-train-84836", "mrqa_squad-train-54820", "mrqa_squad-train-46568", "mrqa_squad-train-17268", "mrqa_squad-train-16907", "mrqa_squad-train-32851", "mrqa_squad-train-56118", "mrqa_squad-validation-3811", "mrqa_searchqa-validation-9394", "mrqa_triviaqa-validation-6548", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8530", "mrqa_newsqa-validation-1879", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9160", "mrqa_hotpotqa-validation-5448", "mrqa_naturalquestions-validation-4674", "mrqa_triviaqa-validation-6580", "mrqa_naturalquestions-validation-4103", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-4059", "mrqa_hotpotqa-validation-411", "mrqa_triviaqa-validation-2781", "mrqa_searchqa-validation-2579", "mrqa_hotpotqa-validation-2848", "mrqa_searchqa-validation-16546", "mrqa_newsqa-validation-169", "mrqa_naturalquestions-validation-7896", "mrqa_squad-validation-9484", "mrqa_triviaqa-validation-4886"], "EFR": 1.0, "Overall": 0.75546875}, {"timecode": 48, "before_eval_results": {"predictions": ["Democratic VP candidate", "Stuttgart", "Three", "Long troop deployments", "over 1,000 pounds", "This is not a project for commercial gain.", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Eleven", "a man who said he had found it in the desert five months before.", "Johan Persson and Martin Schibbye", "anti-trust laws.", "Ferraris, a Lamborghini and an Acura NSX", "eight", "1831", "frees up a place", "Michael Krane,", "Russian bombers", "Three aid workers", "10 a.m.", "2-1", "more than 200.", "\"black is beautiful,\"", "a one-shot victory in the Bob Hope Classic", "whites", "a cancer-causing toxic chemical.", "\"It was perfect work, ready to go for the stimulus package,\"", "US Airways Flight 1549", "the southern city of Naples", "racial intolerance.", "Friday", "\"Twilight\"", "Robert Kimmitt", "22-year-old", "stars of TLC's \"The Little Couple,\"", "Ciudad Juarez, across the border from El Paso, Texas.", "10", "Retailers who don't speak out against it", "Anil Kapoor", "Samoa", "authorizing killings and kidnappings by paramilitary death squads.", "E. coli", "Ma Khin Khin Leh,", "love and loss", "\"The train ride up there is spectacular.", "1998.", "NATO to do more to stop the Afghan opium trade", "London's O2 arena", "The EU naval force", "Azzurri", "the group must recommend a national policy on the subject that's designed to protect ocean ecology, address climate change and promote sustainable ocean economies.", "between $10,000 and $30,000", "1595", "Number 4, Privet Drive, Little Whinging in Surrey, England", "saccharides", "$100", "a rain hat", "five times", "E Street Band", "England", "Chippewa", "salt-free", "Everybody Have Fun Tonight", "the forex market"], "metric_results": {"EM": 0.5, "QA-F1": 0.59055533008658}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.25, 0.47619047619047616, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3, 1.0, 1.0, 0.0, 0.0, 0.32, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-4169", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-3970", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-2918", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12129", "mrqa_naturalquestions-validation-3236"], "SR": 0.5, "CSR": 0.5707908163265306, "retrieved_ids": ["mrqa_squad-train-63156", "mrqa_squad-train-23228", "mrqa_squad-train-13380", "mrqa_squad-train-54779", "mrqa_squad-train-76407", "mrqa_squad-train-12312", "mrqa_squad-train-67171", "mrqa_squad-train-37106", "mrqa_squad-train-26128", "mrqa_squad-train-59360", "mrqa_squad-train-38189", "mrqa_squad-train-70244", "mrqa_squad-train-28481", "mrqa_squad-train-43580", "mrqa_squad-train-83761", "mrqa_squad-train-83816", "mrqa_squad-train-72682", "mrqa_squad-train-11983", "mrqa_squad-train-40740", "mrqa_squad-train-29446", "mrqa_squad-train-5120", "mrqa_squad-train-60226", "mrqa_squad-train-8622", "mrqa_squad-train-1946", "mrqa_squad-validation-3985", "mrqa_triviaqa-validation-6328", "mrqa_squad-validation-6409", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-6046", "mrqa_newsqa-validation-2020", "mrqa_triviaqa-validation-5044", "mrqa_hotpotqa-validation-2213", "mrqa_triviaqa-validation-5500", "mrqa_newsqa-validation-2541", "mrqa_naturalquestions-validation-588", "mrqa_newsqa-validation-11", "mrqa_naturalquestions-validation-3559", "mrqa_squad-validation-9484", "mrqa_triviaqa-validation-1348", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-2462", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-6252", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-8068", "mrqa_hotpotqa-validation-2887", "mrqa_naturalquestions-validation-9639", "mrqa_squad-validation-4326"], "EFR": 0.9375, "Overall": 0.7426737882653061}, {"timecode": 49, "before_eval_results": {"predictions": ["Pierre Laval", "Beckett", "james morris", "Michaela Tabb", "Alpha Orionis", "Edward VIII", "melon Armstrong", "falcon", "Stephen Fry", "Libya", "Darshaan", "Robinson", "the ascetics", "The Daily Telegraph", "William Shakespeare", "Handley Page", "psychology, sociology, anthropology, religious studies, medicine and forensic science", "july goolagong Cawley", "Texas", "(to be performed) in a fiery manner", "the Strait of Messina", "hoy", "country-specific", "Brian Deane", "Volkswagen Golf", "Emilia Fox", "October", "I", "catherine zeta", "South Africa", "Jim Braddock", "mediterranean", "1840", "ocellae", "children of Israel", "Hawaii - The Aloha State", "vomiting", "Richard Strauss", "an albino sperm whale", "Croatian woman residing in Britain", "penguin", "golf", "purple coneflower", "Amnesty International", "Oliver Harmon Jones", "skills", "Kingdom of Lesotho", "BBC - Radio 3 - Elgar/Enigma Variations", "Mauricio Pochettino", "Westminster Abbey", "myxoma", "the season - five premiere episode `` Second Opinion ''", "significant production of peaches as early as 1571, with exports to other states occurring around 1858", "Parker's pregnancy at the time of filming", "Eileen Atkins", "Battle of Chester", "James Gandolfini", "Bill Stanton", "Los Angeles Angels", "56,", "Twenty three", "the 16.5-inch (420-mm) howitzer", "parabola", "Patty Duke"], "metric_results": {"EM": 0.5, "QA-F1": 0.5831732503607503}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1818181818181818, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.6666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-3752", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-4066", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-2853", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7507", "mrqa_hotpotqa-validation-831", "mrqa_searchqa-validation-12134", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-4996"], "SR": 0.5, "CSR": 0.569375, "retrieved_ids": ["mrqa_squad-train-86402", "mrqa_squad-train-60806", "mrqa_squad-train-9679", "mrqa_squad-train-46591", "mrqa_squad-train-71401", "mrqa_squad-train-78531", "mrqa_squad-train-48437", "mrqa_squad-train-4702", "mrqa_squad-train-41401", "mrqa_squad-train-33114", "mrqa_squad-train-83799", "mrqa_squad-train-55419", "mrqa_squad-train-30676", "mrqa_squad-train-23788", "mrqa_squad-train-28029", "mrqa_squad-train-41531", "mrqa_squad-train-7514", "mrqa_squad-train-71876", "mrqa_squad-train-15478", "mrqa_squad-train-82248", "mrqa_squad-train-67490", "mrqa_squad-train-26457", "mrqa_squad-train-308", "mrqa_squad-train-14296", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-2102", "mrqa_squad-validation-3130", "mrqa_naturalquestions-validation-5550", "mrqa_searchqa-validation-4044", "mrqa_naturalquestions-validation-5702", "mrqa_searchqa-validation-12684", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-10057", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-2476", "mrqa_naturalquestions-validation-359", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-2976", "mrqa_naturalquestions-validation-5928", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-2656", "mrqa_triviaqa-validation-2413", "mrqa_naturalquestions-validation-2222", "mrqa_searchqa-validation-15202", "mrqa_naturalquestions-validation-5168", "mrqa_hotpotqa-validation-2205"], "EFR": 0.78125, "Overall": 0.711140625}, {"timecode": 50, "UKR": 0.806640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4431", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-962", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2365", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-893", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5943", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-899"], "OKR": 0.875, "KG": 0.49921875, "before_eval_results": {"predictions": ["A Christmas Carol", "Robert De Niro", "Bangladesh", "Dan Dare", "SUNSET BOULEVARD", "Denmark", "Berlin", "The Rocky Horror Picture Show", "1925", "Prince Albert", "bill", "Pakistan", "pigs", "Popeye (Thimble Theatre)", "james boswell", "Bull Moose Party", "Genoa", "Shanosars Sister", "syria", "Jamaica", "Jessica Simpson", "fredwood", "earthquake", "Campania", "Charlie Chan", "chiba", "Colette", "Louis XVIII", "Anne Boleyn", "playoff basketball", "Thailand", "Aron Ralston ( James Franco)", "Cannes Film Festival", "fred Perry", "Fort Nelson near Portsmouth", "louis armstrong", "Zeus", "The Merry widow", "PHYSICS", "Wolfgang Amadeus Mozart", "Anne-Marie Duff", "Joan Rivers", "water and salt", "13th", "phobias", "Pears soap", "guitar", "Toby", "Argentina", "Mary Peyton", "Fenn Street School", "1804", "spraying the whole atmosphere as if drawing letters in the air ( `` penciling '' )", "November 3, 2007", "John Bingham, 7th Earl of Lucan", "Marktown", "Bit Instant", "well over 1,000 pounds).", "Jet Republic", "Won Sei Hoon, who heads South Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim", "(W.C. Handy)", "a lamb", "legs", "1978"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6257575757575757}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-5137", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-23", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-1371", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-5281", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-3244", "mrqa_searchqa-validation-10670"], "SR": 0.578125, "CSR": 0.569546568627451, "retrieved_ids": ["mrqa_squad-train-18369", "mrqa_squad-train-22576", "mrqa_squad-train-35352", "mrqa_squad-train-52025", "mrqa_squad-train-39239", "mrqa_squad-train-10756", "mrqa_squad-train-72873", "mrqa_squad-train-46949", "mrqa_squad-train-17619", "mrqa_squad-train-61280", "mrqa_squad-train-68405", "mrqa_squad-train-67078", "mrqa_squad-train-79778", "mrqa_squad-train-2044", "mrqa_squad-train-36543", "mrqa_squad-train-67335", "mrqa_squad-train-39962", "mrqa_squad-train-51023", "mrqa_squad-train-66040", "mrqa_squad-train-23746", "mrqa_squad-train-69221", "mrqa_squad-train-37567", "mrqa_squad-train-49960", "mrqa_squad-train-11176", "mrqa_searchqa-validation-3633", "mrqa_hotpotqa-validation-3090", "mrqa_newsqa-validation-1569", "mrqa_triviaqa-validation-3717", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-1372", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9141", "mrqa_newsqa-validation-3982", "mrqa_naturalquestions-validation-3028", "mrqa_newsqa-validation-950", "mrqa_hotpotqa-validation-5703", "mrqa_naturalquestions-validation-10009", "mrqa_triviaqa-validation-5296", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-3732", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-8161", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-332", "mrqa_naturalquestions-validation-4134", "mrqa_squad-validation-384", "mrqa_naturalquestions-validation-875"], "EFR": 0.9259259259259259, "Overall": 0.7352663739106753}, {"timecode": 51, "before_eval_results": {"predictions": ["Carthage", "blue", "Robin Ellis", "mortadella", "an album", "lead", "between the 'Tarsal' bones of the hind-foot and the 'Phalanges' bones in the toe", "natural world and mysticism", "Dancing with the Stars", "South Pacific", "sarah bacall", "Bosnia and Herzegovina", "France", "Sparta", "seekers", "squash", "Northwestern University", "Turkey", "yvonne", "China", "diffusion", "David Bowie", "Robben Island", "bukwus", "myrrh", "a zoom lens", "jordan", "Rocky Marciano", "benny hanley", "tinie Tempah", "Ruth Ellis", "can be called an Egyptian Christian or a Christian in the Middle East or Afro-Asiatic region", "the wren", "Eton College", "annelies", "Siamese", "aug 24", "Boojum", "hindu", "Valentine Dyall", "lesley Hornby", "Portugal", "Opus Dei", "the Flying Pickets", "Dry Ice", "Kenya", "british festivals", "brenda", "norman tebbit", "reanne Evans", "blood", "ummat al - Islamiyah", "Rachel Kelly Tucker", "Honor\u00e9 Mirabeau", "Big Bad Wolf", "Daniel Radcliffe", "Christopher Whitelaw Pine", "President Obama", "Croatia", "Tom Hanks", "Earl Louis \"Curly\" Lambeau", "the candy bar", "zora neale Hurston", "a bed bug"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6208333333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-4257", "mrqa_triviaqa-validation-2964", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-5154", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-6925", "mrqa_naturalquestions-validation-1455", "mrqa_searchqa-validation-7204", "mrqa_searchqa-validation-2456", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-11960"], "SR": 0.578125, "CSR": 0.5697115384615384, "retrieved_ids": ["mrqa_squad-train-23006", "mrqa_squad-train-61406", "mrqa_squad-train-31275", "mrqa_squad-train-17973", "mrqa_squad-train-14461", "mrqa_squad-train-16772", "mrqa_squad-train-54021", "mrqa_squad-train-8485", "mrqa_squad-train-30946", "mrqa_squad-train-17875", "mrqa_squad-train-27768", "mrqa_squad-train-41898", "mrqa_squad-train-48176", "mrqa_squad-train-72570", "mrqa_squad-train-54659", "mrqa_squad-train-19674", "mrqa_squad-train-37070", "mrqa_squad-train-33767", "mrqa_squad-train-90", "mrqa_squad-train-26710", "mrqa_squad-train-70996", "mrqa_squad-train-66298", "mrqa_squad-train-37302", "mrqa_squad-train-42411", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-7212", "mrqa_naturalquestions-validation-8903", "mrqa_newsqa-validation-3035", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-8907", "mrqa_searchqa-validation-149", "mrqa_naturalquestions-validation-9271", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-16130", "mrqa_naturalquestions-validation-8161", "mrqa_newsqa-validation-2823", "mrqa_hotpotqa-validation-1864", "mrqa_triviaqa-validation-5632", "mrqa_newsqa-validation-3986", "mrqa_triviaqa-validation-3079", "mrqa_newsqa-validation-3978", "mrqa_naturalquestions-validation-10354", "mrqa_newsqa-validation-2449", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-1166", "mrqa_searchqa-validation-2728", "mrqa_newsqa-validation-4169", "mrqa_triviaqa-validation-5161"], "EFR": 0.9629629629629629, "Overall": 0.7427067752849001}, {"timecode": 52, "before_eval_results": {"predictions": ["Ringo Starr", "Apprendi v. New Jersey", "8th", "Erreway", "North Queensland", "George Clooney, Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "Pamelyn Ferdin", "Chancellor of Austria", "The Social Network", "$10.5 million", "2017", "Dutch", "2014", "rapper", "Missouri", "Rochdale", "50 best cities to live in", "Virginia", "\"The Godfather Part II\"", "two", "Rigoletto", "Scunthorpe", "Talib Kweli", "motor", "American", "1 September 1864", "Eugene O'Neill", "Colonel Gaddafi", "Sony Music and Syco Music", "figure-eight variety", "Sofia the First", "Sufism", "approximately $700 million", "Roy Spencer", "Magnate", "The Saturdays", "New York and New Jersey campaign", "North Carolina", "John Joseph Travolta", "ice hockey", "Hong Kong", "2006", "Pacific Place", "science fiction", "sarod", "2009", "Northern Ireland", "1999", "Russian Ark", "Delacorte Press", "the voice of The Beast", "17th Century", "April 12, 2017", "in the original Star Wars film in 1977", "an ancient optical illusion toy", "fluorine", "Supermarine", "food, music, culture and language of Latin America", "Amy", "school,", "to pay lip service", "because of the extensive pine forests that have covered the state", "Henry Clay", "Richard Crispin Armitage"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7130208333333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-1351", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2802", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3422", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-6834", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-11977"], "SR": 0.65625, "CSR": 0.5713443396226415, "retrieved_ids": ["mrqa_squad-train-4575", "mrqa_squad-train-61963", "mrqa_squad-train-54267", "mrqa_squad-train-27111", "mrqa_squad-train-79494", "mrqa_squad-train-75121", "mrqa_squad-train-43903", "mrqa_squad-train-36569", "mrqa_squad-train-10245", "mrqa_squad-train-69849", "mrqa_squad-train-73542", "mrqa_squad-train-51038", "mrqa_squad-train-7639", "mrqa_squad-train-34807", "mrqa_squad-train-17898", "mrqa_squad-train-67742", "mrqa_squad-train-5189", "mrqa_squad-train-39261", "mrqa_squad-train-85171", "mrqa_squad-train-39107", "mrqa_squad-train-8108", "mrqa_squad-train-80060", "mrqa_squad-train-32541", "mrqa_squad-train-57251", "mrqa_hotpotqa-validation-3469", "mrqa_newsqa-validation-1426", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-2426", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1402", "mrqa_naturalquestions-validation-9597", "mrqa_squad-validation-9161", "mrqa_triviaqa-validation-4411", "mrqa_triviaqa-validation-6237", "mrqa_searchqa-validation-9185", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1948", "mrqa_naturalquestions-validation-681", "mrqa_newsqa-validation-1265", "mrqa_triviaqa-validation-1125", "mrqa_newsqa-validation-3933", "mrqa_squad-validation-7612", "mrqa_triviaqa-validation-4210", "mrqa_naturalquestions-validation-243", "mrqa_hotpotqa-validation-1534", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-5464", "mrqa_newsqa-validation-25"], "EFR": 0.9545454545454546, "Overall": 0.7413498338336192}, {"timecode": 53, "before_eval_results": {"predictions": ["Mike Mills", "1998", "Nazareth", "Kittie", "American", "People!", "34.9 kilometres", "The Vanguard Group", "American", "Ready to Die", "to steal the plans for the Death Star, the Galactic Empire's super weapon", "Danish", "York County", "Seventeen", "Wake Island", "Australian Defence Force", "June 11, 1973", "Arthur William Bell III", "Boston", "Erreway", "Tampa Bay Lightning", "CBS", "Boston, Massachusetts", "Elise Stefanik", "Jennifer Taylor", "Correcaminos UAT", "9Lives brand cat food", "Black Ravens", "2005", "Las Vegas Strip in Paradise, Nevada", "42,972", "9,000", "Michael Seater", "Drunken Master II", "more than 100 countries", "bassline", "E22", "Allies of World War I", "Geraldine Sue Page", "Kristina Ceyton and Kristian Moliere", "\"Linda McCartney's Life in Photography\"", "Philip Billard Municipal Airport", "1964 to 1974", "Big Fucking German", "law", "Hamlet", "Bow River and the Elbow River", "Gillian Anderson", "segues", "united Ireland", "\"Queen In-hyun's Man\"", "American musical group founded by Marcus Bowens and Jermaine Fuller", "Virgil Ogletree", "4 School of Public Health in the country", "Topiary", "arpad \u2018Arki\u2019 Busson", "1929", "Luca di Montezemolo", "near the Somali coast", "blind,", "deckhand", "automobiles", "Marky Mark", "cheese"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6375800414862915}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.8571428571428571, 0.8, 0.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-3615", "mrqa_hotpotqa-validation-2025", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1033", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-6121", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1641", "mrqa_searchqa-validation-5501", "mrqa_searchqa-validation-3970", "mrqa_searchqa-validation-16209"], "SR": 0.515625, "CSR": 0.5703125, "retrieved_ids": ["mrqa_squad-train-19203", "mrqa_squad-train-86513", "mrqa_squad-train-64871", "mrqa_squad-train-81551", "mrqa_squad-train-49805", "mrqa_squad-train-70642", "mrqa_squad-train-57531", "mrqa_squad-train-6203", "mrqa_squad-train-61429", "mrqa_squad-train-57707", "mrqa_squad-train-21216", "mrqa_squad-train-44333", "mrqa_squad-train-3907", "mrqa_squad-train-82489", "mrqa_squad-train-63110", "mrqa_squad-train-70248", "mrqa_squad-train-62559", "mrqa_squad-train-32834", "mrqa_squad-train-35371", "mrqa_squad-train-82627", "mrqa_squad-train-1951", "mrqa_squad-train-67376", "mrqa_squad-train-16190", "mrqa_squad-train-76199", "mrqa_hotpotqa-validation-3919", "mrqa_triviaqa-validation-4886", "mrqa_newsqa-validation-3018", "mrqa_searchqa-validation-4996", "mrqa_newsqa-validation-1837", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-5237", "mrqa_newsqa-validation-1988", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-2495", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2792", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-1649", "mrqa_hotpotqa-validation-1996", "mrqa_searchqa-validation-1053", "mrqa_newsqa-validation-3541", "mrqa_triviaqa-validation-4150", "mrqa_hotpotqa-validation-1475", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-6237", "mrqa_naturalquestions-validation-8907", "mrqa_squad-validation-7449"], "EFR": 0.967741935483871, "Overall": 0.7437827620967742}, {"timecode": 54, "before_eval_results": {"predictions": ["her brother, Brian", "e-books", "callable bonds", "the sixth season episode `` My Heart Will Go On ''", "Waylon Jennings", "August 2, 1990", "Charlene Holt", "eight episode series", "the courts", "English author Rudyard Kipling", "public road", "18", "Jewel Akens", "New England", "Irsay", "Abid Ali Neemuchwala", "summer", "Marie Fredriksson", "Cadillac", "Jones'then - wife, Peggy Lipton, who knew Vincent Price, suggested Price for the vocal part", "Union", "the chest, back, shoulders, torso and / or legs", "Authority", "drizzle, rain", "1967", "Virginia", "due to Parker's pregnancy at the time of filming", "lakes or reservoirs at high altitudes", "merengue", "Times Square in New York City west to Lincoln Park in San Francisco", "1960s", "IBM", "American singer Elvis Presley", "New England", "1998", "Karen Gillan", "part of the present Indian constitutive state of Meghalaya ( formerly Assam )", "rear - view mirror", "April 29, 2009", "flawed democracy", "2026", "William Chatterton Dix", "Donna Mills", "Selena Gomez", "Steve Russell", "1881", "an armed conflict without the consent of the U.S. Congress", "Timothy B. Schmit", "Games played", "Sir Henry Bartle Frere", "Games", "Cambridge", "Oklahoma City", "choroid", "1982", "2015", "film", "22 felony counts", "one", "economic opportunities", "teacher", "Ukrainian Soviet Socialist Republic", "his own words", "Arthur Schnitzler's 1926 novella \"Traumnovelle\" (\"Dream Story\")"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6908578580063428}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false], "QA-F1": [0.5, 0.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.1904761904761905, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 0.823529411764706, 0.42857142857142855, 1.0, 0.631578947368421, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6086956521739131, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5785", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-6211", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2996", "mrqa_newsqa-validation-830", "mrqa_searchqa-validation-354", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-56", "mrqa_hotpotqa-validation-5124"], "SR": 0.546875, "CSR": 0.5698863636363636, "retrieved_ids": ["mrqa_squad-train-47737", "mrqa_squad-train-78647", "mrqa_squad-train-32067", "mrqa_squad-train-64758", "mrqa_squad-train-54365", "mrqa_squad-train-33345", "mrqa_squad-train-35554", "mrqa_squad-train-83673", "mrqa_squad-train-43035", "mrqa_squad-train-52890", "mrqa_squad-train-42329", "mrqa_squad-train-19609", "mrqa_squad-train-67865", "mrqa_squad-train-46146", "mrqa_squad-train-86511", "mrqa_squad-train-69845", "mrqa_squad-train-6518", "mrqa_squad-train-72489", "mrqa_squad-train-64108", "mrqa_squad-train-76037", "mrqa_squad-train-61240", "mrqa_squad-train-44377", "mrqa_squad-train-37378", "mrqa_squad-train-38613", "mrqa_hotpotqa-validation-2522", "mrqa_triviaqa-validation-1463", "mrqa_hotpotqa-validation-71", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-3760", "mrqa_newsqa-validation-3651", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-6618", "mrqa_searchqa-validation-14194", "mrqa_naturalquestions-validation-4190", "mrqa_newsqa-validation-1429", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-6500", "mrqa_newsqa-validation-3721", "mrqa_triviaqa-validation-2151", "mrqa_searchqa-validation-12363", "mrqa_naturalquestions-validation-8530", "mrqa_hotpotqa-validation-132", "mrqa_naturalquestions-validation-7535", "mrqa_squad-validation-7338", "mrqa_triviaqa-validation-2632", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1496"], "EFR": 0.8620689655172413, "Overall": 0.722562940830721}, {"timecode": 55, "before_eval_results": {"predictions": ["1960s", "Charlton Heston", "without deviating from basic strategy", "Doreen Mantle", "Felicity Huffman", "March 18, 2005", "is actually wise", "when the forward reaction proceeds at the same rate as the reverse reaction", "28 July 1914", "Terry Kath", "In 1922", "2017 season", "Sylvester Stallone", "2008", "Liberia", "Scots law", "Abid Ali Neemuchwala", "Hodel", "online instant messenger", "James Fleet", "one", "ulnar nerve", "Border Collie", "Massachusetts", "citizens", "commercial at", "the star", "60 by West All - Stars", "August 22, 1980", "Jack Nicklaus", "2020", "General George Washington", "7.6 % Per Annum'( compounded annually )", "9.0 -- 9.1 ( M )", "Part 2", "1966", "As of January 17, 2018, 201 episodes", "2026", "1926", "October 20, 1977", "Cetshwayo", "115", "al - Mamlakah al - \u02bbArab\u012byah", "Garbi\u00f1e Muguruza", "23 %", "Detroit Tigers", "Rockwell", "on many space probes and on manned lunar missions", "Charlotte Hornets", "the lumbar enlargement", "February 7, 2018", "muezzin", "Elizabeth Taylor", "Sweden", "fourth-ranking", "\"Queen In-hyun's Man\"", "James Franco", "step up attacks against innocent civilians.\"", "a delegation of American Muslim and Christian leaders", "completely changed the business of music", "\"if a man does not keep pace with his companions, perhaps it is because he hears a different drummer", "the Manchus", "the Black Sox Scandal", "five"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7397073412698413}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.4, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.5, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-10163", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5034", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-2622", "mrqa_searchqa-validation-2367", "mrqa_newsqa-validation-1458"], "SR": 0.640625, "CSR": 0.5711495535714286, "retrieved_ids": ["mrqa_squad-train-65521", "mrqa_squad-train-23312", "mrqa_squad-train-67012", "mrqa_squad-train-84801", "mrqa_squad-train-42734", "mrqa_squad-train-50459", "mrqa_squad-train-43644", "mrqa_squad-train-71525", "mrqa_squad-train-55949", "mrqa_squad-train-7452", "mrqa_squad-train-17114", "mrqa_squad-train-46588", "mrqa_squad-train-29422", "mrqa_squad-train-57748", "mrqa_squad-train-13936", "mrqa_squad-train-38194", "mrqa_squad-train-45864", "mrqa_squad-train-21616", "mrqa_squad-train-82537", "mrqa_squad-train-82326", "mrqa_squad-train-2893", "mrqa_squad-train-27422", "mrqa_squad-train-30681", "mrqa_squad-train-24031", "mrqa_hotpotqa-validation-260", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-10057", "mrqa_triviaqa-validation-5229", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3703", "mrqa_triviaqa-validation-1371", "mrqa_hotpotqa-validation-5808", "mrqa_naturalquestions-validation-4470", "mrqa_hotpotqa-validation-482", "mrqa_newsqa-validation-2449", "mrqa_naturalquestions-validation-553", "mrqa_triviaqa-validation-4966", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-4064", "mrqa_hotpotqa-validation-3162", "mrqa_naturalquestions-validation-8161", "mrqa_searchqa-validation-14371", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14446", "mrqa_squad-validation-2943", "mrqa_searchqa-validation-14569", "mrqa_squad-validation-2336", "mrqa_triviaqa-validation-5852"], "EFR": 1.0, "Overall": 0.7504017857142857}, {"timecode": 56, "before_eval_results": {"predictions": ["l'homme des bois", "Sweden", "the U.S. President who exudes a country-lawyer charisma that belies his brilliance, his deep conviction and devotion to what he believes is right for the country.", "Adam Smith", "Luxembourg", "El Hiero", "Salvador Domingo Felipe Jacinto", "stave", "tyne", "sprint for victory", "The Blues Brothers", "onion", "1984", "frottage", "Penhaligon", "Kevin Painter", "Betsy", "Messenger orbiter", "cutis anserina", "duck-billed platypus", "Montr\u00e9al", "Jeffrey Archer", "Four Tops", "Velazquez", "Restless Leg Syndrome", "Aviva plc", "Charlie Chan", "Apocalypse Now", "taekwondo", "Ishmael", "jubilee line", "d'Artagnan", "delphiniums", "head", "Passepartout", "Chuck Hagel", "haute", "zephyr, Billy Cobham, Alphonse Mouzon, the James Gang, Deep Purple, and Moxy", "300", "speedway", "France", "James Garner", "marinated dried fruits", "Jay-Z", "bird", "false positives", "George lV", "Margaret Beckett", "the Washington Post", "White Ferns", "United States", "in the 18th century", "Austria - Hungary", "Sean O' Neal", "140 million", "The New Yorker", "In Pursuit", "Aung San Suu Kyi", "his brother to surrender.", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "Gene Wilder", "South Park", "florida", "\"The Simpsons\""], "metric_results": {"EM": 0.625, "QA-F1": 0.7209535256410257}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6930", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-4810", "mrqa_newsqa-validation-2308", "mrqa_searchqa-validation-13467", "mrqa_searchqa-validation-11576"], "SR": 0.625, "CSR": 0.5720942982456141, "retrieved_ids": ["mrqa_squad-train-10713", "mrqa_squad-train-54166", "mrqa_squad-train-56965", "mrqa_squad-train-60339", "mrqa_squad-train-79623", "mrqa_squad-train-25972", "mrqa_squad-train-60842", "mrqa_squad-train-29548", "mrqa_squad-train-55145", "mrqa_squad-train-77613", "mrqa_squad-train-84735", "mrqa_squad-train-60673", "mrqa_squad-train-35440", "mrqa_squad-train-58756", "mrqa_squad-train-21753", "mrqa_squad-train-49843", "mrqa_squad-train-10024", "mrqa_squad-train-68167", "mrqa_squad-train-58314", "mrqa_squad-train-57304", "mrqa_squad-train-76568", "mrqa_squad-train-42949", "mrqa_squad-train-43630", "mrqa_squad-train-33885", "mrqa_triviaqa-validation-4715", "mrqa_searchqa-validation-15508", "mrqa_newsqa-validation-551", "mrqa_naturalquestions-validation-138", "mrqa_newsqa-validation-1360", "mrqa_naturalquestions-validation-7896", "mrqa_squad-validation-7845", "mrqa_newsqa-validation-1383", "mrqa_squad-validation-7613", "mrqa_triviaqa-validation-3525", "mrqa_naturalquestions-validation-6903", "mrqa_newsqa-validation-1290", "mrqa_squad-validation-10316", "mrqa_triviaqa-validation-285", "mrqa_newsqa-validation-2735", "mrqa_naturalquestions-validation-4192", "mrqa_triviaqa-validation-3208", "mrqa_newsqa-validation-1144", "mrqa_squad-validation-6031", "mrqa_naturalquestions-validation-6453", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-4059", "mrqa_searchqa-validation-12134", "mrqa_naturalquestions-validation-707"], "EFR": 0.9166666666666666, "Overall": 0.7339240679824561}, {"timecode": 57, "before_eval_results": {"predictions": ["CBS", "Lord Nelson", "leeds", "Utah", "black light", "lacrosse", "the Green BayPackers", "The Hague", "Operation Overlord", "leibniz", "Virginia", "robbie coltrane", "yachts", "tomato", "1215", "pullover", "diffusion", "the Wye", "jack London", "South Carolina", "ellesmere port", "Parsley the Lion", "latin", "Santiago", "the A406 North Circular Road (there are no Junctions 1 to 3)", "Lynda Baron", "Robert Guerrero", "Alcatraz", "90%", "Sven Goran Eriksson", "michael miles", "the 11th Century Church", "A", "Jordan", "a written record", "Motown", "Sudan", "deep blue", "robins", "colony", "Dublin", "anschluss", "silk", "Irving Berlin", "medical", "Leo Tolstoy", "Austria", "oasis", "red bull", "the Roman Republic", "planes", "the university's science club", "2003", "Magnavox Odyssey", "Clark County", "created the American Land-Grant universities and colleges", "fifth", "a ruthless cartel whose area of influence includes the eastern state of Veracruz.", "Japan and Singapore,", "Guo Shuzhong,", "a line", "Harry Potter and the Goblet of Fire", "apricot", "the Red Sea"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5918402777777778}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-5920", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-1395", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-6233", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5877", "mrqa_naturalquestions-validation-8707", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2021", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-1678", "mrqa_searchqa-validation-16366"], "SR": 0.53125, "CSR": 0.5713900862068966, "retrieved_ids": ["mrqa_squad-train-33380", "mrqa_squad-train-32422", "mrqa_squad-train-44963", "mrqa_squad-train-67638", "mrqa_squad-train-75603", "mrqa_squad-train-15965", "mrqa_squad-train-54558", "mrqa_squad-train-40099", "mrqa_squad-train-6260", "mrqa_squad-train-81536", "mrqa_squad-train-3678", "mrqa_squad-train-62183", "mrqa_squad-train-36027", "mrqa_squad-train-32488", "mrqa_squad-train-24550", "mrqa_squad-train-8125", "mrqa_squad-train-9528", "mrqa_squad-train-40657", "mrqa_squad-train-53213", "mrqa_squad-train-24873", "mrqa_squad-train-3006", "mrqa_squad-train-25232", "mrqa_squad-train-33067", "mrqa_squad-train-77657", "mrqa_searchqa-validation-9185", "mrqa_hotpotqa-validation-4612", "mrqa_naturalquestions-validation-9157", "mrqa_newsqa-validation-4059", "mrqa_triviaqa-validation-2199", "mrqa_squad-validation-5724", "mrqa_newsqa-validation-655", "mrqa_hotpotqa-validation-996", "mrqa_triviaqa-validation-6846", "mrqa_newsqa-validation-1372", "mrqa_searchqa-validation-14471", "mrqa_naturalquestions-validation-6442", "mrqa_hotpotqa-validation-4655", "mrqa_newsqa-validation-1634", "mrqa_naturalquestions-validation-10009", "mrqa_hotpotqa-validation-5792", "mrqa_triviaqa-validation-2357", "mrqa_searchqa-validation-3451", "mrqa_hotpotqa-validation-2452", "mrqa_naturalquestions-validation-1282", "mrqa_newsqa-validation-1893", "mrqa_searchqa-validation-2761", "mrqa_hotpotqa-validation-5320", "mrqa_hotpotqa-validation-3563"], "EFR": 0.8333333333333334, "Overall": 0.7171165589080459}, {"timecode": 58, "before_eval_results": {"predictions": ["Christian Louboutin", "apples", "Galapagos Islands", "For Gallantry", "west Side Story", "onions", "l Slovakia, and Hungary", "Mariah Carey", "blancmange", "Daily Herald", "four", "Isaac", "Dick Francis", "larkin", "dictes or Sayengis of the Philosophres", "opossum", "Soviets", "UKIP", "William Wallace", "charlie", "mASH", "helene hanff", "Cum mortuis in lingua mortua", "California condor", "lead", "France", "laos", "CNN", "Puerto Rico", "John Huston", "posh", "cat", "bajan", "aurochs", "dame sarah de Valois", "michael", "dame dame", "mercury", "the Kamikaze", "Sir Humphry Davy", "clarinets", "Mary Poppins", "The Mayor of Casterbridge", "Queensland", "blofeld", "George Eastman", "United Nations of Football", "Kenya", "george iv", "tuscany", "n Nissan", "Copernicus", "Toto", "commemorating fealty and filial piety", "Heather Elizabeth Langenkamp", "Operation Iceberg", "quarterly", "Rambosk", "Revolutionary Armed Forces of Colombia,", "a preliminary injunction", "Dr Jekyll and Mr Hyde", "pole-vaulting", "Maine", "(Coleridge) Coleridge"], "metric_results": {"EM": 0.5625, "QA-F1": 0.643646978021978}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-1383", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-1832", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-1708", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-4635", "mrqa_naturalquestions-validation-6149", "mrqa_hotpotqa-validation-2639", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-384", "mrqa_searchqa-validation-10238", "mrqa_searchqa-validation-444", "mrqa_searchqa-validation-5746"], "SR": 0.5625, "CSR": 0.571239406779661, "retrieved_ids": ["mrqa_squad-train-65758", "mrqa_squad-train-1096", "mrqa_squad-train-53476", "mrqa_squad-train-25113", "mrqa_squad-train-16305", "mrqa_squad-train-32049", "mrqa_squad-train-81683", "mrqa_squad-train-21320", "mrqa_squad-train-61809", "mrqa_squad-train-52598", "mrqa_squad-train-65698", "mrqa_squad-train-43508", "mrqa_squad-train-58808", "mrqa_squad-train-32172", "mrqa_squad-train-83015", "mrqa_squad-train-80624", "mrqa_squad-train-68614", "mrqa_squad-train-51783", "mrqa_squad-train-74801", "mrqa_squad-train-18301", "mrqa_squad-train-25478", "mrqa_squad-train-76857", "mrqa_squad-train-85319", "mrqa_squad-train-34005", "mrqa_triviaqa-validation-6380", "mrqa_searchqa-validation-11621", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-5140", "mrqa_newsqa-validation-3594", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-3542", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3508", "mrqa_searchqa-validation-15202", "mrqa_squad-validation-6477", "mrqa_newsqa-validation-1855", "mrqa_searchqa-validation-12611", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8617", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-5476", "mrqa_searchqa-validation-2743", "mrqa_hotpotqa-validation-5358", "mrqa_searchqa-validation-14371", "mrqa_triviaqa-validation-3447", "mrqa_hotpotqa-validation-2262", "mrqa_triviaqa-validation-4844"], "EFR": 1.0, "Overall": 0.7504197563559322}, {"timecode": 59, "before_eval_results": {"predictions": ["Jesus", "Aristotle", "Eliot Cutler", "goalkeeper", "David Weissman", "lexy gold", "comedy", "November 29, 1895", "Goddess of Pop", "Anthony Hopkins", "near Philip Billard Municipal Airport", "Big 12 Conference", "usually last two years", "Walt Disney and Ub Iwerks at the Walt Disney Studios in 1928", "Martin \"Marty\" McCann", "WB Television Network", "Gainsborough Trinity Football Club", "Artie turns out to be a terrible date", "$7.3 billion", "best known for his ten seasons with the Charlotte Hornets,", "George I of Great Britain", "sixteen", "Rural Electrification Act of 1936", "2015", "Nicholas \"Nick\" Offerman", "Golden Globe Award", "Jahseh Dwayne Onfroy", "Dire Straits", "American reality television series", "MGM Grand Garden Special Events Center", "Best Rock Song", "Pieter van Musschenbroek", "1979", "the 70 m and 90 m events", "Viscount Cranborne", "film and short novels", "Bulgarian-Canadian", "KXII", "James Bond", "Eastern College Athletic Conference", "Gujarat", "William Corcoran Eustis", "World Outgames", "Adelaide", "Saturday", "Shooter Jennings", "Can't Be Tamed", "Bolton, England", "Stephen Hawking", "Kaitlyn Wallace", "Saoirse Ronan", "Nacio Herb Brown ( music ) and Arthur Freed", "a region in Greek mythology", "Todd Bridges", "lemon", "dungarees", "Jaipur", "southern Gaza city of Rafah,", "the U.S. Consulate in Rio de Janeiro", "CNN", "All's Well That ends Well", "Alaska", "Rock Island, Illinois", "Captain James Cook"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7707200351731602}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.625, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.7499999999999999, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.28571428571428575, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-561", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-2429", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-3404", "mrqa_triviaqa-validation-3147", "mrqa_newsqa-validation-2731", "mrqa_searchqa-validation-6145", "mrqa_searchqa-validation-2444", "mrqa_searchqa-validation-15613"], "SR": 0.59375, "CSR": 0.5716145833333333, "retrieved_ids": ["mrqa_squad-train-44495", "mrqa_squad-train-54666", "mrqa_squad-train-51568", "mrqa_squad-train-27403", "mrqa_squad-train-37136", "mrqa_squad-train-70304", "mrqa_squad-train-5495", "mrqa_squad-train-1956", "mrqa_squad-train-24370", "mrqa_squad-train-85619", "mrqa_squad-train-55384", "mrqa_squad-train-61640", "mrqa_squad-train-68350", "mrqa_squad-train-15244", "mrqa_squad-train-40761", "mrqa_squad-train-45715", "mrqa_squad-train-7411", "mrqa_squad-train-63261", "mrqa_squad-train-43716", "mrqa_squad-train-37911", "mrqa_squad-train-13252", "mrqa_squad-train-70231", "mrqa_squad-train-76313", "mrqa_squad-train-25764", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-4561", "mrqa_hotpotqa-validation-650", "mrqa_searchqa-validation-1621", "mrqa_searchqa-validation-11271", "mrqa_triviaqa-validation-660", "mrqa_newsqa-validation-2244", "mrqa_hotpotqa-validation-1506", "mrqa_naturalquestions-validation-4824", "mrqa_newsqa-validation-3043", "mrqa_hotpotqa-validation-482", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-7330", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-9822", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-1135", "mrqa_newsqa-validation-423", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-2058", "mrqa_newsqa-validation-412", "mrqa_naturalquestions-validation-10128", "mrqa_hotpotqa-validation-4451"], "EFR": 0.9615384615384616, "Overall": 0.7428024839743589}, {"timecode": 60, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1496", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2256", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2456", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8282", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1668", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3627", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-468", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5622", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6755", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7112", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7558", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-899"], "OKR": 0.861328125, "KG": 0.48203125, "before_eval_results": {"predictions": ["Batman", "Part XI of the Indian constitution", "Frank Oz", "786 -- 802", "Patris et Filii et Spiritus Sancti", "19 July 1990", "John Ernest Crawford", "Andy Warhol", "December 19, 1971", "in Paradise", "France", "BC Jean and Toby Gad", "the BBC", "22 days", "961", "Jay Baruchel", "December 1886", "allowing private corporations and banks to sell or loan money to either side", "at the state and national governmental level", "judges", "Lori Rom", "Justin Johnson", "2018", "Coroebus of Elis", "giant planet", "Crepuscular animals", "Hank Williams", "after 5 years, it was earning $300,000,000 a year", "in a nearby river bottom", "the nature of Abraham Lincoln's war goals", "10 : 30am", "David Ben - Gurion", "RMS Titanic", "a warrior", "in San Francisco Bay", "Eight full seasons", "traditional dance", "Vasoepididymostomy", "the fourth quarter of the preceding year", "Rosalind Bailey", "God forgave / God gratified", "Broken Hill and Sydney", "Reverse - Flash", "save, rescue, savior", "sedimentary rock", "Sir Ronald Ross", "NFL Scouting combine", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "post translational modification", "nine hours from Coordinated Universal Time ( UTC \u2212 09 : 00 )", "near Camarillo, California", "goneril", "tomato", "Guru Nanak", "footballer", "mixed martial arts", "James Tinling", "Rima Fakih", "165-room", "David Bowie,", "Peach", "The Queen Charlotte Sound", "Godiva", "Sri Lanka Freedom Party"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6528521825396825}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.2222222222222222, 0.8, 1.0, 1.0, 0.16666666666666669, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.5714285714285715, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-437", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1966", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-7489", "mrqa_triviaqa-validation-1154", "mrqa_hotpotqa-validation-4952", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-12527"], "SR": 0.546875, "CSR": 0.5712090163934427, "retrieved_ids": ["mrqa_squad-train-4259", "mrqa_squad-train-41919", "mrqa_squad-train-65189", "mrqa_squad-train-4537", "mrqa_squad-train-35365", "mrqa_squad-train-36201", "mrqa_squad-train-23430", "mrqa_squad-train-2105", "mrqa_squad-train-53023", "mrqa_squad-train-74749", "mrqa_squad-train-50448", "mrqa_squad-train-79511", "mrqa_squad-train-65040", "mrqa_squad-train-17401", "mrqa_squad-train-36514", "mrqa_squad-train-5120", "mrqa_squad-train-7905", "mrqa_squad-train-66269", "mrqa_squad-train-78466", "mrqa_squad-train-55722", "mrqa_squad-train-12991", "mrqa_squad-train-63138", "mrqa_squad-train-83641", "mrqa_squad-train-64588", "mrqa_naturalquestions-validation-8765", "mrqa_searchqa-validation-3618", "mrqa_naturalquestions-validation-1423", "mrqa_triviaqa-validation-2843", "mrqa_newsqa-validation-3733", "mrqa_hotpotqa-validation-4322", "mrqa_naturalquestions-validation-9271", "mrqa_newsqa-validation-2024", "mrqa_searchqa-validation-9183", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-6915", "mrqa_naturalquestions-validation-3598", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-14371", "mrqa_squad-validation-3998", "mrqa_searchqa-validation-11621", "mrqa_naturalquestions-validation-10057", "mrqa_squad-validation-6848", "mrqa_searchqa-validation-2463", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-1905", "mrqa_searchqa-validation-1261", "mrqa_triviaqa-validation-7220"], "EFR": 1.0, "Overall": 0.7407261782786885}, {"timecode": 61, "before_eval_results": {"predictions": ["for the 1994 season", "Cuauhtemoc", "Conrad Lewis", "Bart Millard", "Pangaea or Pangea", "111", "Kiss", "Justice Harlan", "full '' sexual intercourse", "Valene Kane", "Georgia Groome", "the passing of the year", "November 28, 1973", "T.J. Miller", "condemns rural depopulation and the pursuit of excessive wealth", "Malina Weissman", "Pasek & Paul", "state sector", "937 total weeks", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "neutral", "1957", "cat in the hat", "1999", "the first to develop lethal injection as a method of execution", "the concentration of a compound exceeds its solubility", "February 9, 2018", "a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "Andrew Lloyd Webber", "Dollree Mapp", "the 15th century", "electron donors", "Nehemiah 1 : 5", "in a cell", "Beijing", "Mickey Mantle", "Shawn", "Kirsten Simone Vangsness", "dress shop", "the First Epistle of John", "1603", "September 25, 1987", "In 1987", "1939", "Randy Newman", "1956", "Ravi River", "Organisms in the domains of Archaea and Bacteria", "# 4", "Sweden's long - standing policy of neutrality was tested on many occasions during the 1930s", "New York City", "shoes and boots", "(Prince) Eriksson", "horses", "Macclesfield Town Football Club", "Polonius", "40 million", "Dr. Maria Siemionow,", "Brian Smith.", "gun charges,", "Ronald Reagan", "titanium", "Hastings", "urien"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6793161288638262}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.8421052631578948, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5555555555555556, 1.0, 0.5, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-7356", "mrqa_triviaqa-validation-5981", "mrqa_triviaqa-validation-2177", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-5479", "mrqa_searchqa-validation-15585", "mrqa_searchqa-validation-13746", "mrqa_triviaqa-validation-3768"], "SR": 0.578125, "CSR": 0.571320564516129, "retrieved_ids": ["mrqa_squad-train-10376", "mrqa_squad-train-233", "mrqa_squad-train-61743", "mrqa_squad-train-15772", "mrqa_squad-train-49511", "mrqa_squad-train-32558", "mrqa_squad-train-35629", "mrqa_squad-train-73376", "mrqa_squad-train-18406", "mrqa_squad-train-59279", "mrqa_squad-train-28995", "mrqa_squad-train-21385", "mrqa_squad-train-29885", "mrqa_squad-train-53553", "mrqa_squad-train-35736", "mrqa_squad-train-21175", "mrqa_squad-train-5003", "mrqa_squad-train-35338", "mrqa_squad-train-84590", "mrqa_squad-train-21043", "mrqa_squad-train-53804", "mrqa_squad-train-32723", "mrqa_squad-train-34746", "mrqa_squad-train-31250", "mrqa_squad-validation-1765", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-4653", "mrqa_squad-validation-3113", "mrqa_triviaqa-validation-2478", "mrqa_hotpotqa-validation-5311", "mrqa_newsqa-validation-3491", "mrqa_squad-validation-4326", "mrqa_triviaqa-validation-1348", "mrqa_naturalquestions-validation-52", "mrqa_searchqa-validation-13251", "mrqa_naturalquestions-validation-7812", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-6046", "mrqa_naturalquestions-validation-5155", "mrqa_squad-validation-8958", "mrqa_naturalquestions-validation-6466", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-4844", "mrqa_newsqa-validation-2001", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-5790"], "EFR": 0.9259259259259259, "Overall": 0.725933673088411}, {"timecode": 62, "before_eval_results": {"predictions": ["\"bay of geese,\"", "horseracing", "Italy", "honeybees", "adare", "linesider", "63 to 144 inches", "patcham", "squash", "(Jack) London", "AFC Wimbledon", "Scotland", "Edward VIII", "rabbit", "high gross national income (by some estimates the fourth-largest in Africa) gives the country a modest standard of living", "ambidextrous", "bear Grylls", "Japan", "wake", "silver", "yahoo!", "Klaus Barbie", "honey", "Joanne Harris", "The Cave Club", "Kunigunde Mackamotski", "Moldovan", "Chatsworth House", "India and Pakistan", "Bull Moose Party", "Mar Pac\u00edfico", "eagle", "Stockholm", "lapajne", "Hercules", "Real Madrid", "Tennessee Whiskey", "Matthew Pinsent", "Khomeini", "salsa", "Cuba", "John McEnroe", "kia", "Robert Stroud", "cat Stevens", "cuticle", "tyne", "oxygen", "mulberry", "trumpet", "Cockermouth", "more rural in its character and more readily adopted Latin as its common language", "October 1941", "Natya Shastra", "Luc Besson", "near Philip Billard Municipal Airport", "gull-wing doors", "July 8", "sexual harassment", "more than 5,600", "tuna", "banzai", "The Man of Other Days", "If These Dolls Could Talk"], "metric_results": {"EM": 0.625, "QA-F1": 0.6510416666666666}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-4965", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-740", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-5993", "mrqa_naturalquestions-validation-4416", "mrqa_hotpotqa-validation-652", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-230", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-9158"], "SR": 0.625, "CSR": 0.5721726190476191, "retrieved_ids": ["mrqa_squad-train-80456", "mrqa_squad-train-18337", "mrqa_squad-train-81429", "mrqa_squad-train-63112", "mrqa_squad-train-85947", "mrqa_squad-train-10571", "mrqa_squad-train-4585", "mrqa_squad-train-64534", "mrqa_squad-train-24642", "mrqa_squad-train-58375", "mrqa_squad-train-1908", "mrqa_squad-train-46516", "mrqa_squad-train-34032", "mrqa_squad-train-83221", "mrqa_squad-train-47345", "mrqa_squad-train-26213", "mrqa_squad-train-55246", "mrqa_squad-train-59797", "mrqa_squad-train-33510", "mrqa_squad-train-55051", "mrqa_squad-train-15453", "mrqa_squad-train-3500", "mrqa_squad-train-68810", "mrqa_squad-train-22693", "mrqa_searchqa-validation-11621", "mrqa_triviaqa-validation-5154", "mrqa_squad-validation-5818", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7281", "mrqa_squad-validation-4150", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-191", "mrqa_newsqa-validation-1520", "mrqa_triviaqa-validation-6233", "mrqa_squad-validation-7136", "mrqa_newsqa-validation-2371", "mrqa_naturalquestions-validation-7080", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-14194", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-143", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-4563"], "EFR": 0.9166666666666666, "Overall": 0.724252232142857}, {"timecode": 63, "before_eval_results": {"predictions": ["his mother", "the southern city of Naples", "\"CNN Heroes: An All-Star Tribute\"", "the rest of the year", "\"It was never our intention to offend anyone,\"", "Bob Bogle", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "his business dealings", "Saturday", "Samoans", "Haiti,", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Darrin Tuck,", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "a review of state government practices completed in 100 days.", "prostate cancer", "90", "a birdie four at the last hole", "Rima Fakih", "JBS Swift Beef Company,", "37", "slayings of actress Sharon Tate and four others.", "33-year-old", "Christopher Savoie", "12-hour-plus", "Judge Herman Thomas", "Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "bikinis", "laundry", "angry over the treatment of Muslims,", "twice.", "learn in safer surroundings.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "Friday", "Gavin de Becker", "400 years ago", "the U.S. Consulate in Rio de Janeiro", "Apple employees", "heavy turbulence", "$3 billion", "Nkepile M abuse", "resources", "mother's memories of his mother", "Lance Cpl. Maria Lauterbach", "then-Sen. Obama", "Technological Institute of Higher Learning of Monterrey,", "female soldier,", "\"It was incredible. We've had so much rain, and yet today it was beautiful.", "David Russ,", "Arnold Drummond", "Chinese", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "stability, security, and predictability of British law and government", "six", "Israel", "algebra", "Chuck Yeager", "Detroit", "on the north bank of the North Esk", "singer", "sea turtles", "atmosphere", "Tie", "Fort Kent, Maine, at the Canada -- US border, south to Key West, Florida"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7543840975408772}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 0.14545454545454548, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.9152542372881356, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.9600000000000001]}}, "before_error_ids": ["mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-3450", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3087", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-1827", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-2249", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-9739", "mrqa_searchqa-validation-9458", "mrqa_naturalquestions-validation-6670"], "SR": 0.609375, "CSR": 0.57275390625, "retrieved_ids": ["mrqa_squad-train-77714", "mrqa_squad-train-64588", "mrqa_squad-train-33618", "mrqa_squad-train-38443", "mrqa_squad-train-52961", "mrqa_squad-train-12568", "mrqa_squad-train-26090", "mrqa_squad-train-18397", "mrqa_squad-train-25451", "mrqa_squad-train-40741", "mrqa_squad-train-1295", "mrqa_squad-train-49748", "mrqa_squad-train-3031", "mrqa_squad-train-63233", "mrqa_squad-train-53501", "mrqa_squad-train-33727", "mrqa_squad-train-39429", "mrqa_squad-train-21149", "mrqa_squad-train-14592", "mrqa_squad-train-44817", "mrqa_squad-train-41820", "mrqa_squad-train-76373", "mrqa_squad-train-4425", "mrqa_squad-train-18844", "mrqa_naturalquestions-validation-9715", "mrqa_triviaqa-validation-7304", "mrqa_squad-validation-8786", "mrqa_triviaqa-validation-4922", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-3090", "mrqa_naturalquestions-validation-437", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-4729", "mrqa_newsqa-validation-3018", "mrqa_triviaqa-validation-705", "mrqa_newsqa-validation-3138", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-1062", "mrqa_squad-validation-9173", "mrqa_hotpotqa-validation-3063", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-2456", "mrqa_squad-validation-3130", "mrqa_naturalquestions-validation-4359", "mrqa_hotpotqa-validation-4879", "mrqa_triviaqa-validation-1125"], "EFR": 0.88, "Overall": 0.7170351562499999}, {"timecode": 64, "before_eval_results": {"predictions": ["Slavic culture", "beer", "sweepstakes", "General McClellan", "(John) Keats", "a great deal of wine", "Toto", "talc", "Bologna", "potatoes", "Princeton", "China", "the knight", "Volvic", "unicorns", "heaven", "Andes", "Jim Jarmusch", "Martin Luther", "Miles Davis", "Tennessee", "Audrey Hepburn", "Falafel", "Aladdin", "history of Lake County, Indiana, and the Calumet region", "Derek Jeter", "Arthur C. Clarke", "Washington Redskins", "Vietnam War", "Carl Sagan", "Sydney", "Christian Louboutin", "monk seal", "beer", "communication", "less than 1% fat", "Ginger Rogers", "Beijing", "plants", "Lafayette", "the Osmonds", "Pickwick Club", "pemmican", "comet tails", "Chuck Yeager", "Newton", "sheep", "Inheritance Cycle", "Russia", "french toast", "the Fifth Amendment", "Elvis Presley", "at the Louvre Museum in Paris", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "Cybill Shepherd", "jaws", "denarius", "18 November [O.S. 6 November] 1860", "White Horse", "the 1824 Constitution of Mexico", "Nirvana frontman,", "Glasgow, Scotland", "Christopher Savoie", "London"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6024548368298368}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.7878787878787877, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9479", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-13851", "mrqa_searchqa-validation-214", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-6953", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-6692", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-1377", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-11467", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-6284", "mrqa_searchqa-validation-13122", "mrqa_searchqa-validation-6828", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10795", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-5831", "mrqa_triviaqa-validation-51", "mrqa_hotpotqa-validation-4263", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-2011"], "SR": 0.53125, "CSR": 0.5721153846153846, "retrieved_ids": ["mrqa_squad-train-4118", "mrqa_squad-train-76144", "mrqa_squad-train-37058", "mrqa_squad-train-61287", "mrqa_squad-train-48513", "mrqa_squad-train-20341", "mrqa_squad-train-13201", "mrqa_squad-train-28374", "mrqa_squad-train-9794", "mrqa_squad-train-22665", "mrqa_squad-train-22599", "mrqa_squad-train-22033", "mrqa_squad-train-65429", "mrqa_squad-train-44529", "mrqa_squad-train-45148", "mrqa_squad-train-47061", "mrqa_squad-train-30510", "mrqa_squad-train-52373", "mrqa_squad-train-1171", "mrqa_squad-train-16028", "mrqa_squad-train-60234", "mrqa_squad-train-31630", "mrqa_squad-train-37296", "mrqa_squad-train-76638", "mrqa_searchqa-validation-14104", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-7727", "mrqa_squad-validation-8229", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-2010", "mrqa_triviaqa-validation-2073", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-3741", "mrqa_searchqa-validation-16076", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-4143", "mrqa_naturalquestions-validation-875", "mrqa_squad-validation-7876", "mrqa_squad-validation-3113", "mrqa_newsqa-validation-2074", "mrqa_triviaqa-validation-970", "mrqa_squad-validation-1765", "mrqa_naturalquestions-validation-715", "mrqa_triviaqa-validation-6896", "mrqa_hotpotqa-validation-2021", "mrqa_squad-validation-606", "mrqa_newsqa-validation-11"], "EFR": 0.9, "Overall": 0.7209074519230769}, {"timecode": 65, "before_eval_results": {"predictions": ["Montana", "the San Jose Mercury News", "Casino Royale", "William Tell", "The Apprentice", "Aeschylus", "the College of William and Mary", "Intelligence Quotient", "Stranger in a Strange Land", "cracker barrel", "an official batting statistic", "Cowpoke", "Monty Python and the Holy Grail", "Ludwig van Beethoven", "Stalin", "In God We Trust", "Portland", "Taiwan", "Absalom", "Castle Rock", "Bollywood", "Senator, you're no Jack Kennedy", "Habsburg", "joy", "a Twinkie", "the altitude", "The Book of Laughter and forgetting", "Richard", "Anne of Cleves", "SUFFIXES", "Won't you be mine", "Lili`uokalani", "the pituitary gland", "South African War", "the pulp", "Bush Sr.", "Aswan", "Billy Ray Cyrus", "a bolt", "The Body", "Impostor syndrome", "reducing", "Davy Crockett", "Sagittarius", "magma", "copper", "Dubliners", "Jules Verne", "Cuba", "the Taliban", "the Kennedy memorial", "Ali", "Otis Timson", "during prenatal development", "Exile", "Carmen Miranda", "bacall", "Harvey Birdman", "Chief of the Operations Staff of the Armed Forces High Command", "Erich Maria Remarque", "black civil rights leaders and prominent Democrats have largely bitten their tongues, unwilling to publicly take on the president and some of his decisions.", "ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "15-year-old's", "Qutab - ud - din Aibak"], "metric_results": {"EM": 0.546875, "QA-F1": 0.616607343951094}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.25, 0.0, 0.5, 0.1081081081081081, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15873", "mrqa_searchqa-validation-9446", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-2194", "mrqa_searchqa-validation-5095", "mrqa_searchqa-validation-10927", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-9120", "mrqa_searchqa-validation-11135", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-16276", "mrqa_searchqa-validation-3734", "mrqa_searchqa-validation-11604", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-8840", "mrqa_searchqa-validation-6273", "mrqa_searchqa-validation-1852", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-8703", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-2440", "mrqa_triviaqa-validation-6882", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-722"], "SR": 0.546875, "CSR": 0.5717329545454546, "retrieved_ids": ["mrqa_squad-train-74407", "mrqa_squad-train-47279", "mrqa_squad-train-48639", "mrqa_squad-train-22740", "mrqa_squad-train-1189", "mrqa_squad-train-9250", "mrqa_squad-train-21479", "mrqa_squad-train-3014", "mrqa_squad-train-14401", "mrqa_squad-train-18690", "mrqa_squad-train-86568", "mrqa_squad-train-20137", "mrqa_squad-train-84466", "mrqa_squad-train-67840", "mrqa_squad-train-62810", "mrqa_squad-train-16111", "mrqa_squad-train-6729", "mrqa_squad-train-62483", "mrqa_squad-train-27822", "mrqa_squad-train-66682", "mrqa_squad-train-10308", "mrqa_squad-train-38272", "mrqa_squad-train-20864", "mrqa_squad-train-60383", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8555", "mrqa_triviaqa-validation-3341", "mrqa_newsqa-validation-230", "mrqa_squad-validation-4838", "mrqa_newsqa-validation-1413", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-1747", "mrqa_triviaqa-validation-4150", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-3", "mrqa_searchqa-validation-10372", "mrqa_squad-validation-8459", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-767", "mrqa_triviaqa-validation-5209", "mrqa_naturalquestions-validation-8175", "mrqa_newsqa-validation-3232", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-6998", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-7424", "mrqa_hotpotqa-validation-5237", "mrqa_triviaqa-validation-3963"], "EFR": 0.7931034482758621, "Overall": 0.6994516555642634}, {"timecode": 66, "before_eval_results": {"predictions": ["eight", "function like an endocrine organ", "December 20, 1951", "the ninth w\u0101", "Terry Kath", "inability to comprehend and formulate language", "in the fascia surrounding skeletal muscle", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "1546", "Banquo", "January 1923", "Alex Drake", "a habitat", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie )", "Geophysicists who specialize in paleomagnetism", "free floating", "the United States", "the fifth studio album by English rock band the Beatles", "30 October 1918", "Austria - Hungary", "Domhnall Gleeson", "from 13 to 22 June 2012", "T - Bone Walker", "Professor Kantorek", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "the Executive Residence of the White House Complex", "Article Two", "April 13, 2018", "Bush", "Yuzuru Hanyu", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "off the southernmost tip of the South American mainland", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "in Rome", "January 1, 2016", "Leonardo da Vinci", "absolute temperature", "Thawne", "Philippe Petit", "Proposition 103", "2008", "inside the cell nucleus", "Julie Adams", "775", "Pakistan", "100 % owned by Xiu Li Dai and Yongge Dai", "the Miracles", "Americans who served in the armed forces and as civilians during World War II", "eight years", "James Fleet", "the year 1 BC", "David Davis", "Dirty Dancing", "mumps", "Delphi Lawrence", "2 May 2015", "International Boxing Hall of Fame", "J. Crew,", "Pakistani officials, India", "Almost all British troops in Iraq", "Jamaica", "tuna", "Python", "not guilty"], "metric_results": {"EM": 0.625, "QA-F1": 0.7493220459803968}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8095238095238095, 1.0, 1.0, 1.0, 0.5, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.9387755102040816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.923076923076923, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-4419", "mrqa_hotpotqa-validation-5549", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1259"], "SR": 0.625, "CSR": 0.5725279850746269, "retrieved_ids": ["mrqa_squad-train-50320", "mrqa_squad-train-62949", "mrqa_squad-train-42721", "mrqa_squad-train-56440", "mrqa_squad-train-23035", "mrqa_squad-train-25754", "mrqa_squad-train-36424", "mrqa_squad-train-82236", "mrqa_squad-train-54248", "mrqa_squad-train-57669", "mrqa_squad-train-76312", "mrqa_squad-train-63584", "mrqa_squad-train-6970", "mrqa_squad-train-48289", "mrqa_squad-train-43746", "mrqa_squad-train-33254", "mrqa_squad-train-66780", "mrqa_squad-train-74856", "mrqa_squad-train-61767", "mrqa_squad-train-34915", "mrqa_squad-train-21671", "mrqa_squad-train-17865", "mrqa_squad-train-80023", "mrqa_squad-train-18620", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-2399", "mrqa_searchqa-validation-4032", "mrqa_newsqa-validation-3500", "mrqa_triviaqa-validation-5981", "mrqa_searchqa-validation-1428", "mrqa_triviaqa-validation-2356", "mrqa_searchqa-validation-11406", "mrqa_hotpotqa-validation-741", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-15477", "mrqa_newsqa-validation-1762", "mrqa_triviaqa-validation-6396", "mrqa_searchqa-validation-16627", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3612", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-8703", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-2078", "mrqa_naturalquestions-validation-5951", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-9445", "mrqa_hotpotqa-validation-989"], "EFR": 0.9583333333333334, "Overall": 0.732656638681592}, {"timecode": 67, "before_eval_results": {"predictions": ["Pyeongchang County, Gangwon Province, South Korea", "Padawan", "in a liquid solution", "April 1917", "London", "Utah", "1970", "by October 1986", "the team that lost the pre-game coin toss", "English law", "all - female population to reproduce", "Concetta Tomei", "Reproductive system", "Taiwan", "local authorities", "terrier", "Gibraltar", "September 1947", "7 July", "in the bone marrow", "Sophia Akuffo", "who the better fighters are relative to their weight", "the team", "Sarah Josepha Hale", "Ingrid Bergman", "Jessica Simpson", "on the microscope's stage", "the Old Testament", "Daren Maxwell Kagasoff", "The municipal outdoor pool scenes at the beginning and the end of the movie were filmed at Steveston Outdoor pool in Richmond, BC", "Thomas Lennon", "Michigan and surrounding states and provinces", "a recognized group of people who jointly oversee the activities of an organization", "Friedman Billings Ramsey", "Miami Heat", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys", "Toronto Islands", "lighter fluid", "the final episode of the series", "Richard Carpenter", "Konakuppakatil Gopinathan Balakrishnan", "Logan Williams", "Border Collie", "Vesta's fire and the sun", "1665 to 1666", "Peptidoglycan, also known as murein, is a polymer consisting of sugars and amino acids that forms a mesh - like layer outside the plasma membrane of most bacteria", "Aegisthus", "on the continent of Antarctica", "California", "during Christmas season in the late 1970s", "December 1349", "Ipswich Town", "post-impressionist", "British Airways", "Genderqueer", "14,673", "YouTube", "five", "Joel \"Taz\" DiGregorio", "will give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "the Death Valley", "2016", "Ichabod Crane", "Thomas Jefferson"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6955824592663986}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5517241379310345, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.1904761904761905, 1.0, 0.1739130434782609, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.5925925925925926, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1269", "mrqa_naturalquestions-validation-4558", "mrqa_triviaqa-validation-263", "mrqa_hotpotqa-validation-621", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-4207", "mrqa_searchqa-validation-324"], "SR": 0.578125, "CSR": 0.5726102941176471, "retrieved_ids": ["mrqa_squad-train-14940", "mrqa_squad-train-41330", "mrqa_squad-train-71285", "mrqa_squad-train-69195", "mrqa_squad-train-45280", "mrqa_squad-train-60511", "mrqa_squad-train-76546", "mrqa_squad-train-21888", "mrqa_squad-train-85053", "mrqa_squad-train-18010", "mrqa_squad-train-68766", "mrqa_squad-train-60647", "mrqa_squad-train-41975", "mrqa_squad-train-51298", "mrqa_squad-train-56038", "mrqa_squad-train-28675", "mrqa_squad-train-14900", "mrqa_squad-train-85378", "mrqa_squad-train-40713", "mrqa_squad-train-13168", "mrqa_squad-train-22985", "mrqa_squad-train-37187", "mrqa_squad-train-76616", "mrqa_squad-train-24097", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-7062", "mrqa_searchqa-validation-123", "mrqa_hotpotqa-validation-227", "mrqa_naturalquestions-validation-7554", "mrqa_squad-validation-3130", "mrqa_triviaqa-validation-7011", "mrqa_searchqa-validation-13907", "mrqa_hotpotqa-validation-4560", "mrqa_searchqa-validation-6953", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-4132", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-3018", "mrqa_squad-validation-10316", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-4064", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-1621", "mrqa_naturalquestions-validation-10357", "mrqa_triviaqa-validation-4593", "mrqa_searchqa-validation-14569", "mrqa_squad-validation-6957", "mrqa_triviaqa-validation-2705"], "EFR": 0.9259259259259259, "Overall": 0.7261916190087147}, {"timecode": 68, "before_eval_results": {"predictions": ["Malaysia", "nomadic people, also known as nomads", "33.4%", "Parkinson's Disease", "Patrick Henry", "Warsaw", "CAPITAL", "Murfreesboro", "South Africa", "(Clay) Aiken", "Imam Ali", "Melanesia", "Joe Namath", "high and dry", "a doll", "the inquistador", "Cleopatra VII", "the International Space Station", "Iran", "Gaius Cassius", "the cakewalk", "South Africa", "(John) Deere", "Thames", "Oxford", "William Wordsworth", "Elphaba", "Tuscaloosa", "Germany", "Sabino Canyon", "Frasier Crane", "Brian Slade", "Sicily", "Herbert Hoover", "Zhou Enlai", "ham", "Lake Geneva", "(Barbie) Doll", "The Mole", "HIV/AIDS", "Today show", "Golden", "liver cancer", "Bern", "hollandaise", "Jackie Robinson", "uploadFiles", "Diane Arbus", "(Willa) Cather", "overruled", "the marathon", "Masha Skorobogatov", "Kyla Pratt", "Dumont d'Urville Station", "Union Gap", "Charlotte\\'s Web", "china", "Ding Sheng", "May 5, 2015", "Massapequa", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "December 7, 1941", "transit bombings", "acid phosphate"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5507383241758241}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.2857142857142857, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3218", "mrqa_searchqa-validation-2956", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-13698", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-2330", "mrqa_searchqa-validation-6130", "mrqa_searchqa-validation-1918", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-8156", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-5611", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-2941", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-6816", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-12621", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3166", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1457", "mrqa_triviaqa-validation-3820"], "SR": 0.453125, "CSR": 0.5708786231884058, "retrieved_ids": ["mrqa_squad-train-60811", "mrqa_squad-train-72420", "mrqa_squad-train-76489", "mrqa_squad-train-68838", "mrqa_squad-train-74358", "mrqa_squad-train-68927", "mrqa_squad-train-52754", "mrqa_squad-train-18165", "mrqa_squad-train-52428", "mrqa_squad-train-38456", "mrqa_squad-train-52874", "mrqa_squad-train-2215", "mrqa_squad-train-63418", "mrqa_squad-train-25028", "mrqa_squad-train-38722", "mrqa_squad-train-37219", "mrqa_squad-train-30889", "mrqa_squad-train-3939", "mrqa_squad-train-13598", "mrqa_squad-train-39031", "mrqa_squad-train-74066", "mrqa_squad-train-82236", "mrqa_squad-train-8380", "mrqa_squad-train-1588", "mrqa_searchqa-validation-16908", "mrqa_newsqa-validation-2308", "mrqa_searchqa-validation-15508", "mrqa_triviaqa-validation-7424", "mrqa_newsqa-validation-3415", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-1782", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-257", "mrqa_newsqa-validation-1136", "mrqa_triviaqa-validation-3917", "mrqa_naturalquestions-validation-3385", "mrqa_newsqa-validation-2476", "mrqa_squad-validation-5818", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-7464", "mrqa_naturalquestions-validation-5928", "mrqa_triviaqa-validation-5137", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-1813", "mrqa_triviaqa-validation-7727", "mrqa_hotpotqa-validation-4069"], "EFR": 0.9714285714285714, "Overall": 0.7349458139233954}, {"timecode": 69, "before_eval_results": {"predictions": ["an aqueduct", "a quark", "Christopher Reeve", "Bucharest", "ambition", "John Jacob Astor", "acting", "50th Street", "The Sun Also Rises", "Cherokee", "Ferrari", "banquet", "the High Plains area", "Joe Hill", "Revelation", "Kentucky", "Supernatural", "Jean Foucault", "Montana", "Deep brain stimulation", "krukhit", "Amazon", "Park Hill Oklahoma", "Anne Hathaway", "the Model T", "Iraq", "Vietnam", "William Wordsworth", "Canuck", "Jonathan", "Isaac Newton", "the Blue Ridge Mountain range", "Frdric Chopin", "Susan B. Anthony", "Dexter", "the opossum rat", "Washington Bullets", "a seagull", "the Prisoner of Azkaban", "Knocked Up", "Space Chimps", "Chick Corea", "jazz", "Massachusetts", "Han Solo", "george bk", "a proscenium arch", "president Porfirio Daz", "a veil", "Barry Goldwater", "Heineken", "Portugal. The Man", "1983", "Sally Dworsky", "oscar", "skee", "Greek", "Academy Award for Best Animated Feature", "1937", "Stephen James Ireland", "a 20-year-old Stanford University student,", "Copts", "\"an eye for an eye,\"", "Retina display"], "metric_results": {"EM": 0.5, "QA-F1": 0.5932291666666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.5, 1.0, 1.0, 0.5, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-14157", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-6956", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-14736", "mrqa_searchqa-validation-10184", "mrqa_searchqa-validation-9078", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-2333", "mrqa_searchqa-validation-6185", "mrqa_searchqa-validation-5427", "mrqa_triviaqa-validation-3274", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-7182", "mrqa_hotpotqa-validation-987", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-2435"], "SR": 0.5, "CSR": 0.5698660714285715, "retrieved_ids": ["mrqa_squad-train-34007", "mrqa_squad-train-75079", "mrqa_squad-train-50949", "mrqa_squad-train-54960", "mrqa_squad-train-9507", "mrqa_squad-train-36631", "mrqa_squad-train-19475", "mrqa_squad-train-5924", "mrqa_squad-train-53868", "mrqa_squad-train-29599", "mrqa_squad-train-1704", "mrqa_squad-train-68233", "mrqa_squad-train-85038", "mrqa_squad-train-6959", "mrqa_squad-train-74233", "mrqa_squad-train-38466", "mrqa_squad-train-5937", "mrqa_squad-train-55261", "mrqa_squad-train-85134", "mrqa_squad-train-3349", "mrqa_squad-train-36904", "mrqa_squad-train-17836", "mrqa_squad-train-73340", "mrqa_squad-train-12770", "mrqa_searchqa-validation-9446", "mrqa_hotpotqa-validation-1239", "mrqa_naturalquestions-validation-10009", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-326", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-2766", "mrqa_triviaqa-validation-2063", "mrqa_naturalquestions-validation-1974", "mrqa_newsqa-validation-1879", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4280", "mrqa_newsqa-validation-4032", "mrqa_naturalquestions-validation-1462", "mrqa_searchqa-validation-2456", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3660", "mrqa_naturalquestions-validation-2143", "mrqa_searchqa-validation-5149", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-5588", "mrqa_searchqa-validation-11576"], "EFR": 0.90625, "Overall": 0.7217075892857143}, {"timecode": 70, "UKR": 0.80078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2888", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-39", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-978", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1120", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4890", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-963", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14334", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3809", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-2467", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3832", "mrqa_squad-validation-3852", "mrqa_squad-validation-386", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-3994", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4467", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5493", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6638", "mrqa_squad-validation-6875", "mrqa_squad-validation-6957", "mrqa_squad-validation-7064", "mrqa_squad-validation-739", "mrqa_squad-validation-7549", "mrqa_squad-validation-7688", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-7917", "mrqa_squad-validation-8309", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-893", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-143", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5861", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-595", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917"], "OKR": 0.873046875, "KG": 0.51171875, "before_eval_results": {"predictions": ["Kathy Najimy", "2006 -- 06", "The Divergent Series : Ascendant", "Mel Tillis", "2026", "Clare Torry", "Andrew Lloyd Webber", "at the TV studio in the Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Health or vitality", "Stephen Graham", "6 - 6 with one win", "1955", "April", "Parthenogenesis", "fertilization", "Tevye", "famous figures", "in the stems and roots of certain vascular plants", "a Czech word, robota", "in skeletal muscle and the brain", "Nazi Germany and Fascist Italy", "Gunpei Yokoi", "David Motl", "the Senate has explicitly rejected twelve Supreme Court nominees in its history", "September 9, 2010", "A medium of exchange is a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "the late 18th century", "Buddhist", "Kiss", "Syco Music", "Trace Adkins", "the optic chiasm", "to manage the characteristics of the beer's head", "United States, the United Kingdom, and their respective allies", "letter series", "James Intveld", "15 February 1998", "Christopher Allen Lloyd", "100,000 writes", "January 2004", "Bartolomeu Dias", "Isabela Moner", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "potential of hydrogen", "Fall 1998", "the Qianlong Emperor", "Guwahati", "74 languages", "South Africa", "Rufus and Chaka Khan", "eight hoops", "Venado Tuerto", "Jamaica", "mead", "Tomorrowland", "Tallahassee City Commission", "January 18, 1977", "National Infrastructure Program,", "Martin Buber, Emanuel Levinas, or Primo Levi", "123 pounds of cocaine and 4.5 pounds of heroin", "In Memoriam", "Mercury", "Law & Order: Special Victims Unit", "UNICEF"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6387383276445777}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2, 0.18181818181818182, 0.888888888888889, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.75, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-1888", "mrqa_triviaqa-validation-4646", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1309", "mrqa_searchqa-validation-9696"], "SR": 0.515625, "CSR": 0.5691021126760563, "retrieved_ids": ["mrqa_squad-train-24835", "mrqa_squad-train-80484", "mrqa_squad-train-33336", "mrqa_squad-train-72363", "mrqa_squad-train-7551", "mrqa_squad-train-58462", "mrqa_squad-train-18609", "mrqa_squad-train-23680", "mrqa_squad-train-17043", "mrqa_squad-train-55380", "mrqa_squad-train-51285", "mrqa_squad-train-3557", "mrqa_squad-train-13244", "mrqa_squad-train-82875", "mrqa_squad-train-49287", "mrqa_squad-train-70787", "mrqa_squad-train-17208", "mrqa_squad-train-37654", "mrqa_squad-train-21446", "mrqa_squad-train-78667", "mrqa_squad-train-43257", "mrqa_squad-train-24349", "mrqa_squad-train-54021", "mrqa_squad-train-39611", "mrqa_triviaqa-validation-1062", "mrqa_naturalquestions-validation-2482", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-9196", "mrqa_newsqa-validation-1078", "mrqa_hotpotqa-validation-2395", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-5418", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-1415", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1714", "mrqa_newsqa-validation-2179", "mrqa_triviaqa-validation-3447", "mrqa_searchqa-validation-4044", "mrqa_triviaqa-validation-3752", "mrqa_naturalquestions-validation-6442", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-1678", "mrqa_hotpotqa-validation-2325", "mrqa_searchqa-validation-1654", "mrqa_squad-validation-4298", "mrqa_squad-validation-5724", "mrqa_searchqa-validation-3218"], "EFR": 0.8387096774193549, "Overall": 0.7186717330190822}, {"timecode": 71, "before_eval_results": {"predictions": ["Easter Island", "George Balanchine", "Jimi Hendrix", "the Cretaceous Era", "Austen", "Leonardo DiCaprio", "Basques", "Cherry Jones", "Happy Feet", "a guardian angel", "the Army", "the Tame", "Law", "the Caucasus", "June Carter Cash", "Bloemfontein", "a hermatypic coral", "David Glasgow Farragut", "1:24 a.m.", "salaried", "Skull", "Marie Osmond", "Scrabble", "suckers", "Catholicism", "London", "Burgenland", "Halliburton", "the macula", "Boston", "Anamosa", "Spelling Bee", "poetry", "the Battle of Fort Donelson", "1950s", "(born 1941)", "sucrose", "Cheshire", "Cuba", "The Prince and the Pauper", "Thomas Paine", "Abraham Lincoln", "Lord North", "Charles I", "Jemima", "Diane Arbus", "Gujarat", "Paul Redhead", "Utah", "Humulin", "Kublai Khan", "pulmonary heart disease ( cor pulmonale )", "Kimberlin Brown", "Henry Selick", "Caviar", "July", "argentina", "the vicar of Wantage", "Marc Bolan", "Polish-Jewish", "apartment building in Cologne, Germany,", "Kurdish region of Kurdish.", "$40 and a loaf of bread.", "argentina"], "metric_results": {"EM": 0.5, "QA-F1": 0.5749007936507937}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-2210", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-5919", "mrqa_searchqa-validation-4309", "mrqa_searchqa-validation-4000", "mrqa_searchqa-validation-12776", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-15548", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-12497", "mrqa_searchqa-validation-6350", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-15590", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-3746", "mrqa_hotpotqa-validation-3593", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3002", "mrqa_triviaqa-validation-1782"], "SR": 0.5, "CSR": 0.5681423611111112, "retrieved_ids": ["mrqa_squad-train-78586", "mrqa_squad-train-5732", "mrqa_squad-train-15840", "mrqa_squad-train-36885", "mrqa_squad-train-69859", "mrqa_squad-train-13346", "mrqa_squad-train-58111", "mrqa_squad-train-4356", "mrqa_squad-train-12681", "mrqa_squad-train-34982", "mrqa_squad-train-84980", "mrqa_squad-train-11994", "mrqa_squad-train-86254", "mrqa_squad-train-10963", "mrqa_squad-train-41359", "mrqa_squad-train-66197", "mrqa_squad-train-17947", "mrqa_squad-train-17943", "mrqa_squad-train-7528", "mrqa_squad-train-6191", "mrqa_squad-train-21591", "mrqa_squad-train-76145", "mrqa_squad-train-45813", "mrqa_squad-train-61567", "mrqa_triviaqa-validation-7439", "mrqa_newsqa-validation-2205", "mrqa_searchqa-validation-14371", "mrqa_naturalquestions-validation-3523", "mrqa_triviaqa-validation-1605", "mrqa_newsqa-validation-2288", "mrqa_hotpotqa-validation-3930", "mrqa_newsqa-validation-1144", "mrqa_naturalquestions-validation-186", "mrqa_hotpotqa-validation-2522", "mrqa_squad-validation-7613", "mrqa_naturalquestions-validation-1850", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-45", "mrqa_newsqa-validation-2998", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-16276", "mrqa_hotpotqa-validation-3059", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-6019", "mrqa_triviaqa-validation-7376", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-9364"], "EFR": 0.9375, "Overall": 0.7382378472222222}, {"timecode": 72, "before_eval_results": {"predictions": ["Vienna", "peninsulas", "(Midnight Special)", "Brazil", "TGI Fridays", "England", "Backgammon", "Steely Dan", "Artemis", "Hobart", "Colorado", "Cheap trick", "poached eggs", "Islam", "Cerberus", "Robert E. Lee", "Tobago", "Brigadoon", "Columbus", "Elijah Muhammad", "(Paul) Stevens", "Federico Fellini", "Fenway Park", "C.T. Eisler", "The Princess Diaries", "fluoridation", "Herman Melville", "Korea", "John Henry", "Babe Ruth", "Barbara Boxer", "Chicago", "Wallace & Gromit: The Curse of the Were-Rabbit", "sesame", "Nike", "Jack Nicholson", "nitrates", "Omaha", "dogs", "Paul Gauguin", "Francis Scott Key", "Mexico", "the Peashooter", "(Elbert) Gary", "money earned from recycling cans", "Massachusetts", "ACTIVE voice", "box office", "Alfred Hitchcock", "Basques", "Ambrose Bierce", "The president", "around 2.45 billion years ago", "Mary Margaret ( Ginnifer Goodwin )", "trotsky colombia", "meteoroids", "Argentina", "Edinburgh", "Campbellsville", "a French mathematician and physicist", "Hearst Castle.", "Sixteen years ago", "Brian Smith,", "Ballon d'Or"], "metric_results": {"EM": 0.625, "QA-F1": 0.7298363095238095}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4160", "mrqa_searchqa-validation-10654", "mrqa_searchqa-validation-1079", "mrqa_searchqa-validation-7797", "mrqa_searchqa-validation-1805", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-10948", "mrqa_searchqa-validation-5372", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-775", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-10907", "mrqa_searchqa-validation-3922", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-2889", "mrqa_triviaqa-validation-6507", "mrqa_triviaqa-validation-5187", "mrqa_hotpotqa-validation-662", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-1962"], "SR": 0.625, "CSR": 0.5689212328767124, "retrieved_ids": ["mrqa_squad-train-49349", "mrqa_squad-train-36913", "mrqa_squad-train-47166", "mrqa_squad-train-27604", "mrqa_squad-train-21144", "mrqa_squad-train-22881", "mrqa_squad-train-20656", "mrqa_squad-train-8808", "mrqa_squad-train-28405", "mrqa_squad-train-57184", "mrqa_squad-train-38222", "mrqa_squad-train-44564", "mrqa_squad-train-48777", "mrqa_squad-train-3231", "mrqa_squad-train-40301", "mrqa_squad-train-46682", "mrqa_squad-train-49121", "mrqa_squad-train-15599", "mrqa_squad-train-6309", "mrqa_squad-train-9372", "mrqa_squad-train-38951", "mrqa_squad-train-79448", "mrqa_squad-train-46758", "mrqa_squad-train-20103", "mrqa_naturalquestions-validation-10451", "mrqa_searchqa-validation-14101", "mrqa_hotpotqa-validation-652", "mrqa_searchqa-validation-6284", "mrqa_triviaqa-validation-899", "mrqa_searchqa-validation-11367", "mrqa_triviaqa-validation-6558", "mrqa_hotpotqa-validation-5792", "mrqa_triviaqa-validation-1334", "mrqa_naturalquestions-validation-4302", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-585", "mrqa_naturalquestions-validation-10128", "mrqa_triviaqa-validation-970", "mrqa_searchqa-validation-6463", "mrqa_hotpotqa-validation-3703", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-1372", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-2693", "mrqa_naturalquestions-validation-365", "mrqa_triviaqa-validation-4843", "mrqa_newsqa-validation-1514", "mrqa_squad-validation-10168"], "EFR": 0.9166666666666666, "Overall": 0.7342269549086758}, {"timecode": 73, "before_eval_results": {"predictions": ["barcarole", "Sinclair Lewis", "Hilary Swank", "sun Lust Pictures", "Coast Australia", "Israel", "Lundy", "Van Morrison", "Carbon", "Stuart Bingham", "Frank Darabont", "Proclamation of Neutrality", "Adam Smith", "latte", "organizational theory", "Volkswagen", "Bedser", "Oldham", "Netherland", "jabba the Hutt", "Andy Murray", "Crystal Gayle", "Zachary Taylor", "baku", "Chechnya", "John Buchan", "green", "Chester", "Hippety Hopper", "a bodice", "Mt Kenya", "a pumpkin", "london", "Sicily", "Switzerland", "magic circle", "Julie Andrews Edwards", "Pancho Villa", "Nigeria", "l Leeds", "Palm Sunday", "c Cologne", "Oliver!", "nippon", "Fidel Castro", "indiopia", "Renzo Piano", "duchamp", "Mexico", "n Carolina", "Friends", "water can flow from the sink into the faucet without modifying the system", "Tom Brady", "January to May 2014", "Forrest Gump", "John Anderson", "Mel Blanc", "Arnold Drummond", "dining scene", "Abhisit Vejjajiva", "Jackie Moon", "Maria Callas", "Edie", "intelligent design"], "metric_results": {"EM": 0.625, "QA-F1": 0.6859375000000001}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-6319", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-7621", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-6862", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-4028", "mrqa_hotpotqa-validation-1306", "mrqa_newsqa-validation-1828", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16531"], "SR": 0.625, "CSR": 0.5696790540540541, "retrieved_ids": ["mrqa_squad-train-76593", "mrqa_squad-train-63304", "mrqa_squad-train-32181", "mrqa_squad-train-21597", "mrqa_squad-train-49959", "mrqa_squad-train-53600", "mrqa_squad-train-9422", "mrqa_squad-train-36123", "mrqa_squad-train-36243", "mrqa_squad-train-52425", "mrqa_squad-train-842", "mrqa_squad-train-77521", "mrqa_squad-train-79063", "mrqa_squad-train-42475", "mrqa_squad-train-68274", "mrqa_squad-train-47109", "mrqa_squad-train-10989", "mrqa_squad-train-11834", "mrqa_squad-train-21312", "mrqa_squad-train-70669", "mrqa_squad-train-7263", "mrqa_squad-train-45847", "mrqa_squad-train-42457", "mrqa_squad-train-34412", "mrqa_hotpotqa-validation-227", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-1156", "mrqa_searchqa-validation-10473", "mrqa_triviaqa-validation-4864", "mrqa_newsqa-validation-2732", "mrqa_naturalquestions-validation-4674", "mrqa_searchqa-validation-13710", "mrqa_hotpotqa-validation-66", "mrqa_newsqa-validation-2399", "mrqa_hotpotqa-validation-1241", "mrqa_searchqa-validation-5919", "mrqa_searchqa-validation-4160", "mrqa_squad-validation-6957", "mrqa_triviaqa-validation-4057", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-1919", "mrqa_searchqa-validation-2568", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2239", "mrqa_newsqa-validation-1007", "mrqa_triviaqa-validation-1166", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-831"], "EFR": 0.8333333333333334, "Overall": 0.7177118524774775}, {"timecode": 74, "before_eval_results": {"predictions": ["james bardon", "james bond", "Royal Navy", "towed by a tow truck", "apple", "yellow", "dreamgirls", "spain", "the Antilles Current", "viola", "hay fever", "gin", "Canada", "it means that the rent doesn't include additional costs such as insurance or business rates", "whooping cough", "Peter Stuyvesant", "apple", "India and Pakistan", "The Labyrinth", "chiricahua", "the sinus node", "blucher", "Pope Pius XII", "sense of smell", "30", "george i", "Lincolnshire", "mabeleland", "republic of Ireland", "Skittles", "Anwar Sadat", "David Bowie", "Silent Spring", "b Bath and Wells", "Glenn Close and Rade Serbedzija", "Michael Sheen", "The Archers", "Tottenham Court Road", "Montmorency", "California condor", "12", "pudding Lane", "Pinocchio", "Permian", "president james Earl \" Jimmy\" Carter", "Jamie Oliver", "gear system", "wy Russell", "Petula Clark", "Coalition of the Radical Left (Syriza)", "The Blue Boy", "Border Collie", "Donald Sutherland", "fourth season", "George Whitefield", "Hermione Baddeley", "Floyd Casey Stadium", "David Bowie,", "economic growth and creating opportunity for our people.", "Robert Kimmitt", "Mammoth Cave", "recessive", "Charles Dickens", "Fayetteville, North Carolina"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6543154761904761}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-3482", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-4407", "mrqa_triviaqa-validation-2699", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-5738", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8404", "mrqa_newsqa-validation-3008", "mrqa_searchqa-validation-4464", "mrqa_hotpotqa-validation-3787"], "SR": 0.59375, "CSR": 0.5700000000000001, "retrieved_ids": ["mrqa_squad-train-32942", "mrqa_squad-train-67618", "mrqa_squad-train-59221", "mrqa_squad-train-7630", "mrqa_squad-train-53548", "mrqa_squad-train-74504", "mrqa_squad-train-33113", "mrqa_squad-train-26479", "mrqa_squad-train-41080", "mrqa_squad-train-30979", "mrqa_squad-train-63233", "mrqa_squad-train-21779", "mrqa_squad-train-82210", "mrqa_squad-train-72536", "mrqa_squad-train-57937", "mrqa_squad-train-12753", "mrqa_squad-train-15125", "mrqa_squad-train-14120", "mrqa_squad-train-58642", "mrqa_squad-train-23137", "mrqa_squad-train-44414", "mrqa_squad-train-50628", "mrqa_squad-train-29713", "mrqa_squad-train-14612", "mrqa_naturalquestions-validation-2605", "mrqa_searchqa-validation-5919", "mrqa_newsqa-validation-412", "mrqa_naturalquestions-validation-7164", "mrqa_newsqa-validation-908", "mrqa_triviaqa-validation-4965", "mrqa_hotpotqa-validation-876", "mrqa_naturalquestions-validation-4192", "mrqa_searchqa-validation-2623", "mrqa_searchqa-validation-3926", "mrqa_naturalquestions-validation-5550", "mrqa_newsqa-validation-3678", "mrqa_naturalquestions-validation-8500", "mrqa_triviaqa-validation-1917", "mrqa_searchqa-validation-12621", "mrqa_naturalquestions-validation-5826", "mrqa_newsqa-validation-2802", "mrqa_naturalquestions-validation-7009", "mrqa_hotpotqa-validation-5117", "mrqa_squad-validation-3718", "mrqa_newsqa-validation-4054", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-8254", "mrqa_triviaqa-validation-2495"], "EFR": 0.9230769230769231, "Overall": 0.7357247596153846}, {"timecode": 75, "before_eval_results": {"predictions": ["new Zealand", "1961", "rag\u00f9", "daleks", "duke orsino", "james iv", "Miguel de Cervantes", "Budapest", "Gillette", "some", "spain", "Bash Street", "Milwaukee", "the innermost digit of the forelimb", "swallow Sidecar Company", "Chicago", "Brett Favre", "Netherlands", "Gryffindor", "gold hallmarks", "John Buchan", "Pyrenees", "17", "spain", "montreal", "Elysium", "algebra", "Eddie Murphy", "Crete", "spain", "Copenhagen", "vena cava", "james boswell", "orca", "Christopher Nolan", "purple", "chess", "Ireland", "diana vickers", "February", "Damian Green", "argon", "bagel", "France", "South Dakota", "Alexander Dubcek", "Denver", "Chicago Cubs", "st. Louis", "Iberia", "Rosetta Stone", "in the basic curriculum -- the enkuklios paideia or `` education in a circle '' -- of late Classical and Hellenistic Greece", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "the nucleus", "August 14, 1848", "1892", "Merck & Co.", "1,500", "Shenzhen in southern China.", "Iran", "cola", "Washington, D.C.", "sedimentary rock", "golf"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5911458333333333}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5653", "mrqa_triviaqa-validation-2012", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-156", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-4902", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-366", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4506", "mrqa_searchqa-validation-10770", "mrqa_searchqa-validation-2183"], "SR": 0.546875, "CSR": 0.5696957236842105, "retrieved_ids": ["mrqa_squad-train-32766", "mrqa_squad-train-74121", "mrqa_squad-train-7490", "mrqa_squad-train-33872", "mrqa_squad-train-64435", "mrqa_squad-train-67322", "mrqa_squad-train-29202", "mrqa_squad-train-40708", "mrqa_squad-train-67789", "mrqa_squad-train-19654", "mrqa_squad-train-314", "mrqa_squad-train-57389", "mrqa_squad-train-66441", "mrqa_squad-train-51630", "mrqa_squad-train-46537", "mrqa_squad-train-29788", "mrqa_squad-train-67429", "mrqa_squad-train-50052", "mrqa_squad-train-16010", "mrqa_squad-train-82859", "mrqa_squad-train-34164", "mrqa_squad-train-15369", "mrqa_squad-train-69862", "mrqa_squad-train-71644", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-5427", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-423", "mrqa_triviaqa-validation-957", "mrqa_newsqa-validation-3781", "mrqa_searchqa-validation-5611", "mrqa_hotpotqa-validation-1030", "mrqa_triviaqa-validation-781", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-270", "mrqa_naturalquestions-validation-1884", "mrqa_newsqa-validation-2346", "mrqa_hotpotqa-validation-2769", "mrqa_naturalquestions-validation-3602", "mrqa_searchqa-validation-7724", "mrqa_naturalquestions-validation-5017", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-6748", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-3285"], "EFR": 0.8620689655172413, "Overall": 0.7234623128402904}, {"timecode": 76, "before_eval_results": {"predictions": ["English author Rudyard Kipling", "Andrew Garfield", "California, Utah and Arizona", "prospective studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "William Chatterton Dix", "1924", "September 27, 2017", "Montgomery", "Scheria", "Sanchez Navarro", "Thomas Jefferson", "August 2, 1990", "Joe Pizzulo and Leeza Miller", "Julie Adams", "Richard Bremmer", "from the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "a Native American nation from the Great Plains", "in capillaries, alveoli, glomeruli, outer layer of skin", "the ARPANET", "April 1979", "Tbilisi", "a security feature for `` card not present '' payment card transactions instituted to reduce the incidence of credit card fraud", "Tom Robinson", "four", "Liam Cunningham", "2013", "ummat al - Islamiyah", "# 4", "1980", "2017 season", "W. Edwards Deming", "Saphira", "usernames, passwords, commands and data", "1900 -- U.S. population exceeds 75 million", "the final years of the Third Republic", "Ajay Tyagi", "fr\u00e9d\u00e9ric Bazille", "Paul Revere", "Augustus", "Thespis", "1986", "Zeus", "For a single particle in a plane two coordinates define its location so it has two degrees of freedom", "April 10, 2018", "Lee County, Florida, United States", "mid November", "Kevin Spacey", "Fa Ze YouTubers", "two installments", "the French", "excessive growth", "Lingerie", "Amy Dorrit", "imperator", "Karolina Dean,", "four hundred", "Caesars Entertainment Corporation", "the U.S. intelligence community does not believe North Korea intends to launch a long-range missile in the near future,", "the sins of the members of the church", "Marcus Schrenker,", "horse-master", "a quartet", "ask for help", "Charice"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6631673580043145}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.17391304347826084, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.5714285714285715, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.8, 1.0, 0.7692307692307693, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-8608", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-860", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-5761", "mrqa_newsqa-validation-212", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-1590"], "SR": 0.5625, "CSR": 0.5696022727272727, "retrieved_ids": ["mrqa_squad-train-1429", "mrqa_squad-train-46910", "mrqa_squad-train-82241", "mrqa_squad-train-53329", "mrqa_squad-train-55169", "mrqa_squad-train-69788", "mrqa_squad-train-27662", "mrqa_squad-train-3390", "mrqa_squad-train-26196", "mrqa_squad-train-46766", "mrqa_squad-train-81278", "mrqa_squad-train-53188", "mrqa_squad-train-61532", "mrqa_squad-train-44973", "mrqa_squad-train-15888", "mrqa_squad-train-10553", "mrqa_squad-train-42538", "mrqa_squad-train-48033", "mrqa_squad-train-59947", "mrqa_squad-train-68669", "mrqa_squad-train-4215", "mrqa_squad-train-2575", "mrqa_squad-train-68520", "mrqa_squad-train-32384", "mrqa_newsqa-validation-3138", "mrqa_naturalquestions-validation-9979", "mrqa_hotpotqa-validation-2582", "mrqa_naturalquestions-validation-9419", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-290", "mrqa_newsqa-validation-3968", "mrqa_searchqa-validation-11467", "mrqa_newsqa-validation-3978", "mrqa_triviaqa-validation-2233", "mrqa_newsqa-validation-2591", "mrqa_triviaqa-validation-2495", "mrqa_naturalquestions-validation-7286", "mrqa_hotpotqa-validation-3844", "mrqa_newsqa-validation-2284", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-10654", "mrqa_newsqa-validation-2735", "mrqa_searchqa-validation-13003", "mrqa_naturalquestions-validation-6993", "mrqa_hotpotqa-validation-5667", "mrqa_newsqa-validation-1520", "mrqa_searchqa-validation-16209", "mrqa_naturalquestions-validation-1310"], "EFR": 0.7857142857142857, "Overall": 0.7081726866883117}, {"timecode": 77, "before_eval_results": {"predictions": ["President", "Istanbul", "Cana of Galilee", "fetch", "The Marriage of Figaro", "Jenny Craig", "Glitter", "Bayer", "picture book", "Karl Rove", "ethnic Latvians", "Ireland", "The Library", "Portland", "Florida Keys", "Doctor John Dolittle", "fish", "transmission", "hot air balloons", "vacuum tubes", "The Bridges of Madison County", "Italy", "iron", "LOUIS XIV", "ice cream", "Louis XIV", "catfish", "Alien", "the JFK assassination", "Indira Gandhi", "rodents", "Stephen Decatur", "Mary J. Blige", "his father", "Ms Lois Cassatt", "sister cities", "hurricanes", "The Wall Street Journal", "a fragmentation grenade", "Tinactin", "Virgin Atlantic", "Perrier", "Eastwick", "Richard III", "trout", "India", "Minnesota", "San Francisco", "rabbit", "latte", "handguns", "Brazil, Turkey and Uzbekistan", "Nicole DuPort", "species", "Farlake", "Piero da Vinci", "dada", "July 25 to August 4", "1755", "Trey Parker and Matt Stone", "more than 1.2 million", "Ferrari president Luca di Montezemolo", "Roger Federer", "agnolo Bronzino"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6638888888888889}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6923", "mrqa_searchqa-validation-692", "mrqa_searchqa-validation-8250", "mrqa_searchqa-validation-2938", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-15029", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-15498", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-5989", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-15164", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11923", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-2858", "mrqa_naturalquestions-validation-9830", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-3041", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-1364", "mrqa_triviaqa-validation-5253"], "SR": 0.578125, "CSR": 0.5697115384615384, "retrieved_ids": ["mrqa_squad-train-9967", "mrqa_squad-train-67027", "mrqa_squad-train-49283", "mrqa_squad-train-36835", "mrqa_squad-train-34598", "mrqa_squad-train-55739", "mrqa_squad-train-56580", "mrqa_squad-train-59868", "mrqa_squad-train-29520", "mrqa_squad-train-81291", "mrqa_squad-train-82393", "mrqa_squad-train-85347", "mrqa_squad-train-84286", "mrqa_squad-train-78805", "mrqa_squad-train-2131", "mrqa_squad-train-32443", "mrqa_squad-train-53841", "mrqa_squad-train-47295", "mrqa_squad-train-34611", "mrqa_squad-train-46802", "mrqa_squad-train-72662", "mrqa_squad-train-75888", "mrqa_squad-train-83575", "mrqa_squad-train-7822", "mrqa_newsqa-validation-1086", "mrqa_squad-validation-6655", "mrqa_searchqa-validation-10353", "mrqa_hotpotqa-validation-2262", "mrqa_searchqa-validation-4836", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5187", "mrqa_searchqa-validation-3479", "mrqa_newsqa-validation-1550", "mrqa_naturalquestions-validation-8175", "mrqa_searchqa-validation-3991", "mrqa_squad-validation-5818", "mrqa_searchqa-validation-13142", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-4002", "mrqa_newsqa-validation-742", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-5808", "mrqa_searchqa-validation-6185", "mrqa_hotpotqa-validation-631", "mrqa_triviaqa-validation-2853", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-1827", "mrqa_hotpotqa-validation-16"], "EFR": 0.9259259259259259, "Overall": 0.7362368678774929}, {"timecode": 78, "before_eval_results": {"predictions": ["Tycho Brahe", "Little Miss Sunshine", "Philadelphia", "Peter Rabbit", "Tommy Franks", "Baghdad", "Jonny Quest", "Uganda", "Fort Sumter", "Love Story", "Captains Courageous", "Bryan Adams", "Moses", "Mechanical", "Chaucer", "Toronto Blue Jays", "lieutenant", "Isaac Asimov", "Sayonara", "Orient Express", "Dante", "Sir Walter Scott", "a rick", "Louisiana", "The Maltese Falcon", "General Douglas MacArthur", "Teflon", "eastern Kentucky", "PG-13", "occipital", "a spoon", "Little Red Riding Hood", "The Jonas Brothers", "Iceland", "the Popsicle", "Los Angeles", "paladin", "Chelsea Morning", "the comb", "Venice", "Paraguay", "Theodor Wilhelm Hoffmann", "debts", "the Cowardly Lion", "El Supremo", "Foot Locker", "Princess Leia", "artichoke", "tributes", "Hammurabi", "video icon", "the ninth w\u0101", "Matt Monro", "on the two tablets", "Mt kenya", "Elvis Presley", "Boston Legal", "most awarded female act of all-time", "Channel 4", "Mark Neary Donohue Jr.", "the Sadr City and Adhamiya districts of Baghdad City.", "a share in the royalties", "Arizona", "3D computer-animated comedy"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7463541666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12459", "mrqa_searchqa-validation-4423", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-6426", "mrqa_searchqa-validation-10152", "mrqa_searchqa-validation-8259", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-11821", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-12891", "mrqa_naturalquestions-validation-10310", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-4688", "mrqa_newsqa-validation-939"], "SR": 0.6875, "CSR": 0.5712025316455696, "retrieved_ids": ["mrqa_squad-train-4436", "mrqa_squad-train-54964", "mrqa_squad-train-13866", "mrqa_squad-train-83398", "mrqa_squad-train-33793", "mrqa_squad-train-65859", "mrqa_squad-train-61757", "mrqa_squad-train-19021", "mrqa_squad-train-379", "mrqa_squad-train-166", "mrqa_squad-train-11845", "mrqa_squad-train-34499", "mrqa_squad-train-3481", "mrqa_squad-train-85831", "mrqa_squad-train-9362", "mrqa_squad-train-31426", "mrqa_squad-train-66740", "mrqa_squad-train-10163", "mrqa_squad-train-63664", "mrqa_squad-train-19944", "mrqa_squad-train-79447", "mrqa_squad-train-73333", "mrqa_squad-train-67395", "mrqa_squad-train-51496", "mrqa_searchqa-validation-6463", "mrqa_newsqa-validation-412", "mrqa_hotpotqa-validation-788", "mrqa_squad-validation-2000", "mrqa_searchqa-validation-14446", "mrqa_newsqa-validation-1762", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-3811", "mrqa_searchqa-validation-14310", "mrqa_newsqa-validation-3620", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-2378", "mrqa_naturalquestions-validation-10687", "mrqa_newsqa-validation-830", "mrqa_searchqa-validation-5138", "mrqa_triviaqa-validation-4902", "mrqa_searchqa-validation-8710", "mrqa_triviaqa-validation-1348", "mrqa_searchqa-validation-5943", "mrqa_newsqa-validation-2886", "mrqa_hotpotqa-validation-4089", "mrqa_hotpotqa-validation-2848", "mrqa_searchqa-validation-15770", "mrqa_hotpotqa-validation-5292"], "EFR": 0.85, "Overall": 0.7213498813291139}, {"timecode": 79, "before_eval_results": {"predictions": ["the Confederate flag", "Banquo", "Detroit", "grow", "y", "Ford", "Joseph Campbell", "curmudgeon", "Faith Hill", "Novel", "a brush", "Edinburgh", "engineering", "Cyprus", "savanna", "the tandoor", "a floatplane", "piano", "Sure", "oyster", "Gilbert Grape", "Mrs. Barbara Bush", "Confederate soldier Inman", "an orangutan", "eggshells", "the... Hornet", "the Sadler", "Pakistan", "Delta Force", "Joe Pozzuoli", "Johns Hopkins", "Jason", "Mississippi River", "Damascus", "Oahu", "Devo", "biology", "stuffing", "Reading", "George Eliot", "the Cotton Bowl", "Shiloh", "ventriloquist", "Takana", "apples", "cedar", "Almond Joy", "The Children", "Sam Houston", "Hail Caesar", "cable cars", "July 14, 1969", "on permanent display at the Louvre Museum in Paris", "July 1, 1923", "gertrude", "conation Street", "The Boar", "Willie Nelson and Kris Kristofferson", "Sarajevo", "Anne No\u00eb", "Mother's Day", "Italian Serie A title", "The son of Gabon's former president", "Wildcats"], "metric_results": {"EM": 0.53125, "QA-F1": 0.578125}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-9910", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-12299", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-15892", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-8196", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-3790", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-13472", "mrqa_searchqa-validation-1853", "mrqa_searchqa-validation-9372", "mrqa_naturalquestions-validation-1446", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-6870", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-3155", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3923"], "SR": 0.53125, "CSR": 0.570703125, "retrieved_ids": ["mrqa_squad-train-74658", "mrqa_squad-train-42915", "mrqa_squad-train-20221", "mrqa_squad-train-46026", "mrqa_squad-train-19544", "mrqa_squad-train-63161", "mrqa_squad-train-70547", "mrqa_squad-train-1558", "mrqa_squad-train-74081", "mrqa_squad-train-52800", "mrqa_squad-train-20790", "mrqa_squad-train-37656", "mrqa_squad-train-85431", "mrqa_squad-train-30038", "mrqa_squad-train-54664", "mrqa_squad-train-69218", "mrqa_squad-train-8164", "mrqa_squad-train-73380", "mrqa_squad-train-50404", "mrqa_squad-train-68820", "mrqa_squad-train-45175", "mrqa_squad-train-62999", "mrqa_squad-train-46883", "mrqa_squad-train-33092", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-4675", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-5063", "mrqa_searchqa-validation-12611", "mrqa_newsqa-validation-212", "mrqa_searchqa-validation-7269", "mrqa_naturalquestions-validation-10357", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-4030", "mrqa_searchqa-validation-1182", "mrqa_naturalquestions-validation-7223", "mrqa_newsqa-validation-2723", "mrqa_searchqa-validation-7154", "mrqa_newsqa-validation-152", "mrqa_squad-validation-8229", "mrqa_naturalquestions-validation-10259", "mrqa_searchqa-validation-4000", "mrqa_triviaqa-validation-4922", "mrqa_hotpotqa-validation-3090", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1762", "mrqa_triviaqa-validation-3525", "mrqa_hotpotqa-validation-1273"], "EFR": 0.8666666666666667, "Overall": 0.7245833333333334}, {"timecode": 80, "UKR": 0.8125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3137", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8631", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-8803", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7052", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.884765625, "KG": 0.53515625, "before_eval_results": {"predictions": ["India", "partridge", "long-term effects", "Austria", "George IV", "azerbaijan", "alphabets", "Sisyphus", "state of Italy", "specialist", "Cambodia", "Moldova", "Check Me", "amharic", "Frank McCourt", "Furbys", "Arkansas", "Texas", "Norway", "archer", "will jonson", "paula seattle", "Roger Federer", "Charlie Chan", "Galileo Galilei", "Great British Bake Off", "World War I", "shekel", "George Sand", "michael caine", "Professor Brian Cox", "jack brabham", "Knutsford", "Casualty", "McDonnell Douglas", "tyne", "Missouri", "pam Rhodes", "Buckinghamshire", "Turkey", "domestic cat", "elephant", "nine", "One Direction", "Groucho Marx", "Brazil", "Kate Winslet", "Pakistan", "August 1925", "Benjamin Barker", "Rio Grande", "If waivers are requested outside the playing season, or before November 1, then the player shall be transferred to the team with the lowest points in the preceding season", "8 January 1999", "David Joseph Madden", "The Braes o' Bowhether", "Mary Astor", "al-Qaeda", "natural gas", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Madonna", "Hawaii", "Monaco", "P. D. James", "MacFarlane"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6885416666666666}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3969", "mrqa_triviaqa-validation-1559", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2733", "mrqa_triviaqa-validation-3764", "mrqa_naturalquestions-validation-215", "mrqa_hotpotqa-validation-2718", "mrqa_newsqa-validation-4110", "mrqa_searchqa-validation-12999"], "SR": 0.671875, "CSR": 0.5719521604938271, "retrieved_ids": ["mrqa_squad-train-70647", "mrqa_squad-train-56329", "mrqa_squad-train-65219", "mrqa_squad-train-50267", "mrqa_squad-train-68652", "mrqa_squad-train-58227", "mrqa_squad-train-6366", "mrqa_squad-train-74368", "mrqa_squad-train-41732", "mrqa_squad-train-35809", "mrqa_squad-train-69290", "mrqa_squad-train-41822", "mrqa_squad-train-45237", "mrqa_squad-train-30335", "mrqa_squad-train-66788", "mrqa_squad-train-86047", "mrqa_squad-train-8376", "mrqa_squad-train-81864", "mrqa_squad-train-21964", "mrqa_squad-train-74407", "mrqa_squad-train-48377", "mrqa_squad-train-45502", "mrqa_squad-train-2069", "mrqa_squad-train-50419", "mrqa_naturalquestions-validation-2210", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-12440", "mrqa_newsqa-validation-3469", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-1680", "mrqa_newsqa-validation-3502", "mrqa_hotpotqa-validation-3615", "mrqa_newsqa-validation-1364", "mrqa_searchqa-validation-3203", "mrqa_naturalquestions-validation-8095", "mrqa_triviaqa-validation-6324", "mrqa_naturalquestions-validation-289", "mrqa_newsqa-validation-3781", "mrqa_hotpotqa-validation-1239", "mrqa_triviaqa-validation-1154", "mrqa_hotpotqa-validation-5237", "mrqa_naturalquestions-validation-7549", "mrqa_squad-validation-4572", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-7523", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-16209", "mrqa_naturalquestions-validation-4212"], "EFR": 0.8095238095238095, "Overall": 0.7227795690035274}, {"timecode": 81, "before_eval_results": {"predictions": ["the main highway entrance at California State Route 1,", "the Latin alphabet", "Minneapolis Lakers", "John Dalton", "the Alamodome and city of San Antonio", "rear - view mirror", "The Golden Gate Bridge", "RAF, Fighter Command had achieved a great victory in successfully carrying out Sir Thomas Inskip's 1937 air policy of preventing the Germans from knocking Britain out of the war", "BC Jean and Toby Gad", "UNESCO", "September 2017", "Universal Pictures and Focus Features", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "September 29", "nine", "Tbilisi, Georgia", "April 1917", "1900", "Bryan Cranston", "Geothermal gradient", "around 10 : 30am", "frontal lobe", "Napoleon's planned invasion of the United Kingdom", "potential of hydrogen", "volcanic activity", "held that `` a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "the biblical Book of Exodus", "As of January 17, 2018, 201 episodes", "The pia mater", "members of the gay ( LGBT ) community", "Burbank, California", "1986", "2018 Winter Olympics", "rapid destruction of the donor red blood cells by host antibodies", "1603", "English author Rudyard Kipling", "March 16, 2018", "Fusajiro Yamauchi", "the rez", "2013", "the breast or lower chest", "Nick Wilton", "to be developed with Flash", "2018", "Saint Peter", "1960", "August 19, 2016", "Madison, Wisconsin", "Washington, Jay and Franklin", "ABC", "Brevet Colonel Robert E. Lee", "glockenspiel", "alaskan", "Hercules", "Elbow", "County Louth", "NCAA Division II", "Airbus A330-200", "about 50", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "Portugal", "gravity", "Lafayette C. Baker", "Yemen,"], "metric_results": {"EM": 0.625, "QA-F1": 0.7488863485237137}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.07407407407407407, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.7692307692307692, 1.0, 1.0, 0.7710843373493976, 0.8571428571428571, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5555555555555556, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4210526315789474, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-9521", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-2319", "mrqa_triviaqa-validation-4319", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-4136", "mrqa_searchqa-validation-2328"], "SR": 0.625, "CSR": 0.5725990853658536, "retrieved_ids": ["mrqa_squad-train-84077", "mrqa_squad-train-6249", "mrqa_squad-train-29879", "mrqa_squad-train-4058", "mrqa_squad-train-52026", "mrqa_squad-train-34334", "mrqa_squad-train-15051", "mrqa_squad-train-51970", "mrqa_squad-train-51362", "mrqa_squad-train-59083", "mrqa_squad-train-8170", "mrqa_squad-train-5632", "mrqa_squad-train-26116", "mrqa_squad-train-2162", "mrqa_squad-train-15976", "mrqa_squad-train-50593", "mrqa_squad-train-78076", "mrqa_squad-train-15924", "mrqa_squad-train-42417", "mrqa_squad-train-85059", "mrqa_squad-train-53", "mrqa_squad-train-70888", "mrqa_squad-train-3649", "mrqa_squad-train-35584", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-45", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-2768", "mrqa_searchqa-validation-971", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-4274", "mrqa_naturalquestions-validation-437", "mrqa_newsqa-validation-4064", "mrqa_triviaqa-validation-4886", "mrqa_searchqa-validation-16176", "mrqa_hotpotqa-validation-4408", "mrqa_triviaqa-validation-6846", "mrqa_newsqa-validation-1496", "mrqa_hotpotqa-validation-3304", "mrqa_newsqa-validation-3678", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-1534", "mrqa_naturalquestions-validation-7881", "mrqa_newsqa-validation-3697", "mrqa_squad-validation-5588", "mrqa_searchqa-validation-9222", "mrqa_newsqa-validation-3138"], "EFR": 0.8333333333333334, "Overall": 0.7276708587398374}, {"timecode": 82, "before_eval_results": {"predictions": ["Jon Stewart", "king henry i", "Ross Kemp", "Jumanji", "James Donald,", "William Shakespeare", "Christmas", "African violet", "Rod Stewart", "Gerald Ford", "bassoon", "Pembrokeshire Coast National Park", "Imola", "South Africa", "sows", "Persistence of Memory", "orangutan", "Time Machine", "uranus", "Tacitus", "Lady Gaga", "Mecca", "cirrus uncinus", "Georgia", "myxomatosis", "Jasper Fforde", "Philippines", "xerophyte", "Blur", "getting to know you", "The Last King of Scotland", "jaws", "Pearson PLC", "John Steinbeck", "The Bulletin", "violin", "Ross Bagdasarian", "Mark Hamill", "Shirley Bassey", "Burma", "rural producers who often don't own land and work small plots, with the family constituting most or all of the labor", "cryonics", "j\u00f8rn Utzon", "Another Day in Paradise", "decorate", "donaustadt", "Department of Justice", "South Africa", "rapid eye movement", "Antonio Vivaldi", "Corfu", "Walter Brennan", "a solitary figure who is not understood by others, but is actually wise", "Continental drift", "Major Charles White Whittlesey", "2015 Orange Bowl", "White Knights of the Ku Klux Klan", "6,000", "fill a million sandbags and place 700,000 around our city,\"", "Judge Herman Thomas", "Nanosecond", "Mazurka", "Fontdeck", "1945 to 1951"], "metric_results": {"EM": 0.71875, "QA-F1": 0.75}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2050", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4711", "mrqa_searchqa-validation-2044"], "SR": 0.71875, "CSR": 0.5743599397590362, "retrieved_ids": ["mrqa_squad-train-69450", "mrqa_squad-train-70393", "mrqa_squad-train-73997", "mrqa_squad-train-85449", "mrqa_squad-train-42098", "mrqa_squad-train-39011", "mrqa_squad-train-40975", "mrqa_squad-train-44925", "mrqa_squad-train-32995", "mrqa_squad-train-59659", "mrqa_squad-train-67112", "mrqa_squad-train-76258", "mrqa_squad-train-19316", "mrqa_squad-train-83645", "mrqa_squad-train-84024", "mrqa_squad-train-5212", "mrqa_squad-train-19297", "mrqa_squad-train-26517", "mrqa_squad-train-23697", "mrqa_squad-train-33016", "mrqa_squad-train-34259", "mrqa_squad-train-44187", "mrqa_squad-train-62787", "mrqa_squad-train-62786", "mrqa_triviaqa-validation-7497", "mrqa_squad-validation-7612", "mrqa_searchqa-validation-1182", "mrqa_triviaqa-validation-3076", "mrqa_naturalquestions-validation-3119", "mrqa_triviaqa-validation-3456", "mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-10279", "mrqa_squad-validation-606", "mrqa_hotpotqa-validation-3290", "mrqa_newsqa-validation-2001", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-5627", "mrqa_squad-validation-2932", "mrqa_triviaqa-validation-2754", "mrqa_naturalquestions-validation-3392", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-1303", "mrqa_hotpotqa-validation-1033", "mrqa_naturalquestions-validation-1823", "mrqa_triviaqa-validation-2361", "mrqa_naturalquestions-validation-3442", "mrqa_squad-validation-6072", "mrqa_naturalquestions-validation-1165"], "EFR": 0.6111111111111112, "Overall": 0.6835785851740295}, {"timecode": 83, "before_eval_results": {"predictions": ["Libya", "Syriza", "Fernez", "wrigley", "PJ Harvey", "pearl slaghoople", "seven", "Charles Taylor", "Palm Sunday", "dollar", "The Wicker Man", "lungs", "chess", "a spiderion", "Peter Nichols", "bear Grylls", "Count Basie", "John Glenn", "Xenophon", "Amundsen", "abbeys", "St. Augustine, Florida", "Ireland", "Michael Hordern", "Gerald Durrell", "Ishmael", "Switzerland", "climate", "battle tanks", "emmannuelle", "the Etruscan army", "James Van Allen", "wednesday", "Bulls Eye", "Britain", "welding boots", "Helen Gurley Brown", "city of Thebes", "The Jungle Book", "james", "Massachusetts", "Josh Brolin", "Hamlet", "Great Britain", "The Penguin", "His Majesty\u2019s Airship R34", "Banking", "Rock Follies of \u201977", "australia", "Ann Darrow", "lovechild", "1996", "Texas - style chili con carne", "16 June", "Squam Lake", "3D computer-animated comedy", "1902", "due to a shortage of landing fields available for practice, an offer to land near the Middleton house on April 3 was readily accepted.", "The Impeccable", "Justicialist Party, or PJ by its Spanish acronym,", "Ming Dynasty", "Prince Albert", "a crossword clue", "al Qaeda."], "metric_results": {"EM": 0.546875, "QA-F1": 0.651328689370485}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 0.9411764705882353, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-2611", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-3264", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2857", "mrqa_triviaqa-validation-7713", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-4848", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-5596", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-3703", "mrqa_searchqa-validation-581", "mrqa_searchqa-validation-6285"], "SR": 0.546875, "CSR": 0.5740327380952381, "retrieved_ids": ["mrqa_squad-train-38261", "mrqa_squad-train-82556", "mrqa_squad-train-18561", "mrqa_squad-train-42754", "mrqa_squad-train-81748", "mrqa_squad-train-58680", "mrqa_squad-train-39733", "mrqa_squad-train-4704", "mrqa_squad-train-37296", "mrqa_squad-train-8338", "mrqa_squad-train-10156", "mrqa_squad-train-10821", "mrqa_squad-train-8117", "mrqa_squad-train-72055", "mrqa_squad-train-17799", "mrqa_squad-train-68882", "mrqa_squad-train-81202", "mrqa_squad-train-430", "mrqa_squad-train-63080", "mrqa_squad-train-58016", "mrqa_squad-train-82179", "mrqa_squad-train-61635", "mrqa_squad-train-81656", "mrqa_squad-train-43106", "mrqa_hotpotqa-validation-260", "mrqa_naturalquestions-validation-527", "mrqa_hotpotqa-validation-4451", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-70", "mrqa_newsqa-validation-1290", "mrqa_searchqa-validation-2648", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4687", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14104", "mrqa_triviaqa-validation-2544", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-3274", "mrqa_hotpotqa-validation-4515", "mrqa_triviaqa-validation-2853", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5583", "mrqa_hotpotqa-validation-4545", "mrqa_naturalquestions-validation-10310", "mrqa_newsqa-validation-1383", "mrqa_triviaqa-validation-6920", "mrqa_searchqa-validation-13232", "mrqa_triviaqa-validation-5873"], "EFR": 0.8620689655172413, "Overall": 0.7337047157224958}, {"timecode": 84, "before_eval_results": {"predictions": ["ganges", "David Hilbert", "Halifax", "Portugal", "Q", "Franklin Delano Roosevelt", "Buncefield Depot", "leon", "coffee", "turanga leela", "stills galleries", "OK", "Brad Pitt,", "florida", "viscount Melbourne", "Jupiter Mining Corporation", "phil hartman", "Nouakchott", "lothbrok", "verona", "once every two weeks", "gail Webb", "Noah", "jimzebel", "budge", "Queen Victoria and Prince Albert", "Quentin Tarantino", "Dick Whittington", "The Comitium", "rowing", "ouwerks", "gin", "Supertramp", "leicestershire", "halogens", "Jackie Kennedy", "blue", "calcium carbonate", "utensils", "Cuba", "Lorraine", "Nicola Adams", "new york", "Andes", "Essex Eagles", "carry On quip", "American History X", "endometriosis", "music venue", "Brighton", "reneHiguita", "approximately 26,000 years", "Travis Tritt and Marty Stuart", "Norway", "Perdita", "Stormzy", "Tottenham Hotspur", "a potential military strike", "one count of attempted murder in the second degree", "Ma Khin Khin Leh,", "Nintendo Entertainment System (NES)", "Germaine Greer", "Jacob and Esau", "Surrey"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6541666666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.4, 1.0, 0.4, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3183", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-3345", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-3125", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-2381", "mrqa_triviaqa-validation-4587", "mrqa_hotpotqa-validation-3085", "mrqa_hotpotqa-validation-875", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-729", "mrqa_newsqa-validation-2718", "mrqa_searchqa-validation-4541", "mrqa_searchqa-validation-14852"], "SR": 0.5625, "CSR": 0.5738970588235295, "retrieved_ids": ["mrqa_squad-train-54663", "mrqa_squad-train-20566", "mrqa_squad-train-40779", "mrqa_squad-train-81693", "mrqa_squad-train-71011", "mrqa_squad-train-9081", "mrqa_squad-train-18177", "mrqa_squad-train-30388", "mrqa_squad-train-40086", "mrqa_squad-train-34405", "mrqa_squad-train-80708", "mrqa_squad-train-30414", "mrqa_squad-train-1494", "mrqa_squad-train-77863", "mrqa_squad-train-81975", "mrqa_squad-train-58699", "mrqa_squad-train-66270", "mrqa_squad-train-44192", "mrqa_squad-train-1422", "mrqa_squad-train-70536", "mrqa_squad-train-44808", "mrqa_squad-train-17532", "mrqa_squad-train-13563", "mrqa_squad-train-84998", "mrqa_naturalquestions-validation-2462", "mrqa_hotpotqa-validation-482", "mrqa_newsqa-validation-1879", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-399", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2146", "mrqa_searchqa-validation-5095", "mrqa_newsqa-validation-3476", "mrqa_searchqa-validation-13332", "mrqa_newsqa-validation-1309", "mrqa_naturalquestions-validation-10613", "mrqa_searchqa-validation-5501", "mrqa_squad-validation-8294", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7896", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3469", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-2429"], "EFR": 0.75, "Overall": 0.7112637867647059}, {"timecode": 85, "before_eval_results": {"predictions": ["eagle", "teacher", "Shaft", "semicubical parabola", "jets", "region of SW Asia", "canc\u00fan", "back", "Rudyard Kipling", "eat porridge", "The Life and Opinions of Tristram Shandy", "vincenzo Nibali", "160", "earth", "pram", "Relpromax Antitrust Inc.", "c Cyprus", "sheep", "Laos", "Toilet Lid Lock", "Andes Mountains of Chile", "George Sand", "18", "minder", "shepherd neame", "shoulder", "severn", "legs", "leighton Park School in Reading", "Saturday Night and Sunday Morning", "afterlife", "Monday of September", "1982", "bea", "Danish", "priesthood", "Pablo Escobar", "South Africa", "Microsoft", "Bolivia", "Napoleon Bonaparte", "secretary", "Apocalypse Now", "gumm sisters", "Amnesty International", "Wizard", "the Land of the Long White Cloud", "southern", "Renzo Piano", "50", "Russia", "before the first year begins", "2,579 steps", "Hold On", "1919", "\"Apatosaurus\"", "La Scala, Milan", "Virgin America", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "Police", "Daredevil", "Dr. George Washington Carver", "panda", "California, Texas and Florida,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6666046626984127}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2922", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-3394", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4290", "mrqa_triviaqa-validation-193", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-3109", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-3040", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-2820", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-672", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-3561", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-237", "mrqa_newsqa-validation-2338"], "SR": 0.59375, "CSR": 0.5741279069767442, "retrieved_ids": ["mrqa_squad-train-74745", "mrqa_squad-train-12455", "mrqa_squad-train-35777", "mrqa_squad-train-3348", "mrqa_squad-train-28272", "mrqa_squad-train-32799", "mrqa_squad-train-55430", "mrqa_squad-train-8148", "mrqa_squad-train-45971", "mrqa_squad-train-14413", "mrqa_squad-train-12324", "mrqa_squad-train-51170", "mrqa_squad-train-25898", "mrqa_squad-train-70564", "mrqa_squad-train-48489", "mrqa_squad-train-54808", "mrqa_squad-train-6290", "mrqa_squad-train-73076", "mrqa_squad-train-76400", "mrqa_squad-train-14572", "mrqa_squad-train-9863", "mrqa_squad-train-43219", "mrqa_squad-train-72744", "mrqa_squad-train-12066", "mrqa_naturalquestions-validation-10377", "mrqa_newsqa-validation-4030", "mrqa_naturalquestions-validation-1315", "mrqa_triviaqa-validation-3917", "mrqa_hotpotqa-validation-4545", "mrqa_squad-validation-5588", "mrqa_naturalquestions-validation-129", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3476", "mrqa_naturalquestions-validation-4824", "mrqa_triviaqa-validation-7401", "mrqa_newsqa-validation-3181", "mrqa_triviaqa-validation-3264", "mrqa_naturalquestions-validation-7896", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4201", "mrqa_hotpotqa-validation-5358", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3390", "mrqa_searchqa-validation-10770", "mrqa_searchqa-validation-9739", "mrqa_naturalquestions-validation-3848", "mrqa_hotpotqa-validation-5292", "mrqa_searchqa-validation-12119"], "EFR": 0.6538461538461539, "Overall": 0.6920791871645796}, {"timecode": 86, "before_eval_results": {"predictions": ["police car sits outside the Westroads Mall in Omaha, Nebraska,", "money or other discreet aid", "41,", "top designers,", "to promote the attempts but simply to oversee them in a fair and independent manner and ratify successful efforts.", "suicide bombing", "iCloud service", "Seasons of My Heart", "Johannesburg", "shot in the head", "Crandon, Wisconsin,", "Kenneth Cole", "$17,000", "137", "dental", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "School-age girls", "Theoneste Bagosora,", "\"The Lost Symbol,\"", "Haiti's", "54", "German authorities", "his brother to surrender.", "Roy Foster", "Mogadishu", "the \"face of the peace initiative has been attacked.\"", "16", "Jackson sitting in Renaissance-era clothes and holding a book.", "fighting charges of Nazi war crimes", "Boys And Girls alone", "London and Buenos Aires", "ALS6,", "public-television", "Dead Weather's \"Horehound\"", "\"he puts more heart and more passion in what he's doing than some of the other dancers.\"", "his parents", "an empty water bottle", "Samuel Herr, 26, and Juri Kibuishi,", "forged credit cards and identity theft", "the two were embedded with the rebels while working on a story about the region.", "\"I don't think I'll be particularly extravagant.\"", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "five female pastors", "Facebook and Google,", "central Cairo,", "NATO's International Security Assistance Force", "his death cast a shadow over festivities ahead of South Africa's highly- anticipated appearance in the rugby World Cup final with England this weekend.", "U.S. military helicopter", "mental health", "Dr. Death in Germany", "abusing its dominant position in the computer processing unit (CPU) market.", "Thomas Edison", "Randy", "Thomas Lennon", "1947", "bacall", "Mariette", "Boston Celtics", "Australian", "Oahu", "Florida", "cinnamon rolls", "the Seine", "Mary Tyler Moore Show"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6373244554707167}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.10526315789473684, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.10526315789473684, 1.0, 1.0, 0.5, 0.19512195121951217, 0.09090909090909091, 0.18181818181818182, 1.0, 0.5, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-982", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-227", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-3913", "mrqa_naturalquestions-validation-10724", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-4625", "mrqa_searchqa-validation-14248"], "SR": 0.59375, "CSR": 0.5743534482758621, "retrieved_ids": ["mrqa_squad-train-33656", "mrqa_squad-train-69600", "mrqa_squad-train-78478", "mrqa_squad-train-73791", "mrqa_squad-train-39042", "mrqa_squad-train-41398", "mrqa_squad-train-37467", "mrqa_squad-train-57458", "mrqa_squad-train-975", "mrqa_squad-train-37556", "mrqa_squad-train-84263", "mrqa_squad-train-61901", "mrqa_squad-train-85937", "mrqa_squad-train-1822", "mrqa_squad-train-8655", "mrqa_squad-train-33803", "mrqa_squad-train-72965", "mrqa_squad-train-33794", "mrqa_squad-train-4991", "mrqa_squad-train-53408", "mrqa_squad-train-7750", "mrqa_squad-train-66265", "mrqa_squad-train-42663", "mrqa_squad-train-77076", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-5716", "mrqa_newsqa-validation-3508", "mrqa_naturalquestions-validation-2503", "mrqa_hotpotqa-validation-652", "mrqa_newsqa-validation-3714", "mrqa_triviaqa-validation-23", "mrqa_searchqa-validation-14441", "mrqa_triviaqa-validation-1463", "mrqa_naturalquestions-validation-2482", "mrqa_newsqa-validation-501", "mrqa_searchqa-validation-6492", "mrqa_triviaqa-validation-326", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-6207", "mrqa_triviaqa-validation-3940", "mrqa_searchqa-validation-15247", "mrqa_triviaqa-validation-4798", "mrqa_searchqa-validation-10473", "mrqa_triviaqa-validation-6228", "mrqa_searchqa-validation-11406"], "EFR": 0.5384615384615384, "Overall": 0.6690473723474801}, {"timecode": 87, "before_eval_results": {"predictions": ["four", "yellow", "pertussis", "Kawasaki", "h Harrison Ford", "reservoirs", "equator", "wigan", "sugar baby love", "1981", "Bernardo Bertolucci", "The Seven Year Itch", "Dieppe raid", "Mediterranean", "cabbage", "la Boh\u00e8me", "safE: health secretary Jeremy Hunt", "midsomer Murders", "maggie Wheeler", "ishmael", "Aquaman", "American Civil War", "Christian Louboutin", "York Minster", "domestic chicken", "Mexican orange blossom", "herpes zoster", "Queen Mary", "rupiah", "lisping Violet- Elizabeth Bott", "estrades", "Illinois", "danelaw", "landlord\u2019s game", "aquatic fern", "Christine Keeler", "Silver Hatch", "magic", "Guatemala", "shoes", "butch Cassidy and the Sundance Kid", "dolly", "edwina currie", "Baton Rouge", "Warsaw", "2010", "Carole King", "drizzle", "casualty", "trimdon", "sleepless in seattle", "Telma Hopkins, Joyce Vincent Wilson and her sister Pamela Vincent on backing vocals", "1624", "Milira", "Wiltshire", "Austrian Volksbanks", "Napoleon III", "his dad's son.", "the Kurdish militant group in Turkey", "Elena Kagan", "an abacus", "the Maine", "the Marquis de Lafayette", "Donny Osmond"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6200334821428571}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4000000000000001, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2178", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-5185", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-587", "mrqa_triviaqa-validation-2756", "mrqa_triviaqa-validation-985", "mrqa_triviaqa-validation-2998", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-5775", "mrqa_naturalquestions-validation-2862", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-5487", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-1506", "mrqa_naturalquestions-validation-5696"], "SR": 0.546875, "CSR": 0.5740411931818181, "retrieved_ids": ["mrqa_squad-train-5662", "mrqa_squad-train-30005", "mrqa_squad-train-24274", "mrqa_squad-train-37996", "mrqa_squad-train-85", "mrqa_squad-train-11662", "mrqa_squad-train-75462", "mrqa_squad-train-81387", "mrqa_squad-train-7491", "mrqa_squad-train-42139", "mrqa_squad-train-24710", "mrqa_squad-train-20498", "mrqa_squad-train-18170", "mrqa_squad-train-72360", "mrqa_squad-train-85833", "mrqa_squad-train-73341", "mrqa_squad-train-63526", "mrqa_squad-train-56972", "mrqa_squad-train-62937", "mrqa_squad-train-66142", "mrqa_squad-train-69043", "mrqa_squad-train-25624", "mrqa_squad-train-44628", "mrqa_squad-train-16459", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-5600", "mrqa_searchqa-validation-10098", "mrqa_triviaqa-validation-6507", "mrqa_newsqa-validation-3508", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-5579", "mrqa_triviaqa-validation-6319", "mrqa_naturalquestions-validation-10653", "mrqa_searchqa-validation-11960", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5154", "mrqa_searchqa-validation-909", "mrqa_triviaqa-validation-7571", "mrqa_searchqa-validation-16229", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-6046", "mrqa_hotpotqa-validation-650", "mrqa_searchqa-validation-10536", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3658", "mrqa_triviaqa-validation-2685"], "EFR": 0.7931034482758621, "Overall": 0.7199133032915361}, {"timecode": 88, "before_eval_results": {"predictions": ["a turkey", "luau", "Pat Paulsen", "Paddington Bear", "the Arabian Peninsula", "gambling", "Mensheviks", "Balenciaga", "Calvin Klein", "a hatchet", "Hamlet", "baboon", "Chicken Little", "Bach", "Bangkok", "Eli Whitney", "John Smith", "James Buchanan Eads", "\"A Bug's Life\"", "NASCAR", "quiveir", "the joker", "President Richard Nixon", "Benito Mussolini", "a sheepshank", "Robert Burns", "Ebony", "Jack Nicklaus", "pen", "Las Vegas", "fiber", "lovegrass", "a portrait", "Lord of the Flies", "The Pursuit of Happyness", "Nickelback", "succotash", "Jack London", "Falklands", "acetone", "pecan", "adultery", "frankfurter", "Roanoke Colony", "Blackbeard", "Lindsay Davenport", "Borden", "SO2", "Amish", "dachshund", "Robert Frost", "Virginia Dare", "Ole Einar Bj\u00f8rndalen", "the first quarter of the 19th century", "George Washington", "Puerto Rico", "David Graham", "1987", "Jacobite uprising", "Leinster", "in a tenement in the Mumbai suburb of Chembur,", "Monday's", "Illinois Reform Commission", "t.S. Eliot"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6699425574425575}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.923076923076923, 1.0, 0.18181818181818182, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16628", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-15001", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-14289", "mrqa_searchqa-validation-5949", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-10042", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-2948", "mrqa_searchqa-validation-12850", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-2110", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-5816", "mrqa_triviaqa-validation-3013", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-220", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3631", "mrqa_triviaqa-validation-813"], "SR": 0.59375, "CSR": 0.5742626404494382, "retrieved_ids": ["mrqa_squad-train-6094", "mrqa_squad-train-12856", "mrqa_squad-train-51348", "mrqa_squad-train-62908", "mrqa_squad-train-83831", "mrqa_squad-train-78152", "mrqa_squad-train-52972", "mrqa_squad-train-32899", "mrqa_squad-train-83231", "mrqa_squad-train-27111", "mrqa_squad-train-28821", "mrqa_squad-train-61171", "mrqa_squad-train-80183", "mrqa_squad-train-18849", "mrqa_squad-train-30201", "mrqa_squad-train-28021", "mrqa_squad-train-18341", "mrqa_squad-train-68926", "mrqa_squad-train-25419", "mrqa_squad-train-10482", "mrqa_squad-train-45256", "mrqa_squad-train-70444", "mrqa_squad-train-73159", "mrqa_squad-train-79834", "mrqa_naturalquestions-validation-6040", "mrqa_squad-validation-1765", "mrqa_searchqa-validation-13851", "mrqa_triviaqa-validation-6846", "mrqa_newsqa-validation-2904", "mrqa_squad-validation-7708", "mrqa_newsqa-validation-3002", "mrqa_triviaqa-validation-1500", "mrqa_searchqa-validation-13377", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-456", "mrqa_squad-validation-4572", "mrqa_searchqa-validation-1852", "mrqa_naturalquestions-validation-8404", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-190", "mrqa_triviaqa-validation-630", "mrqa_newsqa-validation-3771", "mrqa_naturalquestions-validation-2245", "mrqa_hotpotqa-validation-5386", "mrqa_searchqa-validation-6816", "mrqa_triviaqa-validation-5187", "mrqa_triviaqa-validation-1562", "mrqa_naturalquestions-validation-6500"], "EFR": 0.7692307692307693, "Overall": 0.7151830569360416}, {"timecode": 89, "before_eval_results": {"predictions": ["Bill Bryson", "pink Panther", "Jordan", "Sweden", "sartre", "Motown", "Carl Johan", "Mars", "riyadh", "margot fonteyn", "Diane Keaton", "plutocracy", "dominoes", "ringway", "Radio 4 Extra", "purse", "cello", "U2", "spain", "australia", "auk", "weir", "belge", "soy", "George Best", "Time Bandits", "Jean-Paul Gaultier", "Red Rock West", "x", "zagreb", "handley Page", "presidential helicopter", "Zachary Taylor", "Hitler", "All Holies Day, or All Saints\u2019 Day,", "Shaft", "bacall", "Louis Le Vau", "Scotland", "Tripoli's", "jubilee line", "Abbey Theatre", "Maine", "willow", "b4425", "Denver", "week of June 14th", "Mel Blanc", "Lily Allen", "terrorism", "oats", "Wisconsin", "season seven", "Whig candidates William Henry Harrison ( the `` hero of Tippecanoe '' ) and John Tyler", "more than 230 tournaments", "Serie B league", "Mark O'Connor", "Colombia.", "a federal judge in Mississippi", "his comments had been taken out of context.", "Wade E. Pickren", "Dalits", "blot", "Karle Warren"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6586805555555555}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-6682", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-7104", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-7107", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1265", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-7737", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-1687", "mrqa_newsqa-validation-3566", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-9329", "mrqa_searchqa-validation-15793"], "SR": 0.609375, "CSR": 0.5746527777777778, "retrieved_ids": ["mrqa_squad-train-53473", "mrqa_squad-train-11642", "mrqa_squad-train-57906", "mrqa_squad-train-83198", "mrqa_squad-train-34914", "mrqa_squad-train-85652", "mrqa_squad-train-18015", "mrqa_squad-train-50655", "mrqa_squad-train-34681", "mrqa_squad-train-70377", "mrqa_squad-train-69890", "mrqa_squad-train-21144", "mrqa_squad-train-20729", "mrqa_squad-train-82266", "mrqa_squad-train-41827", "mrqa_squad-train-39392", "mrqa_squad-train-12526", "mrqa_squad-train-4280", "mrqa_squad-train-42670", "mrqa_squad-train-21497", "mrqa_squad-train-60100", "mrqa_squad-train-29488", "mrqa_squad-train-13906", "mrqa_squad-train-38929", "mrqa_hotpotqa-validation-573", "mrqa_searchqa-validation-792", "mrqa_naturalquestions-validation-5939", "mrqa_newsqa-validation-2346", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-8500", "mrqa_newsqa-validation-3034", "mrqa_naturalquestions-validation-2452", "mrqa_searchqa-validation-5254", "mrqa_naturalquestions-validation-5808", "mrqa_newsqa-validation-551", "mrqa_naturalquestions-validation-5785", "mrqa_triviaqa-validation-5654", "mrqa_searchqa-validation-13061", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-5579", "mrqa_searchqa-validation-8711", "mrqa_newsqa-validation-1144", "mrqa_triviaqa-validation-854", "mrqa_searchqa-validation-7774", "mrqa_triviaqa-validation-5187", "mrqa_newsqa-validation-1032", "mrqa_naturalquestions-validation-7143", "mrqa_newsqa-validation-1956"], "EFR": 0.76, "Overall": 0.7134149305555556}, {"timecode": 90, "UKR": 0.833984375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.880859375, "KG": 0.525, "before_eval_results": {"predictions": ["shooting star", "colleen McCullough", "The Lion King", "Cyprus", "f. Lee Bailey and Barry Scheck", "pokemon", "syria", "dove", "giraffe", "the first weekend in October", "stockton-on-Trent", "Venus", "leiter", "norwich", "colleen McCullough", "Egypt", "ummiorno di Regno", "drew carey", "Three Mile Island", "sp Sicily", "sunset bOULEVARD", "Bombe", "Brussels", "arrows", "The Quatermass Experiment", "spaghetti harvest", "Frogmore Estate or Gardens", "Emmy", "caucasus", "88", "cold Comfort Farm", "new year", "Iceland", "David Hilbert", "mediterranean", "Declaration of Independence", "Marlon Brando", "fish", "Cleopatra", "greenham", "grove schumann", "Whisky Galore", "Grace Slick", "michael caine", "fonds de la Recherche Scientifique", "Robert Boyle", "1929", "The Lone Gunmen", "Sue", "daily Herald", "jim Blake", "Brian Steele", "a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "from the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "Cherokee River", "Benedict of Nursia", "Wichita", "Jewish", "Friday,", "put him in \"solitary confinement.\"", "a dove", "method acting", "Annie Proulx", "\"Nude, Green Leaves and Bust\""], "metric_results": {"EM": 0.6875, "QA-F1": 0.7401041666666666}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-5925", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-4815", "mrqa_triviaqa-validation-3038", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-7098", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4925", "mrqa_naturalquestions-validation-10147", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-508"], "SR": 0.6875, "CSR": 0.5758928571428572, "retrieved_ids": ["mrqa_squad-train-65890", "mrqa_squad-train-51225", "mrqa_squad-train-64258", "mrqa_squad-train-58986", "mrqa_squad-train-59384", "mrqa_squad-train-23313", "mrqa_squad-train-29294", "mrqa_squad-train-12576", "mrqa_squad-train-25512", "mrqa_squad-train-85006", "mrqa_squad-train-47264", "mrqa_squad-train-58107", "mrqa_squad-train-18879", "mrqa_squad-train-44995", "mrqa_squad-train-5787", "mrqa_squad-train-78777", "mrqa_squad-train-24835", "mrqa_squad-train-3153", "mrqa_squad-train-59200", "mrqa_squad-train-11705", "mrqa_squad-train-30970", "mrqa_squad-train-72402", "mrqa_squad-train-68002", "mrqa_squad-train-70638", "mrqa_searchqa-validation-6350", "mrqa_triviaqa-validation-6511", "mrqa_newsqa-validation-1078", "mrqa_hotpotqa-validation-484", "mrqa_searchqa-validation-12129", "mrqa_newsqa-validation-2163", "mrqa_naturalquestions-validation-613", "mrqa_newsqa-validation-2328", "mrqa_triviaqa-validation-1782", "mrqa_hotpotqa-validation-2792", "mrqa_searchqa-validation-1649", "mrqa_hotpotqa-validation-2840", "mrqa_naturalquestions-validation-6993", "mrqa_newsqa-validation-2830", "mrqa_searchqa-validation-2444", "mrqa_squad-validation-384", "mrqa_naturalquestions-validation-8126", "mrqa_triviaqa-validation-3690", "mrqa_newsqa-validation-4118", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2198", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-7164"], "EFR": 0.85, "Overall": 0.7331473214285714}, {"timecode": 91, "before_eval_results": {"predictions": ["evangelical Christian periodical", "2011", "John McClane", "Marika Green", "Princeton University", "conservative", "Lombardy", "writer", "Elton John", "Newcastle upon Tyne, England.", "Lev Ivanovich Yashin", "Blackwood Partners Management Corporation", "1958", "2007", "Robots Overlords", "1776", "Plymouth Regional High School (PRHS)", "public", "1944", "Austria", "Ron Cowen and Daniel Lipman", "The Soloist", "indoor", "Hannaford", "January 30, 1930", "Dr. Alberto Taquini", "John Gotti", "Anderson Silva", "north", "Harrison Ford", "C. H. Greenblatt", "Taoiseach of Ireland", "Dissection", "The Killer", "seacoast region", "Javed Miandad", "Dorothy", "2017", "people working in film and the performing arts", "June 2, 2008", "The 8th Habit", "one", "London", "New Zealand", "Ready Player One", "1981 World Rowing Championships", "1989", "15,024", "North Atlantic Conference", "highland regions of Scotland", "Jeanne Tripplehorn", "March 2018", "New Zealand", "62", "flybe", "perry mason", "oscar goldsmith", "Afghan lawmakers", "James Whitehouse,", "Al-Shabaab", "Ford", "Jason Bourne", "the Kiwanis Club of Columbus", "bartering"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7614583333333333}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4948", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-4077", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-4208", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-3250", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-4341", "mrqa_newsqa-validation-3713", "mrqa_searchqa-validation-9994"], "SR": 0.6875, "CSR": 0.5771059782608696, "retrieved_ids": ["mrqa_squad-train-86125", "mrqa_squad-train-31060", "mrqa_squad-train-51619", "mrqa_squad-train-53734", "mrqa_squad-train-62693", "mrqa_squad-train-14015", "mrqa_squad-train-53263", "mrqa_squad-train-68996", "mrqa_squad-train-80907", "mrqa_squad-train-58460", "mrqa_squad-train-22544", "mrqa_squad-train-79897", "mrqa_squad-train-62640", "mrqa_squad-train-10567", "mrqa_squad-train-41005", "mrqa_squad-train-8798", "mrqa_squad-train-59254", "mrqa_squad-train-82451", "mrqa_squad-train-39821", "mrqa_squad-train-27818", "mrqa_squad-train-83367", "mrqa_squad-train-45573", "mrqa_squad-train-72376", "mrqa_squad-train-19627", "mrqa_triviaqa-validation-6748", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-7154", "mrqa_squad-validation-9161", "mrqa_newsqa-validation-4030", "mrqa_hotpotqa-validation-1239", "mrqa_newsqa-validation-3703", "mrqa_naturalquestions-validation-2462", "mrqa_squad-validation-606", "mrqa_naturalquestions-validation-2586", "mrqa_searchqa-validation-16366", "mrqa_searchqa-validation-16176", "mrqa_newsqa-validation-212", "mrqa_searchqa-validation-1377", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-5281", "mrqa_triviaqa-validation-7727", "mrqa_searchqa-validation-3322", "mrqa_hotpotqa-validation-1033", "mrqa_searchqa-validation-9260", "mrqa_naturalquestions-validation-8036", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-8175", "mrqa_triviaqa-validation-6920"], "EFR": 0.85, "Overall": 0.733389945652174}, {"timecode": 92, "before_eval_results": {"predictions": ["Burl Ives", "the capital of French Indochina", "Mary Ellen Mark", "one of the youngest publicly documented people to be identified as transgender", "Female Socceroos", "Odense Boldklub", "SpongeBob SquarePants 4-D", "Oldham County", "Grammar, logic, and rhetoric", "Wright brothers", "a research university with high research activity", "O.T. Genasis", "science fiction drama", "Speedway World Championship", "Citric acid", "200", "moth", "Gerald Hatten Buss", "Delacorte Press", "close range combat", "twice", "Eli Roth", "South Australia", "Lincoln Riley", "December 13, 1920", "Richard B. Riddick", "John McClane", "rural areas", "Orchard Central", "Art of Dying", "Book of Judges", "Ant Timpson, Ted Geoghegan and Tim League", "the Bahamian island of Great Exuma", "John Ford", "classical", "Marvel Comics", "7,500 and 40,000", "crafting and voting on legislation", "Earvin \"Magic\" Johnson Jr.", "Creech Air Force Base, Nevada", "Yasir Hussain", "Victoria", "Jennifer Aniston", "Long Island", "Volcano Bay", "25 December 2009", "the Hanna-Barbera show \"Birdman and the Galaxy Trio.\"", "Jango Fett", "\"Der Rosenkavalier\", \"Elektra\", \"Die Frau ohne Schatten\" and \"Four Last Songs\"", "Minnesota's 8th congressional district", "NBA 2K16", "Kat ( Jessie Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Professor Eobard Thawne", "India", "14", "beetle", "australia", "Daniel Nestor, from Canada,", "Kearny, New Jersey.", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "noodles", "Tennessee Williams", "Illinois", "faye h. Lawrence"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7050586566211565}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.33333333333333326, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.18181818181818182, 0.6666666666666666, 0.8571428571428571, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-446", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4211", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-41", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-1445", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-4979", "mrqa_hotpotqa-validation-4735", "mrqa_naturalquestions-validation-2472", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-8642", "mrqa_triviaqa-validation-4848"], "SR": 0.5625, "CSR": 0.5769489247311828, "retrieved_ids": ["mrqa_squad-train-14490", "mrqa_squad-train-39483", "mrqa_squad-train-41634", "mrqa_squad-train-51765", "mrqa_squad-train-80136", "mrqa_squad-train-70323", "mrqa_squad-train-33292", "mrqa_squad-train-29116", "mrqa_squad-train-11742", "mrqa_squad-train-22676", "mrqa_squad-train-29090", "mrqa_squad-train-15762", "mrqa_squad-train-81110", "mrqa_squad-train-80573", "mrqa_squad-train-61651", "mrqa_squad-train-14149", "mrqa_squad-train-54515", "mrqa_squad-train-6216", "mrqa_squad-train-25994", "mrqa_squad-train-75980", "mrqa_squad-train-38108", "mrqa_squad-train-74862", "mrqa_squad-train-78281", "mrqa_squad-train-44525", "mrqa_naturalquestions-validation-1224", "mrqa_newsqa-validation-1412", "mrqa_triviaqa-validation-5653", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-2733", "mrqa_newsqa-validation-978", "mrqa_searchqa-validation-2194", "mrqa_naturalquestions-validation-7310", "mrqa_searchqa-validation-16558", "mrqa_triviaqa-validation-4750", "mrqa_hotpotqa-validation-1241", "mrqa_triviaqa-validation-2749", "mrqa_squad-validation-7612", "mrqa_hotpotqa-validation-5627", "mrqa_searchqa-validation-1805", "mrqa_triviaqa-validation-4927", "mrqa_searchqa-validation-7724", "mrqa_triviaqa-validation-4060", "mrqa_naturalquestions-validation-7737", "mrqa_hotpotqa-validation-5792", "mrqa_newsqa-validation-2900", "mrqa_triviaqa-validation-4687", "mrqa_searchqa-validation-10249"], "EFR": 0.5714285714285714, "Overall": 0.6776442492319508}, {"timecode": 93, "before_eval_results": {"predictions": ["YIVO", "Archbishop of Canterbury", "Samuel Beckett", "January 28, 2016", "The Catholic Church in Ireland", "close range combat", "Iran", "Kate Millett", "Timothy Matthew Howard", "\"Lucky\"", "Do Kyung-soo", "John Hunt", "Kongo", "William Finn", "Sam Raimi", "The final of 2011 AFC Asian Cup", "suburb", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "Marine Corps Air Station Kaneohe Bay", "August 17, 2017", "Montagues and Capulets", "The Lancia Rally 037", "left", "Vladimir Menshov", "film", "Denmark and Norway", "Love and Theft", "C. W. Grafton", "Evey's mother", "My Love from the Star", "143,372", "Jack Kilby", "West Point Foundry", "Afghanistan", "Operation Sculpin", "guitar feedback", "Flushed Away", "George Washington Bridge", "Reginald Engelbach", "Van Diemen's Land", "Tampa", "Sergeant First Class", "140 million", "SpongeBob SquarePants 4-D", "StubHub Center", "Argentinian", "the Neotropical realm", "a large portion of rural Maine,", "1998", "The More", "former Ambassador to Japan Caroline Kennedy", "com TLD", "Andrew Lloyd Webber", "John Bull", "NASCAR", "The Avengers", "Yes", "mental health", "Madhav Kumar Nepal", "hardship", "Berlin", "William Golding", "The Odd Couple", "Americans who served in the armed forces and as civilians during World War II"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8452020202020201}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 1.0, 1.0, 0.5, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5676", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4235", "mrqa_naturalquestions-validation-4844", "mrqa_triviaqa-validation-2617", "mrqa_newsqa-validation-1063"], "SR": 0.78125, "CSR": 0.5791223404255319, "retrieved_ids": ["mrqa_squad-train-64937", "mrqa_squad-train-3849", "mrqa_squad-train-4844", "mrqa_squad-train-85176", "mrqa_squad-train-69724", "mrqa_squad-train-53422", "mrqa_squad-train-48106", "mrqa_squad-train-4422", "mrqa_squad-train-70025", "mrqa_squad-train-75409", "mrqa_squad-train-58690", "mrqa_squad-train-22541", "mrqa_squad-train-45912", "mrqa_squad-train-18445", "mrqa_squad-train-7814", "mrqa_squad-train-78468", "mrqa_squad-train-52374", "mrqa_squad-train-71104", "mrqa_squad-train-63117", "mrqa_squad-train-68036", "mrqa_squad-train-69949", "mrqa_squad-train-39168", "mrqa_squad-train-11946", "mrqa_squad-train-51579", "mrqa_searchqa-validation-14852", "mrqa_newsqa-validation-2976", "mrqa_squad-validation-7612", "mrqa_triviaqa-validation-4815", "mrqa_hotpotqa-validation-4546", "mrqa_naturalquestions-validation-191", "mrqa_newsqa-validation-3978", "mrqa_triviaqa-validation-6580", "mrqa_searchqa-validation-16576", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-3922", "mrqa_triviaqa-validation-6870", "mrqa_triviaqa-validation-7175", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-56", "mrqa_searchqa-validation-9762", "mrqa_naturalquestions-validation-215", "mrqa_triviaqa-validation-2945", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2452", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-5060", "mrqa_naturalquestions-validation-5726", "mrqa_triviaqa-validation-4668"], "EFR": 0.42857142857142855, "Overall": 0.649507503799392}, {"timecode": 94, "before_eval_results": {"predictions": ["Nutbush, TN,", "alpha", "6", "golf", "Einstein", "manchester", "southampton", "Bleak House", "Vienna", "Harry S. Truman", "york", "to make wrinkles", "Amy Tan", "Gatsby", "Charlie Chan", "1664", "Good Will Hunting", "earldom", "iain Duncan Smith", "silversmith", "room of george orwell", "Jim Peters", "nitrogen", "tobacco", "delilah", "infante", "cuckoo", "PPTH", "The Wicker Man", "green", "Canada", "Giacomo Meyerbeer", "Guardian", "John Huston", "Passenger Pigeon", "Anne Frank", "manchego", "Texas", "pi\u00f1a colada", "fauntleroy", "kachhi", "Petula Clark", "michael colquhoun", "Flo Rida", "The Comedy of Errors", "beer", "chemical origins of life", "Finland", "fructose", "dolma", "kempton park", "Cress", "Vesta's fire and the sun", "In the Season 5 premiere, `` Business Trip ''", "Tiffany & Company", "2010 to 2012", "Nathan Bedford Forrest", "Friday", "Six", "that things are going well for them personally.", "tanning", "Tulane", "Brody", "the Bactrian"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6895123106060606}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_triviaqa-validation-4669", "mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-1767", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-7655", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2276", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9903", "mrqa_hotpotqa-validation-2141", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-5376", "mrqa_naturalquestions-validation-8046"], "SR": 0.640625, "CSR": 0.5797697368421053, "retrieved_ids": ["mrqa_squad-train-70173", "mrqa_squad-train-37395", "mrqa_squad-train-4561", "mrqa_squad-train-61781", "mrqa_squad-train-39937", "mrqa_squad-train-72600", "mrqa_squad-train-43024", "mrqa_squad-train-59184", "mrqa_squad-train-42295", "mrqa_squad-train-56873", "mrqa_squad-train-69200", "mrqa_squad-train-12261", "mrqa_squad-train-15318", "mrqa_squad-train-32323", "mrqa_squad-train-79810", "mrqa_squad-train-76380", "mrqa_squad-train-23984", "mrqa_squad-train-29780", "mrqa_squad-train-46689", "mrqa_squad-train-51967", "mrqa_squad-train-4250", "mrqa_squad-train-20219", "mrqa_squad-train-78384", "mrqa_squad-train-77688", "mrqa_triviaqa-validation-774", "mrqa_hotpotqa-validation-2198", "mrqa_newsqa-validation-722", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-5161", "mrqa_newsqa-validation-2074", "mrqa_hotpotqa-validation-1864", "mrqa_triviaqa-validation-6575", "mrqa_naturalquestions-validation-4326", "mrqa_newsqa-validation-1962", "mrqa_triviaqa-validation-6746", "mrqa_naturalquestions-validation-953", "mrqa_squad-validation-2000", "mrqa_triviaqa-validation-6319", "mrqa_naturalquestions-validation-2010", "mrqa_searchqa-validation-4898", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-10156", "mrqa_triviaqa-validation-2361", "mrqa_searchqa-validation-508", "mrqa_naturalquestions-validation-2945", "mrqa_searchqa-validation-3970", "mrqa_hotpotqa-validation-4015", "mrqa_naturalquestions-validation-6211"], "EFR": 0.5217391304347826, "Overall": 0.6682705234553776}, {"timecode": 95, "before_eval_results": {"predictions": ["the Korean War", "$1.5 million", "Fernando Caceres", "37", "opposition parties", "\"green-card warriors\"", "\"His first task was to remedy the situation of America wielding a big stick for the last eight years.\"", "Secretary of State", "my wife's name", "menstruation", "U.S. senators", "to put a lid on the marking of Ashura", "Alexey Pajitnov", "Spc. Megan Lynn Touma", "regulators in the agency's Colorado office", "Niger Delta.", "Leo Frank,", "Sri Lanka", "Johannesburg", "last year's Gaza campaign", "people have fled their homes in the Somali capital of Mogadishu as a result of a militant offensive against government forces,", "heavy flannel or wool", "\"You can go from rags to riches there.", "near Pakistan's border with Afghanistan", "walk", "an independent homeland", "bill that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "ancient rituals in Olympia,", "Kim Jong Un", "Arthur E. Morgan III,", "billboards with an image of the burning World Trade Center", "Adidas", "Too many glass shards left by beer drinkers in the city center,", "E! News", "Cologne, Germany,", "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "70,000 or so", "The station", "one of Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "he gave the victims \"assurances of the church's action\"", "large accumulations of ice", "East Java", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "billions of dollars", "between June 20 and July 20", "Rod Blagojevich", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Majid Movahedi,", "violating anti-trust laws.", "Jonas", "billions of dollars in Chinese products each year,", "Windows Media Video ( WMV )", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Joe Pizzulo and Leeza Miller", "Alan Turing", "paisley", "a peasant's wife", "three", "The Apple iPod+HP", "Lithuanian", "Jeopardy!", "Krakauer", "The Sun Also Rises", "Rob Reiner"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6762462517058105}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9333333333333333, 0.0, 1.0, 0.0, 0.9743589743589743, 0.2, 1.0, 0.4, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.35714285714285715, 0.5, 1.0, 0.1111111111111111, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-1646", "mrqa_triviaqa-validation-4402", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-318", "mrqa_searchqa-validation-8089", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-10142"], "SR": 0.53125, "CSR": 0.5792643229166667, "retrieved_ids": ["mrqa_squad-train-38230", "mrqa_squad-train-66244", "mrqa_squad-train-29571", "mrqa_squad-train-18303", "mrqa_squad-train-20621", "mrqa_squad-train-13379", "mrqa_squad-train-66543", "mrqa_squad-train-66675", "mrqa_squad-train-40102", "mrqa_squad-train-23903", "mrqa_squad-train-44973", "mrqa_squad-train-39097", "mrqa_squad-train-74424", "mrqa_squad-train-40462", "mrqa_squad-train-58739", "mrqa_squad-train-82386", "mrqa_squad-train-9480", "mrqa_squad-train-3637", "mrqa_squad-train-81952", "mrqa_squad-train-30373", "mrqa_squad-train-52653", "mrqa_squad-train-42874", "mrqa_squad-train-63195", "mrqa_squad-train-26487", "mrqa_triviaqa-validation-7054", "mrqa_hotpotqa-validation-875", "mrqa_naturalquestions-validation-8907", "mrqa_hotpotqa-validation-2404", "mrqa_naturalquestions-validation-143", "mrqa_squad-validation-1037", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1461", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-4587", "mrqa_searchqa-validation-2463", "mrqa_naturalquestions-validation-9157", "mrqa_hotpotqa-validation-1906", "mrqa_triviaqa-validation-2307", "mrqa_newsqa-validation-744", "mrqa_searchqa-validation-1857", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-3059", "mrqa_searchqa-validation-2938", "mrqa_newsqa-validation-339", "mrqa_naturalquestions-validation-2605", "mrqa_newsqa-validation-1510"], "EFR": 0.4, "Overall": 0.6438216145833333}, {"timecode": 96, "before_eval_results": {"predictions": ["UNICEF", "poppy production", "security breach", "urged NATO to take a more active role in countering the spread of the", "American president toured a mosque, laid a wreath at the grave of the founder of the Turkish republic,", "Christopher Savoie", "Brad Blauser,", "Tuesday afternoon.", "Iowa,", "Dr. Jennifer Arnold and husband Bill Klein,", "Chinese", "Bloomberg", "1.2 million", "the estate with its 18th-century sights, sounds, and scents.", "Nearly eight in 10", "$250,000 for Rivers' charity: God's Love We Deliver.", "Flint, Michigan.", "the Revolutionary Armed Forces of Colombia, better known as FARC,", "Mexico", "Larry King", "Alberto Espinoza Barron,", "spiral into economic disaster.", "Four", "Math teacher Mawise Gumba", "burned over 65 percent of his body", "Brian Smith", "2-1", "motor scooter", "Hank Moody", "April 2010.", "Nothing But Love", "Mandi Hamlin", "people look at the content of the speech, not just the delivery.", "Yemen,", "Trevor Rees,", "an American who entered the country illegally from China", "don't have to visit laundromats", "the company has not yet managed to sell the concept to a buyer", "Manny Pacquiao", "\"I didn't think I was going to learn so much about myself through the process,\"", "did not identify any of the dead.", "President Thabo Mbeki", "Bright Automotive,", "Jeffrey Jamaleldine", "reached an agreement late Thursday", "Haiti,", "in Salt Lake City, Utah,", "his business dealings", "hardship for terminally ill patients and their caregivers,", "nearly 28 years", "Hollywood", "E \u00d7 12", "ThonMaker", "parashah", "syria", "the Benedictine Order", "Cecil", "sexy Star", "March 31, 1944", "Dutch", "the dragon", "Marcus Garvey", "the zodiac", "(William) Obsessive-compulsive"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6549789186507937}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.625, 0.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-905", "mrqa_newsqa-validation-1138", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-6184", "mrqa_hotpotqa-validation-5312", "mrqa_searchqa-validation-2594", "mrqa_searchqa-validation-9869"], "SR": 0.578125, "CSR": 0.5792525773195876, "retrieved_ids": ["mrqa_squad-train-7706", "mrqa_squad-train-50134", "mrqa_squad-train-8242", "mrqa_squad-train-69207", "mrqa_squad-train-60104", "mrqa_squad-train-10179", "mrqa_squad-train-65955", "mrqa_squad-train-12924", "mrqa_squad-train-35844", "mrqa_squad-train-34334", "mrqa_squad-train-57422", "mrqa_squad-train-77518", "mrqa_squad-train-57102", "mrqa_squad-train-52491", "mrqa_squad-train-62336", "mrqa_squad-train-83627", "mrqa_squad-train-71712", "mrqa_squad-train-24873", "mrqa_squad-train-59932", "mrqa_squad-train-23060", "mrqa_squad-train-80253", "mrqa_squad-train-51688", "mrqa_squad-train-4108", "mrqa_squad-train-3261", "mrqa_triviaqa-validation-6046", "mrqa_hotpotqa-validation-4399", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9239", "mrqa_triviaqa-validation-2516", "mrqa_newsqa-validation-3491", "mrqa_naturalquestions-validation-4563", "mrqa_squad-validation-7457", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3592", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-4066", "mrqa_hotpotqa-validation-631", "mrqa_searchqa-validation-11960", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-7269", "mrqa_naturalquestions-validation-3533", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2723", "mrqa_naturalquestions-validation-953", "mrqa_searchqa-validation-2829", "mrqa_naturalquestions-validation-8792", "mrqa_newsqa-validation-276"], "EFR": 0.3333333333333333, "Overall": 0.6304859321305842}, {"timecode": 97, "before_eval_results": {"predictions": ["Johannes Gutenberg", "every four years", "in the books of Exodus and Deuteronomy", "Thomas Jefferson", "about the level of the third lumbar vertebra, or L3, at birth", "Valmiki", "Pakistan", "for the red - bed country of its watershed", "United States, its NATO allies and others", "Rashida Jones", "cut off close by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "Most days are sunny throughout the year", "Peter Andrew Beardsley MBE", "Season two", "Terry Reid", "1273.6 cm", "May 3, 2005", "As of September 18, 2012, the chain operates 639 stores in 43 states", "the Rashidun Caliphs", "British Columbia, Canada", "Koine Greek : apokalypsis", "Pyeongchang County, Gangwon Province, South Korea", "interstellar medium", "a Roman Catholic and fan of The Godfather Part II ( 1974 )", "1943", "Tokyo for the 2020 Summer Olympics", "Lituya Bay in Alaska", "San Jose, California", "Panzerkampfwagen VIII Maus", "July 2, 1776", "accomplish the objectives of the organization", "Domhnall Gleeson", "Simon Callow", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a pancake house which would destroy a colony of burrowing owls who live on the site", "Laura Jane Haddock", "May 2016", "Bacon", "1994", "Cam Clarke", "senators", "origins of replication, in the genome", "the fourth quarter of the preceding year", "April 2016", "Michael Schumacher", "Massachusetts", "Latitude", "2015", "post translational modification", "the rise of literacy, technological advances in printing, and improved economics of distribution", "writ of certiorari", "Jules Shear", "Senegal", "paen\u012bnsula", "brain", "Romeo", "De La Soul", "Delilah Rene", "July", "says conviction of Peru's ex-president is a warning to those who deny human rights.", "83,03013", "the Missouri", "jade", "Frank Sinatra", "Long troop deployments"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6687312111082602}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.2857142857142857, 1.0, 0.15384615384615383, 0.2857142857142857, 1.0, 1.0, 0.4444444444444445, 1.0, 0.07407407407407407, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.29508196721311475, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06060606060606061, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.9600000000000001, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-6687", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-9772", "mrqa_triviaqa-validation-2425", "mrqa_triviaqa-validation-7592", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1987", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2892"], "SR": 0.59375, "CSR": 0.5794005102040816, "retrieved_ids": ["mrqa_squad-train-10578", "mrqa_squad-train-52215", "mrqa_squad-train-76141", "mrqa_squad-train-22131", "mrqa_squad-train-5514", "mrqa_squad-train-56809", "mrqa_squad-train-48265", "mrqa_squad-train-83193", "mrqa_squad-train-12581", "mrqa_squad-train-56500", "mrqa_squad-train-79576", "mrqa_squad-train-84465", "mrqa_squad-train-80787", "mrqa_squad-train-51711", "mrqa_squad-train-77436", "mrqa_squad-train-73728", "mrqa_squad-train-30610", "mrqa_squad-train-69593", "mrqa_squad-train-5856", "mrqa_squad-train-10988", "mrqa_squad-train-70375", "mrqa_squad-train-51287", "mrqa_squad-train-61667", "mrqa_squad-train-15642", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-3338", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3697", "mrqa_triviaqa-validation-2361", "mrqa_searchqa-validation-10856", "mrqa_newsqa-validation-1413", "mrqa_naturalquestions-validation-6019", "mrqa_hotpotqa-validation-2282", "mrqa_triviaqa-validation-6046", "mrqa_naturalquestions-validation-681", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-11502", "mrqa_naturalquestions-validation-1372", "mrqa_newsqa-validation-2011", "mrqa_triviaqa-validation-5106", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-5034", "mrqa_searchqa-validation-8544", "mrqa_naturalquestions-validation-2743", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2632", "mrqa_triviaqa-validation-6548"], "EFR": 0.4230769230769231, "Overall": 0.6484642366562009}, {"timecode": 98, "before_eval_results": {"predictions": ["hair", "Phobos", "Lana Turner", "a Polaroid picture", "New York City", "Eris Carter Cash", "the owl", "Brian", "fat", "poison ivy", "Denny McLain", "road", "Edith Wharton", "Liberia", "Rockabilly", "the Royal Court", "AARP", "Arturo Toscanini", "Bangladesh", "Saturn", "Nancy Pelosi", "Unison", "Dr. Pepper", "misery", "the Black Swallower", "explosives", "Iowa", "kidnapping", "Pope John Paul II", "a photocopier", "Syria", "(William) Inge", "the grain", "the Bean Sidhe", "Japan", "Zephyr Teachout", "a ballistic missile submarine", "Ambrose Bierce", "Walt Whitman", "frequency", "Macbeth", "Colorado River", "vice presidential running mate", "Tommy Franks", "Botswana", "Mouse Hunt", "the Dow Jones", "Winston Churchill", "Vietnam", "a tuba", "a Croque Madam", "Kyla Pratt", "Wisconsin", "March 11, 2018", "Top Cat", "cutis anserina", "Adrian Cronauer", "1 August 1971", "Australia", "Bronwyn Kathleen Bishop", "Jacob", "Madhav Kumar Nepal", "Joe Jackson", "About 200"], "metric_results": {"EM": 0.625, "QA-F1": 0.6930803571428571}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-8500", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-1461", "mrqa_searchqa-validation-11742", "mrqa_searchqa-validation-15155", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-14445", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13824", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-4239", "mrqa_searchqa-validation-1625", "mrqa_searchqa-validation-2884", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-312", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-1955"], "SR": 0.625, "CSR": 0.5798611111111112, "retrieved_ids": ["mrqa_squad-train-7355", "mrqa_squad-train-59715", "mrqa_squad-train-53963", "mrqa_squad-train-60224", "mrqa_squad-train-24207", "mrqa_squad-train-68637", "mrqa_squad-train-67823", "mrqa_squad-train-72023", "mrqa_squad-train-50120", "mrqa_squad-train-81481", "mrqa_squad-train-49981", "mrqa_squad-train-68476", "mrqa_squad-train-54336", "mrqa_squad-train-46770", "mrqa_squad-train-64596", "mrqa_squad-train-61718", "mrqa_squad-train-41909", "mrqa_squad-train-13153", "mrqa_squad-train-25513", "mrqa_squad-train-76943", "mrqa_squad-train-33015", "mrqa_squad-train-34911", "mrqa_squad-train-50375", "mrqa_squad-train-29277", "mrqa_triviaqa-validation-4886", "mrqa_newsqa-validation-501", "mrqa_naturalquestions-validation-5702", "mrqa_newsqa-validation-1750", "mrqa_triviaqa-validation-3769", "mrqa_triviaqa-validation-4848", "mrqa_newsqa-validation-2503", "mrqa_naturalquestions-validation-8628", "mrqa_squad-validation-9161", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-2476", "mrqa_triviaqa-validation-4750", "mrqa_searchqa-validation-12459", "mrqa_searchqa-validation-11532", "mrqa_naturalquestions-validation-10653", "mrqa_triviaqa-validation-4458", "mrqa_searchqa-validation-9329", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-3147", "mrqa_naturalquestions-validation-2503", "mrqa_newsqa-validation-2324", "mrqa_naturalquestions-validation-8896", "mrqa_triviaqa-validation-4408", "mrqa_hotpotqa-validation-1664"], "EFR": 0.25, "Overall": 0.6139409722222222}, {"timecode": 99, "UKR": 0.8359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1078", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3329", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3905", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8120", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-70", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12568", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-2884", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4848", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.865234375, "KG": 0.5390625, "before_eval_results": {"predictions": ["New Orleans", "won't get you a guppy", "Minnesota", "July 19", "Singapore", "crumpets", "Lord Bill Astor", "peripheral vision", "Henry Wadsworth Longfellow", "Canton", "Hormel Foods", "a vowel", "Theodore", "South Carolina", "Roger Williams", "Niels Bohr", "the sun", "Ahab", "monsters", "Surf's Up", "Scorpio", "a cat", "Finding Nemo", "the International Space Station", "Shakira", "Candice Bergen", "a shark", "Ireland", "George J. Mitchell", "Henry Wadsworth Longfellow", "Gauguin", "Joan Beaufort", "bamboo", "Animal Crackers", "Crete", "Frank Sinatra", "George Armstrong Custer", "barney stinson", "March 18", "Marlee Matlin", "Ben- Hur: A Tale of the Christ", "Yu Darvish", "Dan Rather", "KLM", "food combining", "a teacher", "elephants", "Arkansas", "Bank of America", "the piccolo", "the tuba", "Jason Marsden", "1998", "Garfield Sobers", "bavaria", "Jimmy Carter", "sienna", "Detroit, Michigan", "the Troubles", "ARY Films", "Sri Lanka", "Omar Bongo,", "The commission, led by former U.S. Attorney Patrick Collins,", "Muhammad"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7063244047619048}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4116", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-3907", "mrqa_searchqa-validation-3515", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-9926", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-3988", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-1713", "mrqa_searchqa-validation-2630", "mrqa_searchqa-validation-9182", "mrqa_searchqa-validation-7727", "mrqa_triviaqa-validation-812"], "SR": 0.640625, "CSR": 0.58046875, "retrieved_ids": ["mrqa_squad-train-57795", "mrqa_squad-train-49637", "mrqa_squad-train-49927", "mrqa_squad-train-68993", "mrqa_squad-train-58978", "mrqa_squad-train-62223", "mrqa_squad-train-47410", "mrqa_squad-train-465", "mrqa_squad-train-48731", "mrqa_squad-train-70981", "mrqa_squad-train-44296", "mrqa_squad-train-29822", "mrqa_squad-train-54664", "mrqa_squad-train-83881", "mrqa_squad-train-19350", "mrqa_squad-train-35728", "mrqa_squad-train-79527", "mrqa_squad-train-9296", "mrqa_squad-train-31290", "mrqa_squad-train-78601", "mrqa_squad-train-65108", "mrqa_squad-train-72628", "mrqa_squad-train-8308", "mrqa_squad-train-55614", "mrqa_naturalquestions-validation-366", "mrqa_searchqa-validation-1461", "mrqa_newsqa-validation-1339", "mrqa_triviaqa-validation-4961", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-6956", "mrqa_hotpotqa-validation-5124", "mrqa_triviaqa-validation-3922", "mrqa_newsqa-validation-2667", "mrqa_naturalquestions-validation-2010", "mrqa_hotpotqa-validation-1576", "mrqa_naturalquestions-validation-878", "mrqa_triviaqa-validation-2820", "mrqa_searchqa-validation-9372", "mrqa_triviaqa-validation-4554", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-5608", "mrqa_triviaqa-validation-2262", "mrqa_newsqa-validation-1148", "mrqa_searchqa-validation-14371", "mrqa_triviaqa-validation-1348", "mrqa_searchqa-validation-12497", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-1893"], "EFR": 0.34782608695652173, "Overall": 0.6337058423913043}]}