{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_oewc_lr=5e-5_ep=10_lbd=500_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8', ewc_gamma=1.0, ewc_lambda=500.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_oewc_lr=5e-5_ep=10_lbd=500_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4340, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a plug valve", "1550", "French Louisiana west of the Mississippi River", "2012", "carbon dioxide", "the Lisbon Treaty", "all colors", "in the chloroplasts of C4 plants", "An attorney", "democracy", "The Greens", "third", "Enthusiastic teachers", "expositions", "no French regular army troops were stationed in North America", "estates of the Holy Roman Empire", "Stromatoveris", "2011", "Louis Pasteur", "the owner", "Time Lord", "mosaic floors", "economic", "1893", "environmental factors like light color and intensity", "Gandhi", "deforestation", "Middle Rhine Valley", "pump this into the mesoglea", "low-light conditions", "No Child Left Behind", "one way streets", "\u20ac25,000 per year", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "unbalanced torque", "Ulaanbaatar", "power", "very weak", "Judith Merril", "Gender pay gap", "the Ilkhanate", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "University of Chicago campus", "3D printing technology", "1957", "2000", "a certain number of teacher's salaries are paid by the State", "the Dutch Republic", "San Jose Marriott", "April 20", "the Commission", "evacuate the cylinder", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "Hurricane Beryl", "temperature and light", "terra nullius", "growth", "human", "the \u2018combs\u2019", "1978", "non-Catholics", "Sanders", "vice president", "700 employees"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8576388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10143", "mrqa_squad-validation-8841", "mrqa_squad-validation-2145", "mrqa_squad-validation-739", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-7449", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-9764", "mrqa_squad-validation-7051"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["1986", "carotenoid pigment", "standardized", "50% to 60%", "Stromatoveris", "lower incomes", "Fort Duquesne", "Katharina von Bora", "Miller", "women", "Mr. Belvedere, Roseanne, Who's the Boss?, Just the Ten of Us, The Wonder Years, Full House and Perfect Strangers", "Frank Marx", "architect or engineer", "$2 million", "superintendent of New York City schools", "San Francisco Bay Area at Santa Clara, California", "Kingdom of Prussia", "the country in the same league as the Asian Economic Tigers", "Palestine", "Aristotle and Archimedes", "in the chloroplasts of C4 plants", "Outlaws", "increased blood flow into tissue", "Edgar Scherick", "14th to the 19th century", "Gibraltar and the \u00c5land islands", "the Evangelical Lutheran Church", "oxygen", "the BBC National Orchestra of Wales", "Thanksgiving", "the founding of new Protestant churches in Catholic-controlled regions", "it is impossible to determine what the acceleration of the rope will be", "Venus", "those who proceed to secondary school or vocational training", "zoning and building code requirements", "Ikh Zasag", "Central Bridge", "Europe", "King James Bible", "1935", "seven", "Grumman", "1191", "Maciot de Bethencourt", "Euclid", "case law by the Court of Justice", "long, slender tentacles", "mesoglea", "1970s", "white", "misguided", "2014", "Reconstruction and the Gilded Age", "European Parliament and the Council of the European Union", "State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Manakin Episcopal Church", "Nicholas Stone", "due to ongoing tectonic subsidence", "the release of her eponymous debut album the following year", "a form of business network", "the Capitol held its first session of the United States Congress with both chambers in session on November 17, 1800", "It is the currency used by the institutions of the European Union", "Djokovic", "a generic cover and none of the Wyeth illustrations"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7774959415584415}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8661", "mrqa_squad-validation-7332", "mrqa_squad-validation-6031", "mrqa_squad-validation-27", "mrqa_squad-validation-8459", "mrqa_squad-validation-10339", "mrqa_squad-validation-10321", "mrqa_squad-validation-3021", "mrqa_squad-validation-2328", "mrqa_squad-validation-3946", "mrqa_squad-validation-1906", "mrqa_squad-validation-5588", "mrqa_squad-validation-9166", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1187", "mrqa_searchqa-validation-2579"], "SR": 0.734375, "CSR": 0.78125, "EFR": 0.9411764705882353, "Overall": 0.8612132352941176}, {"timecode": 2, "before_eval_results": {"predictions": ["Lek", "prohibited emigration", "The Private Education Student Financial Assistance", "lower-paid professions", "Labor Party", "time and storage", "special efforts", "rhetoric", "the British occupation", "a year", "Genghis Khan's", "a supervisory church body", "35", "a cubic interpolation formula", "King Sigismund III Vasa", "1835", "the exploitation of the valuable assets and supplies of the nation that was conquered", "poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies.", "five", "liquid oxygen", "Gosforth Park.", "the Metropolitan Police Authority", "18 February 1546", "1996", "1.7 billion years ago", "Mike Carey", "coal", "31 October", "Santa Clara Marriott.", "1976", "LOVE Radio", "the term \"civil disobedience\"", "Khasar", "Sky Digital", "99.4", "40% of Egypt's population", "the issue of laity having a voice and vote", "1995", "the genes it donated to the former host's nucleus", "rocketry and manned spaceflight", "linebacker", "water", "Sir Edward Poynter", "oxygen", "August 1967", "Velamen parallelum", "terrorist organisation", "three", "Lowry Digital", "the worst-case time complexity T(n)", "2010", "363 feet", "Buffalo Lookout", "the term originated in Missouri, during the Kirtland period of Latter Day Saint history, circa 1834", "The User State Migration Tool", "1773", "the Four Big Pollution Diseases of Japan", "October 6, 2017", "the day", "Haliaeetus", "Sir Henry Bartle Frere", "James Zeebo", "the weekend", "Tom Hanks"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7604367548301372}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.7777777777777778, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7357", "mrqa_squad-validation-2886", "mrqa_squad-validation-1672", "mrqa_squad-validation-335", "mrqa_squad-validation-2538", "mrqa_squad-validation-322", "mrqa_squad-validation-6664", "mrqa_squad-validation-6171", "mrqa_squad-validation-5064", "mrqa_squad-validation-9876", "mrqa_squad-validation-8754", "mrqa_squad-validation-3812", "mrqa_squad-validation-1708", "mrqa_squad-validation-3909", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-6211", "mrqa_newsqa-validation-1080"], "SR": 0.6875, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 3, "before_eval_results": {"predictions": ["female", "1884", "Sayyid Abul Ala Maududi", "a family member", "James E. Webb", "Lutheran and Reformed", "phycoerytherin", "understaffed", "swimming-plates", "10 July 1856", "130 million cubic foot", "teleforce", "Heinrich Himmler", "Super Bowl XII, XXII, XXIV, and XLVIII", "Baptism", "Decision problems", "without markings", "1957", "The Day of the Doctor", "Muhammad Khan", "Sun Life Stadium", "the Council", "February 9, 1953", "May", "sea gooseberry", "1961", "Trio Tribe", "Dai Setsen", "the Late Medieval Catholic Church", "January 1979", "phagocytic cells", "Rankine cycle", "$2.2 billion", "Seine", "Newton's Law of Gravitation", "15 February 1546", "Marquis de la Jonqui\u00e8re", "BBC Dead Ringers", "Kenyans for Kenya initiative", "Fresno", "Saudi", "Presiding Officer", "an intuitive understanding", "default emission factors", "Inherited wealth", "Michael P. Millardi", "Goldman Sachs", "the BRAAVOO website", "the world's catalog of ideas", "the White States", "he would be like an apprentice to the clergy", "the children were nestled all snug in their beds", "Sarah of the Clue Crew", "the Leyden jar", "an better-paid legislator", "Gaetano Rossi", "the Abbeys of Touraine", "the USGS Water Science", "the Mother Goose float led off the first annual Thanksgiving Day Parade in New York City in 1924.", "through the risk of a fire or a flood in my house", "the British", "early 1960s", "April 1917", "land based powers"], "metric_results": {"EM": 0.625, "QA-F1": 0.6797991071428571}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8595", "mrqa_squad-validation-7525", "mrqa_squad-validation-2595", "mrqa_squad-validation-494", "mrqa_squad-validation-5262", "mrqa_squad-validation-10369", "mrqa_squad-validation-8449", "mrqa_squad-validation-3863", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-10156"], "SR": 0.625, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 4, "before_eval_results": {"predictions": ["boom-and-bust cycles", "Prince of P\u0142ock", "hormones", "1840", "occupational stress", "in the parts of the internal canal network under the comb rows", "a fideist", "Tesla Electric Company", "African-American", "Thomson", "1905", "Nun komm, der Heiden Heiland", "John Fox", "all health care settings", "cut in half", "the study of rocks", "colonies", "lower wages", "geophysical surveys", "Louis XIV", "social power and wealth", "2,900 kilometres", "Elie Metchnikoff", "an algorithm", "Immediately after Decision Time", "Confucian propriety and ancestor veneration", "25-minute", "eight", "elude host immune responses", "Pusey Library", "inequality", "designs", "cytokines", "the final draft of the Edict of Worms", "wide sidewalks", "other members", "Air Force", "an occupancy permit", "lung tissue", "Nederrijn", "a multi-cultural city", "pump water out of the mesoglea", "Tim Johnson", "Canberra", "a judicial officer", "a mathematical model", "Henry Purcell", "Ram Nath Kovind", "embryo", "Todd Griffin", "Sandy Knox and Billy Stritch", "Hudson Bay", "Etienne de Mestre", "a bow bridge", "1922 to 1991", "Nicole Gale Anderson", "1", "sedimentary", "Carol Ann Susi", "plate tectonics", "Columbia", "Isabella II", "Conway", "the Colombian telenovela"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7369791666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2463", "mrqa_squad-validation-7338", "mrqa_squad-validation-8093", "mrqa_squad-validation-6957", "mrqa_squad-validation-2209", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-4002", "mrqa_triviaqa-validation-5855", "mrqa_newsqa-validation-2042", "mrqa_searchqa-validation-172"], "SR": 0.703125, "CSR": 0.715625, "EFR": 1.0, "Overall": 0.8578125}, {"timecode": 5, "before_eval_results": {"predictions": ["former flooded terraces", "at the beginning of the 20th century", "1974", "ABC", "dictatorial", "David Hatcher Childress", "quantumized", "Book of Exodus", "Synthetic aperture radar", "Battlestar Galactica and Bionic Woman", "patients' prescriptions and patient safety issues", "No, that's no good", "1697", "3 January 1521", "a new magma", "as a \"principal hostile country\"", "Jan Hus", "Newton", "Croatia", "2011", "Swynnerton Plan", "machine gun", "Theatre Museum", "August 10, 1948", "they are distinct or equal classes", "the 2004 Treaty establishing a Constitution for Europe", "Serge Chermayeff", "Thomas Edison", "Mnemiopsis", "the flail of God", "Woodward Park", "Melbourne Cricket Ground", "Wednesdays", "most common", "up to a thousand times as many", "tears and urine", "six years", "plants and algae", "Constitution of India", "in 1913", "Javier Fern\u00e1ndez", "Konakuppakatil Gopinathan Balakrishnan", "in 1942", "March 2016", "Texas, Oklahoma, and the surrounding Great Plains", "in a balance sheet as an asset", "Irsay", "1995", "William the Conqueror", "1922", "an anembryonic gestation", "Bemis Heights", "9pm ET ( UTC - 5 )", "twice", "S\u00e9rgio Mendes", "Lituya Bay", "Sarah", "routing protocols", "The euro", "meds-vapor lamps, tanning lamps, and black lights", "March 14, 2000", "KCNA", "R&B", "Rodgers & Hammerstein"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6686546092796093}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.4615384615384615, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3076923076923077, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.4444444444444445, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5818", "mrqa_squad-validation-1827", "mrqa_squad-validation-1566", "mrqa_squad-validation-10388", "mrqa_squad-validation-7613", "mrqa_squad-validation-3770", "mrqa_squad-validation-1780", "mrqa_squad-validation-3985", "mrqa_squad-validation-4572", "mrqa_squad-validation-6439", "mrqa_squad-validation-8471", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-2749", "mrqa_hotpotqa-validation-21", "mrqa_newsqa-validation-2404", "mrqa_searchqa-validation-14371"], "SR": 0.53125, "CSR": 0.6848958333333333, "EFR": 1.0, "Overall": 0.8424479166666666}, {"timecode": 6, "before_eval_results": {"predictions": ["four", "2 million", "ranges from 53% in Botswana to -40% in Bahrain", "Pliocene period", "teacher who stays with them for most of the week and will teach them the whole curriculum", "LeGrande", "After the sixth sermon", "10 Cloverfield Lane", "11.1%", "nearly 60,000", "the University of Chicago College Bowl Team", "the decline of organized labor", "Stanford", "oxygen chambers", "two", "two catechisms", "Cologne, Germany", "1991", "Silk Road", "Surveyor 3 unmanned lunar probe", "145 galleries", "growth and investment", "the centers were computer service bureaus, offering batch processing services", "Vampire bats", "antiforms", "U. S. flags left on the Moon during the Apollo missions were found to still be standing", "weight", "Mongolian national identity has had a powerful revival partly because of his perception during the Mongolian People's Republic period", "oil was priced in dollars", "Beyonc\u00e9 and Bruno Mars", "a university or college", "More than 1 million", "pseudorandom number generators", "Japan", "rotating reference frame", "Mickey Rourke", "May 2016", "Nicklaus", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "the Panamanian government", "silk, hair / fur", "the French First Republic", "two", "Sebastian Vettel", "April 10, 2018", "Gorakhpur railway station", "April 28, 2008", "Representative", "December 15, 2016", "Abraham Gottlob Werner", "Jourdan Miller", "1991", "Mandy '' Moore", "the Greek name", "Broken Hill and Sydney", "159 counties", "China", "Judiththia Aline Keppel", "medellin", "Crown Cork & Seal Company", "Expedia", "the Large Orbiting Telescope", "Zed", "an failure of leadership at a critical moment in the nation's history"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6248829804706164}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.19999999999999998, 1.0, 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.1, 0.09523809523809523, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.782608695652174, 0.17391304347826084, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.125, 0.5714285714285715, 1.0, 0.28571428571428575, 1.0, 0.3333333333333333, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7445", "mrqa_squad-validation-1892", "mrqa_squad-validation-606", "mrqa_squad-validation-6965", "mrqa_squad-validation-5435", "mrqa_squad-validation-327", "mrqa_squad-validation-4000", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_squad-validation-6228", "mrqa_squad-validation-3718", "mrqa_squad-validation-9161", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-6106", "mrqa_hotpotqa-validation-1471", "mrqa_searchqa-validation-10372", "mrqa_newsqa-validation-429"], "SR": 0.53125, "CSR": 0.6629464285714286, "EFR": 0.9666666666666667, "Overall": 0.8148065476190476}, {"timecode": 7, "before_eval_results": {"predictions": ["Director", "travel literature, cartography, geography, and scientific education", "oxygen chambers", "Graham Gano", "Two", "In 1066", "2008", "Mojave Desert", "to build a nationwide network in the UK.", "throughout the St. Lawrence and Mississippi watersheds", "27", "4000 years", "Rhine Gorge", "in the helical thylakoid model,", "highest", "impact process effects", "in schools in some Asian, African and Caribbean countries.", "Kings Row and Casablanca", "pharmacists", "high-voltage", "4:51", "Kabaty Forest", "the seal of the Federal Communications Commission", "strong sedimentation in the western Rhine Delta", "The European Commission", "SAP Center in San Jose.", "fire requires only a part of air", "352 votes", "eliminate the accusing law", "October 6, 2004", "The Day of the Doctor", "Pakistan", "September 2001", "September 6, 2019", "Greek", "During the fourth season", "in Germany ( 1936 ), Yugoslavia ( 1984 ), Russia ( 2014 ) and South Korea ( 2018 )", "Nick Kroll", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Billy Gibbons", "an apprentice of the fictional Wars Order in the Star Wars franchise", "in the brain", "31", "in the 1970s", "the Vital Records Office of the states, capital district, territories and former territories", "Art Carney", "accomplish the objectives of the organization", "in the female's body in the absence of mating", "January 1923", "tropical cyclones", "September 2017", "3 September", "The silk floss tree", "Terrell Owens's", "since 3, 1, and 4 are the first three significant digits of \u03c0", "five points", "Dolph Lundgren", "Hampton Court Palace", "Sela Ward", "her decades-long portrayal of Alice Horton", "in December with a joyous MARC Holiday party.", "Benjamin Britten.", "an isosceles triangle", "in an airplane crash overseas and then framed them for treason."], "metric_results": {"EM": 0.5625, "QA-F1": 0.657718437015312}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.9189189189189189, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.625, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4629", "mrqa_squad-validation-8819", "mrqa_squad-validation-1938", "mrqa_squad-validation-5781", "mrqa_squad-validation-6409", "mrqa_squad-validation-451", "mrqa_squad-validation-3416", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16130", "mrqa_triviaqa-validation-6548"], "SR": 0.5625, "CSR": 0.650390625, "EFR": 0.9642857142857143, "Overall": 0.8073381696428572}, {"timecode": 8, "before_eval_results": {"predictions": ["cytokines", "William Pitt, the Secretary of State responsible for the colonies", "North Carolina and New Mexico", "the p-adic norm", "Hassan al Banna", "Gottfried Fritschel", "the Eighth Doctor", "singular plastoglobulus", "pound-force", "the Song dynasty and the Ming dynasty", "Dorothy and Michael Hintze", "The Small Catechism", "36", "Italian, Medieval, Renaissance, Baroque and Neoclassical sculpture", "April 20", "biomass", "their belief in the validity of the social contract", "K MJ-TV", "the Foreign Protestants Naturalization Act", "southern and central parts of France", "10%", "It consisted of separate descent and ascent stages, each with its own engine", "Metro Trains Melbourne", "BBC 1", "$2 million", "Vince Lombardi Trophy", "Isaac Newton", "the vast majority of plastoglobuli occur singularly, attached directly to their parent thylakoid", "the song is a mid-tempo ballad, musically inspired by Motown, Philly soul, and Earth, Wind & Fire", "river Aniene", "1885", "James Madison", "Ryan Pinkston", "federal republic", "the chyle to the thoracic duct where it is emptied into the bloodstream at the subclavian vein", "2005", "foreign investors", "N\u0289m\u0289n\u0289", "8ft", "Bartolomeu Dias", "William Wyler", "1961", "March 1930", "Mary Simpson", "Thomas Jefferson", "February 2017 in Japan and in March 2018 in North America and Europe", "January 1, 2016", "USCS or USC", "dexter L. Owades, PhD, a biochemist working for New York's Rheingold Brewery", "Sunday night", "Billy Hill", "Mara Jade", "Malina Weissman", "September 6, 2019", "1773", "A lacteal", "April 26, 2005", "dexter bawdy folk song called \"The Ballad of Lydia Pinkham", "Albert", "fighting for his life in a Buenos Aires hospital after being shot in the head during an armed robbery", "Croatia", "her boyfriend, who lived in Fort Lauderdale, Florida", "six", "they were part of a group of 20 similar cars making an annual road trip"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5909273438351312}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7368421052631579, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3157894736842105, 0.0, 0.0, 0.0, 0.375]}}, "before_error_ids": ["mrqa_squad-validation-10266", "mrqa_squad-validation-8958", "mrqa_squad-validation-7876", "mrqa_squad-validation-8786", "mrqa_squad-validation-8184", "mrqa_squad-validation-5724", "mrqa_squad-validation-6706", "mrqa_squad-validation-4715", "mrqa_squad-validation-3848", "mrqa_squad-validation-10410", "mrqa_squad-validation-8769", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5739", "mrqa_triviaqa-validation-4881", "mrqa_hotpotqa-validation-2800", "mrqa_newsqa-validation-3043", "mrqa_searchqa-validation-2141", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-3476"], "SR": 0.53125, "CSR": 0.6371527777777778, "EFR": 1.0, "Overall": 0.8185763888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["an attempt to emphasize academics over athletics", "3,600", "nine", "the individual states and territories", "30%\u201350%", "one of his wife's ladies-in-waiting", "liquid phase", "Dirichlet's theorem on arithmetic progressions", "Europe", "the cell membrane", "a renegade Time Lord who desires to rule the universe", "Laverne & Shirley", "carbohydrates", "his butchery", "Jean Ribault", "March 2011", "Continental Edison Company in France", "2013", "more equality in the income distribution", "X is no more difficult than Y", "age 38", "1887", "1469", "a \"world classic of epoch-making oratory.\"", "up to half", "jennifer", "Mill Supply & Equipment", "a tavern or in a honky", "Georgie Porgie", "a grain spirit or it may be made from other plants", "William Shakespeare", "Joe King", "Venus", "Helen Hayes", "Canberra", "jennifer Lohan", "jekyl Island,", "Anna Pavlova", "an actuary computes premium rates, dividends, risks", "Joe Torre", "jopary Questions page 1305", "Chicago Cubs", "jopary Questions page 1155", "jennifer", "a fictional character on the WB Television Network", "jennifer", "jennifer Wagner", "the White Nile", "Beverly and Elliot Mantle", "the chimney", "Andrew Jackson", "the screams of 25 million adoring fans and I wring the sweat out of her thong", "Louis Vauxcelles", "jedoublen", "jennifer", "jockey", "Egypt", "James Hutton", "Turkey, Armenia and Morocco", "Stuart Neame", "Total Nonstop Action Wrestling", "an American chemist and professor of chemistry at Harvard", "China and Japan", "James Appathurai"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5225344967532467}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.36363636363636365, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-8969", "mrqa_squad-validation-7700", "mrqa_squad-validation-1240", "mrqa_squad-validation-166", "mrqa_squad-validation-1748", "mrqa_squad-validation-374", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-1507", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-7463", "mrqa_triviaqa-validation-6421", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-2179"], "SR": 0.453125, "CSR": 0.61875, "EFR": 1.0, "Overall": 0.809375}, {"timecode": 10, "UKR": 0.7734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8454", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-3245", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-5035", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-10000", "mrqa_squad-validation-10115", "mrqa_squad-validation-10136", "mrqa_squad-validation-1017", "mrqa_squad-validation-10181", "mrqa_squad-validation-10184", "mrqa_squad-validation-10217", "mrqa_squad-validation-10263", "mrqa_squad-validation-10281", "mrqa_squad-validation-10290", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-10361", "mrqa_squad-validation-10369", "mrqa_squad-validation-1038", "mrqa_squad-validation-10410", "mrqa_squad-validation-10454", "mrqa_squad-validation-10496", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1181", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1226", "mrqa_squad-validation-1240", "mrqa_squad-validation-1254", "mrqa_squad-validation-1269", "mrqa_squad-validation-1371", "mrqa_squad-validation-1499", "mrqa_squad-validation-1521", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1651", "mrqa_squad-validation-166", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1748", "mrqa_squad-validation-1780", "mrqa_squad-validation-1787", "mrqa_squad-validation-1848", "mrqa_squad-validation-1863", "mrqa_squad-validation-1892", "mrqa_squad-validation-1924", "mrqa_squad-validation-1938", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-1998", "mrqa_squad-validation-2019", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-2108", "mrqa_squad-validation-2145", "mrqa_squad-validation-2209", "mrqa_squad-validation-2233", "mrqa_squad-validation-2241", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-247", "mrqa_squad-validation-2521", "mrqa_squad-validation-2545", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2642", "mrqa_squad-validation-27", "mrqa_squad-validation-2751", "mrqa_squad-validation-2820", "mrqa_squad-validation-2885", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-3039", "mrqa_squad-validation-305", "mrqa_squad-validation-3076", "mrqa_squad-validation-3144", "mrqa_squad-validation-3164", "mrqa_squad-validation-317", "mrqa_squad-validation-3184", "mrqa_squad-validation-322", "mrqa_squad-validation-3230", "mrqa_squad-validation-3270", "mrqa_squad-validation-334", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3376", "mrqa_squad-validation-3380", "mrqa_squad-validation-3392", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3497", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3687", "mrqa_squad-validation-3703", "mrqa_squad-validation-3718", "mrqa_squad-validation-374", "mrqa_squad-validation-3769", "mrqa_squad-validation-3770", "mrqa_squad-validation-381", "mrqa_squad-validation-3824", "mrqa_squad-validation-3829", "mrqa_squad-validation-3842", "mrqa_squad-validation-3848", "mrqa_squad-validation-3852", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3917", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3985", "mrqa_squad-validation-3986", "mrqa_squad-validation-3998", "mrqa_squad-validation-4000", "mrqa_squad-validation-4009", "mrqa_squad-validation-402", "mrqa_squad-validation-4031", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4181", "mrqa_squad-validation-4187", "mrqa_squad-validation-4213", "mrqa_squad-validation-4291", "mrqa_squad-validation-4312", "mrqa_squad-validation-4348", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4452", "mrqa_squad-validation-4467", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-451", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4539", "mrqa_squad-validation-4557", "mrqa_squad-validation-4557", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4838", "mrqa_squad-validation-491", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5019", "mrqa_squad-validation-5064", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-516", "mrqa_squad-validation-5262", "mrqa_squad-validation-5396", "mrqa_squad-validation-5436", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5493", "mrqa_squad-validation-5527", "mrqa_squad-validation-5546", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5602", "mrqa_squad-validation-5631", "mrqa_squad-validation-5664", "mrqa_squad-validation-5677", "mrqa_squad-validation-57", "mrqa_squad-validation-5726", "mrqa_squad-validation-5750", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5806", "mrqa_squad-validation-5818", "mrqa_squad-validation-5852", "mrqa_squad-validation-5860", "mrqa_squad-validation-5865", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6031", "mrqa_squad-validation-6066", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6176", "mrqa_squad-validation-6206", "mrqa_squad-validation-6222", "mrqa_squad-validation-6229", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6319", "mrqa_squad-validation-6330", "mrqa_squad-validation-6347", "mrqa_squad-validation-6353", "mrqa_squad-validation-6355", "mrqa_squad-validation-6409", "mrqa_squad-validation-6439", "mrqa_squad-validation-6502", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6611", "mrqa_squad-validation-6649", "mrqa_squad-validation-6664", "mrqa_squad-validation-6694", "mrqa_squad-validation-6790", "mrqa_squad-validation-6815", "mrqa_squad-validation-6838", "mrqa_squad-validation-6875", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6898", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7036", "mrqa_squad-validation-7039", "mrqa_squad-validation-7064", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7228", "mrqa_squad-validation-7260", "mrqa_squad-validation-7261", "mrqa_squad-validation-7297", "mrqa_squad-validation-7332", "mrqa_squad-validation-7338", "mrqa_squad-validation-7357", "mrqa_squad-validation-7364", "mrqa_squad-validation-7368", "mrqa_squad-validation-7380", "mrqa_squad-validation-739", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7445", "mrqa_squad-validation-7457", "mrqa_squad-validation-7470", "mrqa_squad-validation-7492", "mrqa_squad-validation-7503", "mrqa_squad-validation-7525", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-762", "mrqa_squad-validation-7693", "mrqa_squad-validation-7700", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7775", "mrqa_squad-validation-7781", "mrqa_squad-validation-7785", "mrqa_squad-validation-779", "mrqa_squad-validation-7863", "mrqa_squad-validation-7871", "mrqa_squad-validation-7917", "mrqa_squad-validation-7943", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8016", "mrqa_squad-validation-8043", "mrqa_squad-validation-8093", "mrqa_squad-validation-8125", "mrqa_squad-validation-8154", "mrqa_squad-validation-8177", "mrqa_squad-validation-8184", "mrqa_squad-validation-8192", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8365", "mrqa_squad-validation-8414", "mrqa_squad-validation-8449", "mrqa_squad-validation-8459", "mrqa_squad-validation-8471", "mrqa_squad-validation-8484", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8568", "mrqa_squad-validation-8585", "mrqa_squad-validation-8661", "mrqa_squad-validation-8670", "mrqa_squad-validation-8670", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-8841", "mrqa_squad-validation-888", "mrqa_squad-validation-8904", "mrqa_squad-validation-8925", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8958", "mrqa_squad-validation-8985", "mrqa_squad-validation-908", "mrqa_squad-validation-9095", "mrqa_squad-validation-9161", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9403", "mrqa_squad-validation-9405", "mrqa_squad-validation-9446", "mrqa_squad-validation-9464", "mrqa_squad-validation-9556", "mrqa_squad-validation-957", "mrqa_squad-validation-9594", "mrqa_squad-validation-9615", "mrqa_squad-validation-9669", "mrqa_squad-validation-9716", "mrqa_squad-validation-9717", "mrqa_squad-validation-9764", "mrqa_squad-validation-9814", "mrqa_squad-validation-9816", "mrqa_squad-validation-9876", "mrqa_squad-validation-9907", "mrqa_squad-validation-9928", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7463"], "OKR": 0.931640625, "KG": 0.4328125, "before_eval_results": {"predictions": ["Northern Europe and the Mid-Atlantic", "$2 million", "fish stocks to collapse", "Chris Keates", "many castles and vineyards", "Cinerama Productions/Palomar theatrical library", "Antigone", "3.5 million", "Denver Broncos", "1997", "A \u2192 G deamination", "since 2001", "Streptococcus", "1767", "Narrow alleys", "another problem", "economic", "John and Benjamin Green", "1530", "The company installed electrical arc light based illumination systems", "Denver", "the poor", "Irish Sweepstakes", "Pearl Jam", "Grey's Anatomy", "blue", "Bruce Springsteen", "Wounded Knee", "Maria Callas", "Henry Moore", "Boston", "Charlotte", "the 10th hole", "Narcissus", "Fred Williamson", "South Africa", "(Prince) Albert", "the Holy Grail", "morocco", "morocco", "Mozart", "Lake Victoria", "(Prince) Albert", "root beer", "morocco", "Sarah Orne Jewett", "Velvet Revolver", "morocco", "polar-front", "You Bet Your Life", "North Korea", "Nova Scotia", "Breathless", "morocco", "Franklin Pierce", "Pearl Harbor", "Michael Schumacher", "a four - page pamphlet in 1876", "(Prince) Albert", "Glasgow", "Joely Richardson", "Hugh Dowding", "NATO fighters", "Congress"], "metric_results": {"EM": 0.5, "QA-F1": 0.585094246031746}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.888888888888889, 1.0, 0.8571428571428571, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.26666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8990", "mrqa_squad-validation-5887", "mrqa_squad-validation-6655", "mrqa_squad-validation-9959", "mrqa_squad-validation-1288", "mrqa_squad-validation-985", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3530", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-11388", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-8760", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-12083", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-6909", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-6181", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6896", "mrqa_hotpotqa-validation-1843"], "SR": 0.5, "CSR": 0.6079545454545454, "EFR": 1.0, "Overall": 0.7491690340909091}, {"timecode": 11, "before_eval_results": {"predictions": ["the Horn of Africa", "Grumman", "the prosecution proposes a plea bargain to civil disobedients", "1671", "St. Johns", "the AS-205 mission was canceled", "The President of the Council and a Commissioner can sit in on ECB meetings, but don't have voting rights", "Ismailiyah, Egypt", "phycobilisomes on the thylakoid membranes", "lupus erythematosus", "December 1878", "bars, caf\u00e9s and clubs", "PNU and ODM camps", "T(n) = O(n2)", "Bill Clinton", "the qu", "International Crops Research Institute for the Semi-Arid Tropics", "a straight line", "Germany", "autoimmune", "2014", "his advocacy of young earth creationism and intelligent design.", "Seoul, South Korea", "2005", "2005 through 2008", "May 21, 2000", "the 100 metres", "January 2016", "seven", "Samuel Beckett", "the island", "Sonic Mania", "Homeland", "Carson City", "the League of the Three Emperors", "Barack Obama", "Brian A. Miller", "Washington, D.C.", "December 13, 2015", "Front Row", "1590", "Vixen", "Revolution Studios", "Mach number", "1990", "Michael A. Cremo", "Gangsta's Paradise", "The A41", "Mary Astor", "five", "Indiana", "Esteban Ocon", "ABC", "the British military", "National Lottery", "2018", "Tim Rice", "Milton Friedman", "Richard Avedon", "two", "a full garden and pool, a tennis court, or several heli-pads", "Darfur", "a pillar", "Yahya Khan"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6488613816738816}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.4, 0.2857142857142857, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9912", "mrqa_squad-validation-6759", "mrqa_squad-validation-3113", "mrqa_squad-validation-3954", "mrqa_squad-validation-4150", "mrqa_squad-validation-8594", "mrqa_squad-validation-4883", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-10161", "mrqa_triviaqa-validation-358", "mrqa_newsqa-validation-3227", "mrqa_searchqa-validation-971", "mrqa_naturalquestions-validation-3485"], "SR": 0.59375, "CSR": 0.6067708333333333, "EFR": 0.9615384615384616, "Overall": 0.7412399839743589}, {"timecode": 12, "before_eval_results": {"predictions": ["a Wi-Fi or Power-line connection", "water pump", "Tesla coil", "1946", "21 to 11", "Parliamentary Bureau", "Japan and Latin America", "force the Huguenots to convert", "Arizona Cardinals", "382", "1540s", "John Fox", "American Indians in the colony of Georgia", "orbit the Moon", "Kusala", "quickly", "innate immune system versus the adaptive immune system", "March 1896", "The Lightning thief", "James `` Jamie '' Dornan", "W. Edwards Deming", "May", "Human anatomy", "2.187 billion", "current day Poole Harbour towards mid-Channel", "Accounting Standards Board", "Ole Einar Bj\u00f8rndalen", "General George Washington", "following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Djokovic", "Longline", "1961", "the dome", "1997", "Procol Harum", "Sheev Palpatine", "billy wild", "punk rock", "the septum", "White House Executive chef", "vaskania", "the church at Philippi", "10 May 1940", "Ethel `` Edy '' Proctor", "bohrium", "prejudice in favour of or against one thing, person, or group compared with another", "nasal septum", "a couple broken apart by the Iraq War", "Spanish American wars of independence", "Tristan Rogers", "Owen Vaccaro", "Walter Brennan", "1872", "Brad Johnson", "1992", "King Richard II", "between Austria and Switzerland", "L'amoureuse", "Gillian Anderson", "French Open", "18", "Swing Low, Sweet Chariot", "mute", "Burrard Inlet"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6822916666666667}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.20000000000000004, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-978", "mrqa_squad-validation-3130", "mrqa_squad-validation-9863", "mrqa_squad-validation-8164", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-10122", "mrqa_triviaqa-validation-4886", "mrqa_hotpotqa-validation-2009", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-765", "mrqa_searchqa-validation-9822", "mrqa_searchqa-validation-10098"], "SR": 0.578125, "CSR": 0.6045673076923077, "EFR": 0.9629629629629629, "Overall": 0.7410841791310541}, {"timecode": 13, "before_eval_results": {"predictions": ["Protestantism", "Extension", "Riverside", "fundamental interactions", "Hamburg merchants and traders", "Department of Justice", "water flow through the body cavity", "67.9", "Fort Duquesne", "Sports Programs, Inc.", "quality rental units", "Pittsburgh Steelers", "Edward Teller", "the geographical area it covers", "stay", "Andrew Lortie", "bilaterally symmetrical", "Thirty years after the Galactic Civil War", "an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "two years", "The Vulcan salute", "Longline fishing", "Redford's adopted home state of Utah", "Stephen A. Douglas", "April 2011", "around 1940", "it is located on about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "Herman Hollerith", "Dr. Sachchidananda Sinha", "Rick Marshall", "hairpin corner", "over two days", "the IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "During metaphase of cell division", "it activates a relay which will handle the higher current load", "Donald Trump", "Liam Cunningham", "the spectroscopic notation for the associated atomic orbitals", "Veronica", "moral tale", "a revolution or orbital revolution", "Sauron", "Gustav Bauer", "2002", "Mohammad Reza Pahlavi", "the southeastern coast of the Commonwealth of Virginia in the United States", "two tectonic plates move towards each other at a convergent plate boundary", "Jourdan Miller", "10,605", "84", "1773", "the Mayor's son", "73", "the sun rises south of due east", "1920", "Catherine Zeta-Jones", "Michael Crawford", "264,152", "10,000", "the two bodies", "Mormon Tabernacle Choir", "carbon dioxide", "Pickwick", "Sunshine"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6084003316715703}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.782608695652174, 0.5, 0.10526315789473684, 1.0, 0.2857142857142857, 1.0, 0.0, 0.8, 0.7499999999999999, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9473684210526316, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 0.5, 0.4, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10475", "mrqa_squad-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-8699", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-4097", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-1166"], "SR": 0.484375, "CSR": 0.5959821428571428, "EFR": 1.0, "Overall": 0.7467745535714285}, {"timecode": 14, "before_eval_results": {"predictions": ["a Tulku", "the Quaternary", "between Nova Scotia and Acadia in the north, to the Ohio Country in the south,", "Brad Nortman", "Museum of the Moving Image", "BBC Dead Ringers", "1206", "Louis Pasteur", "The Brain of Morbius (1976)", "the oceans and seas", "two fumbles", "a mainline Protestant Methodist denomination", "Albert Einstein", "Lombardi Trophy", "death in body and soul, if only as highwaymen and murderers.", "Candice Susan Swanepoel", "AT&T", "Australian", "German", "Chris Anderson", "just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "1949", "Red", "Australia", "David in the first series of \"Citizen Khan\" (2012)", "Jena Malone", "John M. Dowd", "twelfth", "Hawaii Republican", "New York", "Southern Rock Allstars", "a tragedy", "cricket fighting", "14th Street", "guitar", "Brad Wilk", "2012", "New Orleans, Louisiana", "William Hoffman & Lake Headley", "May 4, 1924", "Australian", "1966", "2012", "1926", "Texas's 27th congressional district", "florida", "a mother of the Olympian gods and goddesses", "VAQ-135", "1892", "Ludwig van Beethoven", "Sam Phillips", "Manchester United", "Turkmenistan", "1942", "October 6, 2017", "the lower the normal boiling point of the liquid", "lupus", "Ganga", "January 24, 2006", "South Africa", "a pager", "Baltimore", "In 1917", "near Front Royal, and the southern terminus is at an interchange with US 250 near Interstate 64 ( I - 64 ) in Rockfish Gap, where the road continues south as the Blue Ridge Parkway"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5957778322202865}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.7499999999999999, 0.0, 1.0, 1.0, 0.8, 0.625, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6341463414634146]}}, "before_error_ids": ["mrqa_squad-validation-10168", "mrqa_squad-validation-7674", "mrqa_squad-validation-8229", "mrqa_squad-validation-7688", "mrqa_squad-validation-802", "mrqa_squad-validation-218", "mrqa_squad-validation-402", "mrqa_squad-validation-2383", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-10613", "mrqa_triviaqa-validation-775", "mrqa_newsqa-validation-1879", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-1813"], "SR": 0.484375, "CSR": 0.5885416666666667, "EFR": 1.0, "Overall": 0.7452864583333334}, {"timecode": 15, "before_eval_results": {"predictions": ["1937", "the Educational Institute of Scotland and the Scottish Secondary Teachers' Association", "June 4, 2014", "Journey's End", "tech-oriented", "John Houghton", "heterokontophyte", "NP-complete", "Tenggis Khan", "128,843", "by a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections", "56.2%", "20\u201318", "Chaplain to the Forces serving at the Tower of London and in Bengal, Caterham, South Africa (where he was Mentioned in despatches) and Portsmouth until his Archdeacon\u2019s appointment", "KlingStubbins", "Eddie Albert", "Louis Mountbatten", "Alcorn State", "The Guest", "The Today Show", "Philadelphia, Pennsylvania", "9", "The A41", "Bad Meets Evil", "Pimp My Ride", "2003", "casting, job opportunities, and career advice", "Mary Harron", "Flashback", "Eenasul Fateh", "Chicago", "Australian", "2014", "the Second World War", "Lismore", "rural areas", "teenage actor or teen actor", "Summerlin, Clark County, Nevada", "Lester Ben \"Benny\" Binion", "YG Entertainment", "Rusalka", "Don't Look Back in Anger", "our le M\u00e9rite", "Trey Parker and Matt Stone", "Riot Act", "Aqua", "American Longhair", "four operas", "Christy Walton took her husband John's place after his death", "Lt. Gen. Ulysses S. Grant", "Hechingen in Swabia", "Black Sabbath", "Associazione Sportiva", "8,211", "Kristin Beth Baxter", "in the cell nucleus", "a Bristol Box Kite", "1961", "Diprivan", "South Dakota State Penitentiary", "Douglas Fir", "drink wine or kiss a fool", "military commissions", "Philip Markoff"], "metric_results": {"EM": 0.4375, "QA-F1": 0.55199303011803}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.07407407407407407, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.28571428571428575, 0.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2094", "mrqa_squad-validation-6279", "mrqa_squad-validation-4298", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-2621", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-5320", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-2178", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-289", "mrqa_triviaqa-validation-7461", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-13595", "mrqa_searchqa-validation-1757", "mrqa_newsqa-validation-4202"], "SR": 0.4375, "CSR": 0.5791015625, "EFR": 1.0, "Overall": 0.7433984375}, {"timecode": 16, "before_eval_results": {"predictions": ["the Central Secretariat (Zhongshu Sheng)", "Puritan", "James Wolfe", "March 1974", "2003", "Frederick II the Great", "Lower taxes, increased economic development, unification of the community, better public spending and effective administration by a more central authority", "Armenians vassal-states of Sassoun and Taron", "redistributive taxation", "Seattle Seahawks", "paid professionals", "a polynomial-time reduction", "revelry", "Krishna Rajaram", "Padre Alberto", "The BBC", "5-0", "Kim Il Sung", "second-degree aggravated battery", "John McCain", "\"I dropped a tank top behind the door as I was leaving for work, and there would be just weird lights on in my house", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews", "be silent", "200", "2,000", "several weeks", "auction off one of the earliest versions of the Magna Carta later this year", "injuries", "Michael Jackson", "Caylee Anthony", "10 below in Chicago, Illlinois", "The First Stop Resource Center", "Manmohan Singh", "jazz", "1983", "cancer", "Al-Shabaab", "Casalesi Camorra", "KVBC", "Appathurai", "Eintracht Frankfurt", "opium poppies", "The Bronx County District Attorneys Office", "1,073", "Arthur E. Morgan III", "an army major assigned to a guard unit protecting Mexican President Felipe Calderon", "Las Vegas", "Pakistan", "a senior U.S. defense official", "I, the chief executive officer, the one on the very top", "18", "\"Viva Zapatero,\"", "India", "Mumbai", "Miami Heat", "a combination of genetics and the male hormone dihydrotestosterone", "Senegal", "Windermere", "Field of Dreams", "Bill Paxton", "a star", "American Airlines LAS-PHL", "Sex Pistols", "an ambitious Jewish boy growing up in a poor neighborhood in Montreal"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6019043036936182}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.06451612903225806, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.5454545454545454, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8294", "mrqa_squad-validation-7296", "mrqa_squad-validation-1136", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-2900", "mrqa_triviaqa-validation-4966", "mrqa_hotpotqa-validation-5742", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-4356"], "SR": 0.546875, "CSR": 0.5772058823529411, "EFR": 1.0, "Overall": 0.7430193014705881}, {"timecode": 17, "before_eval_results": {"predictions": ["higher economic inequality", "private individuals, private organizations or religious groups", "primary education ( primary schools) and tertiary education (universities and/or TAFE colleges)", "a glass case suspended from the lid", "phagocytic cells", "2000", "five", "weight", "Leukocytes", "3D printing technology", "Ong Khan", "colonel", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "\"The oceans are kind of the last frontier for use and development,\"", "Wigan Athletic", "Vertikal-T,", "Graeme Smith", "228", "\"a striking blow to due process and the rule of law.\"", "her father's home in Satsuma, Florida,", "St. Francis De Sales", "has been the Magneto to my Wolverine, the Saruman to my Frodo, the Dr. Octopus to my Spiderman.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "power lines downed by Saturday's winds,", "African National Congress", "French", "bikinis", "Bobby Darin,", "Adam Yahiye Gadahn,", "Mark Sanford", "150", "they couldn't accept an offer from the Southeastern Pennsylvania Transportation Authority", "the equator,", "Chinese President Hu Jintao", "183", "alert patients of possible tendon ruptures and tendonitis.", "Too many glass shards left by beer drinkers in the city center,", "(l-r)", "\"Goldstone Report\"", "11th year in a row", "stole four Impressionist paintings worth about $163 million (180 million Swiss francs)", "fastest circumnavigation of the globe in a powerboat", "Guinea, Myanmar, Sudan and Venezuela", "Austin Wuennenberg,", "Diversity,", "forcibly injecting them with psychotropic drugs", "Oaxacan countryside of southern Mexico", "buckling under pressure from the ruling party.", "ties", "more than 100", "bribing other wrestlers to lose bouts,", "Alfredo Astiz,", "Tom Tucker", "convert single - stranded genomic RNA into double - stranded cDNA", "Andy Garcia, and Michael Douglas", "17,060 feet", "Peter Robert Auty", "2009", "\"The Jack Paar Show\"", "Marilyn Monroe", "\"The Who\"", "Julie Andrews", "a triple letter score with a Q or X in this"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5011436163779914}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 0.16666666666666666, 0.0, 1.0, 0.4, 0.0, 1.0, 0.15384615384615383, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615385, 0.2857142857142857, 0.0, 1.0, 1.0, 0.25, 0.7272727272727272, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4864864864864865, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7136", "mrqa_squad-validation-2000", "mrqa_squad-validation-7845", "mrqa_squad-validation-6477", "mrqa_newsqa-validation-412", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1123", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-552", "mrqa_hotpotqa-validation-1968", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1649"], "SR": 0.40625, "CSR": 0.5677083333333333, "EFR": 1.0, "Overall": 0.7411197916666665}, {"timecode": 18, "before_eval_results": {"predictions": ["Jason Bourne", "isobaric (constant pressure) processes", "1999", "mesoglea", "a body of treaties and legislation, such as Regulations and Directives,", "liquid", "socially owned", "Mark Twain's", "in amylopectin starch granules that are located in their cytoplasm,", "Tower District", "threat level", "pro-democracy activists clashed Friday with Egyptian security forces in central Cairo,", "at a construction site in the heart of Los Angeles.", "overthrow the socialist government of Salvador Allende in Chile,", "school was repurposed and commissioned for civilian exploration voyages in 1825", "a rally at the State House next week", "2,000", "Michael Schumacher", "Ventures", "seven", "\"The noose incident occurred two weeks after Black History Month", "resigned", "\"I'm just getting started.\"", "21,", "diplomatic relations", "hand-painted Swedish wooden clogs", "Daniel Radcliffe", "Muslim", "five", "mother", "$10 billion", "a seven-member Spanish flight crew", "school on the coastal island,", "9-week-old", "a form of liquid morphine used by terminally ill patients will remain on the market even though it is an \"unapproved drug,", "Lucky Dube,", "a college English professor", "James Newell Osterberg", "At least 40", "NATO", "Lindsey Vonn", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed.\"", "a grizzly bear", "International Polo Club Palm Beach in Florida.", "Nevaeh (heaven spelled backward)", "The cervical cancer vaccine,", "poor families", "28 of them seriously enough to go to a hospital,", "creation of an Islamic emirate in Gaza,", "an \"unnamed international terror group\"", "a UPS delivery box where it shouldn't be.", "Manchester United", "Hamas ministry spokesman Taher Nunu", "the final episode airing in the UK on 25 December 2015 on ITV", "initially absent from the original game release, but were added in the January 2017 patch", "30", "cuffs", "Waylon Albright \"Shooter\" Jennings", "people working in film and the performing arts,", "Nestle Chocolate", "school holidays", "Marlborough, New Hampshire", "1968", "The Krypto Report"], "metric_results": {"EM": 0.375, "QA-F1": 0.477127690018315}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.2564102564102564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.375, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3444", "mrqa_squad-validation-7516", "mrqa_squad-validation-1235", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2733", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-5209", "mrqa_hotpotqa-validation-2986", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-3428"], "SR": 0.375, "CSR": 0.5575657894736843, "EFR": 1.0, "Overall": 0.7390912828947368}, {"timecode": 19, "before_eval_results": {"predictions": ["the blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "an Executive Committee,", "San Francisco Bay Area's Levi's Stadium", "the death of Elisabeth Sladen in early 2011.", "annual NFL Experience", "English", "61%", "plastoglobulus", "three", "Turkey", "wombat", "Sri Lanka", "a maturational", "Peyton Place", "carats", "Gin Rummy", "Pilate", "enamel", "a special grade", "\" Help yourself to happiness\"", "Battle of Hastings", "the delta at the mouth of the river where the city of Astrakhan is... the flow of water from the Caspian into the Zaliv Kara-Bogaz-Gol.", "a work journal", "\"The 1,001 Nights\"", "Gannett Company", "\"Won't Get Fooled Again\"", "\"Don Juan De Marco\"", "Fes", "the World Cup", "Interlaken", "The Sandlot Kids", "Princeton", "Mandy Well you came.", "the Russian Collection at the Library of Congress", "Malay Peninsula", "Herman Wouk", "Frederick IV,", "\"The heart of a fool\"", "poetry", "Europe", "Stock Dinosaurs", "unassisted", "wheat", "Derek Smalls", "Dalits", "Harry Houdini", "\"Randy\"", "chronic fatigue syndrome", "dollop", "Vincent van Gogh", "goat cheese", "James Ross Clemens", "aces", "1991", "to universalize the topic of the song into something everyone could relate to and ascribe personal meaning to in their own way", "cuban cigars", "\"Thrilla in Manila\"", "Kansas City, Missouri.", "841", "Ike,", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "East", "the Peace of Westphalia of 1648", "near the city of Cairo, Illinois"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4778306512184266}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.7272727272727273, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.10526315789473684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.13953488372093023, 1.0, 0.5, 0.8, 1.0, 1.0, 0.9090909090909091, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-127", "mrqa_squad-validation-7872", "mrqa_squad-validation-8465", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-11299", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-2769", "mrqa_newsqa-validation-3214", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-3559"], "SR": 0.359375, "CSR": 0.54765625, "EFR": 0.975609756097561, "Overall": 0.7322313262195121}, {"timecode": 20, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-1252", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1967", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-3519", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-782", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-920", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11270", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2607", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4469", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-971", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10143", "mrqa_squad-validation-10168", "mrqa_squad-validation-10241", "mrqa_squad-validation-10266", "mrqa_squad-validation-10370", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1141", "mrqa_squad-validation-115", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1254", "mrqa_squad-validation-127", "mrqa_squad-validation-1288", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1747", "mrqa_squad-validation-1765", "mrqa_squad-validation-1827", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-218", "mrqa_squad-validation-22", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2379", "mrqa_squad-validation-2383", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-2538", "mrqa_squad-validation-2545", "mrqa_squad-validation-257", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2886", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-305", "mrqa_squad-validation-3052", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3567", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3864", "mrqa_squad-validation-3917", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3975", "mrqa_squad-validation-3986", "mrqa_squad-validation-3994", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4187", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4883", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5097", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-5237", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5604", "mrqa_squad-validation-5677", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5859", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6347", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6594", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7034", "mrqa_squad-validation-7039", "mrqa_squad-validation-7051", "mrqa_squad-validation-71", "mrqa_squad-validation-7125", "mrqa_squad-validation-7136", "mrqa_squad-validation-7192", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7449", "mrqa_squad-validation-7521", "mrqa_squad-validation-7576", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7708", "mrqa_squad-validation-7751", "mrqa_squad-validation-7814", "mrqa_squad-validation-7863", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7881", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8471", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8670", "mrqa_squad-validation-8710", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9095", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9367", "mrqa_squad-validation-9405", "mrqa_squad-validation-942", "mrqa_squad-validation-9594", "mrqa_squad-validation-9614", "mrqa_squad-validation-9669", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-4881", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-7496"], "OKR": 0.88671875, "KG": 0.4796875, "before_eval_results": {"predictions": ["the main contractor", "widespread education", "300", "an attack on New France's capital, Quebec", "two-thirds", "Decompression sickness", "1979", "Parliament Square, High Street and George IV Bridge", "1959", "the Lincoln Laboratory", "grizzly bear", "Dracula", "Sid Vicious", "Nitrous oxide", "the Tchaikovsky 1812 Overture", "Frederic Remington", "the Nicaraguan Depression", "Arkansas", "a programming language model organized around objects rather than \"actions\"", "2010", "a discharge based on military", "a Whig candidate for president", "the NBC serial medical drama ER", "a yellow lotus", "a genie", "The Princess Diaries", "Arkansas", "Mao Zedong", "a multilingual person", "a dialect or a bell clapper", "the Wells Fargo", "Sundance Kid", "prayer", "amber", "Holly", "Umbria", "a Roth IRA", "Quentin Tarantino", "the Palatine Hill", "Kentucky", "axiom", "Daylight Saving Time", "a genie", "\"Airplane\"", "Scooter Libby", "a great flood", "\"a pH factor of more than 7\"", "Equatorial Guinea", "a Nazi POW camp", "bowling", "Cecil Rhodes", "Aerobic", "Anaheim", "Steve Hale", "the distribution and determinants of health and disease conditions in defined populations", "Belgium's", "David Jason", "the 137th", "Merck & Co.", "the computer processing unit (CPU) market", "the Moscow Correspondent at Star City,", "France", "a Hungarian Horntail", "Phil Mickelson"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6253906250000001}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.625, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4918", "mrqa_searchqa-validation-4028", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-16826", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-7628", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-12996", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-12477", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-3514", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-4992", "mrqa_hotpotqa-validation-3157", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-4118"], "SR": 0.515625, "CSR": 0.5461309523809523, "EFR": 1.0, "Overall": 0.7266480654761904}, {"timecode": 21, "before_eval_results": {"predictions": ["a lesson plan", "the laws of physics", "1893", "Welsh", "the people themselves", "criminal", "a monthly subscription", "1526", "novella", "the President of the United States", "above the light source and under the sample in an upright", "November 3, 2007", "1939", "April 1917", "1959", "the orphanage where he was raised", "September 19 - 22, 2017", "tolled ( quota ) highways", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "Bobby Eli", "Nagaland", "Jane Eisner", "Paracelsus", "January 2004", "members of the gay ( LGBT ) community", "the 1930s", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Jerry Lee Lewis", "push the food down the esophagus", "Splodgenessabounds", "drive", "Mary Berry", "a yellow background instead of a white one", "a diastema ( plural diastemata )", "Eddie Murphy", "television", "high officials", "flour and water", "the National Football League ( NFL )", "the Golden Age of India", "card verification value", "T - Bone Walker", "Ray Charles", "Francis Hutcheson", "1937", "Cairo, Illinois", "Barbara Windsor", "the British colonial government", "Norman Whitfield", "the Executive Residence of the White House Complex", "the eighth episode of Arrow's second season", "The courts", "the Kanawha River", "athletics", "isosceles", "1898", "Sir Matthew Arundell", "WFTV", "tennis", "Las Vegas", "Austria", "women and breast cancer", "Harry Nicolaides", "reached under the counter, grabbed his gun and told the robber to drop the bat and get down on his knees."], "metric_results": {"EM": 0.546875, "QA-F1": 0.5996763804346525}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5925925925925926, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.9428571428571428, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 0.08]}}, "before_error_ids": ["mrqa_squad-validation-2336", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-5041", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-9330", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-4715", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1985"], "SR": 0.546875, "CSR": 0.5461647727272727, "EFR": 0.896551724137931, "Overall": 0.7059651743730407}, {"timecode": 22, "before_eval_results": {"predictions": ["instruction in literacy and numeracy, craftsmanship or vocational training, the arts, religion, civics, community roles, or life skills.", "the bark of mulberry trees.", "drama series", "1806", "\"distributive efficiency\"", "on issues related to the substance of the statement.", "in 23 cities", "Continental drift", "Frank Oz", "1975", "775", "Kimberlin Brown", "in Ephesus in AD 95 -- 110", "the status line", "the disk, about 26,000 light - years from the Galactic Center, on the inner edge of the Nebula Arm, one of the spiral - shaped concentrations of gas and dust", "an amalgamated mutant human / Kree physiology that rendered her resistant to most toxins and poisons, with the added effect of making her virtually invulnerable", "Dr. Joel S. Engel of Bell Labs, his rival", "in Weston - super-Mare, which stood in for Clevedon", "a type II endoprotease, cleaves the C peptide - A chain bond", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Javier Fern\u00e1ndez", "a lightning strike destroyed the top storey", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Coton in the Elms", "an ascender", "Wakanda", "1992", "ATP, generated by the root respiration : as the root cells actively take part in the process, it is called active absorption", "shortwave radio", "Mansa Musa came to the throne after a series of civil wars and ruled for thirty years", "Edward Kenway", "Robert Hooke", "the study of the interstellar medium ( ISM ) and giant molecular clouds ( GMC )", "Alicia Vikander as Lara Croft, who embarks on a perilous journey to her father's last - known destination, hoping to solve the mystery of his disappearance", "Jepsen", "the name announcement of Kylie Jenner's first child is the most - liked picture with over 17 million likes", "5 liters", "somatic cell nuclear transfer ( SCNT )", "Betty", "it results from rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "the 17 - inch MacBook Pro", "a head - up display", "a presidential representative democratic republic, whereby the President of El Salvador is both head of state and head of government, and of an Executive power is exercised by the government", "Ferm\u00edn Francisco de Lasu\u00e9n", "a moral tale", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "the final capped year under that agreement, the cap was $128 million per team, while the floor was 87.6 % of the cap", "of Spanish / Basque origin", "Alison", "a group of seemingly unconnected people in Atlanta come to terms with the relationships they have with their mothers", "the Copa Catalunya final, a 4 -- 1 victory over Espanyol, became known in club lore as the partido de la m\u00e1scara, the final of the mask", "a male is larger than that of a female", "a coffee house", "Director of Prisons", "Cheshire", "#364", "24800 mi long", "the liberal revolutions of 1848", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Matamoros, Mexico,", "airport caused arrival delays and mainly affected Continental Airlines, which is the airport's largest tenant.", "The X-Files", "biometrics", "Inseparable"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5551981583231582}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, true, false, true], "QA-F1": [0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6842105263157895, 0.1142857142857143, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.7407407407407407, 1.0, 0.0, 0.2857142857142857, 1.0, 0.10526315789473684, 0.0, 0.19047619047619044, 0.0, 1.0, 0.4, 0.3571428571428571, 0.0, 0.5, 0.2857142857142857, 0.888888888888889, 0.14285714285714288, 0.8, 0.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.1111111111111111, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.8, 1.0, 0.23076923076923078, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1802", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-1120", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4496", "mrqa_hotpotqa-validation-1679", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-3633"], "SR": 0.390625, "CSR": 0.5394021739130435, "EFR": 0.9487179487179487, "Overall": 0.7150458995261983}, {"timecode": 23, "before_eval_results": {"predictions": ["around 100,000 soldiers", "an extensive neoclassical centre referred to as Tyneside Classical", "algebraic", "his birthtown, Smiljan", "Persia", "ABC-DuMont", "the \"bathtub curve\"", "the First World War", "John Constable", "Charlie Harper", "a genie", "James I", "Everton", "September 17th", "cogito ergo sum", "Bull Moose Party", "Augusta National", "Demi Moore", "by secret ballot", "Cornell", "Robert Stroud", "Alice in Wonderland", "caffeine", "Crash", "11", "17 pink", "Achille Lauro", "harrison for everyone to enjoy every detail on every frame.", "Michael Miles", "Swansea", "Wyatt Earp", "Chuck Hagel", "Haiti", "Bangladesh", "arm form", "Sean Maddox", "one king, one queen, two knights, two bishops, and eight pawns", "Bath, Coventry, Gloucester,", "tawny", "Andy Murray", "Independence Day", "a phantom eight-ender", "the Hanseatic League", "the Crusades", "King Henry I", "ThunderCats", "Mursery Comics", "the European Council", "Volkswagen", "George IV", "the Kalavinka", "China", "Justice Lawrence John Wargrave", "Thomas Jefferson", "North Dakota, South Dakota, Minnesota, Iowa, Nebraska, Colorado, Kansas, Oklahoma, New Mexico, Texas, Maine, Vermont, Oregon, and Idaho", "William Adelin", "Barbary pirates", "Sir William Collins", "to hold onto his land", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "as neural devices start to become more and more complicated, it gets easier and easier for people to overlook a bug that could become a very serious risk.", "mirela Valeanu", "the Chilean flag", "Bahadur Shah Zafar"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5607886904761905}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false], "QA-F1": [0.8, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1904761904761905, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6205", "mrqa_squad-validation-5180", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-5816", "mrqa_triviaqa-validation-7339", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-6138", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-1683", "mrqa_naturalquestions-validation-7661", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2371", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-13686"], "SR": 0.484375, "CSR": 0.537109375, "EFR": 1.0, "Overall": 0.72484375}, {"timecode": 24, "before_eval_results": {"predictions": ["Napoleon", "the northeast", "skin damage", "three", "European Parliament and the Council of the European Union", "Steve McQueen", "\u00c9dith Piaf", "piano", "Midtown", "bogeyman", "shoes", "tennis", "Geneva", "charlie", "Woodrow Wilson", "hispan", "Cardiff", "bristol", "bulldog", "distance", "Edward VI", "linseed", "mercury", "trumpet", "architecture", "james bond", "Iain Banks", "Valencia", "gluten", "Jan Van Eyck", "claire", "red", "George Troughton", "stephen", "hispan", "the Oracle at Delphi", "augusta", "Yosemite", "the Sandstone", "king duncan", "8 minutes", "uranium", "pretty Betsy", "red bull", "West Point", "Saint Cecilia", "red bull", "Dr Ichak", "George Whittle", "charlie", "We Interrupt This Week", "Chester", "his confluence with the Paran\u00e1 River north of Corrientes and Resistencia", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "2005", "Jonathan Craven", "1698", "President Bill Clinton", "an Airbus A320-214", "a fall at her home in suburban Los Angeles.", "246", "Hurricane", "The Treasure of the Sierra Madre", "smallpox"], "metric_results": {"EM": 0.40625, "QA-F1": 0.509765625}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.6, 0.625, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2911", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-3769", "mrqa_triviaqa-validation-3085", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1951", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-4073", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-142", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-6358", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-2426", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-3034", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-1857"], "SR": 0.40625, "CSR": 0.531875, "EFR": 1.0, "Overall": 0.7237968749999999}, {"timecode": 25, "before_eval_results": {"predictions": ["The majority", "Thomas Piketty", "1,548", "zoning and building code requirements", "Science and Discovery", "Lake Placid", "the Central line", "Vietnam", "bullseye", "bluebird", "wherry", "300", "1894", "The Colossus of Rhodes", "rwii", "Morticia", "Billie Holiday", "the National Council for the Unmarried Mother and her Child", "wales", "jon pertina flessibile Acquisto verificato", "Len Deighton", "the Highland garb", "Alex Garland", "L. Pasteur", "Dionysus", "Benjamin Disraeli", "Johannesburg", "Martin Luther King", "wales", "thomas thomas", "a bagel set", "ocellaris", "David Cameron", "Newfoundland", "Eddie Cochran", "Alessandro Volta", "OutKast", "wales", "sunny afternoon", "the Biafra secession", "Anna Mae Bullock", "spain", "Cuba", "a dove", "Heston Blumenthal", "Harold Godwinson", "jack Johnson", "ritchie Valens", "wales", "carWale", "wales", "Gargantua", "Kryptonite", "hair jelly", "if the concentration of a compound exceeds its solubility", "Morning Edition", "Nicholas John \" Nick McCarthy\" McCarthy", "Paul John Manafort Jr.", "underprivileged", "tripplehorn", "80", "Maldives", "Matt Leinart", "Max Planck"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5452390491452992}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.888888888888889, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6973", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-3418", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-6724", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2965", "mrqa_hotpotqa-validation-4832", "mrqa_hotpotqa-validation-3714", "mrqa_newsqa-validation-439", "mrqa_searchqa-validation-13257"], "SR": 0.484375, "CSR": 0.5300480769230769, "EFR": 0.9696969696969697, "Overall": 0.7173708843240092}, {"timecode": 26, "before_eval_results": {"predictions": ["paid professionals", "Basel", "\"we want to practice Christian love toward them and pray that they convert,\"", "Informal rule", "fear of animals phobia", "raven", "helium", "John Logie Baird", "city of chicago", "Pickwick", "the Titanic", "Benjamin Britten", "taekwondo", "morocco", "Rome", "lolita", "teeth", "john Emburey", "bury", "nitrogen", "morocco", "john astan", "Venus", "Ben Watson", "chantal", "god Zeus", "\"If\u2013\u201d", "posh", "chicago", "city of chicago", "Australia", "cerebrum", "chicago Chaplin", "chicago", "the Netherlands", "port of Vladivostok", "simmering", "insects", "phoenicia", "neil", "Gulf of Aden", "beard", "lichfield", "lithium", "albert", "posh", "chancery", "emulsion", "Rio", "peacock", "laos", "chicago", "6ft 1in", "Judith Cynthia Aline Keppel", "94 by 50 feet", "Elisha Nelson Manning", "photographs, film and television", "March 17, 2015", "AbdulMutallab", "\"an eye for an eye,\"", "two", "island of the sea", "Dennis Miller", "May"], "metric_results": {"EM": 0.5, "QA-F1": 0.5391927083333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.375, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2368", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-7030", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-1605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-6046", "mrqa_hotpotqa-validation-3563", "mrqa_newsqa-validation-1535", "mrqa_searchqa-validation-8872"], "SR": 0.5, "CSR": 0.5289351851851851, "EFR": 0.96875, "Overall": 0.716958912037037}, {"timecode": 27, "before_eval_results": {"predictions": ["CBS", "17th", "60%", "Xbox One", "two", "will not", "sleepless in denial", "to \"wipe out\" the United States if provoked.", "blew himself up", "Pittsburgh", "wildland", "oil paintings", "\"Americans always believe things are better in their own lives than in the rest of the country,\" said CNN polling director Keating Holland", "Elena Kagan", "Mandi Hamlin", "750", "imminent threat of a North Korean missile strike or confrontation between the two countries at sea.", "Clarkson", "Expedia", "a tuatara", "severe", "10 mPG in efficiency", "Islamabad", "Rawalpindi", "Santaquin City, Utah,", "Sunday", "four", "jAKARTA, Indonesia", "Johannesburg", "nearly $2 billion", "Six members of Zoe's Ark", "2002", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "\"It has never been the policy of this president or this administration to torture.\"", "Herman Thomas", "Costa Rica", "Lee Myung-Bak", "kepplinger", "\"They don't feelMisty Cummings is the last person known to have seen Haleigh the night she disappeared from the family's rented mobile home.", "bleakness", "Melbourne", "into the Southeast,", "Sunday,", "$273 million", "Salt Lake City", "millionaire's surtax", "an animal tranquilizer", "at Fort Bragg, North Carolina,", "kabul's International Security Assistance Force", "Jaime andrade", "1994", "dance Your Ass Off", "Santiago Ram\u00f3n y Cajal", "Los Angeles", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "magic", "jethro Tull", "the Sutherland Brothers", "Lin-Manuel Miranda", "15,024", "novelist and poet", "the lower part of the Earth's crust", "sleepless in september", "marshmallows"], "metric_results": {"EM": 0.375, "QA-F1": 0.4991456512976543}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.5, 0.13793103448275862, 1.0, 1.0, 1.0, 0.11764705882352941, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.09523809523809523, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.8000000000000002, 1.0, 1.0, 1.0, 0.0, 0.0, 0.07142857142857142, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_squad-validation-5644", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-282", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-5756", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-4411", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1864", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-13251"], "SR": 0.375, "CSR": 0.5234375, "EFR": 0.975, "Overall": 0.7171093749999999}, {"timecode": 28, "before_eval_results": {"predictions": ["Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager.", "illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins", "Pittsburgh", "Cress", "molecular clouds in interstellar space", "Stefanie Scott", "white rapper B - Rabbit ( Eminem ) and his attempt to launch a career in a genre dominated by African - Americans", "Ram Nath Kovind", "Senator Joseph McCarthy's Senate Permanent subcommittee on Investigations, an investigation known as the Army -- McCarthy hearings", "one crore", "members of the gay ( LGBT ) community", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "Twickenham Stadium", "wildlife conservation charity in Europe", "1776", "Continental drift", "Julie Adams", "genetics and the male hormone dihydrotestosterone", "Jonathan Cheban", "Norman Greenbaum", "De Waynene Warren as Jarius `` G - Baby '' Evans", "Thirty years after the Galactic Civil War", "a sequel to the song `` Because of You '' ( 2004 ) but with a `` happy ending ''", "Darth Sidious", "October 22, 2017", "April 17, 1982", "the Speaker of the House of Representatives", "London, United Kingdom", "the majority opinion of the court which gives rise to its judgment", "the Near East", "the population, serving staggered terms of six years ; with fifty states presently in the Union, there are 100 U.S. Senators", "Isle of FERNANDO 'S", "pre-Columbian times", "the central plains", "China ( formerly the Republic of China ), Russia", "a costume party ( American English ) or a fancy dress party ( British English )", "directly into the bloodstream", "Kenny Anderson", "beneath the liver", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "Nathan Hale", "Jesse Frederick James Conaway", "the naos", "defense against rain rather than sun", "the colonization of the Americas began and the cocoa plant was discovered in regions of Mesoamerica, until the present", "September 19, 2017", "West Egg on prosperous Long Island in the summer of 1922", "it was first published on November 12, 1976 by Ballantine Books", "Butter Island off North Haven, Maine in the Penobscot Bay", "wintertime", "Tony Rydinger", "Moira Kelly", "Flanagan and Allen", "Havana,", "the Mediterranean Sea", "Manor of More", "Craig William Macneill", "the Cash for Ash scandal", "Michael Partain,", "1962", "The Orchid Thieves", "\"Saw the nakedness of his father\"", "Balfour Declaration", "Arthur Gordon Pym"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5872791224244881}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 0.11764705882352941, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 0.2, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.5714285714285715, 0.18181818181818182, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.125, 1.0, 0.7368421052631577, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.5, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-386", "mrqa_squad-validation-6848", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1135", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-3145", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-2150", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1570", "mrqa_searchqa-validation-4495", "mrqa_searchqa-validation-12829"], "SR": 0.453125, "CSR": 0.5210129310344828, "EFR": 0.9142857142857143, "Overall": 0.7044816040640394}, {"timecode": 29, "before_eval_results": {"predictions": ["Sophocles' play Antigone", "the Meuse", "In 1806", "Chandigarh -- Chandigsarh   Dadra and Nagar Haveli -- Silvassa   Daman and Diu -- Daman   Lakshwadweep -- Kavaratti", "MacFarlane", "the final two games", "Hon July Moyo", "many forested parts of the world", "Narendra Modi", "land, fresh water, air, rare earth metals and heavy metals including ores such as gold, iron, copper, silver", "Andrew Michael Harrison", "The White House Executive Chef", "Michael Crawford", "9 February 2018", "the red bone marrow of large bones", "Drew Barrymore", "Pangaea or Pangea", "Jonathan Breck", "the outermost layer", "Joe Pizzulo and Leeza Miller", "Ming", "201", "Chuck Noland", "Toronto Maple Leaf ( 6 from 1956 to 1995 )", "Max Martin", "Waylon Jennings", "Nancy Jean Cartwright", "Beyonc\u00e9 and Bruno Mars", "Ephesus", "Boston Red Sox", "1996", "when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "United States customary units", "Joe Spano", "Michael Moriarty", "Rock Island, Illinois", "2002", "September 1959", "Louis Hynes", "Bonnie Lipton", "`` Tippecanoe and Tyler Too ''", "the probability of rejecting the null hypothesis", "the third stanza", "Elijah, Rebekah, Klaus and Davina", "the central plate", "Tagalog or English", "Ernest Rutherford", "Napoleon Bonaparte", "the 12th century", "Yosemite National Park", "Norman Pritchard", "2014", "florida", "stars and galaxies", "Jack Cade", "October 13, 1980", "250cc world championship", "Polihale State Park", "Defense of Marriage Act", "Bronx.", "almost 9 million", "\"Tennessee Waltz\"", "abacus", "Cyrus"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6599307586454763}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, true], "QA-F1": [0.5, 1.0, 0.0, 0.5555555555555556, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 1.0, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.39999999999999997, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6638", "mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-3760", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-5106", "mrqa_hotpotqa-validation-1489", "mrqa_newsqa-validation-1426"], "SR": 0.53125, "CSR": 0.5213541666666667, "EFR": 0.9333333333333333, "Overall": 0.7083593749999999}, {"timecode": 30, "UKR": 0.66015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-392", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-974", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-13874", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15740", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-1649", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9269", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10181", "mrqa_squad-validation-10268", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1765", "mrqa_squad-validation-1791", "mrqa_squad-validation-1848", "mrqa_squad-validation-1890", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-2019", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2243", "mrqa_squad-validation-2411", "mrqa_squad-validation-2456", "mrqa_squad-validation-247", "mrqa_squad-validation-2545", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2742", "mrqa_squad-validation-305", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3507", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3718", "mrqa_squad-validation-3770", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-386", "mrqa_squad-validation-3863", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3986", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-4046", "mrqa_squad-validation-4054", "mrqa_squad-validation-4175", "mrqa_squad-validation-4213", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4629", "mrqa_squad-validation-4883", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5097", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5588", "mrqa_squad-validation-5692", "mrqa_squad-validation-5724", "mrqa_squad-validation-5781", "mrqa_squad-validation-5818", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-6019", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6353", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6543", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6965", "mrqa_squad-validation-6973", "mrqa_squad-validation-6999", "mrqa_squad-validation-7039", "mrqa_squad-validation-71", "mrqa_squad-validation-7192", "mrqa_squad-validation-7368", "mrqa_squad-validation-7426", "mrqa_squad-validation-7521", "mrqa_squad-validation-7612", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7814", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-829", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9176", "mrqa_squad-validation-9196", "mrqa_squad-validation-942", "mrqa_squad-validation-9445", "mrqa_squad-validation-957", "mrqa_squad-validation-9614", "mrqa_squad-validation-9764", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4465", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7461"], "OKR": 0.810546875, "KG": 0.434375, "before_eval_results": {"predictions": ["space suit materials", "In 1992", "at least four", "Genesis", "real estate", "cajun cooking from the heart of south louisiana", "metric carat", "Mission: Impossible", "dikonos", "Edinburgh", "Paul Gauguin", "Galpagos", "Mark Twain", "Battle of Chancellorsville", "fight film", "Wii", "the Suez Canal", "Dave Matthews", "henry", "dentures", "BACON", "Kinko's", "a platypus", "bang", "the tongue", "Cherokee", "necropolis", "henry", "Oyster", "fortune", "henry Gill", "bamboos", "Isaac Newton", "Unabomber", "Narnia", "Freud", "henry", "librettos", "henry vi", "henry", "business", "bison", "a New Orleansstyle iced coffee concentrate that's brewed and bottled by hand in Brooklyn, New York.", "Botswana", "Susan B. Anthony dollar", "Mattel", "Little Red Riding Hood", "splint", "milky circle", "vojvodina", "muscle", "latkes", "infection, irritation, or allergies", "Castleford", "usually in May", "henry", "Sahara", "henry", "You're Next", "Headless Body in Topless Bar", "political correctness", "Zoabi", "Princess Diana", "homicide"], "metric_results": {"EM": 0.5, "QA-F1": 0.5976810515873016}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true], "QA-F1": [0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3969", "mrqa_squad-validation-9666", "mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-12471", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-3542", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-12684", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-5586", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-4851", "mrqa_naturalquestions-validation-2666", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-6237", "mrqa_newsqa-validation-1291"], "SR": 0.5, "CSR": 0.5206653225806452, "EFR": 1.0, "Overall": 0.6851486895161291}, {"timecode": 31, "before_eval_results": {"predictions": ["the West", "Begter", "2016", "Sheev Palpatine", "the Church of England in the 18th century", "July 1, 1923", "a pH indicator, a color marker, and a dye", "winter", "South Africa", "East Jaintia Hills district", "the largest part of the brain", "Janie Crawford", "Rob Jacobs", "the straight - line distance from A to B", "786", "the Immigration and Naturalization Service", "Blind carbon copy to tertiary recipients who receive the message", "the `` round '', the rear leg of the cow", "1957", "Terrence Howard", "does not count as a hit", "Andreas Vesalius", "Moscazzano", "Kristy Swanson", "bacteria", "Asuka", "Jay Baruchel", "the island of Tasmania", "a pole", "the Houston Astros", "Six Degrees of Separation", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "the retina", "the fascia surrounding skeletal muscle", "Pangaea", "2017", "near the inner rim of the Orion Arm", "Ricky Nelson", "a pen or pencil", "Debbie Gibson", "Mad - Eye Moody and Hedwig", "Rabbinic Judaism's Oral Torah", "the Omnipotent Lord", "the winter", "Algeria", "in saecula saeculorum in Ephesians 3 : 21", "Harlem River", "1998", "R.E.M.", "332", "above the light source and under the sample in an upright microscope", "Georgia Bulldogs football team", "Illinois", "Northumberland", "Ireland", "Travis County", "Boston Bruins", "Adam Dawes", "Democrats", "the mammoth", "Mumbai", "a dummy", "Aristotle", "nothing"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6323215116389458}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.4444444444444445, 0.1, 0.5, 0.5, 1.0, 1.0, 0.18181818181818182, 0.16666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2, 0.5, 0.888888888888889, 1.0, 0.0, 0.052631578947368425, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.6666666666666666, 0.0, 1.0, 0.7000000000000001, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6428571428571429, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-9791", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-5599", "mrqa_hotpotqa-validation-5831", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3518", "mrqa_searchqa-validation-6752"], "SR": 0.453125, "CSR": 0.5185546875, "EFR": 0.8857142857142857, "Overall": 0.6618694196428572}, {"timecode": 32, "before_eval_results": {"predictions": ["magnitude and direction", "May 21, 2013", "1982", "Albert Lee \"Al\" Ueltschi", "Giotto di Giovanni", "1985", "26,000", "Lakshmibai", "Premier League", "French", "2009", "wargame", "Bonobo", "Robin David Segal", "Greg Gorman", "Shameless", "stolperstein", "November 11, 1901", "Carl Zeiss AG", "YouTube", "Bambi, a Life in the Woods", "Robert \"Bobby\" Germaine", "2004", "DTM", "one season", "\"Twice in a Lifetime\"", "the Sun", "Greg Hertz", "Srinagar", "The Walking Dead", "Ted Nugent", "jewelry designer", "Gust Avrakotos", "Maleficent", "Coll\u00e8ge de France", "Miami-Dade County", "Marty Ingels", "1945", "Edward R. Murrow", "Conservatorio Verdi", "Mindy Kaling", "June 10, 1982", "beer and soft drinks", "Liga MX", "Donald Duck", "The School Boys", "Lord Chancellor of England", "Taoiseach", "The English Electric Canberra", "Richa Sharma", "48,982", "The Trapp Family", "53", "Michigan State Spartans", "Frank Langella", "elephant", "a fool", "Nitric acid", "Kensington Gardens, west London", "homicide by undetermined means,", "maintain an \"aesthetic environment\" and ensure public safety", "Charles de Gaulle", "tanks", "Hannah Montana"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6271577380952381}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.5, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-444", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-1362", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-1030", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-5471", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3726", "mrqa_searchqa-validation-4641", "mrqa_searchqa-validation-4628"], "SR": 0.546875, "CSR": 0.5194128787878788, "EFR": 1.0, "Overall": 0.6848982007575758}, {"timecode": 33, "before_eval_results": {"predictions": ["CEPR", "in special university classes, called Lehramtstudien (Teaching Education Studies)", "CTV Television Network", "13\u20133,", "American", "July 25 to August 4", "in 1958", "Norway", "twenty-three", "The Crips", "The Crowned Prince of the Philadelphia Mob", "the Kentucky Derby", "Charles Edward Stuart", "historic buildings, arts, and published works", "on November 6, 2018,", "Batman Forever", "Tennessee", "Jean de Florette", "books, films and other media", "King Duncan", "Europop", "1835", "Mayor Ed Lee", "Jamaica", "Norwegian", "Dutch", "1976", "1898", "Motorised quadric recycling", "30.9%", "Charlyn Marie \" Chan\" Marshall", "in 1968", "76,416", "Father Dougal McGuire", "June 17, 2007", "political geographer", "The United States of America (USA)", "The Ryukyuan people", "coaxial cables", "November 15, 1903", "Ant Timpson, Ted Geoghegan and Tim League", "1936", "1952", "Indian", "Steve Jobs", "Pablo Escobar", "ZZ Top", "Larry Gatlin & the Gatlin Brothers Band", "Russian Empire", "bi-fuel", "National Park Service", "\"King of Cool\"", "a subduction zone", "Barry Bonds", "Owen Vaccaro", "Machu Picchu", "Exile", "in the noirish \u201cNighthawks\u201d", "only normal maritime traffic around Haiti, and it has not intercepted any Haitians attempting illegal crossings into U.S. waters.", "U Win Tin,", "$1.45 billion", "onomatopoeia", "Singapore", "a femur"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7052980006105006}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.6153846153846153, 0.5, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1904761904761905, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2043", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-498", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-316", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-3703", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-742", "mrqa_searchqa-validation-15477"], "SR": 0.5625, "CSR": 0.5206801470588236, "EFR": 0.9642857142857143, "Overall": 0.6780087972689076}, {"timecode": 34, "before_eval_results": {"predictions": ["Manning", "no acceleration", "over 20 million", "the Guilden Morden boar", "Vienna", "that Fama and French's research is period dependent", "the Harpe brothers", "Bill Clinton", "Dirk Werner Nowitzki", "Detroit, Michigan,", "Bury St Edmunds, Suffolk, England", "novelty songs", "Mahoning County", "6 January 1915", "There Is Only the Fight", "Bohemia", "New York", "The Washington Post", "400 MW", "Ghanaian", "Household Words", "Gatwick Airport", "Tokyo International Airport", "Minette Walters", "CTV", "Firestorm", "2013", "Leslie Edwin Miles", "40 Days and 40 Nights", "James Tinling", "2014", "Ray Teal", "gull-wing doors", "Terry Malloy", "Operation Neptune", "Attack the Block", "House of Commons", "Hessians", "Battle of Chester", "Wayne County, Michigan.", "India", "mistress of the Robes", "Eleanor of Aquitaine", "Rachel Anne Maddow", "November 13, 2015", "2", "Director of Central Intelligence", "1963", "Bologna Process", "Pittsburgh", "women's basketball team", "Salman Rushdie", "FUTA", "the Hongwu Emperor of the Ming Dynasty", "commemorating fealty and filial piety", "Mexico", "It is pronounced: \u02c8\u0251rk\u0259ns\u0254\u02d0/", "streptococcal", "1979", "the son", "$8.8 million", "Applyn Danko", "Miriam Makeba", "a mesio-occlusal cavity"], "metric_results": {"EM": 0.5, "QA-F1": 0.6116815476190476}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.4, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.8571428571428571, 0.5, 0.0, 0.33333333333333337, 0.25, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-384", "mrqa_squad-validation-10316", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-1226", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1797", "mrqa_hotpotqa-validation-2975", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-1420", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-2316", "mrqa_newsqa-validation-501", "mrqa_searchqa-validation-73", "mrqa_searchqa-validation-9394"], "SR": 0.5, "CSR": 0.5200892857142857, "EFR": 1.0, "Overall": 0.6850334821428572}, {"timecode": 35, "before_eval_results": {"predictions": ["the DuMont Television Network", "Mount Kenya", "Albany", "1908", "3 May 1958", "The 1995\u201396 season", "Ronald Wilson Reagan", "Chiltern Hills", "\"A Million Ways to Die in the West\"", "Bay of Fundy", "CD Castell\u00f3n", "2001", "former White Zombie bassist Sean Yseult", "country", "Previously she was a Democratic member of the Hawaii Senate, representing the 13th District since 1996", "The Guadalcanal Campaign", "Paul W. S. Anderson", "15 February 1970", "Yasiin Bey", "Shooter Jennings", "Cincinnati", "\"Bad Moon Rising\"", "Kris Kristofferson", "held the American record for the most time in space (381.6 days)", "Atomic Kitten", "Trey Parker and Matt Stone", "Matt Gonzalez", "Helensvale", "1979", "\u00c6lfgifu of York", "PlayStation 4", "Malta", "1966", "Key West", "Europe", "Black Mountain College", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "a defender", "Comedy Central", "Prince George's County", "Pittsburgh, Pennsylvania.", "1891", "A\u00e9reas", "Gainsborough Trinity", "Los Angeles", "October 13, 1980", "water sprite", "India", "Syracuse", "FIFA Women's World Cup", "Orange County", "76,416", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Quantitative psychological research", "Brian Steele", "Deep Blue", "albert reynolds", "George W. Bush", "Sen. Thomas Carper,", "Current TV", "two", "one bath", "The Lost Boys", "mickquatash"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5996617965367965}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-257", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-2932", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-10491"], "SR": 0.53125, "CSR": 0.5203993055555556, "EFR": 0.9333333333333333, "Overall": 0.6717621527777778}, {"timecode": 36, "before_eval_results": {"predictions": ["between June and September", "whether to close some entrances, bring in additional officers, and make security more visible.", "topless behavior", "a bank", "July for A Country Christmas", "Casalesi Camorra clan", "Tulsa, Oklahoma.", "41,280 pounds", "Old Trafford", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's war zone.", "Number Ones", "Zac Efron", "the Indian embassy in Kabul", "\"Manson murders.\"", "11 kilometers from the finish", "Annie Duke", "brutal choice to step up attacks against innocent civilians.\"", "\"Omar Jadwat, is concerned that the legislation will foster racial profiling, arguing that most police officers don't have enough training to look past race while investigating a person's legal status.\"", "for producing rock music with a country influence", "The Kirchners", "The move frees up a place for another non-European Union player in Frank Rijkaard's squad.", "to root out terrorists within its borders.", "violent separatist campaign", "at the ancient Greek site of Olympia", "3,000", "closing these racial gaps", "Behar", "22", "3-0", "150", "for the two remaining crew members from the helicopter,", "U.N. agencies", "more than 30 Latin American and Caribbean nations", "The man who was killed had been part of a hunting party of three men,", "one", "23 million square meters (248 million square feet)", "Now Zad in Helmand province, Afghanistan.", "Virgin America", "jobs up and down the auto supply chain: from dealers to assembly workers and parts markers", "American Civil Liberties Union against the government.", "summer", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "3rd District of Utah.", "mental health and recovery", "38 percent of the seats in the 601-member constituent assembly", "\"Gruesome photos from the scene show the man facing up, with his arms out to the side.", "Judge Sonia Sotomayor", "men in white hoods", "90", "Cash for Clunkers program", "Argentina", "in 1997", "103", "Carolyn Sue Jones", "vanilla", "the Nemean Lion", "six climate regions", "Gian Carlo Menotti", "National Basketball Association (NBA)", "Semites", "his mother", "the Stravinsky character first played by Tamara Karsavina", "Zugtelephonie A. G.", "Apollo"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5508946985319667}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2666666666666667, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.375, 0.923076923076923, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.5454545454545454, 0.2666666666666667, 0.0, 0.15384615384615385, 1.0, 1.0, 0.09523809523809522, 0.8, 1.0, 0.9565217391304348, 0.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-978", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-4038", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-2634", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4389", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-2623"], "SR": 0.390625, "CSR": 0.5168918918918919, "EFR": 1.0, "Overall": 0.6843940033783784}, {"timecode": 37, "before_eval_results": {"predictions": ["over 760 mm (2 ft 6 in)", "the state's attorney", "Abdullah Gul,", "Ed McMahon,", "success of genocide and mass atrocities virtually anywhere they occur.", "off Somalia's coast.", "clogs", "an upper respiratory infection", "two", "tells stories of different women coping with breast cancer in five vignettes.", "Bobby Jindal", "husband Bill Klein,", "Facebook and Google,", "Damon Bankston", "Venus Williams", "Won Sei Hoon, who heads South Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim", "Robert Mugabe", "a female soldier,", "slayings of 12 off-duty federal agents in southwestern Mexico,", "J. Crew,", "more than $17,000", "her", "Sheikh Abu al-Nour al-Maqdessi,", "\"momentous discovery\"", "downtown Nairobi.", "Robert Barnett", "Asian qualifying Group 2", "Matthew Fisher", "Zimbabwean", "Ben Roethlisberger", "Isao, Masao and Don", "Pew Research Center", "Sgt. Jason Bendett", "Brazil", "hercules", "$24.1 million", "a jury", "Salt Lake City, Utah,", "Sunday.", "Robert Mugabe", "13.", "One of Osama bin Laden's sons", "for security reasons and not because of their faith.", "\"We tortured (Mohammed al.) Qahtani,\"", "autonomy.", "the Arctic north of Murmansk down to the southern climes of Sochi", "Long Island convenience store", "her mom,", "several months", "Kerstin Fritzl,", "Washington State's decommissioned Hanford nuclear site,", "the breast or lower chest of beef or veal", "winter", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "hercules", "cambiado", "Viennese", "1887", "Atlantic Coast Conference", "uncle", "Pennsylvania", "Rabbit", "hercules", "Labrador"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5859455240429505}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.75, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8235294117647058, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2949", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-2297", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-740", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-2905", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-9856", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-5703", "mrqa_searchqa-validation-12609"], "SR": 0.484375, "CSR": 0.5160361842105263, "EFR": 1.0, "Overall": 0.6842228618421053}, {"timecode": 38, "before_eval_results": {"predictions": ["vector quantities", "Transport Workers Union leaders", "March 24,", "Eleven", "Mexican military", "Pakistani officials,", "$7.8 million", "Stratfor's", "Madeleine K. Albright", "Roland Martin", "Polo", "German Foreign Ministry,", "10,000 refugees,", "the IV cafe.", "Red Lines", "in body bags", "40", "georgia peach", "Islamic militants", "a construction site in the heart of Los Angeles.", "last week,", "Sunni Arab and Shiite tribal leaders", "credit card information", "an antihistamine and an epinephrine auto-injector", "North Korea", "Hong Kong", "from the families of three missing military men,", "the release of the four men", "Polo", "Ahmed,", "in pictures.", "from the Southern African Development Community,", "misconduct involving a dormitory parent.", "in an artificial coma", "7-1", "Africa", "fear of losing their licenses to fly.", "Human Rights Watch", "FBI.", "first grand Slam,", "\"it should stay that way.\"", "CNN", "McCain people.", "the strength of its brand name and the diversity of its product portfolio,", "U.S. State Department and British Foreign Office", "Monday's suicide blast outside the district courthouse in Peshawar.", "Fiona MacKeown", "sculptures", "Pakistan's High Commission in India", "Bryant Purvis,", "pain-relief drugs.", "In 1871 A.D.", "Frederick Chiluba, Levy Mwanawasa, Rupiah Banda, Michael Sata,", "psychology", "animals", "Treaty of Utrecht", "Massachusetts", "James Harrison", "Ford Island", "Tomorrowland", "Tiger Woods", "greed", "three", "Sesame Street"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5848807442557442}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.2857142857142857, 0.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 0.30769230769230765, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-1060", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-6409", "mrqa_searchqa-validation-13907"], "SR": 0.484375, "CSR": 0.515224358974359, "EFR": 1.0, "Overall": 0.6840604967948718}, {"timecode": 39, "before_eval_results": {"predictions": ["Andrew Lortie", "side effects of chemotherapy", "cones", "high sewing", "Silver Hatch", "peripheral nerves", "Ethiopia", "red Admiral", "two young people", "4G ready smartphones", "cosmonaut", "special administrative zones", "Alastair Cook", "Enterprise", "Three interior die-cut holes", "Asia", "glenoid", "Frank Sinatra", "meninges", "Official Languages Act", "Guildford Dudley", "Munich,", "Henry Mancini", "Fred Astaire", "a swamp", "grace", "Sudan.", "Flemish", "one-off drama", "Proverbs", "TV personality", "Jamaica", "Tornado,", "drama", "Indonesia,", "pancreas", "puff", "football", "Antoine Lavoisier", "a Marxist revolutionary", "Pangaea", "societies or amalgamations of persons for the purpose of", "Pet Shop Boys", "Matthew Boulton", "Algiers", "Marks & Co,", "Anabaptists", "Agnes", "Hebrew", "John Virgo.", "herpes virus,", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods )", "Garfield Sobers", "In the mountains outside City 17", "aliens from the fictional planet Rigel VII", "Johannes Vermeer", "O.T. Genasis", "Climatecare,", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Schalke", "Lost in America", "Autumn", "Soviet Union", "Republicans"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5104301948051948}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-970", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-86", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3132"], "SR": 0.484375, "CSR": 0.514453125, "EFR": 1.0, "Overall": 0.68390625}, {"timecode": 40, "UKR": 0.669921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1389", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10606", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-1264", "mrqa_searchqa-validation-12829", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16453", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10263", "mrqa_squad-validation-10317", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10369", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-127", "mrqa_squad-validation-1408", "mrqa_squad-validation-1430", "mrqa_squad-validation-1453", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2094", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3124", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3444", "mrqa_squad-validation-3497", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3946", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-4715", "mrqa_squad-validation-491", "mrqa_squad-validation-4918", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5664", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6706", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7147", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7296", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7751", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8154", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-8841", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-568", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-899"], "OKR": 0.814453125, "KG": 0.44453125, "before_eval_results": {"predictions": ["\"chameleon circuit\"", "La Motta", "Denmark", "Joshua", "quick brown fox", "Live and Let Die", "Bette Davis", "salford", "in Brazil", "Robert Hooke,", "Hadrian", "Napier", "Sony Interactive Entertainment", "Henry I of England", "green", "1215", "delphiniums", "Robinson Crusoe", "Ebenezer Scrooge", "belgian", "Egypt", "neutron star", "otalgia", "New York Yankees", "Four Tops", "hudd", "July 20,", "9", "bali", "lilac", "Hilary Swank", "scarlet", "dove", "a mole", "McCarthy", "Broadway", "three", "George I", "Who's the cat", "between the lower end of tibia and fibula of the lower leg", "Daily Mirror", "zon trotsky", "horse", "India", "southerly", "Machu Picchu", "london philharmonic Orchestra", "Madness", "Jane Eyre", "Kansas", "australia", "A marriage officiant", "Ricky Nelson", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "Papua New Guinea", "A bass", "Security Management", "eight", "Russia", "three years ago was like a class to help women \" learn how to dance and feel sexy,\"", "Malacca", "733", "Gina", "Visible Hand"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5191462862318841}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.7499999999999999, 0.0, 0.4, 0.0, 1.0, 0.6086956521739131, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4551", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-1746", "mrqa_triviaqa-validation-3618", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-3771", "mrqa_triviaqa-validation-6915", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-6885", "mrqa_triviaqa-validation-774", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-9938", "mrqa_searchqa-validation-5984", "mrqa_naturalquestions-validation-6903"], "SR": 0.40625, "CSR": 0.5118140243902439, "EFR": 0.9473684210526315, "Overall": 0.6776177390885751}, {"timecode": 41, "before_eval_results": {"predictions": ["Edward Teller", "food, music, culture and language of Latin America", "Los Angeles", "they would not be making any further comments,", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Tim Clark, Matt Kuchar and Bubba Watson", "philip Markoff,", "Haeftling", "forgery", "at Sea World in San Antonio,", "Mafia", "Nat King", "the 1800s", "convicts caught with phones", "16", "cancer for years", "$40 and a loaf of bread.", "fast food and ancient art", "she's in love,", "France", "President Obama", "eight", "South Africa's", "Madeleine K. Albright", "air support.", "back at work", "\"What I would want to do when I got out of the game.\"", "The cause of the blast was unknown,", "at least seven", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Ronald Cummings", "Elisabeth's father,", "his club", "they", "$60 billion", "$50 less", "J.G. Ballard,", "Patty Murray", "\"in the worst possible manner by taking two hours of footage and condensing it down to a minutes-long segment.\"", "the Airbus A330-200", "United States, NATO member states, Russia and India", "fatally shooting a limo driver on February 14, 2002", "ties", "dogs who walk on ice in Alaska.", "are accused along with a third teen, Jesus Mendez,", "China", "Steve Jobs", "\"Rin Tin: The Life and the Legend\"", "Sri Lanka", "Brett Hampton", "the state's attorney", "John Brown", "126", "Brevet Colonel Robert E. Lee", "Telegraph Media Group Limited 2017", "prime minister yitzhak rabin", "la Marseilla", "Kristy Lee Cook", "John Samuel Waters Jr.", "Norman Mark Reedus", "charles dickens", "Douglas MacArthur", "rice", "no license or advanced training beyond just firearm familiarization ( for rentals )"], "metric_results": {"EM": 0.375, "QA-F1": 0.5102884962259961}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.04761904761904762, 1.0, 0.4444444444444445, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.25, 0.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.5454545454545454, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.25, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_squad-validation-7880", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2366", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-3039", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-3949", "mrqa_naturalquestions-validation-2418", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-2819", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2138", "mrqa_searchqa-validation-1621", "mrqa_searchqa-validation-149", "mrqa_naturalquestions-validation-8617"], "SR": 0.375, "CSR": 0.5085565476190477, "EFR": 1.0, "Overall": 0.6874925595238095}, {"timecode": 42, "before_eval_results": {"predictions": ["ITT", "all 246", "beloved and admired and dreamed about for nearly a hundred years.", "The cause of the child's death will be listed as homicide by undetermined means,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Dean Martin, Katharine Hepburn", "Iran's parliament speaker", "20", "\"Dance Your Ass Off.\"", "\"doodles\"", "15-year-old's", "Umar Farouk AbdulMutallab", "London", "Democratic National Convention", "his native Philippines", "Kitty Kelley, biographer of the rich and famous,", "nepal", "\"inappropriate,\" according to WFTV.", "\"It was terrible, it was gut-wrenching just to hear them", "Green Apple Barter Services in Pittsburgh, Pennsylvania.", "more than 15,000", "about 12 million", "the Gulf", "May 4", "3-2", "Zimbabwean President Robert Mugabe", "three", "\"murder dozens of people with a focus on murdering African-Americans\"", "10", "less than 500", "50 percent-plus", "Ignazio La Russa", "Egyptian striker Amir Zaki", "$40 and a loaf of bread.", "a 2,700-acre sanctuary in rural Tennessee.", "Tulsa, Oklahoma.", "2-1", "nearly $2 billion", "Russian concerns that the defensive shield could be used for offensive aims.", "1981", "\"They are, of course, shattered.", "London's", "her love for the \"Golden City,\" what she learnt from growing up in a communist country", "more than 100", "Microsoft.", "40 former U.S. Marines or sons of Marines", "Mitt Romney", "Islamic", "prisoners at the South Dakota State Penitentiary", "part of the proceeds from sales", "\"We've got more work to do to ensure that government treats all its citizens equally, to fight injustice and intolerance in all its forms", "season five", "886 AD", "1940", "Afghanistan", "Martin Howe", "maray firth", "\"Shake It Off\"", "Cheshire", "Brookhaven", "Transamerica", "shows the culture of the United States at a specific time", "pirate captain", "16"], "metric_results": {"EM": 0.34375, "QA-F1": 0.46253955510069644}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 0.26666666666666666, 0.7272727272727273, 0.0, 0.0, 0.4, 0.0, 1.0, 0.5, 1.0, 0.5, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.43478260869565216, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.7499999999999999, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-643", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1429", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-143", "mrqa_triviaqa-validation-1380", "mrqa_triviaqa-validation-6580", "mrqa_hotpotqa-validation-1900", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-709", "mrqa_naturalquestions-validation-1640"], "SR": 0.34375, "CSR": 0.5047238372093024, "EFR": 1.0, "Overall": 0.6867260174418603}, {"timecode": 43, "before_eval_results": {"predictions": ["400", "14", "3-0", "how health care can affect families.", "a \"procedure on her heart,\"", "the Oaxacan countryside of southern Mexico", "the punishment for the player", "wings, included in the sale,", "Vernon Forrest,", "passengers", "U.S. State Department and British Foreign Office", "five female pastors", "Phoenix, Arizona,", "\"We tortured (Mohammed al.) Qahtani,\"", "The elephant Sanctuary", "Six", "the International Space Station at 9:20 p.m. ET", "Washington,", "Russia", "doctors", "Hamas", "a senior at Stetson University", "his health and about a comeback.", "1,500", "through the weekend,", "three", "\"novel\"", "Aniston, Demi Moore and Alicia Keys", "January", "his land", "Miguel Cotto", "\"It wasn't kissing and hugging --", "A family friend of a U.S. soldier", "in a stream in shark River Park in Monmouth County", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "all buses, subways and trolleys that carry almost a million people daily.", "five", "Long troop deployments", "St. Louis, Missouri.", "is not your car.", "\"After the presser, we got a number of calls, and those calls were intriguing, and we're chasing those down now,\"", "Clifford Harris,", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Republican Party,", "almost 9 million", "Asashoryu", "an upper respiratory infection,", "Adriano", "\"Zed,\"", "prime minister's handling of the L'Aquila earthquake,", "human rights abuses, including the massacre of innocent civilians,", "1973", "Roanoke", "Scarlett Johansson", "the skull", "Red Sea", "Atlantic Ocean", "\"Passions\"", "Robert Allen Iger", "Salgaocar", "Novel", "Abercrombie & Fitch", "soap opera", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6390984563681932}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 0.0, 0.7272727272727272, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.5714285714285715, 1.0, 0.14285714285714288, 0.5333333333333333, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.21052631578947367]}}, "before_error_ids": ["mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-2084", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-375", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-802", "mrqa_searchqa-validation-6252", "mrqa_naturalquestions-validation-4915"], "SR": 0.53125, "CSR": 0.5053267045454546, "EFR": 1.0, "Overall": 0.6868465909090908}, {"timecode": 44, "before_eval_results": {"predictions": ["help transfer and dissipate excess energy", "on Chesapeake Bay, south of Annapolis in Maryland", "the Isthmus of Corinth", "Alex planning to propose to her during a party", "Sindbad the Sailor", "2001", "August 2015", "Betty", "Leonard Nimoy", "Rodney Crowell", "Jason Momoa", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "a donor molecule to an acceptor molecule", "Jamie Foxx", "Iowa ( 36.6 % )", "December 31, 1971", "The uvea", "the Director of National Intelligence", "biological taxonomy", "Zeebo", "1977", "Department of Health and Human Services", "the Centennial Exposition in Philadelphia in 1876, and in Madison Square Park in Manhattan", "the development of electronic computers in the 1950s", "a contemporary drama in a rural setting", "1938", "Kristy Swanson", "Jyotirindra Basu", "roughly five hundred", "1998", "the Naturalization Act of 1790", "The Jamestown settlement in the Colony of Virginia", "Arkansas", "December 24, 1836", "at slightly different times when viewed from different points on Earth", "during the American Civil War", "the Executive Residence of the White House Complex", "420 mg", "Timothy B. Schmit", "March 2, 2016", "Thirty years after the Galactic Civil War", "Woody Paige", "Christy Plunkett ( Anna Faris )", "$75,000", "four", "Blood is the New Black ''", "between the stomach and the large intestine", "USS Chesapeake", "18 - season", "to offer the hope that a happy day being marked would recur many more times", "President Lyndon Johnson", "Sarah Palin's", "Pacino", "pomegranate", "Kinnairdy Castle", "Teenage Mutant Ninja Turtles", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona.", "his business dealings for possible securities violations", "40", "16 times.", "(John) Deere", "snowboarding", "dollop", "algiers"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5903227529049897}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.9333333333333333, 0.0, 0.2, 0.0, 1.0, 0.6666666666666666, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.2857142857142857, 1.0, 1.0, 0.060606060606060615, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.2666666666666667, 0.6, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9445", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-824", "mrqa_triviaqa-validation-3099", "mrqa_triviaqa-validation-5874", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-522", "mrqa_searchqa-validation-2656"], "SR": 0.484375, "CSR": 0.5048611111111111, "EFR": 0.9696969696969697, "Overall": 0.6806928661616161}, {"timecode": 45, "before_eval_results": {"predictions": ["to run the length of their bodies and bear comb-like bands of cilia, called \"ctenes,\"", "The genetic basis", "Cheryl Campbell", "The Satavahanas", "Michael Moriarty", "Canada", "111", "Virginia", "modern random - access memory ( RAM )", "1940", "Doug Diemoz", "Charlene Holt", "Kari Wahlgren", "Thomas Edison", "Hedwig", "O'Meara", "Atlanta Hawks", "Missi Hale", "2001", "Semmi", "Asa Taccone", "John Findley Wallace", "1940", "parthenogenic", "Lyle Waggoner", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "the town of Coconut Cove", "the third season", "the biblical name of a Canaanite god associated with child sacrifice", "a mid-size four - wheel drive luxury vehicle", "1940", "Bill Pullman", "minor", "because they could help him find Purgatory, the afterlife of monsters,", "2010", "786 -- 802", "eliminate or reduce the trade barriers", "Franklin", "Justin Timberlake", "generally lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Remus Lupin", "Ella Mitchell", "Allison", "to symbolize connection between Vesta's fire and the sun as sources of life", "Jurchen Aisin Gioro clan", "Abu Talib", "Americans", "1980", "Professor Kantorek", "Yondu Udonta", "the next episode, `` Seeing Red ''", "blue dye", "bushfire", "Alexei Sayle", "Figaro", "Big 12 Conference", "Debbie Reynolds", "Gulf of Aden", "the legitimacy of that race.", "in Salt Lake City,", "georgicus", "OPEC", "bird", "California-based"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5422025908616779}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false], "QA-F1": [0.21052631578947364, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 0.5, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.11764705882352941, 0.0, 1.0, 0.5555555555555556, 0.0, 1.0, 0.967741935483871, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4437", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-3856", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-4640", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3339", "mrqa_newsqa-validation-637", "mrqa_searchqa-validation-10249", "mrqa_newsqa-validation-2590"], "SR": 0.4375, "CSR": 0.5033967391304348, "EFR": 0.9722222222222222, "Overall": 0.6809050422705314}, {"timecode": 46, "before_eval_results": {"predictions": ["the main porch", "horticulture", "The Nitty Gritty Dirt Band", "the ACU", "Luther Ingram", "Johnny, a teenage apes who wants to sing, though his father would rather have him follow his criminal footsteps", "Lucius Verus", "Ishaan Anirudh Sinha", "Ray Harroun", "manga", "Frank Morris", "to establish an electrochemical gradient ( often a proton gradient ) across a membrane, resulting in an electrical potential or ion concentration difference across the membrane", "the Heliocentric model of Copernicus", "a single, implicitly structured data item in a table", "the President pro tempore", "the fuel tank ( fount )", "electron donors", "T.J. Miller", "Ren\u00e9 Descartes", "2006 -- 06", "the dealer sets the cards face - down on the table near the player designated to make the cut, typically the player to the dealer's right", "1955", "the town of Acolman, just north of Mexico City", "23 September 1889", "indigenous to many forested parts of the world", "January 2018", "Colon Street", "The higher the vapor pressure of a liquid at a given temperature", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T )", "1923", "Hugh S. Johnson", "their axons", "the Thane of Lochaber", "Schadenfreude", "lithium", "al - khimar", "291", "Anthony Hopkins", "the middle of the 15th century", "Joan Alison", "a political ideology is a certain ethical set of ideals, principles, doctrines, myths, or symbols of a social movement, institution, class, or large group that explains how society should work", "c. 1000 AD", "Identification of alternative plans / policies", "Missouri River", "2,140 kilometres ( 1,330 mi )", "Anakin Skywalker", "private sector", "the chief lawyer of the United States government", "2018", "C\u03bc and C\u03b4", "August 29, 2017", "Beaujolais Nouveau", "Edward Woodward", "Black Swan", "Lake Wallace", "Paul W. S. Anderson.", "1828\u20131866", "43 percent", "the world's tallest building,", "system of military trials", "V", "the Rig Veda", "Gertrude Stein", "Sedbergh in Dentdale"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5888459206216783}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.4166666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.13793103448275862, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3870967741935484, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4929", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-4197", "mrqa_hotpotqa-validation-1605", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-744", "mrqa_searchqa-validation-1063", "mrqa_triviaqa-validation-5511"], "SR": 0.53125, "CSR": 0.5039893617021276, "EFR": 0.9333333333333333, "Overall": 0.6732457890070922}, {"timecode": 47, "before_eval_results": {"predictions": ["Germany", "Tim Russert", "Meredith Brody ( Zoe McLellan )", "Celtic", "1978", "two", "September 6, 2019", "Earle Hyman", "September 19, 2017", "the Anglo - Norman French waleis", "31 October 1972", "23 September 1889", "the Senate", "Pittsburgh", "the frontal lobe", "the 1940s", "increased productivity, trade, and secular economic trends", "2 %", "approximately 5 liters, with females generally having less blood volume than males", "G -- Games", "the 17th episode in the third season of the television series How I Met Your Mother and 61st overall", "the balance sheet", "the New York Yankees", "94 by 50", "Sam Waterston", "the One Ring", "knowledge, piety, and fear of the Lord", "a type of Stollen", "bohrium", "Tilak Raj", "the main road through the gated community of Pebble Beach", "a place of trade, entertainment, and education", "electrons", "September 2014", "Alice's Adventures in Alice", "Janis Joplin", "Australia", "T'Pau", "Africa", "Blue laws in the United States vary by state", "in South America", "Edgar Lungu", "Donna Reed", "A status line", "treats it off with a special marker called a `` dabber '' or `` dauber ''", "Will", "the centre of Munich", "Gary Grimes", "1998", "Thomas Chisholm", "Tommy James", "nausea", "credit issues with government debt", "\"The best is yet to come.\"", "Chattahoochee", "Patterns of Sexual Behavior", "\"Futurama\"", "Argentina", "ice jam", "Facebook and Google,", "caffeine", "One Flew Over the Cuckoo's Nest", "Stephen Hawking", "treats about the state and its citizens."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5525111607142857}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false], "QA-F1": [0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.47619047619047616, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.125, 1.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-485", "mrqa_triviaqa-validation-4834", "mrqa_triviaqa-validation-2385", "mrqa_hotpotqa-validation-114", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14104", "mrqa_newsqa-validation-2608"], "SR": 0.453125, "CSR": 0.5029296875, "EFR": 1.0, "Overall": 0.6863671874999999}, {"timecode": 48, "before_eval_results": {"predictions": ["the Democratic VP candidate", "Stuttgart", "Three", "Long troop deployments", "some of the most gigantic pumpkins in the world,", "not a project for commercial gain.", "a key witness -- Dennis Davern,", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Eleven", "a man who said he had found it in the desert five months before.", "The prosecutor sought 18 years imprisonment, so if you look at it that way, it is a positive thing that they got the lowest possible sentence,\"", "Microsoft.", "Ferraris, a Lamborghini and an Acura NSX", "20 times during the 1992 campaign.", "a cargo ship,", "frees up a place", "Michael Krane,", "Russian bombers", "Three aid workers", "9 a.m.-1 p.m. Friday", "2-1", "more than 200.", "a powerful brand to boost sales, especially if that brand is a dynamic first family.", "a one-shot victory in the Bob Hope Classic", "a relatively neglected topic and needs to come out of the shadows.", "a cancer-causing toxic chemical.", "\"It was perfect work, ready to go for the stimulus package,\"", "155 people aboard in the frigid river waters", "to the southern city of Naples", "racial intolerance.", "Friday,", "\"Twilight\"", "Robert Kimmitt.", "26,", "both have skeletal dysplasia, a bone-growth disorder that causes dwarfism,", "around Ciudad Juarez, across the border from El Paso, Texas.", "10", "Retailers who don't speak out against it \"should be thrown out of their Mari jobs.\"", "Anil Kapoor", "Samoa", "authorizing killings and kidnappings by paramilitary death squads.", "E. coli", "Ma Khin Khin Leh,", "a traditional form of lounge music that flourished in 1940's Japan.", "it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "1998.", "to give up opium production.", "London's O2 arena,", "The EU naval force", "Cyprus next week,", "a national policy on the subject that's designed to protect ocean ecology, address climate change and promote sustainable ocean economies.", "between $10,000 and $30,000", "1602", "Number 4, Privet Drive, Little Whinging in Surrey, England", "fructose", "5 $10 $20 $50 $100)", "a nylon base material coated with a flexible, 100% waterproof, PVC", "five times", "E Street Band", "England", "chippewa", "salt-free", "a British new wave musical group", "a specialist US financial institution that provides settlement services to its members in the foreign exchange market (FX )"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5364805703052986}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.47619047619047616, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.125, 0.8, 0.15384615384615383, 1.0, 0.09523809523809525, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 0.6666666666666666, 0.08695652173913043, 1.0, 0.12500000000000003, 1.0, 1.0, 0.5, 0.0, 0.32, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-4169", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-3970", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-2918", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12129", "mrqa_naturalquestions-validation-3236"], "SR": 0.4375, "CSR": 0.5015943877551021, "EFR": 0.9722222222222222, "Overall": 0.6805445719954648}, {"timecode": 49, "before_eval_results": {"predictions": ["Pierre Laval", "Harriet Harman", "Samuel Johnson", "Michaela Tabb", "Orion", "Edward VIII", "Helen Shapiro", "falcon", "Stephen Fry", "Libya", "Darshaan,", "Robinson.", "the ascetics", "The Daily Mail", "William Shakespeare", "HP.56", "death and how human beings respond to the inevitability of their mortality and the reality of loss", "Fred Stolle", "Texas", "(Classical Music) music (to be performed) in a fiery manner", "the Strait of Messina", "\"ahoy\"", "a wide range of human rights issues,", "Brian Deane", "Volkswagen Golf,", "Emilia Fox", "October", "PETER FRAMPTON LYRICS", "Catherine Zeta-Jones", "South Africa", "Jim Braddock", "Mediterranean", "1840", "the kidney-shaped electric organs", "children of Israel", "Hawaii - The Aloha State", "vomiting", "Richard Strauss", "sperm", "Syria", "a black or brown-speckled seabird", "golf", "purpurea", "Amnesty International", "Oliver Harmon Jones", "her skills,", "the Kingdom of Lesotho", "Enigma\u2019 Variations", "Mauricio Pochettino", "The Duke and Duchess of York", "myxoma", "the season - five premiere episode `` Second Opinion ''", "The U.S. state of Georgia", "Parker's pregnancy at the time of filming", "D. H. Lawrence", "Battle of Chester", "Denzel Washington and John Travolta", "Bill Stanton", "Los Angeles Angels", "56,", "23", "the 20th", "a semi-major axis", "Patty Duke"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6394097222222223}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666667, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-5470", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-3752", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-4066", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-2853", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7507", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-2003", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-4996"], "SR": 0.546875, "CSR": 0.5025, "EFR": 0.9655172413793104, "Overall": 0.679384698275862}, {"timecode": 50, "UKR": 0.630859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4431", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-962", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2365", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-893", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5943", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-899"], "OKR": 0.794921875, "KG": 0.4125, "before_eval_results": {"predictions": ["A Christmas Carol", "Robert De Niro", "Bangladesh", "Dan Dare", "50th century", "Denmark", "Berlin", "The Rocky Horror Picture Show", "1925", "Princess Stephanie", "bill", "Pakistan", "spider", "Popeye", "james boswell", "Bull Moose Party", "Genoa", "sarah ferguson", "james boswell", "Jamaica", "Jessica Simpson", "james boswell", "earthquake", "Tarentum (Taranto)", "Charlie Chan", "chiba", "Colette", "louis lenislas-Xavier", "Catherine Parr", "basketball", "Thailand", "slumdog Millionaire", "james boswell", "Norman Brookes", "London", "sarah armstrong", "pierce", "Danilo", "curved mirrors", "sigmund haffner", "Anne-Marie Duff", "james boswell", "salt", "19-9", "phobias", "pears soap", "electric guitar", "Toby", "Buenos Aires", "Kenny Everett", "fenn Street School", "1804", "spraying the whole atmosphere as if drawing letters in the air", "November 3, 2007", "jockey William Shand Kydd", "Marktown", "Coinapult", "well over 1,000 pounds).", "japan Breeze,", "from TV news coverage,", "handy", "Lamb of God", "legs", "1978"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5052083333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-2943", "mrqa_triviaqa-validation-5137", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-5652", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-5624", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5281", "mrqa_newsqa-validation-2506", "mrqa_searchqa-validation-9447", "mrqa_searchqa-validation-3244", "mrqa_searchqa-validation-10670"], "SR": 0.46875, "CSR": 0.5018382352941176, "EFR": 1.0, "Overall": 0.6680238970588236}, {"timecode": 51, "before_eval_results": {"predictions": ["Carthage", "blue", "Robin Ellis", "mortadella", "album", "hydrogen gas", "the foot & foot", "priests or the priesthood", "bacall", "South Pacific", "bacall", "Bosnia and Herzegovina", "France", "Sparta", "seekers", "squash", "argentina", "Turkey", "bacall", "China", "diffusion", "David Bowie", "Robben Island", "bukwus", "frankincense", "medium", "albert", "bacall", "zsa zsa Gabor", "bacall", "bacall", "Egypt", "sparrow", "Eton College", "margot betti", "Siamese", "argentina", "barrister", "Sanskrit", "Valentine Dyall", "bacall", "Portugal", "Opus Dei", "the Flying Pickets", "respiration", "Kenya", "Parliament Square", "bakers van", "bercow", "bacall", "blood", "ummat al - Islamiyah", "Rachel Kelly Tucker", "Honor\u00e9 Mirabeau", "Big Bad Wolf", "Brian Kirk", "Christopher Whitelaw Pine", "President Obama", "Tottenham", "Ewan McGregor", "Curly Lambeau", "the Curly Wurly", "bacall", "bug"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4921875}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-4257", "mrqa_triviaqa-validation-2964", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-5154", "mrqa_triviaqa-validation-114", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-6925", "mrqa_naturalquestions-validation-1455", "mrqa_hotpotqa-validation-3177", "mrqa_newsqa-validation-318", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-2456", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-11960"], "SR": 0.453125, "CSR": 0.5009014423076923, "EFR": 1.0, "Overall": 0.6678365384615386}, {"timecode": 52, "before_eval_results": {"predictions": ["Ringo Starr", "Apprendi v. New Jersey", "Minnesota's 8th congressional district", "Erreway", "Queensland", "George Clooney", "Pamelyn Wanda Ferdin", "Chancellor of Austria", "\"The Fog\"", "$10.5 million", "2017", "Dutch", "2014", "rapper", "Missouri", "Rochdale, North West England", "\"50 best cities to live in.\"", "Virginia", "The Godfather Part II", "two", "Rigoletto", "Scunthorpe", "Talib Kweli", "motor vehicles", "a retired ski racer", "March, 1904", "Eugene O'Neill", "Colonel Gaddafi", "Irish rock band U2", "wooden roller ride", "Sofia the First", "Sufism", "pharmaceutical companies", "Roy Spencer", "Magnate", "The Saturdays", "a series of battles", "North Carolina", "John Travolta", "ice hockey", "Hong Kong", "2006", "Pacific Place", "jazz", "sarod", "2009", "Northern Ireland", "1999", "Russian Ark", "Delacorte Press", "the voice of The Beast", "1624", "April 14, 2017", "the First Order", "an ancient optical illusion toy", "nickel", "vickers-Armstrong", "food, music, culture and language of Latin America", "gopi Podila", "school", "patrick", "Maine's", "Henry Clay", "Richard Crispin Armitage"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6339743589743589}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1351", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-1044", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4939", "mrqa_hotpotqa-validation-2802", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3422", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-468", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-11977"], "SR": 0.578125, "CSR": 0.5023584905660378, "EFR": 1.0, "Overall": 0.6681279481132075}, {"timecode": 53, "before_eval_results": {"predictions": ["Mike Mills", "1998", "Israel", "Kittie", "American", "People!", "South Australian town", "John C. Bogle", "American", "Ready to Die", "stunt performer", "Danish", "York County", "Kye Bumzu", "Wake Island", "Australian Defence Force", "June 11, 1973", "Arthur William Bell III", "Boston", "Erreway", "Tampa Bay Lightning", "CBS", "Boston, Massachusetts", "Elisabeth Marie Stefanik", "Jennifer Taylor", "Correcaminos UAT", "9Lives brand cat food", "Black Ravens", "September 10, 1993", "Las Vegas Strip in Paradise, Nevada", "42,972", "over 9,000 employees", "Michael Seater", "Drunken Master II", "more than 100 countries", "bassline", "E22", "Allies of World War I, or Entente Powers", "Geraldine Page", "Kristina Ceyton and Kristian Moliere", "TASCHEN", "near Philip Billard Municipal Airport", "1964 to 1974", "Big Fucking German", "law firm", "Hamlet", "Bow River and the Elbow River", "Gillian Anderson", "segues", "a united Ireland", "\"Queen In-hyun's Man\"", "American", "Virgil Ogletree", "University of Michigan", "topiary", "Uma Thurman", "1961", "Luca di Montezemolo", "near the Somali coast", "was a 24-year-old electronics student.", "deckhand", "American Car Collector", "Marky Mark", "cheese"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5786791333666333}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.4, 1.0, 1.0, 0.9090909090909091, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.7692307692307693, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.8, 0.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-3615", "mrqa_hotpotqa-validation-2025", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1033", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-6121", "mrqa_triviaqa-validation-115", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1641", "mrqa_searchqa-validation-5501", "mrqa_searchqa-validation-3970", "mrqa_searchqa-validation-16209"], "SR": 0.453125, "CSR": 0.5014467592592593, "EFR": 1.0, "Overall": 0.6679456018518519}, {"timecode": 54, "before_eval_results": {"predictions": ["her brother", "audio CDs", "call premium", "The original timeline is eventually restored", "Waylon Jennings", "August 2, 1990", "Charlene Holt", "eight episode series", "the provincial governments", "wolves", "an official document", "18", "Jewel Akens", "Connecticut", "Irsay", "Abid Ali Neemuchwala", "winter", "Roxette", "the lunch box", "Peggy Lipton", "The Union", "the chest", "Authority", "drizzle", "1967", "Virginia", "due to Parker's pregnancy at the time of filming", "lakes or reservoirs", "merengue and bachata music", "New Jersey", "the 1960s", "IBM", "American singer Elvis Presley", "1958", "1998", "Karen Gillan", "headquarter Khliehriat", "rearview mirror", "April 29, 2009", "democracy", "2026", "William Chatterton Dix", "ethan president", "Selena Gomez", "spacewar", "1881", "an armed conflict without the consent of the U.S. Congress", "Timothy B. Schmit", "Games played", "Cetshwayo", "Games", "Cambridge", "downtown Oklahoma City", "The choroid", "1982", "2015", "Indian epic historical drama", "22", "one", "economic opportunities", "Mr. Hand", "Ukrainian Soviet Socialist Republic", "Napoleon", "\"Traumnovelle\" (\"Dream Story\")"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6351686951954696}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.823529411764706, 0.24000000000000002, 0.4, 0.25, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.09523809523809523, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6086956521739131, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-5785", "mrqa_naturalquestions-validation-10331", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3483", "mrqa_searchqa-validation-354", "mrqa_searchqa-validation-7780"], "SR": 0.546875, "CSR": 0.5022727272727272, "EFR": 0.8620689655172413, "Overall": 0.6405245885579938}, {"timecode": 55, "before_eval_results": {"predictions": ["the 1960s", "Charlton Heston", "to acquire an advantage without deviating from basic strategy", "Doreen Mantle", "Felicity Huffman", "March 18, 2005", "the titular `` fool '', a solitary figure who is not understood by others, but is actually wise", "the forward reaction proceeds at the same rate as the reverse reaction", "28 July 1914", "Terry Kath", "1922", "2017 season", "Sylvester Stallone", "2008", "Ethiopia and Liberia", "Scottish law", "Abid Ali Neemuchwala", "Hodel", "the concept of a fully centralized service with individual user accounts focused on one - on - one conversations set the blueprint for later instant messaging services like AIM, and its influence is seen in modern social media applications", "James Fleet", "one", "The ulnar nerve", "Border Collie", "Massachusetts", "Augustus", "`` at ''", "The Sun", "60 by West All - Stars", "August 22, 1980", "Jack Nicklaus", "2020", "General George Washington", "7.6 % Per Annum'( compounded annually )", "9.0 -- 9.1 ( M )", "Part 2", "1966", "As of January 17, 2018, 201 episodes", "2026 -- the centenary of Gaud\u00ed's death", "1926", "October 20, 1977", "Cetshwayo", "50", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "Garbi\u00f1e Muguruza", "17 % of the GDP", "the White Sox", "Rockwell", "nuclear power in outer space, typically either small fission systems or radioactive decay for electricity or heat", "Charlotte Hornets", "the lumbar cistern", "February 7, 2018", "muezzin", "Elizabeth Taylor", "Sweden", "U.S. Representative for Oklahoma's 4 congressional district", "\"Queen In-hyun's Man\"", "James Franco", "step up attacks against innocent civilians.\"", "a delegation of American Muslim and Christian leaders", "iTunes, which completely changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\" in 1995,", "drummer", "the Manchus", "Black Sox Scandal", "seven"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7175346218109375}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.923076923076923, 0.9473684210526316, 0.6, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.10256410256410256, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7857142857142858, 0.4444444444444445, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-4520", "mrqa_naturalquestions-validation-1144", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-10163", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5034", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1458"], "SR": 0.578125, "CSR": 0.5036272321428572, "EFR": 1.0, "Overall": 0.6683816964285715}, {"timecode": 56, "before_eval_results": {"predictions": ["Mrs. Bolton, Clifford's nurse\u2013vary significantly from one version to another", "Sweden", "\"The West Wing (1999)", "Adam Smith", "Luxembourg", "El Hiero", "Salvador Domingo Felipe Jacinto Dal\u00ef\u00bf\u00bdbol", "eighth note", "tyne", "a motorcycle, but can be a derny or tandem bicycle", "The Blues Brothers", "onion", "1984", "Frottage", "Penhaligon", "Kevin Painter", "Mickey and the Beanstalk", "NASA's Messenger orbiter", "goose bump", "duck-billed platypus", "Montreal", "Jeffrey Archer", "The Four Tops", "Velazquez", "WED", "\"Aviva plc\"", "Charlie Chan", "Apocalypse Now", "taekwondo", "Ishmael", "jubilee line", "Aramis", "\"Elijah's Chariot,\"", "the head", "Passepartout", "president Clinton", "haute", "James Gang and 1974\u2019s Miami", "300", "motorcycle speedway", "France", "James Garner", "marinated dried fruits", "Jay-Z", "bird", "The Psychiatric Association\u2019s Diagnostic and Statistical Manual of Mental disorders", "George lV", "Margaret Beckett", "Washington Post", "the White Ferns", "the CPI or the Creel Committee", "the 18th century", "Austria - Hungary", "Sean O' Neal", "140 million", "The New Yorker", "In Pursuit", "and Nobel Peace Prize winner Aung San Suu Kyi", "his brother to surrender.", "\"Walk -- Don't Run\"", "Gene Wilder", "\"South Park\"", "Prescott", "The Simpsons Movie"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6755380036630036}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.8, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6437", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-3195", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-4810", "mrqa_newsqa-validation-739", "mrqa_newsqa-validation-2308", "mrqa_searchqa-validation-13467"], "SR": 0.578125, "CSR": 0.5049342105263157, "EFR": 1.0, "Overall": 0.6686430921052632}, {"timecode": 57, "before_eval_results": {"predictions": ["CBS", "Lord Nelson", "united states", "Utah", "black light", "lacrosse", "Packers", "utrecht", "Operation Overlord", "leibniz", "Virginia", "Fred Perry", "dinghy", "potatoes", "1215", "cashmere", "diffusion", "wye", "jack London", "South Carolina", "ellesmere port", "Parsley", "Hindi", "Santiago", "cambridge", "Lynda Baron", "Oscar De La Hoya", "salvos", "90%", "Sven Goran Eriksson", "salvors", "Monaco Historic 2018 Grand Prix", "star", "Jordan", "coins", "motown", "Sudan", "cows", "times", "\"army\"", "united states", "Operation Otto", "silk", "Irving Berlin", "medical", "Leo Tolstoy", "Austria", "united states", "caffeine", "times of the Roman Republic", "planes", "the university's science club", "in 2003", "Magnavox Odyssey", "Clark County", "the Morrill Acts of 1862 and 1890", "Championnat National 3", "to kill members", "Japan", "Dr. Maria Siemionow", "times", "the Goblet of Fire", "praecox", "the Red Sea"], "metric_results": {"EM": 0.5, "QA-F1": 0.5715277777777779}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444444, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-6233", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-4578", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-923", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5877", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-2674", "mrqa_hotpotqa-validation-5140", "mrqa_newsqa-validation-2792", "mrqa_searchqa-validation-16366", "mrqa_searchqa-validation-5198", "mrqa_searchqa-validation-8276"], "SR": 0.5, "CSR": 0.5048491379310345, "EFR": 0.9375, "Overall": 0.656126077586207}, {"timecode": 58, "before_eval_results": {"predictions": ["Christian Louboutin", "apples", "Galapagos Islands", "\u201cFor Gallantry\u201d", "west side Story", "onions", "united states of Austria", "mariah Carey", "blancmange", "the Daily Herald", "four inches", "nathan", "dicksaid", "larkin", "dictes", "united states", "united states", "united states", "wales", "charlie", "m*A*S*H", "helene hanff", "capua mortua", "condors", "nickel alloys", "France", "laos", "play a big", "united states", "John Huston", "peterborough united", "capone", "bajan", "usa", "united states", "mike andrew atherton", "chine\u017fe", "mercury", "capone", "drew andrew atherton", "clarinets", "Mary Poppins", "Chapters I\u2013II", "Queensland", "Blofeld", "Kodak", "united states", "Kenya", "george iv", "tuscany", "n Nissan", "Ptolemy", "Toto", "commemorating fealty and filial piety", "Heather Langenheim", "Operation Iceberg", "0.500", "mesac Damas", "Revolutionary Armed Forces of Colombia", "a preliminary injunction", "michael andrew atherton", "pole vaulting", "Maine", "\"Dejection: An Ode\""], "metric_results": {"EM": 0.390625, "QA-F1": 0.4650011446886447}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-1089", "mrqa_triviaqa-validation-1383", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-3453", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-1832", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-2487", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1708", "mrqa_triviaqa-validation-1637", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-4635", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1545", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-384", "mrqa_searchqa-validation-10238", "mrqa_searchqa-validation-444", "mrqa_searchqa-validation-5746"], "SR": 0.390625, "CSR": 0.5029131355932204, "EFR": 1.0, "Overall": 0.6682388771186442}, {"timecode": 59, "before_eval_results": {"predictions": ["Victor Garber", "Aristotle", "Eliot Cutler", "goalkeeper", "David Weissman", "get a Clue", "comedy", "November 29, 1895", "the Goddess of Pop", "Sir Philip Anthony Hopkins", "near Philip Billard Municipal Airport", "Floyd Casey Stadium in Waco, Texas", "usually last two years", "Walt Disney and Ub Iwerks at the Walt Disney Studios in 1928", "Martin \"Marty\" McCann", "WB Television Network", "gainsborough Trinity Football Club is a football club based in Gainsborough, Lincolnshire, England", "a terrible date", "$7.3 billion", "best known for his ten seasons with the Charlotte Hornets", "george i", "sixteen", "Rural Electrification Act of 1936", "2015", "Nicholas \"Nick\" Offerman", "Golden Globe Award", "XXXTentacion", "Dire Straits", "reality television series", "the MGM Grand Garden Special Events Center", "Best Rock Song", "Pieter van Musschenbroek", "1979", "the 70 m and 90 m events", "prime minister", "video game", "Bulgarian-Canadian", "KXII", "Thunderball", "Eastern College Athletic Conference", "Indian state of Gujarat", "William Corcoran Eustis", "World Outgames", "Norwood", "Saturday", "Shooter Jennings", "Can't Be Tamed", "Bolton, England", "Stephen Hawking", "Samantha Rockwell", "Saoirse Ronan", "Gene Kelly and Donald O'Connor", "a region in Greek mythology", "Todd Bridges", "lemon", "dungarees & Jeans", "henbridge Jaipur", "southern Gaza city of Rafah", "the U.S. Consulate in Rio de Janeiro", "CNN", "william Shakespeare", "ice cream", "davenport", "Captain James Cook"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6743906700937952}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.625, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.8, 1.0, 0.0, 1.0, 0.8571428571428571, 0.9090909090909091, 0.28571428571428575, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.9090909090909091, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-1007", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1491", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-561", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-1830", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-2429", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-3404", "mrqa_triviaqa-validation-3147", "mrqa_triviaqa-validation-1352", "mrqa_newsqa-validation-2731", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-6145", "mrqa_searchqa-validation-2444", "mrqa_searchqa-validation-15613"], "SR": 0.484375, "CSR": 0.5026041666666667, "EFR": 1.0, "Overall": 0.6681770833333334}, {"timecode": 60, "UKR": 0.630859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1496", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2256", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2456", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8282", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1668", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3627", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-468", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5622", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6755", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7112", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7558", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-899"], "OKR": 0.7890625, "KG": 0.44296875, "before_eval_results": {"predictions": ["Batman", "The Constitution of India", "Frank Oz", "786 -- 802", "Patris", "19 July 1990", "John Ernest Crawford", "Andy Warhol", "December 19, 1971", "west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "in the British Isles of French and Latin origin", "BC Jean", "BT Sport", "57 days", "961", "Jay Baruchel", "December 1886", "President Woodrow Wilson", "at the state and national governmental level", "the courts", "Holly Marie Combs", "Greg Norman", "1998", "Coroebus of Elis", "giant planet", "Crepuscular animals", "Clarence Williams", "due to not being profitable", "Missouri", "issues of the American Civil War", "around 10 : 30am", "David Ben - Gurion", "RMS Titanic", "a warrior", "in San Francisco Bay", "Eight full seasons", "the main section of the parade, or the members of the actual club with the parading permit as well as the brass band", "Vasoepididymostomy", "with the fourth quarter of the preceding year", "James Bolam", "God forgave / God gratified", "in Broken Hill and Sydney", "Detective Eddie Thawne", "save, rescue, savior", "sedimentary rock", "Sir Ronald Ross", "NFL coaches", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "post translational modification", "UTC \u2212 09 : 00", "The Seattle Center", "Goneril", "tomato", "Guru Nanak", "footballer", "mixed martial arts", "James Tinling", "Rima Fakih", "165-room", "David Bowie,", "melba", "December to April", "Godiva Chocolate", "Sri Lanka Freedom Party"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6224474657287158}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true], "QA-F1": [0.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.8181818181818181, 0.2, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.5, 0.07999999999999999, 1.0, 0.9090909090909091, 0.0, 1.0, 0.888888888888889, 0.3333333333333333, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1966", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8599", "mrqa_triviaqa-validation-1154", "mrqa_hotpotqa-validation-4952", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-12527", "mrqa_searchqa-validation-10884"], "SR": 0.453125, "CSR": 0.5017930327868853, "EFR": 1.0, "Overall": 0.6729367315573771}, {"timecode": 61, "before_eval_results": {"predictions": ["1994 season", "Hern\u00e1n Cort\u00e9s and Xicotencatl the Younger", "Conrad Lewis", "lead vocalist Bart Millard", "Pangaea or Pangea", "111", "Kiss", "Justice Harlan", "full '' sexual intercourse", "a woman who had a sexual relationship with Paul whilst at university", "Georgia Groome", "autumpne", "Gina Tognoni / to\u028an\u02c8jo\u028ani / ( born November 28, 1973 )", "Michael K. Williams", "rural depopulation and the pursuit of excessive wealth", "Malina Weissman", "Pasek & Paul", "public services and public enterprises", "937 total weeks", "their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "neutral", "31 December 1960", "Jumping on the Moon", "1999", "the beginning of the American colonies", "the concentration of a compound exceeds its solubility ( such as when mixing solvents or changing their temperature )", "February 9, 2018", "animal, but a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "Andrew Lloyd Webber and lyrics by Charles Hart and Richard Stilgoe", "Dollree Mapp", "the 15th century", "electrons from electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions", "Rich Mullins", "Unwinding of DNA at the origin", "Detroit to Beijing", "Mickey Mantle", "Ben Savage", "FBI Technical analyst Penelope Garcia", "dress shop", "the 9th century", "1603", "September 25, 1987", "In 1987", "1939", "Randy Newman", "1956", "Ravi River", "prokaryotic", "# 4", "an active supporter of the League of Nations", "New York City", "shoes", "Sven Goran Eriksson", "horses", "Vanarama National League", "Laertes", "40 million", "siemionow", "Brian Smith", "gun charges", "Ronald Reagan", "titanium", "(Harold the Seco) Godwinson", "(Urien) Urien"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6811203359021247}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false], "QA-F1": [0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.7777777777777778, 1.0, 1.0, 0.1818181818181818, 0.0, 0.7586206896551725, 0.0, 1.0, 0.0, 1.0, 0.4, 0.5714285714285715, 1.0, 0.125, 0.42857142857142855, 1.0, 1.0, 0.2222222222222222, 1.0, 0.20000000000000004, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-5479", "mrqa_newsqa-validation-1679", "mrqa_searchqa-validation-15585", "mrqa_searchqa-validation-13746", "mrqa_triviaqa-validation-3768"], "SR": 0.546875, "CSR": 0.5025201612903225, "EFR": 1.0, "Overall": 0.6730821572580645}, {"timecode": 62, "before_eval_results": {"predictions": ["\"Boesmansrivier\"", "horse racing", "Italy", "bees", "adare", "2006 Trains", "63 to 144 inches", "Brighton", "squash", "Jack London", "AFC Wimbledon", "Scotland", "Edward VIII", "Bugs Bunny", "2006", "ambidextrous", "legend of hit survival TV series", "Japan", "2006", "mercury", "Yahoo!", "Klaus Barbie", "honey", "Joanne Harris", "The Forbidden Club", "Kunigunde Mackamotski,", "Moldovan", "Chatsworth House", "India and Pakistan", "Bull Moose Party", "arthurian", "eagle", "arlanda", "2006 (thirty) years ago", "hercules", "Real Madrid", "Jack Daniel's - Old Time Tennessee Whiskey", "Matthew Pinsent", "Iran", "salsa", "Cuba", "John McEnroe", "Kia", "Robert Stroud", "Yusuf Islam", "epidermis", "tyne", "oxygen", "avian Aqua Miser", "trumpet", "Cockermouth", "Europe", "October 1941", "the Natya Shastra", "McG", "Philip Billard Municipal Airport", "gull-wing doors", "July 8", "sexual harassment", "5,600", "the recipe page of the Whole30 program", "A movie heartthrob", "A Moon for the Misbegotten", "If These Dolls Could Talk"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6545138888888888}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-945", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6819", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-3276", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-4440", "mrqa_naturalquestions-validation-4416", "mrqa_hotpotqa-validation-2840", "mrqa_newsqa-validation-3652", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-15132", "mrqa_searchqa-validation-9158"], "SR": 0.609375, "CSR": 0.5042162698412698, "EFR": 1.0, "Overall": 0.6734213789682539}, {"timecode": 63, "before_eval_results": {"predictions": ["mom", "southern city of Naples", "\"CNN Heroes: An All-Star Tribute\"", "for the rest of the year", "\"It was never our intention to offend anyone,\"", "Bob Bogle,", "to fight back against Israel in Gaza", "his business dealings", "Saturday just hours before he was scheduled to perform at the BET Hip Hop Awards", "People Against Switching Sides", "Haiti's capital, Port-au-Prince,", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Darrin Tuck,", "from Amsterdam, in the Netherlands, to Ankara, Turkey,", "a review of state government practices completed in 100 days.", "prostate cancer,", "90", "a birdie four at the last hole", "Rima Fakih", "the U.S. Department of Agriculture", "37", "slayings of actress Sharon Tate and four others.", "33-year-old", "Isaac, and daughter, Rebecca.", "12-hour", "Judge Herman Thomas", "President Obama's race", "oldest documented bikinis", "laundry service", "the treatment of Muslims,", "twice", "lessons are simple enough -- confidence-building exercises, critical-thinking lessons -- all framed in the context of Islamic values.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras", "Friday,", "Gavin de Becker", "400 years ago", "the U.S. Consulate in Rio de Janeiro,", "Apple employees", "heavy turbulence", "$3 billion", "Nkepile M abuse", "resources", "women", "Megan Lynn Touma,", "then-Sen. Obama", "Technological Institute of Higher Learning of Monterrey,", "women soldier,", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\" Frankie Neylon,", "David Russ,", "wisecracking youngster Arnold Drummond", "Chinese", "a young husband and wife", "The stability, security, and predictability of British law and government", "six - hoop", "Israel", "mathematics", "james bach", "Detroit, Michigan", "the north bank of the North Esk", "singer", "dinosaurs", "warmth", "women", "at the Canada -- US border, south to Key West, Florida"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6069886318878255}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.19354838709677416, 1.0, 0.13333333333333333, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8181818181818181]}}, "before_error_ids": ["mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3087", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-1827", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-validation-1831", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-130", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-9739", "mrqa_searchqa-validation-6571", "mrqa_searchqa-validation-9458", "mrqa_naturalquestions-validation-6670"], "SR": 0.515625, "CSR": 0.50439453125, "EFR": 1.0, "Overall": 0.67345703125}, {"timecode": 64, "before_eval_results": {"predictions": ["Slavic culture", "a Belgian-owned Canadian beer company", "Sweepstakes", "General McClellan", "John Keats", "Shiraz", "Wizard of Oz", "calcite", "Bologna", "potatoes", "Princeton", "China", "the Knight", "Evian", "unicorns", "heaven", "the Andes", "Jim Jarmusch", "Martin Luther", "Miles Davis", "Tennessee", "Audrey Hepburn", "Falafel", "aladdin", "The Prairie Wolf", "Derek Jeter", "Arthur C. Clarke", "the Denver Broncos", "the Vietnam War", "Silence of the Lambs", "a dynamic, contemporary Australian university, proud of its reputation for quality teaching and the strength of its...", "Christian Louboutin", "monk seal", "beer", "writing", "less than 1% fat", "Ginger Rogers", "Beijing", "Plumeria", "Lafayette", "Side by Side: The True Story of the Osmond Family", "Pickwick Club", "a Buffalo", "comet", "chuck yeager", "two toy cars or roller skates of equal mass at the same time", "sheep", "Inheritance Cycle", "Georgia", "french toast", "the Fifth Amendment", "Elvis Presley", "at the Louvre Museum in Paris", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "bachman", "jaws", "gold aureus", "[O.S. 6 November] 1860", "\"White Horse\"", "the Constitution of Mexico", "Kurt Cobain,", "Glasgow, Scotland", "Christopher Savoie", "London"], "metric_results": {"EM": 0.5, "QA-F1": 0.5747018606393606}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.2, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.7878787878787877, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9479", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-13851", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-6953", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-13675", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-6692", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-1377", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-6284", "mrqa_searchqa-validation-13122", "mrqa_searchqa-validation-16755", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-3875", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-5831", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1604", "mrqa_hotpotqa-validation-4263", "mrqa_hotpotqa-validation-1876", "mrqa_newsqa-validation-2011"], "SR": 0.5, "CSR": 0.5043269230769231, "EFR": 0.96875, "Overall": 0.6671935096153846}, {"timecode": 65, "before_eval_results": {"predictions": ["Iowa", "McClatchy", "Casino Royale", "William Tell", "the Apprentice", "Aeschylus", "the College of William and Mary", "Intelligence Quotient", "Stranger in a Strange Land", "cracker barrel", "24", "cowherd", "Monty Python and the Holy Grail", "bacall", "President of the United States", "In God We Trust", "Portland", "China", "ABIYSHALOWM", "Castle Rock", "Bollywood", "Marcia Brady", "House of Habsburg", "joy", "a Twinkie", "the altitude", "the Unbearable Lightness of Being", "Richard", "henry VIII", "SUFFIXES", "C Dm7", "Liliuokalani", "the pituitary", "the South African Boer War", "the pulp", "Michelle Pfeiffer", "Aswan", "Billy Ray Cyrus", "pony", "The Body", "Impostor syndrome", "to displace", "Davy Crockett", "Sagittarius", "the volcano at Okataina Volcanic Centre", "copper", "Dubliners", "the Trembling Mountain", "Cuba", "the Taliban", "Arlington", "Ali", "Brian", "during prenatal development in the central part of each developing bone", "Exile", "Carmen Miranda", "bette", "the superhero Birdman", "the Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Erich Maria Remarque", "the nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "15-year-old", "Qutab - ud - din Aibak"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5831293616584564}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.33333333333333337, 1.0, 1.0, 0.0, 0.25, 0.4, 0.5, 0.9655172413793104, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5768", "mrqa_searchqa-validation-15873", "mrqa_searchqa-validation-9446", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-2194", "mrqa_searchqa-validation-5095", "mrqa_searchqa-validation-10927", "mrqa_searchqa-validation-10163", "mrqa_searchqa-validation-9120", "mrqa_searchqa-validation-11135", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-9081", "mrqa_searchqa-validation-16276", "mrqa_searchqa-validation-3734", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-11604", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-8840", "mrqa_searchqa-validation-1852", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-1683", "mrqa_searchqa-validation-8703", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-2440", "mrqa_triviaqa-validation-6882", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2982"], "SR": 0.484375, "CSR": 0.5040246212121212, "EFR": 0.9696969696969697, "Overall": 0.6673224431818181}, {"timecode": 66, "before_eval_results": {"predictions": ["eight", "a mutualistic relationship", "December 20, 1951", "the ninth w\u0101", "Terry Kath", "an inability to comprehend and formulate language because of damage to specific brain regions", "in the fascia surrounding skeletal muscle", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "1546", "Banquo", "January 1923", "Bea Vanderwaal", "a habitat", "minced meat", "a geochronologic tool", "free floating", "the Japanese government", "6 August 1965", "30 October 1918", "Austria - Hungary", "Domhnall Gleeson", "from 13 to 22 June 2012", "T - Bone Walker", "Paul Baumer", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "in the Executive Residence of the White House Complex", "Article Two", "April 13, 2018", "Bush", "Yuzuru Hanyu", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "from the southernmost tip of the South American mainland, across the Strait of Magellan", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "in Rome", "January 1, 2016", "Leonardo da Vinci", "for a given mass of an ideal gas at constant pressure", "Thawne", "Philippe Petit", "the passage of Proposition 103 in 1988", "2008", "from eukaryotic cells", "Julie Adams", "775", "Pakistan", "Xiu Li Dai and Yongge Dai", "Norman Whitfield", "Americans who served in the armed forces and as civilians during World War II", "eight years", "James Fleet", "the year 1 BC", "David Davis", "Dirty Dancing", "mumps", "Delphi Lawrence", "child and only daughter of Prince William, Duke of Cambridge, and Catherine, Duchess of Cambridge", "International Boxing Hall of Fame", "a dress", "Pakistani officials,", "Iraq", "Jamaica", "cocoa", "Delphi", "not guilty of affray"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6754300933660178}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.8095238095238095, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.9387755102040816, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.7058823529411764, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6984", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-4419", "mrqa_hotpotqa-validation-5549", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1259", "mrqa_searchqa-validation-10836", "mrqa_searchqa-validation-5072", "mrqa_newsqa-validation-37"], "SR": 0.5625, "CSR": 0.5048973880597014, "EFR": 0.9642857142857143, "Overall": 0.6664147454690832}, {"timecode": 67, "before_eval_results": {"predictions": ["Pyeongchang County, Gangwon Province, South Korea", "Padawan", "in a liquid solution", "April 1917", "Wimbledon", "Redford's adopted home state of Utah", "1969", "by October 1986", "the referee", "Northern Ireland law", "parthenogenesis", "Taquo Applegate as Sue Ellen `` Swell '' Crandell", "reproductive system", "Taiwan", "the Parliament of the United Kingdom", "part - Samoyed terrier", "Gibraltar", "September 1947", "7 July", "in the bone marrow", "Sophia Akuffo", "to determine who would win if all those ranked were the same size", "the last place team is positioned first", "Sarah Josepha Hale", "Ingrid Bergman", "Jessica Simpson", "on the microscope's stage", "the Old Testament", "Daren Maxwell Kagasoff", "at Steveston Outdoor pool in Richmond, BC", "Anne Garcia", "in North America", "by government regulations ( including the jurisdiction's corporations law ) and the organization's own constitution and bylaws", "the investment bank Friedman Billings Ramsey", "the Miami Heat of the National Basketball Association ( NBA )", "vasoconstriction", "the Toronto Islands in Toronto, Ontario, Canada", "lighter", "the final episode of the series", "Roger Nichols and Paul Williams", "Konakuppakatil Gopinathan Balakrishnan", "Logan Williams", "a Border Collie", "to symbolize connection between Vesta's fire and the sun as sources of life", "1665 to 1666", "sugars and amino acids", "Aegisthus", "from the nearest open sea at Bay of Whales", "from their Oklahoma home", "decades after its initial release", "by December 1349", "Ipswich Town Football Club", "Dutch", "British Airways", "genderqueer", "14,673", "YouTube", "Chinese tourists", "Joel \"Taz\" DiGregorio", "expanded legal protections", "the Death Valley", "2016", "Ichabod Crane", "Thomas Jefferson"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5484352453102452}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.2857142857142857, 0.0, 0.4, 0.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.12121212121212123, 0.25, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.4444444444444445, 0.0, 0.1, 0.0, 0.7499999999999999, 0.4444444444444445, 0.0, 0.42857142857142855, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.4, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-8147", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1269", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-5634", "mrqa_hotpotqa-validation-621", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-4207", "mrqa_searchqa-validation-324"], "SR": 0.421875, "CSR": 0.5036764705882353, "EFR": 0.972972972972973, "Overall": 0.6679080137122416}, {"timecode": 68, "before_eval_results": {"predictions": ["Malaysia", "nomadic people", "5.4%", "Parkinson's Disease", "John Jay", "Warsaw", "London", "Murfreesboro", "Africa", "(Clay) Aiken", "Muhammad", "Melanesia", "Joe Namath", "high and dry", "a Doll", "Baroque Modernity in Peru's South Sea Metropolis", "Cleopatra", "the International Space Station", "Iran", "Gaius Cassius Longinus", "\"The Night They Drove Old Dixie Down,\"", "South Africa", "(John) Deere", "Thames", "Oxford", "William Wordsworth", "Elphaba", "Tuscaloosa", "Cyprus", "Sabino Canyon", "Frasier", "Josh Kay", "Sicily", "Herbert Hoover", "Zhou Enlai", "pizzas", "Lake Geneva", "(Bob) Falfa", "The Mole", "HIV/AIDS", "Today", "Golden", "liver cancer", "Bern", "bchamel", "(Jack) Robinson", "Buzzbee", "Diane Arbus", "Willa Cather", "Overruled", "the marathon", "Masha Skorobogatov", "Kyla Pratt", "Dumont d'Urville Station", "Union Gap", "Charlotte's Web", "Cameroon", "Ding Sheng", "May 5, 2015", "Massapequa", "Dayton, Oregon, in the Willamette Valley to the Pacific coast", "December 7, 1941", "London transit bombings", "acid phosphate"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5544034090909091}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3218", "mrqa_searchqa-validation-2956", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-13698", "mrqa_searchqa-validation-11405", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-2883", "mrqa_searchqa-validation-2330", "mrqa_searchqa-validation-6130", "mrqa_searchqa-validation-1918", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-8156", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-2941", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-6816", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-12621", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3166", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-893", "mrqa_triviaqa-validation-3820"], "SR": 0.46875, "CSR": 0.5031702898550725, "EFR": 1.0, "Overall": 0.6732121829710145}, {"timecode": 69, "before_eval_results": {"predictions": ["the aqueduct", "a quark", "Christopher Reeve", "Bucharest", "ambition", "John Jacob Astor", "casting trans actors", "Penn Station", "the Sun Also Rises", "Cherokee", "Ferrari", "banquet", "the High Plains", "Joe Hill", "the Mark of the Beast", "Kentucky", "Supernatural", "Jean Foucault", "Montana", "Deep Brain Stimulation", "kissanhnta", "the Amazon", "Oklahoma", "Anne Hathaway", "the Model T", "Iraq", "Vietnam", "Tintern", "Canada", "Cecil Day-Lewis", "Isaac Newton", "the Blue Ridge Mountain", "Cimitiere", "Susan B. Anthony", "Dexter", "the opossum rat", "the Washington Redskins", "Starsky", "the Prisoner of Azkaban", "Knocked Up", "Space Chimps", "Christopher Wren", "jazz", "Boston", "Han Solo", "Hans Christian Andersen", "the orchestra", "the Mexican Revolution", "a veil", "New York", "Guinness", "Portugal. The Man", "1983", "Sally Dworsky", "Pumas", "red", "Greek", "Best Animated Feature", "1895", "Stephen Ireland", "Shiza Shahid,", "Coptic Christians and Muslims", "blind Majid Movahedi,", "Retina display"], "metric_results": {"EM": 0.53125, "QA-F1": 0.63125}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.8, 0.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-5269", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-6228", "mrqa_searchqa-validation-6770", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-6956", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-14736", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-2333", "mrqa_searchqa-validation-6185", "mrqa_searchqa-validation-2934", "mrqa_searchqa-validation-5427", "mrqa_triviaqa-validation-3274", "mrqa_triviaqa-validation-7182", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-5477", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-1640"], "SR": 0.53125, "CSR": 0.5035714285714286, "EFR": 1.0, "Overall": 0.6732924107142857}, {"timecode": 70, "UKR": 0.666015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2888", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-39", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-978", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1120", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4890", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-963", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14334", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3809", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-2467", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3832", "mrqa_squad-validation-3852", "mrqa_squad-validation-386", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-3994", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4467", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5493", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6638", "mrqa_squad-validation-6875", "mrqa_squad-validation-6957", "mrqa_squad-validation-7064", "mrqa_squad-validation-739", "mrqa_squad-validation-7549", "mrqa_squad-validation-7688", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-7917", "mrqa_squad-validation-8309", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-893", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-143", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5861", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-595", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917"], "OKR": 0.80078125, "KG": 0.46328125, "before_eval_results": {"predictions": ["Kathy Najimy", "2006 -- 07", "2015", "Kenny Rogers and The First Edition", "2026", "Clare Torry", "Andrew Lloyd Webber", "Hollywood Masonic Temple", "vitality", "Stephen Graham", "6 - 6", "1955", "Owen Hunt", "Parthenogenesis", "fertilization", "Perchik", "Judy Garland", "inwards towards the pith", "a Czech word, robota", "skeletal muscle", "Nazi Germany and Fascist Italy", "Gunpei Yokoi", "David Motl", "60", "September 9, 2010", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "scrolls dating back to the 12th century", "Buddhism", "Kiss", "Syco Music", "Trace Adkins", "the optic chiasm", "to manage the characteristics of the beer's head", "United States", "a large, high - performance luxury coupe", "James Intveld", "15 February 1998", "Christopher Allen Lloyd", "100,000", "January 2004", "Bartolomeu Dias", "Isabela Moner", "to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "potential of hydrogen", "episode 108", "the Qianlong Emperor", "Guwahati", "74", "(\u01c0xarra)", "Rufus and Chaka Khan", "eight", "Venado Tuerto, Argentina", "Jamaica", "mead", "Tomorrowland", "the Tallahassee City Commission", "John Kavanagh", "pesos", "Muhammad Ali, Meir Kahane and Baruch Goldstein.", "$7.8 million", "In Memoriam", "Mercury", "(Jerry) Orbach", "UNICEF"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6489169164169164}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.9, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 0.4, 0.33333333333333337, 1.0, 0.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-305", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7374", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-9150", "mrqa_triviaqa-validation-4646", "mrqa_hotpotqa-validation-3032", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-236", "mrqa_searchqa-validation-9696"], "SR": 0.5625, "CSR": 0.5044014084507042, "EFR": 0.9642857142857143, "Overall": 0.6797530495472837}, {"timecode": 71, "before_eval_results": {"predictions": ["Easter Island", "George Balanchine", "Jimi Hendrix", "the Mesozoic Era", "Jane Austen", "Critters 3", "the Basques", "Cherry Jones", "Happy Feet", "a guardian angel", "West Point", "the Tame", "the eighth season of the television series, Law & Order: Special Victims Unit", "the Black and Caspian seas", "Eris", "Bloemfontein", "the Belize atolls", "(George) Farragut", "Mar 17, 2008", "Salaried", "the Memento Mori", "Side by Side: The True Story of the Osmond Family", "Scrabble", "the suckers", "the Catholic Church", "London", "Fert", "TransCanada", "the macula", "Boston", "# Quiz", "the spelling bee", "poetry", "the Battle of Fort Donelson", "1950", "(Larry) Goad", "sucrose", "Merseyside", "Cuba", "The Prince and the Pauper", "(Thomas) Paine", "(Emil) Lincoln", "(Emil) North", "King Charles I", "Jemima", "an interest in horror", "Palitana", "George Bernard Shaw", "Utah", "RNA", "Kublai Khan", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "Kimberlin Brown", "Henry Selick", "Caviar", "July 16, 1969", "argentina", "the vicar of Wantage", "Mark Feld", "Polish-Jewish", "a collapsed apartment building in Cologne, Germany,", "the Iraq's autonomous region of Kurdish.", "$40 and a loaf of bread.", "Ontario"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4351314484126984}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-2210", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-5919", "mrqa_searchqa-validation-4309", "mrqa_searchqa-validation-4000", "mrqa_searchqa-validation-12776", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-6284", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-15548", "mrqa_searchqa-validation-3904", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-9495", "mrqa_searchqa-validation-13841", "mrqa_searchqa-validation-5088", "mrqa_searchqa-validation-3073", "mrqa_searchqa-validation-12497", "mrqa_searchqa-validation-11533", "mrqa_searchqa-validation-6350", "mrqa_searchqa-validation-15590", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-3746", "mrqa_hotpotqa-validation-3593", "mrqa_hotpotqa-validation-3503", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3002"], "SR": 0.359375, "CSR": 0.5023871527777778, "EFR": 1.0, "Overall": 0.6864930555555555}, {"timecode": 72, "before_eval_results": {"predictions": ["Austria", "peninsulas", "(Ray) Reddy", "Brasilia", "Applebee\\'s", "New Jersey", "Backgammon", "Steely Dan", "Artemis", "Tasmania", "Colorado Springs", "Cheap trick", "eggs", "Islam", "Cerberus", "Robert E. Lee", "(Yerette)", "Brigadoon", "Columbus", "Elijah Muhammad", "New York", "Federico Fellini", "Fenway Park", "C.T. Eisler", "The Princess Diaries", "fluoridation", "Herman Melville", "Korea", "John Henry", "Babe Ruth", "Hillary Clinton", "Chicago", "Wallace & Gromit", "sesame", "Nike", "(Jack) Nicholson", "nitrogen", "the Omaha", "dogs", "Paul Gauguin", "Francis Scott Key", "Mexico", "the Peashooter", "(J.P. Morgan)", "$200m", "Massachusetts", "ACTIVE", "a wicket", "Alfred Hitchcock", "the Basque", "Ambrose Bierce", "the President of the United States", "around 2.45 billion years ago", "Jennifer Morrison", "Stephen Cowell", "a meteoroid", "argentina", "Edinburgh", "Campbellsville", "a French mathematician and physicist", "165-room", "Wednesday,", "Brian Smith,", "a Ballon d'Or"], "metric_results": {"EM": 0.625, "QA-F1": 0.6989583333333333}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-4160", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-1079", "mrqa_searchqa-validation-1717", "mrqa_searchqa-validation-1805", "mrqa_searchqa-validation-14670", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-775", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-10907", "mrqa_searchqa-validation-11119", "mrqa_searchqa-validation-13029", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-2889", "mrqa_triviaqa-validation-6507", "mrqa_triviaqa-validation-5187", "mrqa_hotpotqa-validation-662", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-1962"], "SR": 0.625, "CSR": 0.5040667808219178, "EFR": 0.9583333333333334, "Overall": 0.6784956478310502}, {"timecode": 73, "before_eval_results": {"predictions": ["gondola", "Sinclair Lewis", "Hilary Swank", "louis walsh", "Coast New Zealand", "Israel", "Lundy", "Van Morrison", "nickel", "Stuart Bingham", "Frank Darabont", "President of the United States of America", "Napoleon I", "espresso", "the Hierarchy ofneeds", "Volkswagen", "Bedser", "Oldham", "Suriname", "Jabba the Hutt", "louis walsh", "Barbara Ann Mandrell", "Taylor", "Baku", "Chechnya", "Hodder & Stoughton, London", "green", "Chester", "Hippety Hopper", "wear it", "Mt Kenya", "a pumpkin", "Slovenia", "Sicily", "Switzerland", "The Magic Circle", "Julie Andrews Edwards", "Pancho Villa", "Nigeria", "Leeds", "Passover", "Cologne", "Oliver!", "nihon", "Ra\u00fal Castro", "Uganda", "Renzo Piano", "The Penrose triangle", "Mexico", "North Carolina", "The Rembrandts", "a wall mounted faucet and the sink rim", "Tom Brady", "December 11, 2014", "Forrest Gump", "John Anderson", "Leon Schlesinger Productions (later Warner Bros.  Cartoons)", "Arnold Drummond", "dining scene", "Thaksin Shinawatra,", "Jackie Moon", "Maria Callas", "Desperate Housewives", "intelligent design"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5513888888888888}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6526", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2845", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-3345", "mrqa_triviaqa-validation-5684", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-6862", "mrqa_triviaqa-validation-1816", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-4028", "mrqa_hotpotqa-validation-1306", "mrqa_hotpotqa-validation-2694", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-2669", "mrqa_searchqa-validation-16053"], "SR": 0.515625, "CSR": 0.504222972972973, "EFR": 1.0, "Overall": 0.6868602195945945}, {"timecode": 74, "before_eval_results": {"predictions": ["charles", "john smith", "the navy", "a Ford Tutor", "apple", "orange", "Dreamgirls", "spain", "orange", "duke", "allergies", "gin", "Canada", "to find a good shop location", "coughing", "Peter Stuyvesant", "apple", "Pakistan", "Athens", "chiricahua", "cardiac conduction system", "Blucher", "japan", "osmic", "cubed", "george fideric Handel", "Lincolnshire", "Zimbabwe", "Northern Ireland", "orange", "Anwar Sadat", "sue barker", "Silent Spring", "smith", "bali hetrick", "Michael Sheen", "(A Wood) Maypole dance", "London", "Montmorency", "the California condor", "racquet", "port of London", "Pinocchio", "cenozoic", "john smith", "Jamie Oliver", "bikes", "willy Russell", "Petula Clark", "new Democracy", "The Blue Boy", "a Border Collie", "Kristy Swanson", "fourth season", "George Whitefield", "Hermione Youlanda Ruby Clinton-Baddeley", "Floyd Casey Stadium", "david Bowie", "spurring on economic growth and creating opportunity for our people.", "Robert Kimmitt", "Mammoth Cave", "recessive", "seeth Menotti", "Fayetteville"], "metric_results": {"EM": 0.40625, "QA-F1": 0.44375}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-2589", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-6107", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-3482", "mrqa_triviaqa-validation-6722", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-169", "mrqa_triviaqa-validation-4706", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-2699", "mrqa_triviaqa-validation-7353", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-1344", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-5738", "mrqa_naturalquestions-validation-8404", "mrqa_hotpotqa-validation-461", "mrqa_newsqa-validation-3008", "mrqa_searchqa-validation-4464", "mrqa_hotpotqa-validation-3787"], "SR": 0.40625, "CSR": 0.5029166666666667, "EFR": 0.9736842105263158, "Overall": 0.6813358004385965}, {"timecode": 75, "before_eval_results": {"predictions": ["new Zealand", "1961", "bolognese", "cardinal pacelli", "Duke Orsino", "Arthur", "jimmy de cervantes", "Budapest", "Gillette", "word of use", "Mediterranean", "Bash Street", "emerald", "the innermost digit of the forelimb", "swallow sidecar", "Chicago", "quarterback", "france", "Gryffindor", "gold hallmarks", "john Buchan", "Pyrenees", "17", "smith", "Geneva", "labyrinth", "algebra", "Eddie Murphy", "Crete", "spain", "Copenhagen", "pulmonary", "john galsworthy", "the killer whale", "Christopher Nolan", "purple", "chess", "Ireland", "Diana Vickers", "February 14", "Damian Green", "krypton", "Bagel", "france", "South Dakota", "Alexander Dubcek", "Denver", "Chicago Cubs", "St. Louis", "iberia", "Rosetta", "formal education during the Roman Empire", "the Saudi Arab kingdom", "the nucleus", "February 14, 1859", "1892", "outside the United States and Canada", "1,500", "southern China", "Iran", "bone", "Washington", "sedimentary rock", "golf"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6443452380952381}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-5653", "mrqa_triviaqa-validation-2012", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-3005", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-156", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-6463", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-3745", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-366", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2495", "mrqa_searchqa-validation-10770", "mrqa_searchqa-validation-8445", "mrqa_searchqa-validation-2183"], "SR": 0.546875, "CSR": 0.5034950657894737, "EFR": 1.0, "Overall": 0.6867146381578947}, {"timecode": 76, "before_eval_results": {"predictions": ["English author Rudyard Kipling", "Rachel Griffiths", "California, Utah and Arizona", "epidemiology", "William Chatterton Dix", "1924", "The fifth season of Chicago P.D., an American police drama television series with executive producer Dick Wolf, and producers Derek Haas, Michael Brandt, and Rick Eid", "Alabama", "Scheria", "Sanchez Navarro", "Thomas Jefferson, John Adams and Thomas Paine", "August 2, 1990", "Joe Pizzulo and Leeza Miller", "Julie Adams", "Ian Hart", "ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "a Native American nation from the Great Plains", "basal lamina ( one of the two layers of the basement membrane )", "the ARPANET", "April 1979", "Tbilisi", "a security feature for `` card not present '' payment card transactions instituted to reduce the incidence of credit card fraud", "Atticus Finch", "50", "Liam Cunningham", "2013", "ummat al - Islamiyah", "# 4", "1980", "2017 season", "W. Edwards Deming", "Saphira", "encrypted its traffic", "Galveston hurricane", "the final years of the Third Republic ( 1965 -- 72 )", "Ajay Tyagi", "jules pacelli", "Paul Revere", "Julius Caesar", "Aristotle", "1927, 1934, 1938, 1956", "Zeus", "For a single particle in a plane two coordinates define its location", "April 10, 2018", "Lee County, Florida, United States", "mid November", "Kevin Spacey", "Fa Ze wrist", "20th Century Fox, produced by 1492 Pictures and currently consists of two installments", "the French", "hyperinflation", "lingerie", "cerventry patmore", "Augustus", "Karolina Dean", "around four hundred", "Caesars Entertainment Corporation", "not believe North Korea intends to launch a long-range missile in the near future,", "the sins of the members of the church,", "Marcus Schrenker,", "Gimli", "Quartet", "ask for help", "Charice"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5852498611611765}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.06451612903225806, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.5714285714285715, 0.28571428571428575, 1.0, 0.0, 0.10526315789473684, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-1907", "mrqa_hotpotqa-validation-4503", "mrqa_newsqa-validation-212", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-1590"], "SR": 0.53125, "CSR": 0.5038555194805194, "EFR": 1.0, "Overall": 0.6867867288961039}, {"timecode": 77, "before_eval_results": {"predictions": ["Senator John Cornyn", "Istanbul", "Cana of Galilee", "\"Go pick up your toy\"", "Figaro", "Jenny Craig", "glitter", "Bayer", "The Theodor Seuss Geisel Medal", "Rove", "iceland", "Ireland", "The Library, Westminster School, London, S. W. The Oxford and Cambridge Club, c/o Messrs.", "Portland", "florida", "Doctor Dolittle, M.D.", "fish", "transmission", "hot air balloons", "vacuum tube", "Bridges of Madison County", "colombia", "iron", "\"SUN King\" SNIFFED,", "ice cream", "king louis XIV", "hyaena hyaena", "Alien", "John F. Kennedy", "Indira Gandhi", "rodents", "Stephen Decatur", "Patti LaBelle", "the Plowman", "Molly Brown", "other cities", "hurricanes", "The Wall Street Journal", "fragging", "tinactin", "Virgin Atlantic", "Perrier", "Eastwick", "henry", "trout", "india", "Minnesota", "San Francisco", "rabbit", "coffee", "handguns", "Brazil", "Nicole DuPort", "species", "deep purple", "Leonardo Da Vinci", "Surrealist", "July 25 to August 4", "1755", "Trey Parker and Matt Stone", "more than 1.2 million people.", "Luca di Montezemolo", "Roger Federer", "Angelo Bronzino"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6165922619047619}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.8571428571428571, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6923", "mrqa_searchqa-validation-692", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-1044", "mrqa_searchqa-validation-8250", "mrqa_searchqa-validation-2938", "mrqa_searchqa-validation-11220", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-15029", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-13993", "mrqa_searchqa-validation-15498", "mrqa_searchqa-validation-10162", "mrqa_searchqa-validation-8660", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-5989", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-6955", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-11923", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-2858", "mrqa_triviaqa-validation-3098", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-1364", "mrqa_triviaqa-validation-5253"], "SR": 0.515625, "CSR": 0.5040064102564102, "EFR": 1.0, "Overall": 0.6868169070512821}, {"timecode": 78, "before_eval_results": {"predictions": ["Tycho Brahe", "Little Miss Sunshine", "Philadelphia", "Peter Rabbit", "Tommy Franks", "Troy", "Jonny Quest", "Rwanda and its neighboring kingdom to the south, Burundi", "Fort Sumter", "never having to say you're sorry", "Captains Courageous", "Bryan Adams", "Moses", "engineering", "Chaucer", "Toronto Blue Jays", "the R.A.F.", "Dr. Isaac Asimov", "Sayonara", "the Orient Express", "Dante", "Sir Walter Scott", "a cord", "Louisiana", "\"The Maltese Falcon (1941)", "the South West Pacific", "teflon", "the human breast", "PG", "occipital", "a spoonful", "Little Red Riding Hood", "Year 3000", "Wyoming", "a popsicle", "Los Angeles", "a paladin", "\"Chelsea Morning\"", "a comb", "Venice", "Paraguay", "E. T. A. Hoffmann", "debts", "Oz", "El Supremo", "Foot Locker", "Princess Leia", "a thistle", "some dramatic coup occurred three years ago when he stepped in, on one... to conduct the Paris Opera Orchestra in Otello at the Kennedy Center", "Hammurabi", "Alkalinity", "the ninth w\u0101", "Matt Monro", "God's re-telling of the Ten Commandments to the younger generation who were to enter the Promised Land", "mount kenya", "Louis \"Colonel Tom\" Parker", "Boston Legal", "Whitney Houston", "Channel 4", "Mark Neary Donohue Jr.", "U.S. Defense Department", "a share in the royalties for the tune.", "Arizona", "American 3D computer-animated comedy"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7308407738095238}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-12459", "mrqa_searchqa-validation-4423", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-11041", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11646", "mrqa_searchqa-validation-10152", "mrqa_searchqa-validation-15144", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-11821", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-13217", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-12891", "mrqa_naturalquestions-validation-10310", "mrqa_triviaqa-validation-4688", "mrqa_hotpotqa-validation-5344", "mrqa_newsqa-validation-2151", "mrqa_hotpotqa-validation-2673"], "SR": 0.640625, "CSR": 0.5057357594936709, "EFR": 1.0, "Overall": 0.6871627768987343}, {"timecode": 79, "before_eval_results": {"predictions": ["The New Jersey Devils", "Banquo", "Detroit", "antonyms", "w", "Ford", "Joseph Campbell", "antonyms", "Faith Hill", "Novel", "a New Broom", "Edinburgh", "engineering", "Cyprus", "savanna", "a tandoor", "buoyancy", "pianist", "Sure", "oysters", "Gilbert Grape", "Barbara Bush", "Inman", "The Jungle Book", "eggshells", "a Hornet", "The Sadler\\'s Wells Ballet", "The Voice of Punjab(Punjab)", "the FBI's Hostage Rescue Team (HRT)", "Aaron Burr", "Johns Hopkins", "jason", "The Mississippi River", "Damascus", "Oahu", "Devo!", "biosphere", "stuffing", "Reading Railroad", "George Eliot", "the Cotton Bowl", "the Battle of Shiloh", "McCarthy", "Takana", "apples", "a cedar", "the Almond Joy", "James Vance Marshall", "Sam Houston", "Caesar salads", "cable cars", "July 14, 1969", "on permanent display at the Louvre Museum in Paris", "July 1, 1923", "The Crime Wave at Blandings", "Coronation Street", "The Boar\u2019soprano", "Waylon Jennings, Kris Kristofferson", "Sarajevo", "Annie Ida Jenny No\u00eb Haesendonck", "The little feet along the floor.\"", "a fifth successive season", "The son of Gabon's former president", "Wildcats"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5807291666666666}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-15449", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-12299", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-15892", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-7208", "mrqa_searchqa-validation-15134", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-6381", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-13472", "mrqa_searchqa-validation-1853", "mrqa_searchqa-validation-9372", "mrqa_naturalquestions-validation-1446", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-6870", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-3155", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-3923"], "SR": 0.53125, "CSR": 0.5060546875, "EFR": 1.0, "Overall": 0.6872265625}, {"timecode": 80, "UKR": 0.6796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3137", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8631", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-8803", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7052", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.80859375, "KG": 0.46015625, "before_eval_results": {"predictions": ["paul", "a partridge", "hashish", "Austria", "George IV", "azerbaijan", "peter", "Sisyphus", "Italy", "a coffee house", "Cambodia", "moldova", "Taking of Pelham", "Ethiopia", "Frank McCourt", "antifan", "Arkansas", "Texas", "Norway", "archer", "William Blake", "Mar Pac\u00edfico", "Federer", "Charlie Chan", "Christiaan Huygens", "paul hUSSAIN", "Austria-Hungary", "a shekel", "Franz Liszt", "Michael Caine", "Professor Brian Cox", "Australia", "Heathwaite House", "casualty", "McDonnell Douglas", "tyne", "Missouri", "Emma Chambers", "Buckinghamshire", "Turkey", "cat", "David Lynch", "six", "One Direction", "Groucho Marx", "Brazil", "Kate Winslet", "India", "1884", "penny dreadful", "Rio Grande", "November 1", "18 February 2000", "David Joseph Madden", "\"The Braes of Balquhither\"", "Mary Astor", "Taliban's Islamic Emirate of Afghanistan", "natural gas", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Madonna", "Hawaii", "Monaco", "p. D. James", "MacFarlane"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6133928571428572}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3779", "mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-3969", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-909", "mrqa_triviaqa-validation-5733", "mrqa_triviaqa-validation-4756", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-2921", "mrqa_triviaqa-validation-4823", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-2733", "mrqa_triviaqa-validation-3764", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-9986", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-3690", "mrqa_newsqa-validation-4110", "mrqa_searchqa-validation-12999"], "SR": 0.59375, "CSR": 0.5071373456790124, "EFR": 1.0, "Overall": 0.6911149691358024}, {"timecode": 81, "before_eval_results": {"predictions": ["Pebble Beach", "a punctuation mark written before the first letter of an interrogative sentence or clause to indicate that a question follows", "New York Knickerbockers", "John Dalton", "San Antonio", "rear - view mirror", "Wonders of the Modern World", "the RAF", "BC Jean and Toby Gad", "1994", "September 2017", "Focus Features", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "2017", "nine", "Tbilisi, Georgia", "1917", "1900", "Bryan Cranston", "Geothermal gradient", "around 10 : 30am", "frontal lobe", "Napoleon's planned invasion of the United Kingdom", "potential of hydrogen", "volcanic activity", "held that `` a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Egypt", "201", "pia mater", "members of the gay ( LGBT ) community", "Burbank, California", "1986", "2018", "rapid destruction of the donor red blood cells by host antibodies", "1623", "English author Rudyard Kipling", "March 16, 2018", "Fusajiro Yamauchi", "to get rich and escape the cycles of poverty and abuse on the reservation", "2013", "the breast or lower chest", "buchin galin", "Flex Data Services", "2018", "Saint Peter", "1963", "August 19, 2016", "Madison", "John Adams", "ABC", "commander of the 2nd U.S. Cavalry Regiment", "glockenspiel", "alaskan", "Hercules", "Elbow", "Dundalk", "National Collegiate Athletic Association (NCAA) Division II", "the Airbus A330-200", "50", "he never swore in front of women.", "porto", "gravity", "a cat", "Yemen,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5785371113280177}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true], "QA-F1": [0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.7692307692307692, 1.0, 1.0, 0.7710843373493976, 0.0, 0.25, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6, 1.0, 1.0, 0.14285714285714285, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-1395", "mrqa_naturalquestions-validation-9521", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2421", "mrqa_triviaqa-validation-4319", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-5829", "mrqa_newsqa-validation-4136", "mrqa_searchqa-validation-7915", "mrqa_searchqa-validation-2328"], "SR": 0.484375, "CSR": 0.506859756097561, "EFR": 0.9393939393939394, "Overall": 0.6789382390983001}, {"timecode": 82, "before_eval_results": {"predictions": ["Jon Stewart", "henry i", "Charlie Brooks", "At night they fly,", "Kirk Douglas", "William Shakespeare", "Christmas", "African violet", "Rod Stewart", "Gerald Ford", "a wood wind musical instrument of low pitch", "Pembrokeshire coast park", "Imola", "South Africa", "sows", "The Persistence of Memory", "orangutan", "The Time Machine", "Uranus", "Tacitus", "Lady Gaga", "Mecca", "cirrus uncinus", "Ukraine", "myxomatosis", "pakistan fforde", "Japan", "xerophyte", "Blur", "The King and I", "The Last King of Scotland", "jaws", "Pearson PLC", "John Steinbeck", "The Bulletin", "the violin", "Ross Bagdasarian", "Mark Hamill", "Sam Smith", "Burma", "a day commemorating the massacre of 19 landless farmers in Brazil who were demanding land and justice in 1996", "cryonics", "Queen Elizabeth II", "Another Day in Paradise", "decorate", "Vienna", "Department of Justice", "Australia", "wakefulness", "J. S. Bach", "Corfu", "Walter Brennan", "a solitary figure who is not understood by others, but is actually wise", "Continental drift", "Charles White Whittlesey", "December 31, 2015", "White Knights of the Ku Klux Klan", "about 62,000", "fill a million sandbags and place 700,000 around our city,\"", "Judge Herman Thomas", "a clock cycle", "\"Poland is Not Dead as Long as We are Living,\"", "a Font", "1951"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6257440476190477}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-6798", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-5262", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-6450", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-5492", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4711", "mrqa_newsqa-validation-14", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-2044", "mrqa_hotpotqa-validation-5551"], "SR": 0.546875, "CSR": 0.5073418674698795, "EFR": 1.0, "Overall": 0.6911558734939758}, {"timecode": 83, "before_eval_results": {"predictions": ["Benghazi", "Syriza", "a whitsunday", "jrigley", "james Blake", "philippine flinstone", "6-1", "Charles Taylor", "palm sunday", "dollar", "The Wicker Man", "philippine", "endgame", "a lion", "Peter Nichols", "bear grylls", "Count Basie", "vostok", "philippine", "vindictive", "vatican city", "Pensacola, Florida", "whitsunday", "Michael Hordern", "Gerald Durrell", "ishmael", "philippine raurica", "a map", "a tank driver", "vampish", "henatius", "James Van Allen", "owls", "Bulls Eye", "south africa", "boots", "Helen Gurley Brown", "philippine", "king of the Apes", "philippine", "Massachusetts", "douglas", "Hamlet", "madonna", "mad", "vatican islands", "whitsunday", "Rock Follies", "philippine islands", "ann Darrow", "madonna", "1996", "heavy use of shredded cheese, meat ( particularly beef and pork ), beans, peppers and spices, in addition to flour tortillas", "16 June", "Squam Lake", "3D computer-animated comedy", "1898 - December 23, 1946", "\"The Ministry of Defense said the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.\"", "The Impeccable", "Justicialist Party, or PJ by its Spanish acronym", "Ming Dynasty", "Prince Albert", "a crossword clue", "al Qaeda."], "metric_results": {"EM": 0.34375, "QA-F1": 0.43285281217750254}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8947368421052632, 1.0, 0.9411764705882353, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-50", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-5095", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-5103", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-1765", "mrqa_triviaqa-validation-2611", "mrqa_triviaqa-validation-6733", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-3264", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-925", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-722", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-352", "mrqa_triviaqa-validation-2857", "mrqa_triviaqa-validation-7713", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-4848", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-5596", "mrqa_hotpotqa-validation-4407", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-3703", "mrqa_searchqa-validation-581", "mrqa_searchqa-validation-6285"], "SR": 0.34375, "CSR": 0.5053943452380952, "EFR": 1.0, "Overall": 0.690766369047619}, {"timecode": 84, "before_eval_results": {"predictions": ["ganges", "david Hilbert", "Halifax", "florida", "Q", "Franklin Delano Roosevelt", "Buncefield", "hydrogen", "coffee", "cobanga leela", "lord Asquith", "cimarron", "david fincher", "florida", "roald smith", "Jupiter", "david hartman", "Nouakchott", "helen", "verona", "once every two weeks", "crystal gayle", "noah", "neaboth", "roald budge", "queen and prince albert", "quentin tarantino", "lord of city", "the Forum", "rowing", "ouwerks", "gin", "supertramp", "ireland", "hydrochloric acid", "Jackie Kennedy", "blue", "calcium carbonate", "cobble", "cuba", "lorraine", "Nicola Adams", "football league", "Andes", "Essex Eagles", "carry On Cleo", "American History X", "endometriosis", "music", "Brighton", "el Loco", "approximately 26,000 years", "Marty Stuart", "Norway", "Perdita", "Shut Up", "Tottenham Hotspur", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "attempted murder,", "Ma Khin Khin Leh,", "Nintendo Entertainment System (NES)", "Greer", "Jacob and Esau", "Surrey"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6084077380952382}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.4, 0.6666666666666666, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3183", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7646", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-3125", "mrqa_triviaqa-validation-7251", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-4672", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-4587", "mrqa_naturalquestions-validation-10049", "mrqa_hotpotqa-validation-3085", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-729", "mrqa_searchqa-validation-4541", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-14852"], "SR": 0.515625, "CSR": 0.5055147058823529, "EFR": 0.967741935483871, "Overall": 0.6843388282732448}, {"timecode": 85, "before_eval_results": {"predictions": ["Byzantium", "teacher", "shaft", "parabola", "jets", "adle of civilization", "canc\u00fan", "back", "Rudyard Kipling", "won the most Oscars Walt Disney", "The Life and Opinions of Tristram Shandy", "vincenzo Nibali", "200", "scorpion", "pram", "lexis", "c Cyprus", "sheep", "laos", "Toilet Lid Lock", "Andes Mountains", "georgie sand", "18", "bonham", "shepherd neame", "the shoulder", "severn", "legs", "le Blount Lean", "Saturday night and Sunday morning", "afterlife", "on the first Monday of September", "1982", "beberia", "Danish", "priesthood", "Pablo Escobar", "South Africa", "Microsoft", "b Bolivia", "bonaparte", "secretary", "Apocalypse Now", "Judy Garland", "Amnesty International", "rollet", "the Treaty of Waitangi", "portugal", "Renzo Piano", "10", "florence", "before the first year begins", "2,579", "Hold On", "1919", "\"Apatosaurus\"", "la Scala, Milan", "Virgin America", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "police", "Daredevil", "Dr. George Washington Carver", "panda", "California, Texas and Florida,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.613219246031746}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5178", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-3394", "mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-4290", "mrqa_triviaqa-validation-193", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-3319", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-3109", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-3040", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-688", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1317", "mrqa_naturalquestions-validation-5465", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-237", "mrqa_newsqa-validation-2338"], "SR": 0.546875, "CSR": 0.5059956395348837, "EFR": 0.896551724137931, "Overall": 0.6701969727345629}, {"timecode": 86, "before_eval_results": {"predictions": ["keeping malls safe", "money or other discreet aid", "41,", "adidas", "kite boards", "a suicide bombing", "iCloud service", "true", "South Africa's", "shot in the head", "at a house party in Crandon, Wisconsin,", "Kenneth Cole", "$17,000", "183", "\"Dwayne Carter, as he is known legally, pleaded guilty to felony gun charges in a deal with prosecutors October 2009.", "urged NATO to take a more active role in countering the spread of the", "School-age girls", "The truee Bagosora", "\"The Lost Symbol\"", "haitians", "54", "German authorities", "his brother to surrender.", "Roy Foster", "Mogadishu", "the \"face of the peace initiative has been attacked.\"", "16", "Michael Jackson", "fighting charges of Nazi war crimes", "Big Brother", "argentina", "amyotrophic Lateral Sclerosis", "public-television", "\"Leave this Town\"", "joesworld.org.", "Michael Partain,", "an empty water bottle down the touchline", "Daniel Wozniak", "stole the personal credit information of thousands of unsuspecting American and European consumers,", "human rights abuses against ethnic Somalis by rebels and Ethiopian troops are rampant.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "bathing suit", "five female pastors", "Facebook and Google,", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "NATO's International Security Assistance Force", "his death cast a shadow over festivities ahead of South Africa's highly- anticipated appearance in the rugby World Cup final with England.", "a U.S. military helicopter", "mental health", "Dr. Death in Germany", "abusing its dominant position in the computer processing unit (CPU) market.", "Thomas Edison", "Randy", "Thomas Lennon", "true", "Brooklyn", "mariette", "Boston Celtics", "Australian", "Northwestern Hawaiian Islands", "florida", "a s'more", "seine", "Mary Tyler Moore Show"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5198501299232682}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.21276595744680848, 0.9166666666666666, 0.04761904761904762, 0.0, 0.5, 1.0, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-227", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-3913", "mrqa_naturalquestions-validation-10724", "mrqa_triviaqa-validation-6716", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-4625", "mrqa_searchqa-validation-14248"], "SR": 0.453125, "CSR": 0.5053879310344828, "EFR": 1.0, "Overall": 0.6907650862068965}, {"timecode": 87, "before_eval_results": {"predictions": ["four", "yellow", "whooping cough", "Kawasaki", "harrison ford", "Reservoir", "the equator", "Wimbledon Lawn Tennis Championship", "\"Sugar Baby Love\"", "1986", "Bernardo Bertolucci", "\u201cThe Seven Year Itch\u201d", "Dieppe raid", "Nile River", "spinach", "la Boh\u00e8me", "Nicky Morgan", "midsomer Murders", "joe Tribbiani", "Abraham", "Aquaman", "American Civil War", "Christian Louboutin", "watling street", "domestic chicken", "Mexican Orange Blossom", "herpes zoster", "queen's coffin", "indonesian rupiah", "bonita Melody Lysette", "Charles II", "Illinois", "danelaw", "\"Go\",", "birdsville", "Christine Keeler,", "Silver Hatch", "watchmaker", "Guatemala", "clogs", "\"ABC from 1971 to 1973", "Dolores", "edwina Currie", "Baton Rouge", "WarsawWarsaw", "2010", "Carole King", "drizzle", "casualty", "bradgaged his constituency home", "ANNIE", "Telma Hopkins, Joyce Vincent Wilson and her sister Pamela Vincent", "17th Century", "Milira", "Wiltshire, in the south west of England", "Austrian Volksbanks", "1848 to 1852", "head of Gabonese capital of Libreville,", "Turkish President Abdullah Gul,", "Sonia Sotomayor", "the abacus", "USS Maine", "the Marquis de Lafayette", "The Osmonds"], "metric_results": {"EM": 0.5, "QA-F1": 0.5398143523143523}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.0, 0.1818181818181818, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2178", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-1851", "mrqa_triviaqa-validation-5185", "mrqa_triviaqa-validation-1047", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-2756", "mrqa_triviaqa-validation-985", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-7150", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-285", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-4563", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-5487", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-720", "mrqa_searchqa-validation-11553"], "SR": 0.5, "CSR": 0.5053267045454546, "EFR": 1.0, "Overall": 0.6907528409090908}, {"timecode": 88, "before_eval_results": {"predictions": ["a canterbury", "the moolelo", "Pat Paulsen", "Mr. Bennett and Mrs. Brown", "Arabian Peninsula", "casino", "Mensheviks", "Prada", "edward", "a hatchet", "'Tis brief, my lord", "baboon", "Chicken Little", "Bach", "bangkok", "Eli Whitney", "john smith", "James Buchanan Eads", "A Bug's Life", "race car", "a quiver", "the joker", "1972", "Mussolini", "a sheepshank", "Robert Burns", "eony", "Jack Nicklaus", "pen", "Las Vegas", "lipoprotein (LDL)", "poppy", "portrait", "canterbury", "The Pursuit of Happyness", "Nickelback", "succotash", "jack London", "Falklands", "acetone", "a Space Odyssey", "adultery", "frankfurter", "roanoke", "Blackbeard", "smith", "Borden", "volcanic eruptions", "Amish", "a dachshund", "Robert Frost", "Virginia Dare", "Einar Bj\u00f8rndalen", "the first quarter of the 19th century", "George Washington", "spanish", "David Graham", "1987", "Jacobite", "Leinster", "a tenement in the Mumbai suburb of Chembur,", "Monday's", "an initial report outlining its findings and recommendations in about 100 days.", "sweeney agonistes"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5645833333333333}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-10484", "mrqa_searchqa-validation-2857", "mrqa_searchqa-validation-16628", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-14289", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-5949", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-2948", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-12327", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-2110", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-5816", "mrqa_naturalquestions-validation-1538", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-3013", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-220", "mrqa_newsqa-validation-3631", "mrqa_triviaqa-validation-813"], "SR": 0.546875, "CSR": 0.5057935393258427, "EFR": 1.0, "Overall": 0.6908462078651685}, {"timecode": 89, "before_eval_results": {"predictions": ["Bill Bryson", "Pink Panther", "Jordan", "Scandinavia", "Jean-Paul Sartre", "Motown", "Jean Henri Bernadotte", "Mars", "riyadh", "Margot fonteyn", "Kay Adams", "plutocracy", "block", "ringway", "Radio 4 Extra", "purses", "violin", "U2", "port of Barcelona", "australia", "auk", "a barrier", "can be reached through non-stop flights by this busy regional airline", "soybean", "George Best", "one of the best stories I have ever heard about a focus group that actually worked in the director\u2019s favour is in relation to the 1981 movie Frank and Cindy", "Jean-Paul Gaultier", "Red Rock West", "the x-coordinate or abscissa", "Zagreb", "handley page", "Marine One", "Zachary Taylor", "edward hanfstaengl", "saints\u2019 day", "Who\\'s the cat that won't cop out when there\\'s danger all about", "boriken", "Louis Le Vau", "Scotland", "libyans", "jubilee line", "Abbey Theatre", "Maine", "clefts", "canterbury", "Denver", "week of June 14th", "Mel Blanc", "Lily Allen", "an al-Qaeda\u2013inspired terrorist cell", "oats", "Wisconsin", "the season seven finale", "Whig candidates William Henry Harrison ( the `` hero of Tippecanoe '' ) and John Tyler", "more than 230", "Serie B", "Mark O'Connor", "colombia", "a federal judge in Mississippi", "telling CNN his comments had been taken out of context.", "Ladd-Franklin", "the Untouchables", "to eliminate", "Tyne Daly"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5951593137254902}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8235294117647058, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-5922", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-3265", "mrqa_triviaqa-validation-6682", "mrqa_triviaqa-validation-6744", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7104", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-484", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-7107", "mrqa_triviaqa-validation-3771", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1265", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-7737", "mrqa_hotpotqa-validation-87", "mrqa_newsqa-validation-3566", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-9329"], "SR": 0.53125, "CSR": 0.5060763888888888, "EFR": 1.0, "Overall": 0.6909027777777778}, {"timecode": 90, "UKR": 0.669921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.8125, "KG": 0.4671875, "before_eval_results": {"predictions": ["a meteoroid", "colleen McCullough", "The Lion King", "cyprus", "o.J. Simpson", "sprite", "russia", "dove", "giraffe", "the festival", "Conservative", "venus", "bones", "south", "colleen McCullough", "america", "(Arrigo) Boito", "d Drew Carey", "three Mile Island", "island of palermo", "sunset bou", "the Bombe", "brussels", "arrows", "The Quatermass Experiment", "spaghetti harvest", "Frogmore estate or Gardens", "hitler", "Caucasus", "88", "hitler", "june", "iceland", "david hilbert", "mediterranean", "the Declaration of Independence", "jean", "fish", "venus", "dachau", "Robert Schumann", "whisky galore", "june slick", "Michael Caine", "america", "boston", "1929", "The Lone Gunmen", "benny hill", "the Daily Herald", "pj harvey", "Brian Steele", "a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "Philadelphia, which is Greek for brotherly love", "the Cherokee River", "Benedict of Nursia", "Wichita", "june", "Friday,", "put him in \"solitary confinement.", "a dove", "method acting", "Annie Proulx", "\"Nude, Green Leaves and Bust,\" or \"Nu au Plateau de Sculpteur,\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.5951822916666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.625]}}, "before_error_ids": ["mrqa_triviaqa-validation-5508", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-5925", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-7187", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-4815", "mrqa_triviaqa-validation-6049", "mrqa_triviaqa-validation-3038", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-219", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-7098", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4932", "mrqa_triviaqa-validation-1956", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-508", "mrqa_newsqa-validation-901"], "SR": 0.546875, "CSR": 0.5065247252747253, "EFR": 1.0, "Overall": 0.6912268200549451}, {"timecode": 91, "before_eval_results": {"predictions": ["\"evangelicalism\u2019s flagship magazine\"; \"The New York Times\"", "2011", "John McClane", "marika Nicolette Green", "Princeton University", "conservative", "Milan", "writer", "elton john", "Newcastle upon Tyne, England", "(Panther of Berlin)", "Blackwood Partners Management Corporation", "1958", "2007", "robot overlords", "1776", "town in Grafton County", "public house", "1944", "Austria", "Ron Cowen and Daniel Lipman", "The Soloist", "indoor", "new york", "January 30, 1930", "biochemist and academic Dr. Alberto Taquini", "John Gotti", "Anderson Silva", "the north of Argentina", "h Harrison Ford", "C. H. Greenblatt", "Taoiseach of Ireland", "Dissection", "The Killer", "Rockingham", "Ken Rutherford and Pakistan by Javed Miandad", "Dorothy", "2017", "people working in film and the performing arts", "June 2, 2008", "\"The 8th Habit\"", "one", "London", "new Zealand", "\"Ready Player One\"", "the 1981 World Rowing Championships", "1989", "15,023", "the North Atlantic Conference", "the highland regions of Scotland", "Phelan Beale", "March 2018", "Bay of Plenty", "19", "flybe", "perry michael", "Oliver Goldsmith", "Afghan lawmakers", "suzan Hubbard, director of the Division of Adult Institutions,", "al-Shabaab", "george slick", "Jason Bourne", "DECA", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6418402777777777}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-4208", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-4040", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-3966", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-3250", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-3737", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-61", "mrqa_newsqa-validation-3713", "mrqa_searchqa-validation-760", "mrqa_searchqa-validation-9994", "mrqa_newsqa-validation-719"], "SR": 0.578125, "CSR": 0.5073029891304348, "EFR": 1.0, "Overall": 0.6913824728260869}, {"timecode": 92, "before_eval_results": {"predictions": ["Margery Williams", "the capital of the Socialist Republic of Vietnam", "Terry Richardson", "being one of the youngest publicly documented people to be identified as transgender", "the Matildas", "Odense Boldklub", "SpongeBob SquarePants 4-D", "Oldham County", "Grammar, logic, and rhetoric", "The Wright brothers, Orville (August 19, 1871 \u2013 January 30, 1948) and Wilbur", "a research university", "O.T. Genasis", "science fiction", "Speedway World Championship", "Citric acid", "About 200", "moth", "Gerald Hatten Buss", "Delacorte Press", "close range combat", "twice", "Eli Roth", "Adelaide", "Lincoln Riley", "December 13, 1920", "The Fast and the Furious", "John McClane", "rural", "Orchard Central", "Art of Dying", "The Book of Judges", "Ant Timpson, Ted Geoghegan and Tim League", "on the Bahamian island of Great Exuma", "John Ford", "classical", "Marvel Comics", "between 7,500 and 40,000", "for crafting and voting on legislation", "\"The Big Book of Jewish Humor\"", "Nevada", "Yasir Hussain", "Victoria", "Jennifer Aniston", "the eastern terminus of the Ronkonkoma Branch", "Universal Orlando Resort", "Christmas Day, December 25, 2009", "the Hanna-Barbera show \"Birdman and the Galaxy Trio.\"", "his son", "\"Die Nacht \" (\"The Night\")", "6th congressional district", "NBA 2K16", "Little Mo", "Professor Eobard Thawne", "India", "one", "beetle", "Australia", "a fan", "Kearny, New Jersey", "flight delays", "pasta", "Tennessee Williams", "Illinois", "Femina Vie Heureuse Prize"], "metric_results": {"EM": 0.578125, "QA-F1": 0.659449226760082}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.3157894736842105, 0.0, 1.0, 1.0, 0.8, 1.0, 0.2857142857142857, 1.0, 1.0, 0.8, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-1783", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-41", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-1445", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-5501", "mrqa_hotpotqa-validation-4979", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-4735", "mrqa_triviaqa-validation-6079", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-8642", "mrqa_triviaqa-validation-4848"], "SR": 0.578125, "CSR": 0.5080645161290323, "EFR": 1.0, "Overall": 0.6915347782258064}, {"timecode": 93, "before_eval_results": {"predictions": ["YIVO", "Archbishop of Canterbury", "Samuel Beckett", "March 17, 1941", "The Catholic Church in Ireland", "close range combat", "Iran", "Kate Millett", "Tim Howard", "\"Lucky\"", "Do Kyung-soo", "John Hunt", "Kongo language", "William Finn", "Sam Raimi", "The seventeenth edition of the IAAF World Championships", "Klemzig is a suburb of Adelaide in the City of Port Adelaide Enfield", "\"personal earnings\" (such as salary and wages)", "Marine Corps Air Station Kaneohe Bay", "August 17, 2017", "Montagues and Capulets", "Walter R\u00f6hrl", "his left hand", "Vladimir Menshov", "erotic thriller", "Denmark and Norway", "Love and Theft", "C. W. Grafton", "Evey", "My Love from the Star", "143,372", "Robert Noyce", "Cold Spring, New York", "Afghanistan", "Operation Julin", "guitar feedback", "Flushed Away", "George Washington Bridge", "Reginald Engelbach", "Van Diemen's Land", "Tampa", "Sergeant First Class", "140 million", "SpongeBob SquarePants 4-D", "StubHub Center", "Argentinian", "Americas", "Maine, published six days per week in Bangor, Maine", "1998", "The More", "John F. Kennedy Jr.", "com TLD", "Andrew Lloyd Webber", "John Bull", "NASCAR", "The Avengers", "Rick Wakeman", "mental health", "Madhav Kumar Nepal", "hardship", "Berlin", "William Golding", "The Odd Couple", "Americans who served in the armed forces and as civilians during World War II"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7801259686950477}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.16666666666666666, 0.16666666666666669, 0.7368421052631579, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-5676", "mrqa_hotpotqa-validation-5173", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-517", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-4776", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4235", "mrqa_naturalquestions-validation-4844", "mrqa_newsqa-validation-1063"], "SR": 0.703125, "CSR": 0.5101396276595744, "EFR": 1.0, "Overall": 0.6919498005319149}, {"timecode": 94, "before_eval_results": {"predictions": ["Nutbush", "charles", "6", "golf", "Sharbat Gula", "Manchester", "southampton", "Bleak House", "Vienna", "Harry S. Truman", "new york", "furrow", "Frank Chin", "The Great Gatsby", "charlie Chan", "1664", "Good Will Hunting", "dukedom", "Iain Duncan Smith", "engraver", "orwell", "Jim Peters", "oxygen", "pipes", "Delilah", "Infante", "cuckoo", "PPTH", "The Wicker Man", "red", "Canada", "verdi", "guardian", "john Huston", "The Passenger Pigeon", "Anne Frank", "Spain", "bromo", "pi\u00f1a colada", "Fauntleroy", "chicken", "Petula Clark", "Dr Tamseel", "Flo Rida", "The Comedy of Errors", "sake", "chemical origins of life", "Finland", "sugar", "stuffing", "Kempton Park", "Cress", "Vesta's fire and the sun as sources of life", "The Michael Scott Paper Co.", "Tiffany's", "2010 to 2012", "Nathan Bedford Forrest", "Friday,", "Six", "\"Americans always believe things are better in their own lives than in the rest of the country,\"", "tanning", "Tulane University", "The Hills Live", "Bactrian"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5795403079710144}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1739130434782609, 1.0, 0.6666666666666666, 0.0, 0.25]}}, "before_error_ids": ["mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-2951", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-5727", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-7529", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-7655", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2671", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9903", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1300", "mrqa_searchqa-validation-8477", "mrqa_searchqa-validation-5376", "mrqa_naturalquestions-validation-8046"], "SR": 0.5625, "CSR": 0.5106907894736843, "EFR": 0.9642857142857143, "Overall": 0.6849171757518797}, {"timecode": 95, "before_eval_results": {"predictions": ["the Korean War", "$1.5 million", "Fernando Caceres", "37", "opposition parties", "soldier", "the founder of the Turkish republic,", "Secretary of State", "his wife's name", "take a mammogram", "U.S. senators", "to put a lid on the marking of Ashura", "Alexey Pajitnov", "Spc. Megan Lynn Touma,", "regulators in the agency's Colorado office", "west African nation", "Leo Frank,", "Sri Lanka", "Johannesburg", "last year's Gaza campaign", "enormous suffering and massive displacement,\"", "heavy flannel or wool", "it is not something that has gotten lost,\"", "Pakistan's border with Afghanistan", "walk", "an independent homeland", "bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "longest domestic relay in Olympic history,", "Kim Jong Un", "Arthur E. Morgan III,", "billboards with an image of the burning World Trade Center", "adidas", "too many glass shards left by beer drinkers in the city center,", "E! News", "Cologne, Germany,", "that she was lured to a dorm and assaulted in a bathroom stall.", "70,000", "Brett thought it would be best if he resigned,\"", "Antioquia,", "the most important attacks on the church don't come from the outside,", "north Georgia", "East Java", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "billions of dollars", "between June 20 and July 20,", "Blagojevich", "attacked L.K. Chaudhary,", "Majid Movahedi,", "$1.45 billion", "Lindsay and Jonas", "billions of dollars in Chinese products each year,", "Windows Media Player 11", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Joe Pizzulo and Leeza Miller", "alonzo Church", "paisley", "Thumbelina", "three", "The Apple iPod+HP", "Lithuanian", "Wayne's World", "Krakauer", "Hemingway", "Rob Reiner"], "metric_results": {"EM": 0.5, "QA-F1": 0.6428545921922282}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.058823529411764705, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.9047619047619047, 0.6666666666666666, 1.0, 0.4, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1018", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-3916", "mrqa_newsqa-validation-1855", "mrqa_naturalquestions-validation-3851", "mrqa_triviaqa-validation-3391", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-318", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-10142"], "SR": 0.5, "CSR": 0.5105794270833333, "EFR": 0.96875, "Overall": 0.6857877604166667}, {"timecode": 96, "before_eval_results": {"predictions": ["UNICEF", "poppy production", "security breach", "urged NATO to take a more active role in countering the spread of the drug trade,", "situation of America wielding a big stick for the last eight years.\"", "Christopher Savoie", "prisoners at the South Dakota State Penitentiary", "Tuesday afternoon.", "Iowa,", "Dr. Jennifer Arnold and husband Bill Klein,", "Chinese", "New York City Mayor Michael Bloomberg", "1.2 million", "George Washington as that grim, old man on the dollar bill.", "\"Seventy-three percent of those questioned in a CNN/Opinion Research Corporation survey released Monday", "$250,000 for Rivers' charity: God's Love We Deliver.", "Flint, Michigan.", "FARC rebels.", "Mexico", "Larry Zeiger", "Alberto Espinoza Barron,", "independence from Britain", "Four Americans", "Mawise Gumba", "burned over 65 percent of his body", "Brian Smith", "2-1", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "Hank Moody", "April 2010.", "\"Nothing But Love\" comeback tour,", "Mandi Hamlin", "people look at the content of the speech, not just the delivery.", "Yemen", "al Fayed", "Kim Jong Il.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "managed to sell the concept to a buyer.", "Manny Pacquiao", "\"You take a bite of cheesecake and you think'should I be doing this?'", "a red minivan ran a red light and struck two vehicles at an intersection,", "Thabo Mbeki", "Bright Automotive, a small carmaker from Anderson, Indiana,", "Jeffrey Jamaleldine", "not get them to agree on a solution.", "Haiti,", "Salt Lake City,", "his business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court,", "hardship for terminally ill patients and their caregivers,", "nearly 28 years of rule.", "Hollywood", "V \u00d7 2", "ThonMaker", "parashah ( or parshah / p\u0251\u02d0r\u0283\u0259 / or parsha )", "Syria", "Benedict", "Alan Freed", "sexy Star", "March 31, 1944", "Dutch", "a dragon", "Marcus Garvey", "the zodiac", "obsessive-compulsive"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6121493172129299}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9166666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.0, 0.125, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.9166666666666666, 0.0, 1.0, 0.1, 1.0, 0.0, 0.4444444444444445, 0.6666666666666666, 0.11764705882352941, 1.0, 0.0, 0.3157894736842105, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-905", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-1138", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-3634", "mrqa_hotpotqa-validation-5312", "mrqa_searchqa-validation-2594", "mrqa_searchqa-validation-9869"], "SR": 0.484375, "CSR": 0.5103092783505154, "EFR": 1.0, "Overall": 0.6919837306701031}, {"timecode": 97, "before_eval_results": {"predictions": ["Johannes Gutenberg", "2018", "Exodus and Deuteronomy", "John Adams and Benjamin Franklin, as well as Jefferson's notes of changes made by Congress", "about the level of the third lumbar vertebra, or L3, at birth", "King Dasharatha", "Pakistan", "aributary of the Mississippi that flows separately into the Gulf of Mexico", "the United States, its NATO allies and others", "Rashida Jones", "close by the hip, and under the left shoulder", "a tropical desert climate, K\u00f6ppen classification Bwh, because of its location within the Northern desert belt", "Bolton Wanderers", "Season two", "Terry Reid", "1273.6 cm", "May 3, 2005", "43", "Rightly Guided Caliphs", "British Columbia, Canada", "Book of Revelation, often called the Revelation to John, the Apocalypse of John, The Revelation, or simply Revelation or Apocalypse, is a book of the New Testament that occupies a central place in Christian eschatology", "Pyeongchang County, Gangwon Province, South Korea", "a diffuse interstellar medium ( ISM ) of gas and dust", "desperation, with only a small chance of success and time running out on the clock", "1943", "Tokyo for the 2020 Summer Olympics", "Lituya Bay in Alaska", "San Jose, California", "Panzerkampfwagen VIII Maus ( `` Mouse '' )", "July 2, 1776", "to accomplish the objectives of the organization", "Domhnall Gleeson", "Simon Callow", "Florida", "Laura Jane Haddock", "May 2016", "Bacon", "1994", "Matthew Broderick", "senators", "origins of replication, in the genome", "the fourth quarter of the preceding year", "April 2016", "Michael Schumacher", "Massachusetts", "Latitude exerts little influence on the Venezuelan climate, but the altitude changes it dramatically, particularly the temperature, reaching values very different according to the presence of different thermal floors", "2015", "post translational modification", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "a writ of certiorari", "Jules Shear", "Senegal", "a component of paen\u012bnsula", "the head and neck", "Romeo Montague (Italian: \"Romeo and Juliet\" )", "De La Soul", "Delilah Rene", "July as part of the State Department's Foreign Relations of the United States series.\"", "a warning to those who deny human rights.", "$81,4705", "the Missouri River", "jade", "Frank Sinatra", "Long troop deployments"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6447475971874917}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.5, 0.0, 0.15384615384615383, 0.0, 1.0, 0.11764705882352941, 0.4444444444444445, 1.0, 0.2, 0.35294117647058826, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 0.4, 1.0, 0.1875, 1.0, 0.4210526315789474, 0.06153846153846154, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.14285714285714285, 0.7368421052631579, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-6687", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-9772", "mrqa_triviaqa-validation-2425", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1987", "mrqa_newsqa-validation-4199", "mrqa_searchqa-validation-4588", "mrqa_newsqa-validation-2892"], "SR": 0.53125, "CSR": 0.5105229591836735, "EFR": 0.9666666666666667, "Overall": 0.685359800170068}, {"timecode": 98, "before_eval_results": {"predictions": ["beads", "Deimos", "Lana Turner", "a Polaroid picture", "in the heart of Washington Heights", "June Carter (Cash)", "The owl and the Pussycat", "Dr. Quinn", "butter", "poison ivy", "Detroit Tigers", "bicycle", "edith wharton", "Liberia", "Rockabilly", "Buckingham Palace", "AARP", "Arturo Toscanini", "Bangladesh", "Saturn", "Nancy Pelosi", "a song written by Rob Parissi and recorded by the band Wild Cherry", "Dr Pepper", "misery", "an anglerfish", "in the areas of ordnance, explosives, propellants and other energetic materials", "Iowa", "taking and conveying away a person against his or her will", "Pope John Paul II", "a dictopier", "Syria", "(William) Inge", "the grain", "The Bean Sidhe", "Japan", "a propaganda technique to discourage people from voting by convincing them their vote doesn't", "a ballistic missile submarine", "Ambrose Bierce", "Walt Whitman", "frequency", "Macbeth", "the Colorado River", "run as Eisenhower's running mate on the Democratic ticket", "Tommy Franks", "Botswana", "Mousehunt", "the Dow Jones", "Malcolm X", "Vietnam", "a tuba", "croque monsieur", "Kyla Pratt", "Wisconsin", "March 11, 2018", "Spook", "horripilation", "a real deegypt", "1 August 1971", "Australia", "Bronwyn Kathleen Bishop", "Jonas", "Madhav Kumar Nepal", "Joe Jackson", "About 200"], "metric_results": {"EM": 0.625, "QA-F1": 0.6644345238095237}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-8500", "mrqa_searchqa-validation-14933", "mrqa_searchqa-validation-12674", "mrqa_searchqa-validation-8280", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-9104", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-14445", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13824", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-1625", "mrqa_searchqa-validation-2884", "mrqa_triviaqa-validation-475", "mrqa_triviaqa-validation-312", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1955"], "SR": 0.625, "CSR": 0.5116792929292929, "EFR": 1.0, "Overall": 0.6922577335858586}, {"timecode": 99, "UKR": 0.654296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1078", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3329", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3905", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8120", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-70", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12568", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-2884", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4848", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.826171875, "KG": 0.48984375, "before_eval_results": {"predictions": ["New Orleans", "what you did to that puppy", "Minnesota", "a basalt", "Japan", "muffins", "Profumo", "peripheral", "(Henry) Wadsworth", "Canton", "Hormel Foods", "a consonant", "Theodore", "Beaufort", "Roger Williams", "Niels Bohr", "the sun", "Moby Dick", "horror", "Surf's Up", "Scorpius", "cat", "Finding Nemo", "the International Space Station", "Shakira", "Candice Bergen", "a shark", "Ireland", "the Good Friday Agreement", "(Henry) Wadsworth", "Gauguin", "Mary Stuart", "bamboo", "Barbie", "a page", "Frank Sinatra", "Custer", "barney stinson", "March 18", "Marlee Matlin", "Ben-Hur: A Tale of the Christ", "Nomo", "Dan Rather", "KLM", "a Glycemic Index", "a private tutor", "elephants", "Arkansas", "U.S.", "a piccolo", "a tuba", "Jason Marsden", "1998", "Garfield Sobers", "Germany", "Jimmy Carter", "white", "Detroit", "the Troubles", "ARY", "South Africa", "Omar Bongo,", "commission, led by former U.S. Attorney Patrick Collins,", "the Islamic prophet Muhammad"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6598958333333332}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-4116", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-3907", "mrqa_searchqa-validation-3417", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-9926", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-3988", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-1713", "mrqa_searchqa-validation-2630", "mrqa_searchqa-validation-7236", "mrqa_searchqa-validation-9182", "mrqa_searchqa-validation-7727", "mrqa_triviaqa-validation-3439", "mrqa_hotpotqa-validation-2249", "mrqa_hotpotqa-validation-4869", "mrqa_newsqa-validation-2897", "mrqa_naturalquestions-validation-6637"], "SR": 0.578125, "CSR": 0.51234375, "EFR": 1.0, "Overall": 0.6965312499999999}]}