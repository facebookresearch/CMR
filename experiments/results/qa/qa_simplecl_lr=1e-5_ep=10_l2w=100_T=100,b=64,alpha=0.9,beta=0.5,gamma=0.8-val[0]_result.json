{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=1e-5_ep=10_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=100.0, gradient_accumulation_steps=1, kg_eval_freq=50, kg_eval_mode='metric', kr_eval_freq=50, kr_eval_mode='metric', learning_rate=1e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=100, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=1e-5_ep=10_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4050, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "Los Angeles Times", "the Broncos", "anticlines and synclines", "Bells Beach SurfClassic", "Paleoproterozoic", "the end itself", "1894", "Rhenus", "Atlantic", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "Shia", "Royal Ujazd\u00f3w Castle", "hard-to-fill", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "\u00a315\u2013100,000", "mid-Eocene", "infected corpses", "United Kingdom, Australia, Canada and the United States", "11", "forces", "2005", "chief electrician", "lower incomes", "everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "a shortage of male teachers", "Masovian Primeval Forest", "days, weeks and months", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "a squared integer", "Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "human", "Killer T cells", "Swedex GmbH & Co KG", "More than 1 million", "1998", "market", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Buddhism", "Matthew 16:18", "U.S.-flagged Maersk Alabama", "Rwanda", "revelry", "his health", "American Hugo Vihlen", "the outskirts of a small Southern town"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7790865384615384}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-9243", "mrqa_squad-validation-4289", "mrqa_squad-validation-9655", "mrqa_squad-validation-7763", "mrqa_squad-validation-7728", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-4274", "mrqa_squad-validation-7527", "mrqa_squad-validation-8014", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.734375, "CSR": 0.78125, "EFR": 0.6470588235294118, "Overall": 0.7141544117647058}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50%", "mountainous areas", "the coast of Denmark", "quantum mechanics", "75th birthday", "Distinguished Service Medal", "30", "Virgin Media", "destruction of Israel and the establishment of an Islamic state in Palestine", "locomotion", "each six months", "Japanese", "visitation of the Electorate of Saxony", "Mark Twain", "the Commission", "1085", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "presidential representative democratic republic", "the grace that \"goes before\" us", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two", "Arizona Cardinals", "1991", "Mercury/Gemini", "Isiah Bowman", "the poor", "100\u2013150", "John Elway", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "expected to become more integral within the health care system", "declare martial law", "a customs union", "the Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "Ronnie Wood and Brandon Block", "Documents", "the company's factory in Waterford City, Ireland", "nitrogen", "Christopher Nolan", "Agulhas Current Flow Rates", "six", "It always begins with the music", "conductor", "Illinois", "Rafael Palmeiro Corrales", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.8125, "QA-F1": 0.8241758241758241}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9600", "mrqa_squad-validation-1174", "mrqa_squad-validation-9896", "mrqa_squad-validation-235", "mrqa_squad-validation-3967", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936"], "SR": 0.8125, "CSR": 0.7916666666666666, "EFR": 0.5, "Overall": 0.6458333333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "buildings of an Eastern Bloc city", "Sakya", "monumental size", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "treasure", "near the surface", "northern China", "giving her brother Polynices a proper burial", "political figures", "President", "2000", "oxygen", "increase local producer prices by 20\u201325%", "Apollo 1 backup crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "the Edict of Nantes", "reserved to, and dealt with at, Westminster", "multiple revisions", "philanthropic initiative", "integer factorization", "necessity", "Isel", "adapted quickly and often married outside their immediate French communities", "U. S. Secretary of Housing and Urban Development", "Charles-Fer Ferdinand University", "drowned in the Mur River", "yellow fever outbreaks", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "energy stored in an H+ or hydrogen ion gradient", "the late 19th century", "the Channel Islands", "in no way", "Alberich", "9", "How Emeril Really Feels About the Word", "Churchill Downs", "The port of Terneuzen", "the regular polyhedrons", "The Archers: The village of vice", "India", "study insects and their relationship to humans", "the limbic system", "Allan Border", "George Fox", "Maryland", "Great Expectations", "24 hours a day and 7 days a week", "Sponsorship scandal", "\"Krabby Road\""], "metric_results": {"EM": 0.640625, "QA-F1": 0.7029885912698413}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.5, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-874", "mrqa_squad-validation-2597", "mrqa_squad-validation-801", "mrqa_squad-validation-9286", "mrqa_squad-validation-4293", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-392", "mrqa_squad-validation-3069", "mrqa_squad-validation-7240", "mrqa_squad-validation-1189", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-6229", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_hotpotqa-validation-3821"], "SR": 0.640625, "CSR": 0.75390625, "EFR": 0.6086956521739131, "Overall": 0.6813009510869565}, {"timecode": 4, "before_eval_results": {"predictions": ["in higher plants", "Parliament of Victoria", "Zaha Hadid", "the Marquis de Vaudreuil", "Science and Discovery", "the Army", "pedagogy", "the red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "in a number of stages", "The Skirmish of the Brick Church", "the port city of Kaffa in the Crimea", "Henry of Navarre", "reduced moist tropical vegetation cover", "wage or salary", "the Roman Catholic Church", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "12 May 1705", "apicomplexan-related diseases", "Academy of the Pavilion of the Star of Literature", "passenger space", "1639", "biostratigraphers", "the web", "the Song dynasty", "2010", "1606", "The Earth's mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "the Italian government", "22", "terror groups that they say were planning numerous suicide attacks, including in the country's largest city of Karachi", "it was a comment that shouldn't have been made and certainly one that he wished he didn't make", "Brian Smith", "a new model is simply out of their reach", "Muslim", "This will be the first time any version of the Magna Carta has ever gone up for auction", "\"a fantastic five episodes.\"", "15", "militants from Afghanistan", "Chesley \"Sully\" Sullenberger", "backbreaking labor", "CNN's Campbell Brown", "a woman who may have been contacted through a Craigslist ad", "one", "celebrity-inspired names", "$1,500", "National Industrial Recovery Act", "Travis", "Humberside Airport", "in the past few seasons"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7218298796791444}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3, 0.15999999999999998, 1.0, 1.0, 0.0, 0.32, 0.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-10247", "mrqa_squad-validation-4773", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-3733", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.703125, "CSR": 0.74375, "EFR": 0.7368421052631579, "Overall": 0.7402960526315789}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "the United States", "New York City", "Larry Ellison", "the Anglican tradition's Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "the Connectional Table", "Deformational", "a high-level marketing manager", "500,000", "Ofcom", "Scottish independence", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches", "2011", "algae", "part of a rule connected with civil disobedience", "June 1978", "Milton Latham", "1914", "the Philippines", "the Broncos", "the 1970s", "the characteristics of the conquering peoples", "German Te Deum", "1795", "Bermuda 419", "compressing and cooling", "Infinity Broadcasting Corporation", "semi-legal", "1972", "rudimentary", "1957", "mother-of-pearl", "Gene Barry", "President of the United States negotiates treaties with foreign nations", "It is typically found on laptops due to their keyboard size restrictions", "from an Ohio newspaper on 8 February 1925", "President since Woodrow Wilson, with the notable exception of Herbert Hoover, has made at least one State of the Union report as a speech delivered before a joint session of Congress", "the correct angle \u03b8 to aim so as to hit the target at the edge of the turntable", "Panning", "Justin Timberlake", "the following 15 countries or regions have reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "The total size of the peacekeeping force is 98,200 police, troops, and military experts", "unknown origin", "omitted and an additional panel stating the type of hazard ahead", "Lowe's opened its first three stores in Canada on December 10, 2007", "the speech, once given during the day, is now typically given in the evening, after 9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age", "most episodes feature a storyline taking place in the present ( 2016 -- 2018, contemporaneous with airing )", "Morgan Freeman reprising his role as detective Alex Cross", "David Gahan", "it includes a restaurant, spa, and bed - and - breakfast and provides guided tours which feature the history and alleged paranormal activity of the site", "long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "The day is the day before Ash Wednesday and usually falls between February 3 and March 9", "Jaipur", "Swedish Prime Minister Fredrik Reinfeldt", "torpedo boats", "Newport"], "metric_results": {"EM": 0.625, "QA-F1": 0.7103340804403211}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.1111111111111111, 0.0, 0.13793103448275862, 0.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.0, 0.0, 0.3636363636363636, 0.5, 0.0, 0.5833333333333334, 0.23529411764705882, 1.0, 0.20000000000000004, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10011", "mrqa_squad-validation-4836", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.625, "CSR": 0.7239583333333333, "EFR": 0.75, "Overall": 0.7369791666666666}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "more expensive", "an antigen from a pathogen", "their disastrous financial situation", "priest", "receptions, gatherings or exhibition purposes", "New England Patriots", "Charly", "Henry Cole", "steam turbines", "social and political action", "1936", "the New Birth", "gold", "a 3\u20130 lead", "Vivienne Westwood", "reduction", "disease", "TGIF", "Confucian propriety and ancestor veneration", "the rediscovery of \"Christ and His salvation\"", "five", "European Court of Justice and the highest national courts", "1888", "business", "BBC Radio 5 Live", "1876", "a chain or screw stoking mechanism", "#P", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548", "Joy", "teachers in publicly funded schools must be members in good standing with the college, and private schools may also require their teachers to be college peoples.", "end of the season", "10", "Jacob", "African-Americans", "will not support the Stop Online Piracy Act", "Chuck Bass", "always hot and humid and it rains almost every day of the year", "an animal tranquilizer", "in an interview Tuesday on CNN's \"Larry King Live.\"", "Stuttgart on Sunday", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "more than 170", "North Korea's reclusive leader Kim Jong- Il", "first five Potter films", "know what's important in life", "3 to 17", "two suicide bombers", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "James Whitehouse", "there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now", "a series of monthly meals for people with food allergies", "Zimbabwe", "2004", "Mohamed Alanssi", "Ludacris", "Mike Gatting", "Colgate University", "Church of Christ, Scientist", "unsaturated fats are comprised of lipids that contain?", "Luke 6 : 67 -- 71"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7277323932195257}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.5, 0.2666666666666667, 0.5, 0.11764705882352941, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.75]}}, "before_error_ids": ["mrqa_squad-validation-800", "mrqa_squad-validation-486", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.65625, "CSR": 0.7142857142857143, "EFR": 0.7272727272727273, "Overall": 0.7207792207792207}, {"timecode": 7, "before_eval_results": {"predictions": ["1970s", "his friendship", "increased trade with poor countries", "187 feet", "pH or available iron", "90\u00b0", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism in the Small Catechism", "Jim Gray", "unequal", "July 1969", "Hitler's secret police demanded to know if they were hiding a Jew in their house.", "a yellow chlorophyll precursor", "spontaneous", "the courts of member states and the Court of Justice of the European Union", "silver", "TARDIS", "Buckland Valley near Bright", "Scottish rivers", "\"Bricks for Warsaw\"", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "Belgrade", "\u00a320,427", "21 October 1512", "James O. McKinsey", "dance Your Ass Off", "\"Freshman Year\" experience", "India", "Zulfikar Ali Bhutto", "an unofficial strike that has dragged on more than a week at the country's third-largest oil refinery", "April 24 through May 2", "Krishna Rajaram", "early detection and helping other women cope with the disease", "250,000", "Timothy Masters", "homicide", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "12 hours", "from the capital, Dhaka, to their homes in Bhola", "Jared Polis", "William S. Cohen", "\"Dance Your Ass Off\"", "military trials for some Guant Bay detainees", "Gary Brooker", "Herman Cain", "9 a.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian", "a \"stressed and tired force\" made vulnerable by multiple deployments", "Japan", "condition", "British author J.G. Ballard, whose boyhood experience in a World War II internment camp became the novel and film \"Empire of the Sun", "Norman given name Robert", "stronger", "Matthew Ward Winer", "Wyatt Earp", "the Baltic Sea", "Mustique", "green"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6690536892087919}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.9032258064516129, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.3636363636363636, 0.0, 0.5, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.10256410256410256, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.2608695652173913, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7533", "mrqa_squad-validation-2448", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-479", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-417", "mrqa_newsqa-validation-3281", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-991", "mrqa_searchqa-validation-7977"], "SR": 0.609375, "CSR": 0.701171875, "EFR": 0.64, "Overall": 0.6705859375000001}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "The British provided medical treatment for the sick and wounded French soldiers", "Roman Catholic", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe", "Enric Miralles", "25-foot (7.6 m)", "eight", "Monday", "Journey's End", "immediate", "Levi's Stadium", "Wesleyan", "art posters", "Tsakhiagiin Elbegdorj", "Chinggis Khaan", "Einstein", "fast forwarding", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington Railway", "autonomy", "Islamic", "$12.9 million", "Fernando Gonzalez", "Graeme Smith", "a strong work ethic is the reason for his longevity in the movie business.", "finance", "terminal brain cancer", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "Employee Free Choice act", "took him to Brazil", "Animal Planet", "bleeding profusely", "there were no radar outages and said it had not lost contact with any planes", "54 bodies", "early detection and helping other women cope with the disease", "Diversity", "$250,000", "breaking up ice jams", "Nazi Germany", "March 27", "The Kirchners", "directly involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president was declared the winner", "2050", "Alfredo Astiz", "Abdullah Gul", "Carl Froch", "The Everglades, known as the River of Grass,", "The original chromosome and the copy are now called sister chromatids", "Gibraltar territory currently contains an 800 m long section of the isthmus that links the Rock with mainland Spain.", "New Orleans, Louisiana", "many investors paying huge sums for individual bulbs", "Captain Steven Hiller", "Give you now, it's gonna make a lot of noise.... a little restaurant and tea shop... so people could eat the cookies and drink tea.", "get the latest St. Louis Blues news, scores, stats, standings, rumors, videos, photos, injuries, transactions"], "metric_results": {"EM": 0.625, "QA-F1": 0.6796666568725392}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0909090909090909, 1.0, 0.2857142857142857, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.11764705882352941, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-1710", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839", "mrqa_searchqa-validation-9016"], "SR": 0.625, "CSR": 0.6927083333333333, "EFR": 0.5833333333333334, "Overall": 0.6380208333333333}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "The Book of Discipline", "His wife Katharina", "theology and philosophy", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Levi's Stadium", "General Sejm", "Derek Jacobi", "net force", "\"hoos\"", "30%\u201350%", "very badly disposed towards the French, and are entirely devoted to the English.", "United States", "CRISPR sequences", "six", "about 300 km long and up to 40 km wide", "1962", "free radical production", "Video On Demand", "issues related to the substance of the statement.", "Edict of Fontainebleau", "15", "\"Well, about time.\"", "Ronaldinho", "providing the basic securities that Turkey can be a great partner.", "an average of 25 percent", "a gym", "Jennifer Arnold and husband Bill Klein, who both have skeletal dysplasia, a bone-growth disorder that causes dwarfism,", "environmental and political events", "he fears a desperate country with a potential power vacuum that could lash out.", "two and a half hours", "Elin Nordegren", "Europe, Asia, Africa and the Middle East", "6,000", "cortisone", "President Clinton", "delivered three machine guns and two silencers to the hip-hop star", "Morgan Tsvangirai", "policing the world and Africa", "future relations between the Middle East and Washington", "in a canyon in the path of the blaze", "President Thabo Mbeki", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "posting a $1,725 bail", "school", "strife in Somalia", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia Police Department", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "the area was sealed off, so they did not know casualty figures.", "London", "after Shawn's kidnapping, Juliet goes to his apartment with Gus to search for clues", "the immediate physical and social setting in which people live or in which something happens or develops.", "William Tell", "OutKast", "Groundhog Day", "he really didn't mean t", "a singer who takes a job working with a struggling carnival."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6394722468139987}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.08, 0.4, 0.0, 0.5217391304347825, 1.0, 0.0, 0.8, 1.0, 0.2, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.923076923076923, 0.2, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5278", "mrqa_squad-validation-9194", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-2315", "mrqa_hotpotqa-validation-2679", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-11812"], "SR": 0.5625, "CSR": 0.6796875, "EFR": 0.42857142857142855, "Overall": 0.5541294642857143}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "the Cathedral of Saint John the Divine", "pseudorandom", "John Wesley", "Genghis Khan", "water", "internal strife", "yellow fever", "DC traction motor", "The Prince of P\u0142ock", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Lothar de Maizi\u00e8re", "premises of the hospital", "journalist", "Cam Newton", "over $40 million", "Super Bowl XXXIII", "the primary endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Theodor Fontane", "33", "chairman and CEO", "Brazil", "July 18, 1994", "broken pelvis", "issued his first military orders as leader of North Korea", "snow, sleet, freezing drizzle or rain", "Willem Dafoe", "Maude", "Phillip A. Myers", "general astonishment", "two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "58", "two Metro transit trains that crashed the day before, killing nine,", "last summer", "Christopher Savoie", "Lance Cpl. Maria Lauterbach", "Dangjin", "\"novel that you would be embarrassed to buy\"", "Chinese President Hu Jintao", "magazine, GospelToday", "burns over about two-thirds of his body, according to the hospital's associate director, Dr. Carl Schulman.", "October 3,", "Adriano", "Larry Zeiger", "shock, quickly followed by speculation about what was going to happen next.", "President Bush", "Jeffrey Jamaleldine", "35,000", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "Sunday", "lightning strikes", "Bill Stanton", "American Airlines are true or not doesn't really matter", "16 August 1975", "Bonnie Aarons", "one", "kabinett - Wines made with fully ripened grapes.", "Lionsgate.", "James Lofton", "meditation", "hair-like structures that help paramecium move around."], "metric_results": {"EM": 0.625, "QA-F1": 0.6863873106060607}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.36363636363636365, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7230", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1947", "mrqa_triviaqa-validation-1100", "mrqa_triviaqa-validation-7134", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.625, "CSR": 0.6747159090909092, "EFR": 0.75, "Overall": 0.7123579545454546}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "hermaphroditism and early reproduction", "Victoria Department of Education", "transported to the Manhattan Storage and Warehouse Company under the Office of Alien Property (OAP) seal. John G. Trump", "Manned Spacecraft Center", "economic inequality", "refusing to make a commitment", "use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets, and delivery of these messages by store and forward switching.", "Elway", "Philo of Byzantium", "36 acres", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "Euler's totient function", "a better relevant income", "Redwood City, California", "400 m wide", "Netherlands", "David Copperfield", "The Soup Dragon", "antelope", "nipples", "Triassic Periods", "pio-  neers' Society, Ltd.", "Anastasia Dobromyslova", "Lady Gaga", "9", "Blake Griffin", "radish", "Robert Ludlum", "a great power", "(.mov) or (.avi)", "The Donington Grand Prix Collection is, quite simply, the largest showcase of Grand Prix racing cars in the world", "Saturday Night Live", "Hebrew", "London Underground Piccadilly Line", "Wisconsin", "orangutan", "Manet", "The Magic Finger", "Massachusetts", "2005", "1969", "Dodge", "Benny Hill", "Venice", "petticoat", "Enrico Caruso", "Elizabeth Arden", "collapsible support assembly", "Sir Hardy Amies", "Liechtenstein", "son of Edward", "Rob Davis", "Cody Miller", "Brown Square Station", "Nazi Germany", "Golden Gate Yacht Club of San Francisco", "Roger Vivier", "Jamaica", "Buddhism"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6623047940534643}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.23076923076923078, 1.0, 0.8, 1.0, 0.7234042553191489, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-7320", "mrqa_squad-validation-4890", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983"], "SR": 0.59375, "CSR": 0.66796875, "EFR": 0.6153846153846154, "Overall": 0.6416766826923077}, {"timecode": 12, "before_eval_results": {"predictions": ["Southern Border Region", "70-50's", "Panini", "new laws or amendments to existing laws as a bill; a committee of the Parliament can present a bill in one of the areas under its remit", "anti-colonial movements", "Rhine Valley", "A", "by experience", "Zhongshu Sheng", "legitimate medical purpose", "\"so long as\" the EU works towards the democratisation of its institutions, and has a framework that protects fundamental human rights,", "1788", "2006", "Roman Catholic", "Henry of Navarre", "John Wesley", "because the nationalisation law was from 1962, and the treaty was in force from 1958,", "Eternal Heaven", "Lowestoft", "John Mayer", "Sue Ryder", "Val Doonican", "Virgil", "France", "T.S. Eliot", "Eric Pickles", "Sir Hugo Drax", "Vladivostok", "Sheryl Crow", "Telstar", "Camellia sinensis", "AFC Wimbledon", "Charles Hawtrey", "Malaysia", "cosmology", "gin", "George Clooney", "Eric Coates", "James Chadwick", "\"No one was saved\"", "Monopoly", "champagne", "abundant rainfall lasting many thousands of years", "United States", "Brigit Forsyth", "Lord Melbourne", "Japan is a stratovolcanic archipelago of 6,852 islands.", "The History of Troilus and Cressida", "Thomas Edward Lawrence", "Kent", "Renoir\u00b4s", "Standard Motor Company", "white", "Switzerland", "gin", "people of the United States", "79", "ITV", "Scottish national team", "the death of a pregnant soldier", "Derek Mears", "Baden- Wrttemberg, Bavaria, Brandenburg, Bremen, Hamburg, Hesse, Lower Saxony, Mecklenburg-Vorpommern, North", "The Goat Amalthea", "\"Stagecoach\" (John Ford, 1939)"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6538392857142856}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-9452", "mrqa_squad-validation-6655", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_squad-validation-4590", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_searchqa-validation-8450", "mrqa_searchqa-validation-9647", "mrqa_newsqa-validation-3860"], "SR": 0.609375, "CSR": 0.6634615384615384, "EFR": 0.44, "Overall": 0.5517307692307692}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "Barnett Center", "entertainment", "Muhammad ibn Zakar\u012bya R\u0101zi", "Georgia", "articles 1 to 7", "the Black Death", "their own militia", "days, weeks and months", "Over 61", "quality of a country's institutions", "cilia", "friction", "Sky Digital", "2005", "force", "mustelids", "John Connally", "saffron  Saffron, produced from the stigma of the crocus flower, is easily the world's most expensive spice, costing $50 an ounce or more.", "god of fruitfulness", "Zeus", "a chemical", "Suez Canal", "Brigit Forsyth", "Call My Bluff", "March 10, 1997", "crazed Holiday creature Feature That's Good for More Than Just Scares", "The Battle of the Three Emperors", "Velazquez", "Arthur Ashe", "lizards have two lungs, unlike some snakes which have only one lung, and amphibians which breathe through their skins as well as through lungs.", "strong cold southwest wind", "table tennis", "the National Library of Medicine", "penhaligon", "Gandalf", "Sherlock Holmes", "Jinnah International Airport", "Monday", "capital of Venezuela (officially named Bolivarian Republic of Venezuela) is the city of Caracas.", "beads", "Pears soap", "highball", "Avro Lancaster", "Genesis", "Charlie Brooker", "tea that can calm, soothe and relax has great value in today\u2019s often hectic world.", "Harrods", "2007", "Christina Ricci", "Scarface", "pale yellow to golden", "Everest\u2019s aluminium-framed double glazing that turned draught proofing into a national obsession", "bubba", "June 12, 2018", "Filipino American", "London", "Lambic", "Nook", "Steven Green", "c) oh, my God!", "cUNY-CoLAG", "coder", "Synchronicity"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6039128880718954}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.07999999999999999, 0.4, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.0, 0.375, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-4908", "mrqa_squad-validation-6933", "mrqa_squad-validation-2920", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-3162", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-6628"], "SR": 0.53125, "CSR": 0.6540178571428572, "EFR": 0.7, "Overall": 0.6770089285714286}, {"timecode": 14, "before_eval_results": {"predictions": ["seven months old", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium, and the San Francisco Bay Area's Levi's Stadium", "the Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "sleep in peace", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "declining state of mind", "13 May 1899", "The Deadly Assassin and Mawdryn undead", "radioisotope thermoelectric generator", "Cody Fern", "Nicklaus", "Jim Gaffigan", "cat in the hat", "2020", "1974", "272", "1936", "Authority", "senior enlisted sailor", "Spanish moss", "Chinese cooking for over 400 years", "Vienna", "World Trade Center", "Kevin Spacey", "1 November", "78", "main type of cell found in lymph", "International Border", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha", "minor key", "Coppolas and, technically, the Farrow / Previn / Allens", "Chandan Shett", "metamorphic rock", "January 12, 2017", "United States", "claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection, irritation, or allergies", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "Palm Sunday celebrations", "vertebral column ( spine )", "three", "neecups", "long", "Kew Gardens", "Nikita Khrushchev", "$500,000", "young self-styled anarchists", "reaper", "shIELD", "the foyer of the BBC building in Glasgow, Scotland", "a thick stack of paper"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6454082375957375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.6666666666666666, 0.25, 0.8, 0.0, 0.5, 0.2857142857142857, 0.4, 0.5, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2339", "mrqa_squad-validation-1454", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_newsqa-validation-3571", "mrqa_searchqa-validation-726", "mrqa_newsqa-validation-1279"], "SR": 0.5625, "CSR": 0.6479166666666667, "EFR": 0.7142857142857143, "Overall": 0.6811011904761906}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "The Late Late Show", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "declare martial law and sent the state militia to maintain order", "Famous musicians", "CBS", "Jean Ribault", "Tetzel", "the Electorate of Saxony", "$414 million", "Necessity-based", "950 pesos ( approximately $ 18 )", "note number 60", "Seattle, Washington", "Battle of Antietam", "Nicolas Anelka", "In Time", "by the early 3rd century the cross had become so closely associated with Christ that Clement of Alexandria", "Glenn Close", "four times", "Agostino Bassi", "The first five seasons of Prison Break have been released on DVD and Blu - ray in Regions 1, 2, and 4", "Malibu, California", "the church at Philippi", "Dutch navy captain Jurriaen Aernoutsz", "September 2017", "Professor Kantorek", "1546", "Bud '' Bergstein", "Bhupendranath Dutt", "Grey Wardens", "Dr. Lexie Grey", "Majandra Delfino", "September 1972", "Uruguay defeated Argentina 4 -- 2 in front of a crowd of 68,346 people", "Alex Skuby", "Thomas Middleditch", "The National Legal Aid & Defender Association ( NLADA ) is the oldest and largest national, nonprofit membership organization devoted to advocating equal justice for all Americans", "Monk's Caf\u00e9", "domesticated sheep goes back to between 11000 and 9000 BC", "1970s", "Director of National Intelligence", "Defence Against the Dark Arts teacher", "Isaiah Amir Mustafa", "Julie Stichbury", "Saphira", "5.7 million", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Thespis", "Portugal. The Man", "John Coffey", "Rachel Kelly Tucker", "Bohemia", "earwigs", "Code 02PrettyPretty", "musician", "opposition group, also known as the \"red shirts,\"", "abduction of minors", "47 percent of all land in the West", "poetry", "kmart", "1881"], "metric_results": {"EM": 0.484375, "QA-F1": 0.571590141553135}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.10526315789473684, 1.0, 0.6666666666666666, 1.0, 0.09523809523809523, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.37037037037037035, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_squad-validation-551", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-5271", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-2756", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_searchqa-validation-13473", "mrqa_searchqa-validation-5103", "mrqa_hotpotqa-validation-1852"], "SR": 0.484375, "CSR": 0.6376953125, "EFR": 0.5454545454545454, "Overall": 0.5915749289772727}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "Arizona Cardinals", "Bert Bolin", "390 billion", "igneous, sedimentary, and metamorphic", "US", "six", "11", "hydrogen and helium", "Western Xia", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Vancouver", "Office", "SAVE", "SAS Fr\u00f6sundavik Office Building", "1993 to 2001", "1951", "NCAA Division I Football Bowl Subdivision", "Martin Truex Jr.", "Easter Rising of 1916", "45%", "more than two decades", "BAFTA TV Award Best Actor", "Jello Biafra drew on Nardwuar's face with a marker pen", "the 1745 rebellion of Charles Edward Stuart", "Burny Mattinson, David Michener, and the team of John Musker and Ron Clements", "Sir William McMahon", "North Sea", "7.63\u00d725mm Mauser", "Academy Award for Best Animated Feature", "the CAC/PAC JF-17 Thunder", "Delacorte Press", "Neighbourhoods", "Secretariat", "Wake Atoll", "Hydrogen vehicle", "Fort Valley, Georgia", "King of the Polish-Lithuanian Commonwealth", "\"Southern Living\" Reader's Choice Awards", "William Shakespeare", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Alticor", "Parlophone Records", "South Africa", "Surrey", "The Girl", "Charles Russell", "Boyd Gaming", "three different covers", "1991", "Glenn Close", "Florence Glenda Ballard", "Neighbours", "Ewan McGregor", "2011", "Alice Lloyd College", "a enslaved African American who led", "power-sharing talks", "Brown-Waite"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6667106331168831}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.3636363636363636, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4415", "mrqa_squad-validation-3667", "mrqa_squad-validation-8087", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655"], "SR": 0.59375, "CSR": 0.6351102941176471, "EFR": 0.7692307692307693, "Overall": 0.7021705316742082}, {"timecode": 17, "before_eval_results": {"predictions": ["forces", "theology and philosophy", "ITV", "University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese", "charter", "1830", "nonfunctional pseudogenes", "inner chloroplast membrane", "Charlie Harper", "steveland Hardaway Morris", "beaver", "La Boh\u00e8me", "formic acid", "\u00ea\u00bf\u00bdzar", "Zimbabwe", "Mr. Boddy", "Edward \"Ted\" Hankey", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "multi-user dungeon", "Mercury", "hound", "Plato", "Fuller's", "ship\u2019s hull", "Nick Hornby", "The Comedy of Errors", "Charles V", "the Isle of Man", "Lagertha", "weight plates", "big house", "Hadrian", "US", "flea", "Moonee Ponds, a suburb in Melbourne, Victoria", "Hamburg", "mulberry", "Tangled", "\"The French Connection\"", "CBS", "Manchester United (13), Chelsea (4), Arsenal (3), Manchester City (2), Blackburn Rovers and Leicester City (1)", "Robert Cromposer", "John Mayer", "Boy George", "Corsican Republic", "3000m", "Scotland", "Japan", "Travis Tritt and Marty Stuart", "Union", "New Jewel Movement", "sub-Saharan Africa", "U.S. 93", "Anjuna beach in Goa", "Lev Ivanov", "\"s h! du'y van", "two", "jeopardy/1870_Qs.txt", "\"The Sunday Thing\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.5951388888888889}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_naturalquestions-validation-767", "mrqa_newsqa-validation-2981", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-3198", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.546875, "CSR": 0.6302083333333333, "EFR": 0.5862068965517241, "Overall": 0.6082076149425286}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude", "1622", "extremely high", "Manakin Town", "northwest", "fewer than 10 employees", "Middle Miocene", "magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "balance sheet", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "2010", "Coppolas and, technically, the Farrow / Previn / Allens", "Anna Faris", "mainland greece", "ability to comprehend and formulate language", "Splodgenessabounds", "Tyrion", "electron donors", "Laura Jane Haddock", "1985", "19 state rooms", "Solange Knowles & Destiny's Child", "Gupta Empire", "December 2, 1942", "Alice", "20 November 1989", "Coton in the Elms", "55 -- 69 %", "The Vamps, McGregor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "1995", "Identification of alternative plans / policies", "16 August 1975", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Killer Within", "iron ore production in Western Australia, and Australia as a whole, was negligible, in the range of less than 10 million tons a year", "coronary arteries", "July 21, 1861", "Dr. Addison Montgomery", "capital and financial markets", "an optional message body", "on the lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "a violation of nature", "September 2017", "moral", "Rising Sun Blues", "Part 2", "Dumbo", "the duke of Monmouth\u2019s rebellion", "Christian", "Robert L. Stone", "2008", "Yemen", "jegna", "Robert Langdon", "ABC1 and ABC2", "NBA 2K16", "mistress of the Robes"], "metric_results": {"EM": 0.609375, "QA-F1": 0.687823430654313}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 0.0, 0.2857142857142857, 0.0, 0.0, 0.8, 1.0, 0.19999999999999998, 1.0, 1.0, 0.6666666666666666, 0.4, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6242", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_searchqa-validation-7111", "mrqa_hotpotqa-validation-4735"], "SR": 0.609375, "CSR": 0.6291118421052632, "EFR": 0.48, "Overall": 0.5545559210526316}, {"timecode": 19, "before_eval_results": {"predictions": ["everything that is used to work sorrow over sin is called the law, even if it is Christ's life, Christ's death for sin, or God's goodness experienced in creation", "black", "Illinois Country", "Jaime Weston", "1978", "high art and folk music", "warming", "the mid-sixties", "270,000", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "CNN.com", "a Florida girl who disappeared in February, plans to file for divorce from the girl's stepmother, a key witness in the case,", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "32 percent", "sovereignty over them", "Tuesday", "forgery and flying without a valid license", "Anil Kapoor", "49", "President Obama", "unwanted baggage from the 80s and has grown beyond a resort town into something more substantial.", "The Louvre", "snowstorm", "sports cars", "eggs", "Mutassim", "Thursday and Friday to the end of her tour on June 17 and 18, the publicist said.The Glasgow, Scotland concert has been shifted from this Sunday to May 1,", "\"The Screening Room\"", "NATO fighters followed the bombers on their 13-hour flight over the Arctic Ocean and the Atlantic, the defense ministry said Wednesday.", "alcohol", "Atlantic Ocean", "President Sheikh Sharif Sheikh Ahmed", "cortisone", "fortune", "Kingman Regional Medical Center", "CNN", "Manmohan Singh", "Michael Jackson", "be silent", "40 militants and six Pakistan soldiers dead", "Roger Federer", "subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "Louisiana", "Southeast", "new wife", "Carol Browner", "\"thing that I learn at a very early age is I don't get caught up in gossip,\"", "a tracheotomy, a surgical procedure in which an opening is made into the airway through an incision in the neck to allow for suction of fluid out of the lungs.", "back at work", "necropsy or animal autopsy", "27", "Derek Hough", "John Adams, a leader in pushing for independence, had persuaded the committee to select Thomas Jefferson to compose the original draft of the document", "parsley", "Zager & Evans", "Bobby Hurley", "fourth term", "\"electronic identification\" is too vague and chilled protected free speech.", "(Oliver) Cromwell", "Lapland", "2000", "Emad Hashim"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5783443313386147}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true], "QA-F1": [0.6, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.11764705882352941, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.10526315789473684, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.923076923076923, 0.6666666666666666, 0.8695652173913044, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.8, 0.4, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2520", "mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-7208", "mrqa_hotpotqa-validation-4760", "mrqa_searchqa-validation-328", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5120"], "SR": 0.4375, "CSR": 0.61953125, "EFR": 0.6944444444444444, "Overall": 0.6569878472222221}, {"timecode": 20, "before_eval_results": {"predictions": ["late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "Rhenus", "1331", "Death wish Coffee", "L", "Cameroon", "1994", "ballots", "clothes that are consistent and accessible", "three empty vodka bottles,", "secretary of defense on China, Taiwan, Hong Kong and Mongolia, and was deputy director for strategy, plans and policy on the Army staff.", "Bobby Darin", "Felipe Massa", "16", "his former Boca Juniors teammate and national coach Diego Maradona", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "composer", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony, 2,", "Amanda Knox's aunt", "well over 1,000 pounds", "Iran's nuclear program", "12 shades of violet, including a welcoming, bright blue-purple during the day, a softer violet hue after dusk", "using recreational drugs", "ceo Herbert Hainer", "Brett Cummins,", "a nearby day care center whose children are predominantly African-American", "inmates", "Col. Elspeth Cameron-Ritchie", "E! News", "three French journalists, a seven-member Spanish flight crew and one Belgian", "jobs", "saying Tuesday the reality he has seen is \"terrifying.\"", "waterboarding at least 266 times on two top al Qaeda suspects, according to a Bush-era Justice Department memo released by the Obama administration.", "Mexican's attorney general's office responded with a statement saying that it would investigate the video and any group that tries to take justice into its own hands.", "Republicans", "start developing a youth ballpark in his hometown of Aberdeen, Maryland, financed in part by a $75,000 gift from the Major League Baseball Players Association.", "an undated photo of Alexandros Grigoropoulos, whose death", "\"ceremonial,\" the official said.", "a 57-year old male", "North Korea intends to launch a long-range missile in the near future,", "Angola", "Matthew Fisher", "al-Maqdessi, leader of the Salafist jihad group Jund Ansar Allah, or Soldiers of the Partisans of God,", "bogeyman Jason Voorhees", "military prosecutors have accused al-Qahtani of helping to plan the September 11, 2001, terror attacks, and believe he may have sought to participate, possibly as the \"20th hijacker.\"", "Sea World in San Antonio", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "about 50 formal applications for speed attempts during 2008.", "Ku Klux Klan", "1939", "Branford College", "Bury, Oldham, Rochdale, Stockport, Tameside, Trafford, Wigan, and the cities of Manchester and Salford.", "stamens", "Malayalam movies", "August 17, 2017", "\"A Tradition of Love\"", "a mouse followed, in the '80s | clone. right: Dave.", "Hodel", "the benefits of the US privacy Act to Europeans and gives them access to US courts", "Coldplay"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5064636041539523}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.9523809523809523, 0.0, 1.0, 0.23076923076923078, 0.8, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.04878048780487805, 0.14814814814814814, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2564102564102564, 0.0, 0.19047619047619047, 0.19999999999999998, 1.0, 1.0, 0.0, 0.14285714285714288, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5263157894736842, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-465", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_naturalquestions-validation-3788", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-13277", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-3783"], "SR": 0.40625, "CSR": 0.609375, "EFR": 0.5789473684210527, "Overall": 0.5941611842105263}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness", "the poor", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "regulations and directives", "Splash", "Nicola Adams", "copper and zinc", "Russia, France, and also Poland,", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair", "Illinois", "on both shoulders", "Madonna's", "Glasgow", "a satellite-based navigational system that can tell users exactly where they are on Earth", "Australia", "bird's food, often with the aid of stone s or grit swallowed for this purpose.", "Pearson PLC", "Irish Setter", "American Civil War", "Loch Awe", "Jesacomaro", "Tasmania", "a medium-sized cat, fine-boned, long, and firmly muscled.", "capital of China", "Harrisburg", "mink mink", "glockenspiel", "Dr John Sentamu", "Baka hunter-gatherers", "Cruella de Vil", "Anne Boleyn", "EMI", "Holly Johnson", "Emma Chambers", "Charles V", "the community", "Russell Crowe", "Warren G. Harding", "rowing", "Puck", "Senj Nosnibor", "chamomile", "Ireland", "tarn", "Atlantic", "Albert Square", "Newbury", "the Old Testament", "70 million people, at that time 21 % of the world's entire population", "Target Corporation", "Sister, Sister", "Michelle Rounds", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "Global Fund to Fight AIDS, Tuberculosis and Malaria", "Swamp Fox", "better conditions for inmates, like Amnesty International", "talk show queen Oprah Winfrey.", "his mother"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6143563034188034}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-3569", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2484", "mrqa_searchqa-validation-11802", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.546875, "CSR": 0.6065340909090908, "EFR": 0.6896551724137931, "Overall": 0.648094631661442}, {"timecode": 22, "before_eval_results": {"predictions": ["coughing and sneezing", "1765", "along the frontiers between New France and the British colonies,", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials,", "Vicodin, generically known as hydrocodone", "Rome", "Robert Peary", "pearls", "jedenhall", "Carrie Underwood", "liqueur samboie", "he made his horse a consul, his palace a brothel, and his...", "Google", "Langston Hughes", "pain tolerance", "samuel Russell", "Orquestra", "riata", "reed", "lST (Landing Ship, Tank) is an amphibious vessel designed to land battle-ready tanks, troops", "prey drive", "David Beckham", "Arturo Toscanini", "economics", "Miracle in the Andes: 72 Days on the Mountain and My Long Trek", "proscenium arch", "Montenegro", "discus", "kouign", "basidiomycota", "james", "Georgia Thomas", "Idi Amin Dada", "agricultural green & yellow", "a body, body part, or personal object associated with a saint", "terracotta", "Plutarch", "lawyer, businessman, former politician", "sourdough", "50", "the Vikings.", "74 Fairfield Street", "l quatorze juillet", "typhoid fever", "A ria is a coastal inlet formed by the partial submergence of an unglaciated river valley.", "baviere-quebec.org", "james", "telegraph", "University Post Office", "hydrogen peroxide & yeast creates foam, steam & notably causes heat to be given off in this type of 10-letter chemical reaction", "John Knox, theologian and a leader of the Protestant Reformation", "the internal reproductive anatomy ( such as the uterus in females )", "the film sold over 128 million tickets in the US in its initial theatrical run", "epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results ( including peer review and occasional systematic review )", "jape", "Tesco", "A4", "Graham Hill", "Battelle Energy Alliance", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services.", "debris", "$10 billion", "out in the woods"], "metric_results": {"EM": 0.328125, "QA-F1": 0.41241262553665803}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.09090909090909091, 0.4, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10104", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11156", "mrqa_searchqa-validation-15814", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-10188", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-12564", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-11235", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-5091", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-16854", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-1118", "mrqa_hotpotqa-validation-68"], "SR": 0.328125, "CSR": 0.5944293478260869, "EFR": 0.7209302325581395, "Overall": 0.6576797901921132}, {"timecode": 23, "before_eval_results": {"predictions": ["2010", "1493\u20131500", "Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Nintendo", "the Gulf of Mexico", "domestic cat", "the daughter of Tony Richardson and Vanessa Redgrave", "Basel, Switzerland", "The Argonauts", "Zeus", "Altamont Speedway Free Festival", "John F. Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conductor", "multi-user dungeon", "Italy", "khaki", "magma", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "Mendip", "Barack Obama", "the Earth", "Nafea Faa Ipoipo?", "phosphorus", "Mumbai", "Joan Rivers", "Moses Sithole", "the colony of Suriname", "Justin Trudeau", "aircraft, ships, spacecraft, guided missiles, motor vehicles, weather formations", "Denis Law", "Love Is All Around", "William Golding", "Sally Ride", "Earthquake", "Fife", "Money Saving", "Adidas", "Snarks", "Elizabeth Arden", "Buxton", "woe", "Octopussy", "all pieces capture opponent's pieces by moving to the square that the opponent's piece occupies.", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "Musharraf", "The Wall Street Journal Europe", "fox", "60 Minutes", "Earth"], "metric_results": {"EM": 0.625, "QA-F1": 0.6708792892156863}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-692", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-2570", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848", "mrqa_searchqa-validation-1586"], "SR": 0.625, "CSR": 0.595703125, "EFR": 0.625, "Overall": 0.6103515625}, {"timecode": 24, "before_eval_results": {"predictions": ["limited coercion", "the chosen machine model", "Fox", "1997", "a suite of network protocols", "Noriko Savoie", "15", "nine-wicket", "Pyongyang and Seoul", "killed a man, the latter cheated on his wife.", "11", "change course", "Alwin Landry's supply vessel Damon Bankston", "Jared Polis", "money or other discreet aid for the effort if it could be made available,", "Sarah", "normal maritime traffic", "environmental", "Hungary", "Afghan security forces", "Saturday just hours before he was scheduled to perform at the BET Hip Hop Awards.", "38", "70,000 or so", "Climatecare, one of Europe's most experienced providers of carbon offset,", "E! News", "coach", "Steve Williams", "McDonald's", "writing her short stories (she has already published one book) and shows me a cartoon character she has created called \"Tomato Man.\"", "pastor Paula White", "2008", "Diego Maradona", "Dog patch Labs", "The Stooges", "two", "Itawamba County School District", "Romney", "EU naval force", "Plymouth Rock", "Liza Murphy", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "police", "former U.S. secretary of state", "At least 33", "five", "improve health and beauty", "state governments going after them.", "that students can help stop crime from happening.", "Alwin Landry's", "Krishna Rajaram", "Sunday, when a man wearing an explosives-laden vest drove a motorcycle rigged with bombs into a group of police recruits in eastern Baghdad.", "killing", "sinead", "southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "Northern Ireland", "radar", "art photographer", "16th season", "23", "South America", "freestyle", "Florence Nightingale", "the Crystal Skull"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6165758630737079}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5555555555555556, 1.0, 0.8, 0.0, 1.0, 0.0, 0.13333333333333333, 1.0, 0.5, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.12121212121212123, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 0.5, 1.0, 1.0, 0.33333333333333337, 0.16666666666666666, 1.0, 1.0, 0.07692307692307693, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_squad-validation-4673", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_searchqa-validation-3826"], "SR": 0.46875, "CSR": 0.590625, "EFR": 0.5294117647058824, "Overall": 0.5600183823529412}, {"timecode": 25, "before_eval_results": {"predictions": ["its 50th anniversary special", "Thomas Savery", "Vicodin, generically known as hydrocodone", "lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "22,000 years ago onward", "separatist", "Eleven", "269,000", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "38 feet", "Eintracht Frankfurt", "150", "an angry mob.", "Russian bombers", "41", "Los Alamitos Joint Forces Training Base", "Wally", "137", "the Kurdish militant group in Turkey", "3-2", "autonomy", "shoreline of the city of Quebradillas.", "the Russian air force", "34", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "around 3.5 percent of global greenhouse emissions.", "Amanda Knox's aunt", "\"several pieces of aircraft equipment were at fault or had broken down.\"", "ensuring that all prescription drugs on the market are FDA approved", "customers", "Tom Baer", "Pakistan", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "heavy flannel or wool", "Brian Mabry", "iTunes, which completely changed the business of music, to offering the world its first completely full-length computer-generated animated film", "Sunday", "60 euros", "American Civil Liberties Union", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment", "some truly mind-blowing structures", "first name", "he was one of 10 gunmen who attacked several targets in Mumbai", "2006", "San Diego", "Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior.", "that some or all of Bergdahl's remarks were scripted by his captors.", "keystroke", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "order and its tactics", "heart", "Hyderabad", "Sinai Peninsula or simply Sinai ( / \u02c8sa\u026ana\u026a / ; Arabic : \u0633\u064a\u0646\u0627\u0621\u200e S\u012bn\u0101\u02bc", "to stay, abide", "Las Vegas", "Jackson Pollock", "Lyrical", "McComb, Mississippi", "October 4, 1970", "King Duncan", "Brasstown Bald", "the dog, top hat", "taking a walk"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5305877034888763}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.782608695652174, 0.8571428571428571, 0.6666666666666666, 0.0, 0.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.27027027027027023, 1.0, 1.0, 0.0, 0.07407407407407408, 0.6666666666666666, 0.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1111111111111111, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7809", "mrqa_squad-validation-8068", "mrqa_squad-validation-9432", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_triviaqa-validation-1677", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-1982", "mrqa_searchqa-validation-11832", "mrqa_searchqa-validation-9476"], "SR": 0.4375, "CSR": 0.5847355769230769, "EFR": 0.6111111111111112, "Overall": 0.597923344017094}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "chlorophyll b", "Off-Off Campus", "clerical", "pro-democracy activists clashed Friday with Egyptian security forces", "Krishna Rajaram,", "25", "Booches Billiard Hall,", "finance", "Ross Perot", "Hong Kong's Victoria Harbor", "2002", "seven", "legitimacy of that race.", "tears of a Native American Indian", "three", "Monday", "Scarlett Keeling", "two years", "nearly 28 years", "regulators in the agency's Colorado office", "military commissions for some Guant Bay detainees.", "July", "Akshay Kumar", "Graham's wife", "the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "\"disagreements\" with the Port Authority of New York and New Jersey,", "during childbirth", "Michelle Rounds", "Bowie", "the death of Prince George's County police Cpl. Richard Findley, who died Friday after being struck by a truck.", "Phil Spector", "Kim Il Sung", "1994", "planned attacks in the southern port city of Karachi, Pakistan's largest city and the capital of Sindh province.", "Friday", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces", "al-Douri, the highest ranking former member of Saddam Hussein's regime still at large,", "dependable", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Pop star Michael Jackson", "Kingman Regional Medical Center,", "in his 60s, is incarcerated at the Supermax prison in Florence, Colorado, as is Zayed.", "overthrow the socialist government of Salvador Allende in Chile,", "Miguel Cotto", "9 a.m.", "\"don't ask, don't tell\"", "military veterans", "bartering -- trading goods and services without exchanging money", "semi-autonomous organisational units", "one", "Matt Monro", "Superintendent Norman Mullet", "the innermost digit of the forelimb; thumb", "1988", "25 million", "Peoria, Illinois", "Hawaii", "\"The eyes of these croaking critters usually bulge, but they retract & push down on the mouth to help in swallowing", "incense", "Ottoman Empire"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6203584883683568}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.10256410256410256, 0.6666666666666666, 1.0, 1.0, 0.06666666666666668, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.5, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4905", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-4159", "mrqa_searchqa-validation-11586", "mrqa_searchqa-validation-6839"], "SR": 0.546875, "CSR": 0.5833333333333333, "EFR": 0.7931034482758621, "Overall": 0.6882183908045977}, {"timecode": 27, "before_eval_results": {"predictions": ["early 1990s", "leaf-shaped buildings, a grass-roofed branch merging into adjacent parkland and gabion walls formed from the stones of previous buildings.", "silver", "1755", "AbdulMutallab", "trading goods and services without exchanging money", "Kenner, Louisiana", "John Dillinger,", "what caused the collapse of the building which contained the city's historical archives, bringing down parts of the two nearby structures.", "Seasons of My Heart", "Haleigh", "Whitney Houston", "Kris Allen", "Brazil's response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "Lashkar-e-Tayyiba (LeT)", "$1.5 million", "2006", "Rev. Alberto Cutie", "Angels", "eight Indian army troopers, including one officer, and 17 militants,", "\"There's no chance of it being open on time. Work has basically stopped,\"", "Karen Floyd", "14", "in a Starbucks this summer.", "BADBUL", "98", "2008", "near the Somali coast", "Paul Ryan", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "Pakistan's combustible Swat Valley,", "South Dakota State Penitentiary", "Iran", "last month's Mumbai terror attacks", "know what's important in life,", "in July", "Most of the 103 children that a French charity attempted to take to France from Chad for adoption are neither Sudanese nor orphans,", "Four Americans", "Josef Fritzl,", "Glasgow, Scotland", "38", "near the George Washington Bridge,", "President Bush", "fake his own death by crashing his private plane into a Florida swamp.", "Formagruppen", "fractured pelvis and sacrum -- the triangular bone within the pelvis.", "Wednesday at the age of 95.", "the abduction of minors", "gun", "Aniston, Demi Moore and Alicia Keys", "U.S. Vice President Dick Cheney", "19 June 2018", "Flag Day in 1954", "11 p.m. to 3 a.m", "Charlotte Corday", "Thailand", "barley", "Norwood, Massachusetts", "Manchester, England", "Drowning Pool", "apteka", "largest city", "beta blockers"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7393986260492722}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38095238095238093, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5333333333333333, 0.2666666666666667, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 1.0, 0.06451612903225806, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9307", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2108", "mrqa_hotpotqa-validation-4117", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-14535"], "SR": 0.609375, "CSR": 0.5842633928571428, "EFR": 0.84, "Overall": 0.7121316964285713}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "lead author of the Fifth Assessment Report", "ownership of private industries", "1981", "forgery and flying without a valid license,", "a racially-tinged remark made by his former caddy, telling reporters Steve Williams apologized and is not a racist.\"", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "Genocide Prevention Task Force.", "shoot down the satellite", "semiconductors", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need. We need Barack Obama as the next president of the United States.\"", "Kurt Cobain", "13", "\"face of the peace initiative has been attacked,\"", "misdemeanor assault charges", "one day we will have no more oil and we'll have to find another way to live,\"", "Anil Kapoor.", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "The Rosie Show", "Form Design Center.", "collaborating with the Colombian government,", "Christianity and Judaism", "the Dalai Lama", "Russia", "8 p.m. local time Thursday", "Passers-by", "For weeks, Nicole had a funny feeling that something odd was going on in her apartment. \"My gut started feeling like something just wasn't right,\"", "executive director of the Americas Division of Human Rights Watch,", "750", "nearly 100 people", "Matthew Fisher", "The Ski Train", "Boys And Girls alone", "Ozzy Osbourne", "AbdulMutallab,", "U.S. senators who couldn't resist taking the vehicles for a spin.", "inconclusive", "5:20 p.m. at Terminal C when a man walked through an exit on the public side to the secure \"sterile\" side for passengers", "environmental and political events", "$250,000", "byproducts emitted during the process of burning and melting raw materials.", "School-age girls", "5,600 people every year, and about 10 percent of those cases are hereditary.", "a million", "Sen. Arlen Specter", "Deutschneudorf,", "would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "a deceased organ donor,", "bragging about his sex life", "vertebral column ( spine ) ; invertebrates don't", "December 11, 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "Festival of Britain on London's South Bank.", "Douglas Hofstadter", "\"The Dark Tower\" series", "American", "Marmee takes care of her 4 girls while her husband is away serving as an army chaplain in the Civil War", "Castle Rock", "anchovy"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7065115867560348}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.9655172413793104, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.08695652173913045, 1.0, 0.8, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 0.972972972972973, 1.0, 0.1111111111111111, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8616", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_hotpotqa-validation-5376", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-9830"], "SR": 0.5625, "CSR": 0.5835129310344828, "EFR": 0.6428571428571429, "Overall": 0.6131850369458128}, {"timecode": 29, "before_eval_results": {"predictions": ["stagnant", "poison", "estimated 438,000", "Erick Avari, Michael McKean, Amy D. Jacobson,", "coaxial", "Pakistan A", "Everbank Field.", "14 directly elected members, 12 indirectly elected members representing functional constituencies and 7 members appointed by the chief executive.", "Battle of Dresden", "James Fitz James, 1st Duke of Berwick", "1965", "Charles de Gaulle Airport", "fifth", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "seven", "Province of Syracuse", "1963", "coca wine", "puzzle", "Knoxville, Tennessee", "Washington, D.C.", "Oryzomyini", "Tom Kartsotis", "November 6, 2009", "Wayman Tisdale", "Mexico", "Srinagar", "Northern Ireland", "late 19th and early 20th centuries", "political thriller", "22,500", "the Harpe brothers", "Eric Liddell", "2002", "Gregg Harper", "Shohola Falls", "small forward", "ARY Films", "Erinsborough", "Marine Corps", "Robert A. Iger", "Major Charles White Whittlesey.", "Seminole and Miccosukee", "Virginia", "1996 NBA Slam Dunk Contest.", "$10\u201320 million", "January 28, 2016", "Kennedy Road", "Somerset County, Pennsylvania", "Drowning Pool", "Colin Blakely", "two Nobel Peace Prizes, once in 1954 and again in 1981.", "IB Diploma Program and the IB Career - related Program for students aged 15 to 18, the IB Middle Years Program", "Bengal tiger", "off the southernmost tip of the South American mainland", "Cecil B. De Mille", "allergic reaction that can occur very quickly\u2014as fast as within a couple of minutes of exposure to the allergen.", "Peter Townsend", "3,000 kilometers (1,900 miles)", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Swiss art heist follows the recent theft in Switzerland of two paintings by Pablo Picasso,", "Russia", "peel and devein shrimp before putting on a BBQ", "Australia"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6219051411050249}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5333333333333333, 0.0, 0.0, 0.923076923076923, 0.0, 0.21052631578947367, 0.6666666666666666, 1.0, 0.15384615384615383, 0.35294117647058826, 0.0, 0.2222222222222222, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8164", "mrqa_squad-validation-4347", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-4716", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-4356", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-8450", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-4033", "mrqa_searchqa-validation-2585", "mrqa_searchqa-validation-6793"], "SR": 0.53125, "CSR": 0.5817708333333333, "EFR": 0.7333333333333333, "Overall": 0.6575520833333333}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "deforestation", "Prussian army general, adjutant to Frederick William IV of Prussia", "London", "Dave Thomas", "a cooperative where farmers pool their resources in certain areas of activity", "Danish", "1903", "the attack on Pearl Harbor", "other individuals, teams, or entire organizations.", "ten years of probation", "In Pursuit", "Bolton", "Monty Python's Flying Circus", "Kansas City crime family", "Dirk Werner Nowitzki", "Cecil B. DeMille Award honoree", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200", "Theme Park World", "Formula E", "New Jersey", "Norse mythology", "86,112", "Celtic", "Ouse and Foss", "United States and Canada", "British comedian", "Apatosaurus", "1885", "American", "Frank Thomas' Big Hurt", "\"Gliding Dance of the Maidens\"", "Margarine Unie", "Winecoff Hotel fire", "mentalfloss.com", "The Seduction of Hillary Rodham", "2005", "Lambic", "Tom Clancy's The Division", "Argentina", "Larry Alphonso Johnson Jr.", "Mike Mills", "nuclear weapons", "Joseph E. Grosberg", "Chelsea Does", "276,170", "Turkmenistan", "Wembley Stadium, London", "Sally Field", "Tatsumi", "The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "New York", "discus thrower", "Aston Villa", "2005", "228", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Post Traumatic Stress disorder", "Copenhagen", "Nez Perce"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7299133158508159}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 0.8666666666666666, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-332", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5351", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3905", "mrqa_searchqa-validation-6975"], "SR": 0.609375, "CSR": 0.5826612903225806, "EFR": 0.64, "Overall": 0.6113306451612903}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno", "79", "Iceland", "Wyoming", "a short story centering on the thoughts of a... At lunch one day, he ignores his mother when she asks him to pass a plate.", "Knott's Berry Farm", "Iowa", "A People's History of the United States", "Nassau", "mollusks", "Dr. Anthony Gallo", "Thomas Beekman", "a network of rail lines", "Rigoletto", "aardwolf", "Beijing", "a mile in under four minutes", "Inuk", "Death Valley", "Yves Saint Laurent", "reindeer", "Norway", "the fleet", "Anna Mary Robertson", "lunar", "Pompey's wife", "The New York Times fiction Best Sellers of 2004", "spectacled bear", "a tornado", "George Harrison", "Monty Python and the Holy Grail", "a polarized electron source consisting of a 3-electrode photocathode gun and a flashlamp-", "Milton Berle", "george herbert walker bush, george bush", "Congolese independence leader", "lunar module", "Diego de Almagro", "Dan Marino", "Mars", "clownfish", "E = mc2, the energy E of a.... potential energy of its constituents, in calculating the total mass", "Guru Pitka", "Las Vegas", "millet", "a butterfly", "heavy drinking", "orangutan", "New Mexico's (OSU) campus cable network, along with eight other Big Ten", "soothsayer", "an Israeli ultranationalist", "David Becomes Israel's King", "Gettysburg", "Jack Gleeson", "Plank", "Buddhism", "Carl Johan", "Portugal", "Tom Evans", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\" leaving them \"vulnerable to disruption,\"", "knocking the World Cup off the front pages for the first time in days.", "12.3 million people worldwide"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4897324775112444}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.17391304347826084, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.13333333333333333, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-1768", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-6655", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12657", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-6838", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-7406", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-4308", "mrqa_searchqa-validation-8550", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-7151", "mrqa_searchqa-validation-4905", "mrqa_searchqa-validation-2051", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3574"], "SR": 0.453125, "CSR": 0.57861328125, "EFR": 0.6571428571428571, "Overall": 0.6178780691964285}, {"timecode": 32, "before_eval_results": {"predictions": ["Second World War", "62", "Herbert Henry Asquith", "40", "Libya", "Shania Twain", "Sheffield Wednesday", "glucagon", "The New York Yankees", "rapid eye movement", "green, red, white", "Ann Dunham", "Saddam Hussein", "French", "Jim Branning", "Ohio", "Francis Matthews", "photographer", "magnetite", "Noah", "London", "New", "Prince Andrew and Sarah Ferguson", "Mercury", "a power factor of 0.5", "Jack Douglas", "Subway", "Madagascar", "Swansea City", "Gatcombe Park", "Brazil", "optimism", "aged 75", "Jennifer Lopez", "1664", "Morgan Choir", "Fred Perry", "Downton Abbey", "Martina Hingis", "a painter", "Cyclops", "Watch with Mother", "Michael Miles", "Sheryl Crow", "Gulliver's Travels", "Pomona", "Milan", "Mike Skinner", "Appalachian Trail", "a black Ferrari", "algebra", "grizzly bear", "Michael Moriarty", "June 1992", "24", "1952", "Campbell's", "Kirkcudbright", "16 Indiana National Guard soldiers", "banned substance cortisone.", "providing safety and physical security, but also could further assist with the reconstruction projects, such as building hospitals, schools, sanitation facilities and investment projects", "Juno", "a Swiss typeface", "lungs"], "metric_results": {"EM": 0.5, "QA-F1": 0.5774814993564993}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.05405405405405405, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-6378", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-5258", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-2914", "mrqa_triviaqa-validation-6656", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-7650", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-16567"], "SR": 0.5, "CSR": 0.5762310606060606, "EFR": 0.71875, "Overall": 0.6474905303030303}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "Knutsford", "insulin", "a Caesar salad", "Hudson Bay", "florida", "can cause allergic reactions in certain people.", "abigail Masham", "Getafix", "Brighton", "Belfast", "wind", "fire insurance", "Robin Hood's A Holy Grail", "West Point", "Andy Warhol", "Spain", "John Cable", "Paz", "solar system", "tomato and eggplant", "Moldova", "Mitsubishi", "warblers", "Franz Liszt", "Estimate", "a paceline", "leybdenum", "Pet Sounds", "Madness", "Buxton", "discretion", "Christian Dior", "Rudyard Kipling", "Leeds", "Philippines", "beaver", "Mel Blanc", "Envy", "Moffitt", "Ellen Morgan", "Phil Woolas", "5000 meters", "horse racing", "casein", "Newfoundland and Labrador", "crow", "Yellowstone National Park", "St. Thomas", "luzon", "Hugh Laurie", "Buddhism", "Guy Berryman", "Ohio", "Port Melbourne", "\u00c6thelwald Moll", "Scarface", "forgery and flying without a valid license", "Group D, Bundesliga Herder Bremen beat Sporting Lisbon of Portugal 1-0 through Gojko Kacar's second half strike.It meant Dutch side Heerenveen were eliminated despite a 5-0 home victory", "Liza Murphy", "Spock", "Astana", "Andorra, Belgium, Germany, Italy, Luxembourg"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6258680555555556}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-1395", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-2281", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-11382"], "SR": 0.609375, "CSR": 0.5772058823529411, "EFR": 0.68, "Overall": 0.6286029411764706}, {"timecode": 34, "before_eval_results": {"predictions": ["Battle of Fort Bull", "business", "steppes steppe", "Italy", "George Santayana", "opossum", "Alice Cooper", "diastolic", "trumpet", "Peter Kay", "The Cry", "Stockton & Darlington Railway", "Appalachian Trail", "Herald of Free Enterprise", "ballet", "The Last Voyage", "george West", "lizard", "Blackburn", "Frankie Laine", "The Mystery of Edwin Drood", "pommel horse", "cardinal", "Dick Van Dyke", "Egrement", "manhunt", "Francisco de Goya", "Medea", "Basil", "Canada", "ink", "soap", "Some Like It Hot", "Mull", "Ireland", "Mike Meyers", "hippocampus", "plutonium", "magma", "Passepartout", "welcome", "Norway", "Denmark", "Shrek", "26.22", "Cleveland Brown", "Heston Blumenthal", "One Direction", "Flint", "Jupiter", "Stringer", "Charles Lindbergh", "November 1999", "Baaghi", "Lead and lead dioxide", "boxer", "Wiltshire", "beer", "Pittsburgh", "Pakistan's High Commission in India", "astonishment", "Hunter S. Thompson", "tchaikovsky", "Howard Carter"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5899553571428571}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-4418", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-3623", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-4664", "mrqa_newsqa-validation-78"], "SR": 0.515625, "CSR": 0.5754464285714286, "EFR": 0.6451612903225806, "Overall": 0.6103038594470046}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "Leonardo da Vinci", "Matlock", "American Civil War", "Ethiopia", "cetaceans", "Arafura Sea", "The Labyrinth", "the Euphrates River", "Austria", "to make wrinkles in one's face", "Spain", "Carousel", "bullfighting", "Mike Brady", "Tenor", "alpo", "fidelio", "Guys and Dolls", "Julian Fellowes", "Denmark", "Another Day in Paradise", "The Last King of Scotland", "Ghana", "sandstone Trail", "L. Pasteur", "fonda", "rachmaninoff", "Finland", "stars with gravity", "Mille Miglia", "caves", "Billaley & His comets", "spain", "Muriel Spark", "happy birthday to You", "seven", "opossum", "Pickwick Papers", "presliced bread", "The Bridge", "raven", "Jordan", "bPA", "nelsons Column", "Etruscan", "Ken Burns", "grosvenor crescent", "Great Britain", "Pyotr Ilich Tchaikovsky", "Mujib", "libras", "Donna", "season four", "sinoatrial node", "yubin, Yeeun", "tomato", "2002", "people who lose out to British labor to claim they were being discriminated against on the basis of nationality.", "L'Aquila earthquake", "March 24", "duke nedir, ne demek, duke anlam", "chuseok", "Pocahontas"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6634375}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4758", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619"], "SR": 0.59375, "CSR": 0.5759548611111112, "EFR": 0.4230769230769231, "Overall": 0.49951589209401714}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans", "Kim", "mocambique", "branson, Missouri", "Gordon Ramsay", "Manchester City", "Robert Kennedy Sr.", "nitrogen oxides", "Anelies Marie Frank", "Manchester Airport", "Portuguese", "travelocity", "The Avengers", "ryansborough", "blackgate", "Paul Simon", "a ghost does not have flesh and bones, as you see I have (Luke 24:39)", "canola", "Tina Turner", "Benjamin Barker", "Nottingham Forest", "Bolivia", "John Donne", "Uranus", "Rio Grande", "Percheron", "The Graduate", "US", "ginger Rogers", "James I of Scotland", "One Foot in the Grave", "Bronx Mowgli", "Bob Anderson", "George Santayana", "The Finger Tab", "scafell Pike", "Wee Jimmy Krankie and his father", "muy Reverendo Se\u00f1or Fray Thomas de Torquemada", "Daniel Barenboim", "Canada", "rum and cola with a slice of optional lime", "seattlepi.com", "ghee", "George III", "ed Sheeran", "hyperbole", "a cigarette", "June", "blackenelope", "Ceylon", "screwdrivers", "Kansas City Chiefs", "G minor", "My Summer Story", "1974", "Nightmares", "Amberley Village", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "U.S. troops to the ongoing war in Afghanistan,", "kerstin Fritzl,", "cixi", "Brigham Young", "May", "chalk quarry"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5172743055555555}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.75, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.25, 0.25, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-7674", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-6749", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-5196", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-3013", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_hotpotqa-validation-5545", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-12", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-2849"], "SR": 0.453125, "CSR": 0.5726351351351351, "EFR": 0.6857142857142857, "Overall": 0.6291747104247104}, {"timecode": 37, "before_eval_results": {"predictions": ["a not-for-profit United States computer networking consortium", "neutral", "the roofs of the choir side - aisles", "Alex Ryan", "Sakshi Malik", "Columbia River Gorge", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "49 cents", "1876", "geologist James Hutton", "dakar", "joy of living", "more than 420", "George Strait", "the slogan used to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "1990", "Shawn", "Kiss", "January to May 2014", "Los Angeles", "September 28, 2017", "Kelly Reno", "provides the public with financial information about a nonprofit organization", "1770 BC", "Niveditha, Diwakar, Shruti", "two parties", "Jane Lynch", "cell nucleus", "Anakin", "Travis Tritt and Marty Stuart", "1983", "Bee Gees", "Matt Czuchry", "Pradyumna", "compulsory registration of births with the United Kingdom government is a practice that originated at least as far back as 1853", "Lands End", "Psychomachia, '' an epic poem written in the fifth century", "the New Jersey Devils of the National Hockey League ( NHL ) and the Seton Hall Pirates men's basketball team", "two", "0.30 in ( 7.6 mm )", "Cress", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Gloria", "Hudson Bay", "The Maginot Line", "silesia", "Dumbo", "purple", "Charles Guiteau", "Gettysburg Address", "iTunes", "$273 million", "India", "Jeddah, Saudi Arabia", "Desperate Housewives", "Captain chaos", "chilpancingo", "Tuesday"], "metric_results": {"EM": 0.59375, "QA-F1": 0.720544588882984}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.9523809523809523, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07272727272727272, 1.0, 0.6956521739130436, 1.0, 0.3333333333333333, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 0.23529411764705882, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-7642", "mrqa_searchqa-validation-3257", "mrqa_searchqa-validation-2335"], "SR": 0.59375, "CSR": 0.5731907894736843, "EFR": 0.7307692307692307, "Overall": 0.6519800101214575}, {"timecode": 38, "before_eval_results": {"predictions": ["Isaac Newton", "performers must receive the highest number of votes, and also greater than 50 % of the votes", "the United States", "Kim Basinger", "fall of 2015", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Kusha", "in positions Arg15 - Ile16", "Joseph M. Scriven", "Lady Gaga", "Chicago metropolitan area", "president", "Domhnall Gleeson", "eusebeia", "Pastoral farming", "Southampton ( 1902, then in the Southern League ) being the last finalist from outside the top two tiers.", "nobiliary particle", "Stephen A. Douglas", "1984", "a loanword of the Visigothic word guma", "Pakistan", "21 February", "Tagalog or English", "Bryan Cranston", "thylakoid membranes", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Felix Baumgartner", "Wake County, it lies just north of the state capital, Raleigh", "December 1922", "18 Divisional Round", "602", "stable, non-radioactive rubidium - 85", "Membership is believed to cost between $10,000 and $30,000", "September 1980", "1931", "University of Oxford", "about 375 miles ( 600 km ) south of Newfoundland", "Gladys Knight & the Pips", "1959", "The statesmen who led the secession movement", "Randy", "the Infamy Speech of US President Franklin D. Roosevelt", "Joseph Stalin", "into the intermembrane space", "tectonic", "Idaho", "Sara Gilbert", "13", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "black", "Baroness Thatcher", "Roddy Doyle", "Daniil Shafran", "TD Garden", "Venus", "starting a dialogue while maintaining sanctions,", "10 below", "Henry Ford", "David McCullough", "Rendezvous with Rama", "CERN", "Portugal"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6661248972030621}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.962962962962963, 0.0, 1.0, 0.5, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8695652173913044, 0.0, 0.4, 1.0, 0.6666666666666666, 0.4666666666666666, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-5292", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-6088", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2419", "mrqa_searchqa-validation-3219"], "SR": 0.578125, "CSR": 0.5733173076923077, "EFR": 0.5555555555555556, "Overall": 0.5644364316239316}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "A Turtle's Tale : Sammy's Adventures", "Jenny Slate", "active absorption", "Philippe Petit", "September 1980", "January 2004", "southwest and along the Yangtze ; it is planted in March to June and harvested in October and November and also contributed about 34 percent to total rice output in the 1980s", "Toby Keith", "development of electronic computers in the 1950s", "17 - year - old", "punk rock", "Set six months after Kratos killed his wife and child, he has been imprisoned by the three Furies for breaking his blood oath to Ares", "Teri Hatcher", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "XXXX", "the Berlin School of experimental psychology", "53", "between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "Stefanie Scott", "Richard Crispin Armitage", "Brooks & Dunn", "Faceman '' Peck", "Bonnie Aarons", "in either late 2018 or early 2019", "diffuse nebulae", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "McKim Marriott", "Secretary of Homeland Security isirstjen Nielsen", "Santiago Ram\u00f3n y Cajal", "in The Nursery Rhymes of England ( London and New York, c. 1886 )", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "man", "1960s", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "defense against rain rather than sun", "Since 1940", "Tad '' Stone", "Mark Jackson", "Michael Buffer", "one faith, one baptism, one God and Father of all, who is over all and through all and in all", "on location", "federal government", "New England", "Cody Fern", "issues of the American Civil War include questions about the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "prophets and beloved religious leaders", "saliva", "Juan Manuel de Ayala", "Prophet Joseph Smith, Jr.", "funny Folks (1874 - 1894)", "1909", "John Duigan", "179", "Princess Diana", "Mikkel Kessler", "curfew", "Pearl", "spiny dogfish", "Fast Food Nation", "ABBA"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5971238848127873}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1212121212121212, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.56, 0.3076923076923077, 0.967741935483871, 0.0, 0.25, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 0.17142857142857143, 1.0, 0.0, 0.0, 1.0, 0.2608695652173913, 0.7499999999999999, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-4219", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-302", "mrqa_searchqa-validation-8323"], "SR": 0.4375, "CSR": 0.569921875, "EFR": 0.6666666666666666, "Overall": 0.6182942708333333}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment", "eight Indian army troopers, including one officer, and 17 militants", "Joan Rivers", "cancer awareness", "glamour and hedonism", "2-0", "15,000", "58 people", "Michael Schumacher", "that the kinds of devices available today are not susceptible to attack, and that fear of future risks shouldn't impede progress in the field.", "numerous suicide attacks,", "the U.S. Assistant Secretary of State for African Affairs Jendayi Frazer", "2004", "NATO", "Switzerland", "Monday", "second", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "he wants to spend billions to revitalize the nation's economy, a plan the campaign of his likely Republican opponent said would slow economic growth with higher taxes.", "Clifford Harris", "baja California Language College in Ensenada, Mexico", "Robert Barnett", "$627", "41", "Nick Adenhart", "a strict interpretation of the law,", "Derek Mears", "Nieb\u00fcll", "rural Tennessee", "Tuesday afternoon", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "section 60", "Ali Bongo", "her lawyer, celebrity attorney Gloria Allred,", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "two pages -- usually high school juniors who serve Congress as messengers", "A Brazilian supreme court judge", "Derek Mears", "Operation Crank Call", "help rebuild the nation's highways, bridges and other public-use facilities", "East Java", "St. Louis, Missouri", "NATO fighters", "High Court Judge Justice Davis", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "2019", "P.V. Sindhu", "Mexico", "Snickers candy bars", "monoceros", "Berkshire", "Anaheim, California", "uncle", "Bergen", "embalming", "Cartagena", "a graphical user interface", "German Shepherds have a two - layer coat which is close and dense with a thick undercoat"], "metric_results": {"EM": 0.5, "QA-F1": 0.676003889414752}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.1111111111111111, 0.47058823529411764, 1.0, 0.0, 0.375, 1.0, 0.5, 0.6666666666666666, 1.0, 0.2105263157894737, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.3870967741935484, 0.13953488372093023, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 0.29629629629629634, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.5714285714285715, 1.0, 0.0, 0.9523809523809523, 0.3076923076923077, 0.4, 1.0, 1.0, 0.9411764705882353, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5384615384615384, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.10526315789473682]}}, "before_error_ids": ["mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.5, "CSR": 0.5682164634146342, "EFR": 0.78125, "Overall": 0.6747332317073171}, {"timecode": 41, "before_eval_results": {"predictions": ["Battle of Sainte-Foy", "1890s Klondike Gold Rush, when strong sled dogs were in high demand", "Stephen A. Douglas", "1998", "displacement", "sovereign states", "Megan Park", "the currency used by the institutions of the European Union", "Kate Walsh", "September 14, 2008", "Trace Adkins", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "2002", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "pour point of a liquid", "increases the life of the pump, allows a smaller and lighter device to be used, and reduces electrical load", "largest financial inflows to developing countries", "Akshay Kumar", "Shirley Partridge", "15 February 1998", "5.7 million", "believed to cost between $10,000 and $30,000", "mining", "Cedric Alexander, who is in his first reign. He won the vacant title by defeating Mustafa Ali in the finals of a 16 - man tournament on April 8, 2018, on the WrestleMania 34 pre-show.", "interspecific hybridization and parthenogenesis", "David Joseph Madden", "1902", "the Dutch figure of Sinterklaas", "Yuzuru Hanyu", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Phillipa Soo", "A firm, flexible cup - shaped device worn inside the vagina to collect menstrual flow", "pigs", "General George Washington", "Tagalog or English", "Virgil Tibbs", "an integral membrane protein that builds up a proton gradient across a biological membrane", "the atrioventricular node, along the Bundle of His and through bundle branches to cause contraction of the heart muscle", "four distinct levels of protein structure", "Jack Nicklaus", "Norman Greenbaum", "Tim Rice", "six 50 minute ( one - hour with advertisements ) episodes", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups", "the Monsoons from the south atlantic ocean arrives in central Nigeria in July bringing with it high humidity, heavy cloud cover and heavy rainfall", "Missouri River", "the right to vote", "frontal lobe", "10 June 1940", "Tandi", "Alberich", "ear", "brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "the Iraq's autonomous region of Kurdish.", "Denver, Colorado.", "the Sadr City and Adhamiya districts of Baghdad City", "President Charles Logan and Graem Bauer,", "Arthur", "Howie Mandel", "Virgin America"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6022640328776439}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.8, 0.6666666666666666, 0.17391304347826086, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.5, 1.0, 0.125, 0.4, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1111111111111111, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 0.42857142857142855, 1.0, 0.0, 0.2666666666666667, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-5481", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6887", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-1237"], "SR": 0.46875, "CSR": 0.5658482142857143, "EFR": 0.7352941176470589, "Overall": 0.6505711659663866}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "vaporization of water also absorbs heat ; it thereby cools the smoke, air, walls, and objects that could act as further fuel", "in Middlesex County, Province of Massachusetts Bay", "chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "its population", "Zeus", "During Hanna's recovery masquerade celebration, she suddenly regains her memory,", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the south", "to bring", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "Ceramic art", "the Soviet Union", "Covington, Kentucky", "New Mexico", "reduces the back pressure, which in turn reduces the steam consumption, and thus the fuel consumption, while at the same time increasing power and recycling boiler - water", "December 15, 2017", "Paradise, Nevada", "L.K. Advani", "differential erosion", "Glenn Close", "Gospel of Matthew in the middle of the Sermon on the Mount, and the short form in the Gospel of Luke", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "Southampton ( 1902, then in the Southern League ) being the last finalist from outside the top two tiers", "2018", "electricity generation, power distribution, and power transmission on the island", "Tsetse fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "Norman Greenbaum", "notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "electron shells", "compasses", "Charlotte Thornton", "Northeast Monsoon or Retreating Monsoon", "March 16, 2018", "President Lyndon Johnson", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Daya Jethalal Gada", "2,140 kilometres ( 1,330 mi )", "by producing an egg through parthenogenesis", "1926", "South Korea", "starting in 1560s", "Frankie Muniz", "Lou Rawls", "between 1765 and 1783", "Alfheim", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "recall communications", "two", "prostate cancer", "wyvern", "Lord Fauntleroy", "a key ring or a decorative key fob", "yellow"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6168303309662226}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.5384615384615384, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.9, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0625, 1.0, 0.8, 0.14814814814814814, 0.6666666666666666, 1.0, 0.3157894736842105, 1.0, 1.0, 0.0, 1.0, 0.14285714285714285, 0.9767441860465117, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-8157", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-5636", "mrqa_searchqa-validation-11152"], "SR": 0.46875, "CSR": 0.5635901162790697, "EFR": 0.6470588235294118, "Overall": 0.6053244699042408}, {"timecode": 43, "before_eval_results": {"predictions": ["1985", "February 27, 2007", "pick yourself up and dust yourself off and keep going", "relieve families who had difficulty finding jobs during the Great Depression in the United States", "Lynne", "2013", "Blue with a harp of gold", "Miami Heat", "1981", "As late as the 1890s, building regulations in London did not require working - class housing to have indoor toilets ; into the early 20th century,", "in the mid - to late 1920s", "Napoleon Bonaparte", "Juan Francisco Ochoa", "Augustus Waters", "Camille Pissarro", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier,", "a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "active absorption of water from the soil by the root", "Alex Ryan", "habitat", "2018", "Advanced Systems Format ( ASF )", "100 members, two from each of the 50 states", "Toledo", "embryo", "During the last Ice Age, sea levels were lower and the Solent was part of a river flowing south east from current day Poole Harbour towards mid-Channel", "Haikou on the Hainan Island", "Robert Irsay", "Paradise, Nevada", "Alicia Vikander", "annually in late January or early February", "Ashoka", "the name of a work gang", "Robert Andrews Millikan", "Puerto Rico Electric Power Authority ( PREPA )", "Bumblebee", "into the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "armored fighting vehicle", "honey bees", "Mary Chapin Carpenter", "the Louvre Museum in Paris", "over two days in July 2011,", "February 7, 2018", "Florida", "The Ranch is an American comedy web television series", "Caparra", "usually in a way considered to be unfair", "winter", "Pangaea", "Newcastle Brown Ale", "Western Australia", "Vaclav Havel", "Mary Bonauto, Susan Murray, and Beth Robinson", "Chelsea", "North America", "The federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site, once the center of the country's Cold War plutonium production.", "\"We are resetting,\"", "Michigan", "Dean Acheson", "Bob Kerrey", "Jane Goodall", "Forrest Gump"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5776735128297628}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.7142857142857143, 0.962962962962963, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.8333333333333333, 1.0, 0.16666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.2857142857142857, 0.888888888888889, 1.0, 0.0, 0.5714285714285715, 0.4444444444444444, 1.0, 1.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7272727272727273, 1.0, 1.0, 0.25641025641025644, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-9079", "mrqa_triviaqa-validation-2697", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1977"], "SR": 0.421875, "CSR": 0.5603693181818181, "EFR": 0.5945945945945946, "Overall": 0.5774819563882063}, {"timecode": 44, "before_eval_results": {"predictions": ["fixed annual carriage fees of \u00a330m for the channels with both channel suppliers able to secure additional capped payments if their channels meet certain performance-related targets.", "aluminum foil", "Laurel, Mississippi", "mountain-climbing", "Indianola", "insurance", "DuSable", "1992", "Cher", "Alabama", "Jim Harrison", "Toronto", "Tomorrowland", "fennec fox", "United States Army", "stop motion animation", "Jean Acker", "Bracebridge Heath", "Leucippus", "Caesars Entertainment Corporation", "Terrence \"Uncle Terry\" Richardson", "Reinhard Heydrich", "Karl Kraus", "Christopher Rich Wilson", "Maria Brink", "Manitowoc County, Wisconsin", "Northrop P-61 Black widow", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Sri Lanka Freedom Party", "Bishop's Stortford", "ambassador to Ghana", "Emmy, Grammy, Oscar and Tony awards.", "1991\u201392", "Leatherheads", "September 25, 2017", "John Delaney", "Tampa", "OutKast", "Richard Street", "Zaire", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "a pioneering New Zealand food writer.", "South America", "2006", "perjury and obstruction of justice", "Operation Overlord", "Mary Elizabeth Hartman", "over 9,000 employees", "Rosalind Bailey", "potential of hydrogen", "Alamodome in San Antonio, Texas", "Carrie", "The Finger Tab", "Kent", "almost 9 million", "Kenya", "2008", "terrorism", "Moses", "Chapter 5", "Jeff Barry and Andy Kim"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6094551282051281}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [0.07692307692307693, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2837", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-2069", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_triviaqa-validation-6953", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.515625, "CSR": 0.559375, "EFR": 0.7419354838709677, "Overall": 0.6506552419354839}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "early 20th century", "Australian", "1903", "the power to regulate interstate commerce", "Donna Wallace", "Jenson Alexander Lyons", "Tufts University", "People's Republic of China", "Azeroth", "Squam Lake", "Philip Livingston", "Tayeb Salih", "King James II of England", "God Save the Queen", "526", "Scotland", "A hard rock/blues rock band, they have also been considered a heavy metal band, although they have always dubbed their music simply \"rock and roll\"", "GmbH", "Mick Jackson", "Lalit", "her performances of \"khyal\", \"thumri\", and \"bhajans\"", "Tampa Bay Lightning", "Steven Selling", "Sullenberger III", "Manhattan Project", "Pacific War", "Romantic", "Hugh Dowding", "AMC Entertainment Holdings, Inc.", "New York Islanders", "fennec", "1978", "six different constructors taking the first six positions.", "French", "Pacific Place", "Australian women's national soccer team", "\"Bad Blood\"", "\"SexyBack\"", "the E22", "Giuseppe Verdi", "Chief Minister of Tamil Nadu", "Sacramento Kings", "Walldorf, Baden-W\u00fcrttemberg", "Fife", "Fyvie Castle", "Aamina Sheikh, Hasan Ahmed, Neelam Muneer, Zaheen Tahira, Ismat Zaidi, Sami Khan and Faisal Qureshi.", "the British Army", "voting directly or elect representatives from among themselves to form a governing body, such as a parliament.", "Boletus edulis", "Robert Remak", "JackScanlon", "Steve Hale", "Frances Ethel Gumm", "Switzerland", "Model T", "NATO's International Security Assistance Force", "2,000", "Cyprus", "Maroon 5", "Saudi Arabia", "Geraldine A. Ferraro", "two"], "metric_results": {"EM": 0.484375, "QA-F1": 0.579910913228928}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.5714285714285715, 0.5, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.23529411764705882, 1.0, 0.09090909090909091, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1401", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-2129", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-6575", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-8327"], "SR": 0.484375, "CSR": 0.5577445652173914, "EFR": 0.7575757575757576, "Overall": 0.6576601613965745}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "tepuis", "The King and I", "Republican", "1996", "5", "Greenland shark", "The Word", "President Abraham Lincoln", "Thaddeus", "Anthoonij van Diemenslandt", "the death penalty", "xerophyte", "Jackie Robinson", "Provincetown", "Dian Fossey", "MI5", "Harrow", "creme anglaise", "a sauce of lemon juice, parsley, salt, pepper, and drawn butter", "pork", "curling", "Victoria Coren", "Gettysburg Civil War", "Chile", "Majorca (Mallorca)", "Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara Wieck", "Mercury", "Venus", "Barack Obama", "Canada", "Bologna Song Lyrics", "Dominican Republic", "Bowie", "Stephen King", "Hinduism", "caryatid", "feet", "all of its land in North America", "Mary Poppins", "Glyn Jones", "Port Moresby", "Connecticut", "Quentin Blake", "whooping cough", "The Daily Herald", "numerous", "halal meat eaten by Muslims.", "2016", "the courts", "2017", "ambassador to Ghana and to Czechoslovakia", "Diamond White", "1987", "Daniel Nestor, from Canada,", "Jeddah, Saudi Arabia", "death", "Beatrix Potter", "Dan Eggen and Elizabeth Williamson", "Reader's Digest", "the living child"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-9246", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-252", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-6488"], "SR": 0.53125, "CSR": 0.5571808510638299, "EFR": 0.7, "Overall": 0.6285904255319149}, {"timecode": 47, "before_eval_results": {"predictions": ["zebra", "allergic reaction", "David Beckham", "on Scottish soil took place on a nearby moor at Culloden", "Runic", "Spain", "cricket", "Max Planck", "Rotherham United", "heat exchange", "Misery", "Styal", "stately", "Blind Beggar", "Brainwash", "l Leroy Burrell (United States)", "parlophone", "Wild Atlantic Way", "John Denver", "Ankh-Morpork", "noddy Goes To Toyland", "Lackawanna Six", "Brazil", "Backyard Kerplunk Game", "muezzin", "window", "on a ship s bottom next to the keel", "realist", "Apollo 11", "flit gun", "Nikola Tesla", "Nicky Henderson", "Evita", "albino sperm whale", "Randy Turpin", "East Fife", "St Pancras International Station", "the immediate physical and social setting in which people live or in which something happens or develops", "presliced bread", "Dilbert", "Aristotelian Tragedy", "dimittis", "French", "Phrixus", "Burgundy", "cribbage", "Westlife", "Johannesburg", "France", "Muffin Man", "Seoul", "Prince James, Duke of York and of Albany ( later King James II & VII )", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Mike Nesmith", "Pansexuality", "Tony Ducks", "1754", "drugs", "Veracruz, Mexico", "if the airline doesn't perform, the credit card company still has your money and can give it right back to you.Periodically check in on your airline", "robert frost", "King Henry VIII", "Pillsbury", "1995 Mitsubishi Eclipse"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6194951749639249}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.1111111111111111, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857142, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-4691", "mrqa_triviaqa-validation-5130", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4781", "mrqa_triviaqa-validation-582", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_newsqa-validation-1947", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618"], "SR": 0.5625, "CSR": 0.5572916666666667, "EFR": 0.7142857142857143, "Overall": 0.6357886904761905}, {"timecode": 48, "before_eval_results": {"predictions": ["Route 66", "Sesame Street", "meat", "cabbage", "South Australia", "Mr. Magoo", "fleece", "Ash tree", "opossum", "New Zealand", "slide saxophones", "60", "Auric Goldfinger", "1984", "small pikeb", "Mongol Empire", "1875", "tax collector", "penny", "santana", "Wars of the Roses", "Bagram", "maggie Gilkeson", "Chrysler", "fur hat", "korky the cat", "honours", "United States", "Brazil", "pei Tang", "biathlon", "Idaho Falls", "Charlie Chan", "Vienna", "white", "jaws", "Paul Rudd", "rabbit", "Scotland", "jodhpurs", "Orson Welles", "Sanskrit", "menorah", "Dutch", "Texas", "Super Bowl Sunday", "long pole", "Little Tommy Stout", "mzarts", "Rhododendron", "Ireland", "Chuck Noland", "Colony of Virginia", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park", "2010", "through Greece, the birthplace of the Olympics, before being transported to Canada for what will be the longest domestic torch relay in the games' history, officials said.", "10 below", "100 to 150 troops", "silver", "the American Kennel Club", "Omaha", "jedoublen/jeopardy"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5663541666666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.08, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-2687", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-6306", "mrqa_naturalquestions-validation-4803", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-16460", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-4136"], "SR": 0.515625, "CSR": 0.5564413265306123, "EFR": 0.6774193548387096, "Overall": 0.6169303406846609}, {"timecode": 49, "before_eval_results": {"predictions": ["Quin Ivy", "Iran", "alcohol", "Francis Ford Coppola", "mrrick", "Daniel Boone", "Thames Street", "Theodore Roosevelt", "satyrs", "a type known for being very good to eat", "La Boh\u00e8me", "IBM", "wishbone", "Garrick Club", "Lackawanna 6", "Barnaby Rudge", "britten", "American Civil War", "dark", "Cybill Shepherd", "Jimmy Robertson", "Florence", "Basil", "quy Wonka", "Severn", "Australia", "South Africa", "bunch grasses", "Nicaragua", "Churchill", "Wars of the Roses", "Chemnitz", "many home runs and this many walks and give up this many home run, it will generally translate into this many runs and runs allowed.", "trout", "ap\u00e9ro", "Wali Muhammad", "Belize", "Library of Congress", "hair loss", "sprint", "Charlie Drake", "Robin Hood's A Holy Grail", "Chris Martin", "flinstone", "lead detective sergeant (Lee Ingleby)", "rugby", "honda", "deacon Blues", "11", "tobacco", "heifer", "free floating", "Tom Selleck", "New Orleans", "superhuman abilities", "Texas Tech University", "Loughborough Technical Institute", "Herman Cain", "the United States", "provide with gainful employment by allowing them to do jobs that Arizonans wouldn't do.", "George Babbitt", "Oklahoma", "ladies who Lunch", "four"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5010416666666666}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-3615"], "SR": 0.4375, "CSR": 0.5540625, "EFR": 0.6666666666666666, "Overall": 0.6103645833333333}, {"timecode": 50, "UKR": 0.79296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-622", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9188", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-4136", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2877", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-4606", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5556", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5656", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7698", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.751953125, "KG": 0.48828125, "before_eval_results": {"predictions": ["RAF Mount Pleasant", "University of Kansas", "\"the Gentle Don\"", "Walcha", "Michelle Anne Sinclair", "Adam Levine", "Tim Allen", "The Dressmaker", "Oakland, California", "37", "Tufts University", "Owsley Stanley", "The Late Late Show", "Kongo", "Harold Lipshitz", "Spanish", "Ted", "1945", "69.7 million litres", "Neneh Cherry", "Sunyani", "antelope", "Comodoro Arturo Merino Ben\u00edtez International Airport", "Scotty Grainger Jr.", "9", "Vigor, Prelude, CR-X, and Quint", "8,648", "Alfonso Cuar\u00f3n", "1886", "September 30, 2017", "1898", "Nicolas Winding Refn", "devotional", "Fortunino Francesco Verdi", "Laban Movement Analysis", "Cecily Strong", "Sam Waterston", "invoicing", "seasonal television specials", "4 km", "1853", "Love", "The Supremes", "48,982", "Vincent Landay", "Lieutenant Martin \"Marty\" Castillo", "playback singer", "1901", "Pope John X.", "Art Directors Guild's Excellence in Production Design Award", "VAQ-135", "Alex Skuby", "Hold On", "Prince James, Duke of York and of Albany ( later King James II & VII )", "'Q'", "FBI", "jug", "The iconic Abbey Road music studios made famous by the Beatles are not for sale", "UNICEF", "9 a.m.", "lord Byron", "Van", "kufic", "a long-range missile"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6430002289377289}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, false], "QA-F1": [0.8, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.4615384615384615, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-5030", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-2480", "mrqa_hotpotqa-validation-2058", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-495", "mrqa_triviaqa-validation-7487", "mrqa_newsqa-validation-2096", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-12460", "mrqa_searchqa-validation-14277", "mrqa_newsqa-validation-1661"], "SR": 0.515625, "CSR": 0.5533088235294117, "EFR": 0.7419354838709677, "Overall": 0.6656894864800759}, {"timecode": 51, "before_eval_results": {"predictions": ["Detroit, Michigan", "comedy", "Mickey's Christmas Carol", "143,007", "Atat\u00fcrk Museum Mansion", "Realty Bites", "24 NCAA sports", "Razor Ramon", "psychoanalysis", "Forbes, New South Wales", "St. George, Maine", "Heart", "senior men's Lithuanian national team", "International Boxing Hall of Fame (IBHOF)", "35", "Conservatorio Verdi in Milan", "Lester Ben \"Benny\" Binion", "Smoothie King Center", "World Outgames", "Homebrewing", "Umberto II", "Presbyterian Church", "neuro-orthopaedic", "in their home country", "North Sea", "17 October 2006", "67,575", "Oxford", "\"Advanced Dragons\"", "Emad Hashim", "5320 km", "Heinkel Flugzeugwerke", "English", "Eric Edward Whitacre", "largest Mission Revival Style building in the United States", "21 flights daily", "George Adamski", "\"Hand of Thrawn\"", "Switzerland", "McKenna's Fort", "Scunthorpe", "British comedian", "Cook's Landing Place", "Summer Olympic Games unofficial programme in 1900", "1936", "1970", "Royal Albert Hall and The Kennedy Center", "Budget Rent a Car", "Japan", "lion", "1959", "Donna Mills", "a pop and R&B ballad, with Latin pop influences", "400 feet ( 122 m )", "Maine", "Blanche", "maxillae", "Microsoft", "4.6 million", "government", "tea rose", "nine", "black Russian", "Rear Window"], "metric_results": {"EM": 0.4375, "QA-F1": 0.619164862914863}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true], "QA-F1": [0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.5714285714285715, 0.8, 0.0, 0.33333333333333337, 0.9090909090909091, 0.0, 0.6666666666666666, 0.4, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.3333333333333333, 0.5, 0.5, 1.0, 0.5, 0.8571428571428571, 0.6, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-1260", "mrqa_hotpotqa-validation-2502", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1207", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4820", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-580", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-4448", "mrqa_hotpotqa-validation-3576", "mrqa_hotpotqa-validation-558", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-7250", "mrqa_triviaqa-validation-84", "mrqa_triviaqa-validation-4184", "mrqa_searchqa-validation-10653"], "SR": 0.4375, "CSR": 0.5510817307692308, "EFR": 0.6666666666666666, "Overall": 0.6501903044871795}, {"timecode": 52, "before_eval_results": {"predictions": ["September 19, 2017", "Billy Idol", "pools campaign contributions from members and donates those funds to campaign for or against candidates, ballot initiatives, or legislation", ", Debates over the franchise were frequent, and differentiating voters from non-voters must have been done", "3 lines of reflection", "up to 100,000 write / erase cycles", "Sachin Tendulkar and Kumar Sangakkara", "Judiththia Aline Keppel", "Camping World Stadium in Orlando", "Advanced Systems Format ( ASF )", "the Ramones", "Jack Nicholson", "a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Gospel of Matthew in the middle of the Sermon on the Mount, and the short form in the Gospel of Luke", "30 years after Return of the Wars", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract", "Diego Tinoco", "sport utility vehicles", "Melbourne", "The Drew Las Vegas, which rises 735 feet ( 224 m ) and was topped out in November 2008", "October 2008", "John Hancock", "1960", "The British colonial government fell in the region of modern Nova Scotia after several disastrous campaigns in 1757, including a failed expedition against Louisbourg and the Siege of Fort William Henry", "1972", "Columbia River Gorge", "coercivity", "2010", "malicious software", "Cyndi Grecco", "differs in ingredients", "1900", "dorsally on the forearm", "a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "Terry Kath", "birch", "one person", "The Parlement de Bretagne", "a password recovery tool for Microsoft Windows", "declared state laws establishing separate public schools for black and white students to be unconstitutional", "1986", "late - September through early January", "currency option", "1623", "a normally inaccessible mini-game in the 2004 video game Grand Theft Auto : San Andreas", "the medulla oblongata", "2018", "Gibraltar, a British Overseas Territory, located at the southern tip of the Iberian Peninsula, is the subject of an irredentist territorial claim by Spain", "Howard Caine", "May 3, 2005", "Andy Cole", "Harry patch", "septoe and Son", "Paul Maskey", "teen actor", "Saoirse Ronan", "Happy Madison Productions", "The Kirchners", "north London rivals Tottenham", "Gordon Brown", "veterans", "yellow fever", "winter solstice", "Netflix"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6573561661506708}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3157894736842105, 0.8333333333333334, 0.3137254901960785, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.4, 0.058823529411764705, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-9536", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-3959", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-2688", "mrqa_newsqa-validation-318", "mrqa_searchqa-validation-4527"], "SR": 0.5625, "CSR": 0.5512971698113207, "EFR": 0.6785714285714286, "Overall": 0.6526143446765499}, {"timecode": 53, "before_eval_results": {"predictions": ["Alston", "the Big Bang Theory of Creation", "Handel", "Green Acres", "a monthly gourmet lifestyle and food magazine based in Singapore serving the latest dining trends, delicious food recipes, wine & drinks", "konabar", "the Doppler shift", "Lobster Newberg", "Shel Silverstein", "American Eagle", "her coronation", "Vermont", "Plymouth Notch Cemetery", "candy store", "salmon", "pudd'nhead Wilson", "Sydney", "tapir", "a French colony", "Spam", "Dedalus Wagner", "contractions may last for at least an hour, with each one just as intense as the last, and feel strong, then you are probably in labour", "Camembert", "Friday", "the Apocrypha", "centaur", "Mentor", "a Lebanese politician", "Manifest Destiny", "Al Gore", "the Americans with Disabilities Act", "Bali", "Philadelphia", "the European Economic Community (EEC)", "glucosamine", "Madagascar", "a quick search", "a celebration, stunt, spectacle", "busby", "Susan Faludi", "Ice-T", "Al Lang Stadium", "Fidel Castro", "fudge", "a story of the Transcontinental Railroad, 4.0, 0.5, 70789", "Service Employees International Union", "black moor", "auxins", "a dive in which the diver bends in midair to touch the toe, keeping the legs straight, and then...", "yellowtail", "Nitrides of boron & silicon are used to make crucibles", "between the Mediterranean Sea to the north and the Red Sea", "Tokyo", "The technologically superior machine", "Van Morrison", "antelope", "manufacturer, distributor, and marketer of non-alcoholic beverage concentrates", "Rocky Mountain Institute", "21", "stolperstein", "nearly 2,000", "insurgent small arms fire", "about 3,000 kilometers (1,900 miles)", "Lambic"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5166100146198831}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 0.33333333333333337, 0.0, 1.0, 0.10526315789473684, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.888888888888889, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3854", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-7512", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-15543", "mrqa_searchqa-validation-7691", "mrqa_searchqa-validation-7039", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-7145", "mrqa_searchqa-validation-14860", "mrqa_searchqa-validation-8244", "mrqa_searchqa-validation-1451", "mrqa_searchqa-validation-12711", "mrqa_searchqa-validation-1007", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-1108", "mrqa_searchqa-validation-8821", "mrqa_searchqa-validation-7952", "mrqa_searchqa-validation-5611", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-11345", "mrqa_searchqa-validation-10587", "mrqa_searchqa-validation-4481", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-904", "mrqa_searchqa-validation-12755", "mrqa_searchqa-validation-6093", "mrqa_searchqa-validation-379", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10562", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-7747", "mrqa_newsqa-validation-955", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3349"], "SR": 0.4375, "CSR": 0.5491898148148149, "EFR": 0.8055555555555556, "Overall": 0.6775896990740741}, {"timecode": 54, "before_eval_results": {"predictions": ["Keeley Clare Julia Hawes", "1837", "Trace Adkins", "Dan Stevens", "the state legislators of Assam", "a nearby river bottom", "stems and roots of certain vascular plants", "Greek name", "1987", "John F. Kennedy", "in teaching elocution", "Club Bijou on Chapel Street", "seven", "1999", "Rashida Jones", "Hook", "Bush", "tissues in the vicinity of the nose", "December 1, 2009", "the eighth episode in the ninth season of the American animated television series South Park", "70 million people", "a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Nawab Sir Sahib of Turangzai", "approximately 11 %", "mobile telephone call from handheld subscriber equipment", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "1998", "Will Champion", "the Ming dynasty", "for the red - bed country of its watershed", "Thomas Jefferson", "The Intolerable Acts", "National Industrial Recovery Act ( NIRA )", "semi-autonomous organisational units within the National Health Service in England", "Scott Schwartz", "Cyanea capillata", "1996", "Hyderabad", "Johannes Gutenberg", "1885", "the 1964 Republican National Convention in San Francisco, California", "the Finch family's African - American housekeeper", "Somatic motor neurons", "Pyeongchang County, Gangwon Province, South Korea", "New York City", "two - year terms", "The Vamps, McGregor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "invoices raised by a business and delivered to the customer for payment within an agreed time frame", "Tenochtitlan", "Ravi River", "O'Meara", "Philippines", "driving Miss Daisy", "goddess of Revenge", "Marine Corps Air Station Kaneohe Bay", "PlayStation 4", "December 31, 2015", "Seminole", "the Defense of Marriage Act", "Alinghi", "Eiffel Tower", "barnacles", "a dog", "Jim Broadbent, John Cleese and Ricky Gervais"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6526626355670474}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5882352941176471, 0.8, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-6287", "mrqa_naturalquestions-validation-3132", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-5925", "mrqa_triviaqa-validation-892", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-2466", "mrqa_searchqa-validation-11506", "mrqa_triviaqa-validation-7361"], "SR": 0.546875, "CSR": 0.5491477272727272, "EFR": 0.6551724137931034, "Overall": 0.6475046532131661}, {"timecode": 55, "before_eval_results": {"predictions": ["Edward III", "charity", "purple", "aeoline", "Ascot", "euro", "Loretta Lynn", "The Wrestling Classic", "Steppenwolf", "chop suey", "Ross MacManus", "Coronation Street", "The Bill", "Yuvraj Singh", "Saddam Hussein", "New Zealand", "Tyrrhenian Sea", "Bobby Sands", "MauritaniaMauritania", "Hans Lippershey", "Bolivia", "Bob Giraldi", "Mozambique Channel", "ash", "Edward VII", "Edward VI", "testicles", "Guatemala", "muralitharan", "Caroline Aherne", "Byron", "S\u00e8vres", "Mau Mau", "Kipps: The Story of a Simple Soul", "gums", "Serena Williams", "capital of Togo", "Pegida", "Alberich", "Utrecht", "1709", "Mitford sisters", "Kansas", "Miles Morales", "vine plagues", "Skylab", "ostrich", "Hugh Quarshie", "a boat", "Batman", "korea", "Kimberlin Brown", "seven", "August Darnell", "Jack Ridley", "Linux Format", "Stage Stores", "26", "ordered the immediate release", "Shanghai", "Kool", "Treaty of Versailles", "Ken Russell", "Russia and China"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5681818181818181}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5972", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-4775", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-5479", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-3454", "mrqa_triviaqa-validation-1601", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-5930", "mrqa_naturalquestions-validation-8845", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-4642", "mrqa_newsqa-validation-1564", "mrqa_searchqa-validation-5391", "mrqa_searchqa-validation-4261"], "SR": 0.546875, "CSR": 0.5491071428571428, "EFR": 0.4827586206896552, "Overall": 0.6130137777093596}, {"timecode": 56, "before_eval_results": {"predictions": ["France", "Bolivia", "The Telegraph", "liver", "Portugal", "Drunk", "Galway Bay", "Aldo Moro", "calcium carbonate", "Salman Rushdie", "George Eliot", "the north-west corner of the central business district", "koftas", "Benazir Bhutto", "bowler", "Sam Mendes", "Tara King", "Way Back Attack", "the first eight seasons", "business", "godiva", "maise mcManus", "Mexico", "Towy", "dravinsky", "1984", "Swansea", "0-6-0", "Shintoism", "Sussex", "George III", "Mickey Mouse", "oxygen", "Prince Albert", "Talavera de la Reina", "quietly", "Dodoma", "Radiohead", "Wilson", "Loch Lomond", "Pyrenees", "South Korea", "gelatine", "Papua New Guinea", "the Suez Canal", "North Yorkshire", "a\u00e9roport de Gaulle", "Sankt Moritz", "French Revolution", "the old Kent Road", "one of the Vikings nine realms", "anion", "iron", "President since creation of the office in 1789", "Chris Pine", "Yorgos Lanthimos", "Tottenham", "one of the shocks of the year", "off Somalia's coast", "Shanghai", "cape", "John Pershing", "governess", "a large portion of rural Maine, published six days per week in Bangor, Maine."], "metric_results": {"EM": 0.515625, "QA-F1": 0.5987132352941176}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.11764705882352941]}}, "before_error_ids": ["mrqa_triviaqa-validation-3016", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-3116", "mrqa_triviaqa-validation-745", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-6420", "mrqa_triviaqa-validation-4127", "mrqa_triviaqa-validation-6236", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-3964", "mrqa_triviaqa-validation-361", "mrqa_triviaqa-validation-3828", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4370", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-3603", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-10590", "mrqa_searchqa-validation-10653", "mrqa_hotpotqa-validation-4052"], "SR": 0.515625, "CSR": 0.5485197368421053, "EFR": 0.6451612903225806, "Overall": 0.6453768304329371}, {"timecode": 57, "before_eval_results": {"predictions": ["William Shakespeare", "jensen holborn", "casein", "Lee", "Rudolf Nureyev", "Jessica", "placebo", "weather", "Lake Placid", "Vatican City as a sovereign and independent papal state", "contractions", "William Boyd", "Cecilia", "karolina mladenovic", "Morecambe & Wise", "maggie Gilkeson", "butcher", "cowpox", "deer hunting", "Stockholm", "France", "so far away", "anosmia", "Lunar Prospector probe", "Chemnitz", "rue", "yellow", "raven", "caracas", "Ennio Morricone", "Saratoga and Yorktown", "Spain", "Time Team", "Turandot", "adaba", "mauna Kea", "eat porridge", "Howard Keel", "marriage", "boutros gali", "Germany", "Sinclair Lewis", "New Mexico", "in the garden of Gethsemane", "a tree diagram worksheet", "2", "Bild", "France", "Kristiania", "sprint", "selenium", "vehicle that is both four - wheel - drive and primarily a road car", "Noel Kahn", "Tbilisi", "Las Vegas", "2006", "number five", "natural gas", "Michael Jackson was in good health, contrary to media reports he was diagnosed with skin cancer.", "a group of college students of Pakistani background", "Perseid meteor shower", "accordion", "bones", "Marky Mark"], "metric_results": {"EM": 0.5, "QA-F1": 0.6119791666666666}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.4, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-5042", "mrqa_triviaqa-validation-5768", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-5969", "mrqa_triviaqa-validation-7431", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-2366", "mrqa_triviaqa-validation-2804", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-3054", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-2743", "mrqa_triviaqa-validation-1884", "mrqa_triviaqa-validation-5363", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-4499", "mrqa_triviaqa-validation-2116", "mrqa_triviaqa-validation-4425", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-1334", "mrqa_naturalquestions-validation-10066", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-794", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2238", "mrqa_searchqa-validation-3009", "mrqa_searchqa-validation-16209"], "SR": 0.5, "CSR": 0.5476831896551724, "EFR": 0.71875, "Overall": 0.6599272629310345}, {"timecode": 58, "before_eval_results": {"predictions": ["rubbing", "Jonah", "Hughe", "The Delhi", "joseph quotes", "Brazil", "Hudson River", "bones", "Bahamas Junkanoo Festival", "Sons of Liberty", "Napoleon Bonaparte", "Cecil Rhodes", "Hindenburg", "atrium", "Valley Forge", "ruby", "scalpels", "Siberia", "William Pitt the Younger", "five", "Friday the 13th", "New Hampshire", "The Godfather", "melanoma", "Nostradamus", "jihad", "harpoons", "mandy manilow", "financial services", "Conrad Hilton Jr.", "Jasper Johns", "plutonium", "cyanotype", "Zimbabwe", "battle of Trafalgar", "bald eagle", "menudo", "Panax", "hurricanes", "Home Improvement", "Kashmir", "airport", "nu", "new orleans", "statistic", "Little mermaid", "godolphin", "lethal", "college grants that don't have to be paid back & are named for a Rhode Island senator", "beryl", "golden dome", "19 July 1990", "anvil", "Louis XV", "Malawi", "Mansion House", "senior", "London", "Comme des Gar\u00e7ons", "Manasseh Cutler Hall", "danish engineer Karl Kr\u00f8yer", "Nasser Medical Institute in Cairo", "Auckland", "becoming bald or fear of being around bald people."], "metric_results": {"EM": 0.53125, "QA-F1": 0.577359068627451}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-3868", "mrqa_searchqa-validation-1815", "mrqa_searchqa-validation-8324", "mrqa_searchqa-validation-4370", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-11218", "mrqa_searchqa-validation-12288", "mrqa_searchqa-validation-15988", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-4925", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-16444", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-6389", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-4488", "mrqa_searchqa-validation-14124", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-6110", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-737", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1225", "mrqa_triviaqa-validation-2596"], "SR": 0.53125, "CSR": 0.5474046610169492, "EFR": 0.8666666666666667, "Overall": 0.6894548905367233}, {"timecode": 59, "before_eval_results": {"predictions": ["Alabama", "the 1&1 website", "distillation", "Leonard Bernstein", "magnesium", "Attendolo", "the Danube", "the albatross", "Seinfeld", "Smashing Pumpkins", "words", "Ohio State", "William Tecumseh Sherman", "Pakistan", "Theology of God", "Leinster", "Streisand", "Barbara Cartland", "rum", "a Pringles can", "Paul Hamm", "profundo", "East Siberia", "Nimble", "a simple man with a low I.Q. and good", "Clue", "a pair of black magpies, identical in appearance but distinguishably different in personality", "#5367", "alternating current", "Walter Cronkite", "Robert Burns", "Bicentennial Man", "Marshall", "General Motors", "trade winds", "(great-great grandson of George Cabot,... of the United States in 1960; Ambassador to Republic of Vietnam 1963-1964", "silk", "W", "the Unicorn", "Scrabble", "humerus", "The Bodyguard", "Petruchio", "Philippines", "mushrooms", "Ernesto Che Guevara", "Yale University", "Oscar Wilde", "Aeneas", "Dian Fossey", "a map", "an iron -- nickel alloy and some other elements", "ABC", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court", "pear", "Melbourne", "jape", "Ringo Starr", "Do Kyung-soo", "Hanna", "Majid Movahedi", "North Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim", "\"We take this issue seriously,\"", "Priam"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6312761287625417}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6, 0.0, 0.34782608695652173, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6548", "mrqa_searchqa-validation-14429", "mrqa_searchqa-validation-2000", "mrqa_searchqa-validation-4274", "mrqa_searchqa-validation-8167", "mrqa_searchqa-validation-2384", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-116", "mrqa_searchqa-validation-494", "mrqa_searchqa-validation-3853", "mrqa_searchqa-validation-2557", "mrqa_searchqa-validation-12241", "mrqa_searchqa-validation-15665", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-10544", "mrqa_searchqa-validation-2948", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-14151", "mrqa_searchqa-validation-13514", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-15230", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-10610", "mrqa_naturalquestions-validation-5251", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2777"], "SR": 0.546875, "CSR": 0.5473958333333333, "EFR": 0.7931034482758621, "Overall": 0.6747404813218391}, {"timecode": 60, "before_eval_results": {"predictions": ["26 November", "close to 50 million", "Pacific Place", "1002", "Anandji Virji Shah", "Lu\u00eds Carlos Almeida da Cunha", "four", "Sizzin' on Some Syrup", "Chris Stebbins", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Terrina Chrishell Stause", "Karl-Anthony Towns", "five", "Gust Avrakotos", "Sim Theme Park", "a facelifted 850 saloon", "its riverside location", "1858", "Julie Taymor", "actress", "James Edward Kelly", "Spanish", "Indiana University", "lower Manhattan", "Homeland", "2016", "Virgin America", "green and yellow", "Champion Jockey", "March 2012", "Tom Courtenay", "Erinsborough", "2015", "Vladimir Menshov", "The Birds", "Londonderry", "York County", "National Basketball Development League", "Father Dougal McGuire", "Bill Curry", "UFC Fight Pass", "25 August 1949", "Savannah River Site", "Eardwulf", "God and the just cause", "Swiss", "Augustus", "World War I", "October 4, 1970", "Marktown", "five", "Rodney Crowell", "Mendel", "near major hotels and in the parking areas of major Chinese supermarkets", "scales", "d\u00fcsseldorf", "Apollo", "Anil Kapoor", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Arthur E. Morgan III", "quarantina", "Silk Road", "ABBA", "Spain"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6568681318681319}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-4817", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-3132", "mrqa_hotpotqa-validation-3911", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1812", "mrqa_naturalquestions-validation-6949", "mrqa_triviaqa-validation-2692", "mrqa_newsqa-validation-867", "mrqa_searchqa-validation-14894", "mrqa_naturalquestions-validation-6011"], "SR": 0.578125, "CSR": 0.5478995901639344, "EFR": 0.7777777777777778, "Overall": 0.6717760985883425}, {"timecode": 61, "before_eval_results": {"predictions": ["Gaels", "Chicago Bears", "girls aged 11 to 18", "Taylor Swift", "Adolfo Rodr\u00edguez Sa\u00e1", "Freeform", "Adult Swim", "1983", "Rio Gavin Ferdinand", "264,152", "2,664", "841", "first and second segment", "Australian Broadcasting Corporation (ABC)", "MG Car Company Limited", "Walt Disney and Ub Iwerks", "1979", "15", "1898", "John D. Hoyer", "Bolton", "Argentinian", "Them", "137\u201373", "John Snow", "New York and New Jersey campaign", "2013\u201314", "Melbourne Storm", "UNLV", "21", "Dziga Vertov", "Friday", "Oklahoma Sooners", "2011", "7pm", "1866", "Gaahl", "Serie B", "1887", "Isabella (Belle) Baumfree", "RAF Tangmere, West Sussex", "North Holland", "Don Bluth and Gary Goldman", "Golden Calf", "Furious 7", "2011 AFC Asian Cup", "Julia Kathleen McKenzie", "Mercer", "1951", "35,124", "21 years and 154 days", "September 30", "Jimmy Flynn", "the Soviet Union and its satellite states", "his finger", "Ronald Wilson Reagan", "One Thousand and One", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "protecting 50,000 jobs in the last 18 months and would continue to shed jobs without this program.", "forcibly drugging", "James Watt", "T.S. Eliot", "ANastasia", "Games"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6319183600164202}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.8, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.5, 0.6666666666666666, 1.0, 1.0, 0.2666666666666667, 0.06896551724137931, 0.25, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-3044", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-3306", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-1609", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-1285", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-3679", "mrqa_naturalquestions-validation-5180", "mrqa_triviaqa-validation-7611", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-129"], "SR": 0.515625, "CSR": 0.5473790322580645, "EFR": 0.8709677419354839, "Overall": 0.6903099798387097}, {"timecode": 62, "before_eval_results": {"predictions": ["Paris", "13 October 1958", "Walt Disney and Ub Iwerks", "barcode", "Ishtar Gate", "a card (or cards) during a card game", "water", "Sean Yseult", "law", "October 5, 1937", "Hillsborough County", "Charles Nungesser", "Burning Man", "Love Streams", "King George VI", "August 10, 1933", "Dallas", "Black Panthers", "globetrotters", "Francis Schaeffer", "Somerset County, Pennsylvania", "Simon Bolivar Buckner", "German", "Gareth Jones", "consulting", "March 2012", "1978", "actor, producer, and director", "Melanie Owen", "1983", "Indian state of Gujarat", "143,007", "May 4, 1924", "jewelry designer", "Axl Rose", "Nebula Award, the Philip K. Dick Award, and the Hugo Award", "The Big Bang Theory", "the German princely Battenberg family", "dice", "Kal Ho Naa Ho", "Dungeness", "Pendlebury, Lancashire", "25 October 1921", "Canadian comedian", "Martin O'Neill", "Jeremy Hammond", "Reginald Engelbach", "American", "Black Friday", "Minneapolis", "Jean Erdman", "5.7 million customer accounts", "During Hanna's recovery masquerade celebration", "Anakin", "Richard Seddon", "the heart", "Sitka, Alaska", "to take the Rio Group to a new level by creating the organization.", "\"Now that we know Muhammad is an Ennis man, we will be back,\" Ali's wife Lonnie told Britain's Daily Telegraph newspaper.", "Bob Bogle", "circumflex", "The Hague", "a shooting-brake", "2001"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6285511363636362}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727272, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-515", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-3911", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-209", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-2535", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-197", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-3658", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-7459", "mrqa_newsqa-validation-3319", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-12404"], "SR": 0.53125, "CSR": 0.5471230158730158, "EFR": 0.6666666666666666, "Overall": 0.6493985615079365}, {"timecode": 63, "before_eval_results": {"predictions": ["to a in for", "two weevils", "\"twenty times the value of the sum\"", "The Many Colours of Carrot Roots", "Beluga whale", "Nicholas II", "tuna", "shalom", "Russia", "a chimp", "The Larry Sanders Show", "Buenos Aires Times", "Thor", "Orange", "astride", "Borneo", "Versailles", "cereal", "Winston-Salem", "whipped cream", "tuna", "the Thane", "Jean-Michel Basquiat", "Led Zeppelin", "The Hidden Blade", "Dutchman", "The U.S. SENATE", "the outskirts of a small Southern town", "Lieutenant Columbo", "John Tyler", "Atlanta", "a tranfgrelTion", "Wall Street", "sake", "Notre Dame", "The Port of Portland", "Marquis de Lafayette", "The Indianapolis 500", "Toy Story", "Dead Serious Improv", "Charles Askegardshe", "a 13-letter collective name for any & all forms of water", "Nikolai Gogol", "Oscar Wilde", "Fletcher Christian", "weaving", "Pope John Paul II", "Greenland", "John", "The Marx Brothers", "watermelon", "Phillip Schofield and Christine Bleakley", "duodenum", "Reverend J. Long", "violin", "sexual imagination", "a mountain peak in the Karakoram Range in northern Kashmir", "Garrett Morris", "1966 US tour", "12 mi east-southeast of Bridgeport", "Susan Atkins", "almost 9 million", "three thousand", "al-Maliki"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6136904761904761}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.4, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_searchqa-validation-13774", "mrqa_searchqa-validation-6211", "mrqa_searchqa-validation-513", "mrqa_searchqa-validation-6512", "mrqa_searchqa-validation-10557", "mrqa_searchqa-validation-5508", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-7670", "mrqa_searchqa-validation-15074", "mrqa_searchqa-validation-14769", "mrqa_searchqa-validation-724", "mrqa_searchqa-validation-16930", "mrqa_searchqa-validation-14853", "mrqa_searchqa-validation-8527", "mrqa_searchqa-validation-2728", "mrqa_searchqa-validation-8555", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-3642", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-6694", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-15871", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-4356", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-667", "mrqa_newsqa-validation-3671"], "SR": 0.515625, "CSR": 0.546630859375, "EFR": 0.7419354838709677, "Overall": 0.6643538936491936}, {"timecode": 64, "before_eval_results": {"predictions": ["the most gigantic pumpkins in the world", "Seminole Tribe", "billions of dollars in Chinese products each year", "green-card warriors", "228", "a traditional form of lounge music that flourished in 1940's Japan.", "2005", "contaminated groundwater, hundreds of buildings used for plutonium enrichment that need to be torn down, and underground tanks that are full of radioactive sludge.", "consumer confidence", "Fernando Gonzalez", "the southern port city of Karachi, Pakistan's largest city and the capital of Sindh province.", "Dan Parris, 25, and Rob Lehr, 26,", "Jared Polis", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah", "Russia", "Obama", "Sunday", "her husband and her abductors.Mayor Cesar Lobregat, head of a Crisis Management Committee in Zamboanga City,", "France", "41,280", "be silent", "iTunes", "Kenyan and Somali governments", "\"gotten the balance right\" on Myanmar, the military junta-ruled Asian nation formerly known as Burma, by starting a dialogue while maintaining sanctions,", "100 to 150", "10", "Quiet Nights", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England this weekend.", "Iran", "123 pounds of cocaine and 4.5 pounds of heroin", "engineering and construction", "\"I think I killed somebody.\"", "fractured pelvis and sacrum", "five", "to step up", "12 years after the discovery of Hettrick's stabbed and sexually mutilated corpse in a field near his trailer.", "MOSCOW, Russia", "Mashhad", "summer", "one", "Bryant Purvis", "Jeanne Tripplehorn", "al Qaeda", "Garth Brooks", "Oxbow", "an Iranian court", "different women coping with breast cancer in five vignettes.", "Michael Schumacher", "Luiz Inacio Lula da Silva", "the release of the four men", "2006", "12.9 - kilometre ( 8 mi )", "Tim Passmore", "Odoacer", "Ted Heath", "Estonia", "\"Rarely is the question asked, is our children learning?\"", "Anne", "Sergeant First Class", "Champion Jockey", "Frederic Remington", "Hipparchus", "April 2, 1917", "a pirate"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5846495084776335}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5555555555555556, 0.0, 1.0, 0.2222222222222222, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.2727272727272727, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8750000000000001, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-4133", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-57", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-435", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-2545", "mrqa_naturalquestions-validation-1147", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-616", "mrqa_hotpotqa-validation-1077", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-10515", "mrqa_searchqa-validation-11117", "mrqa_searchqa-validation-709"], "SR": 0.453125, "CSR": 0.5451923076923078, "EFR": 0.5142857142857142, "Overall": 0.6185362293956044}, {"timecode": 65, "before_eval_results": {"predictions": ["269,000", "Dillinger and his gang rampaged through the American Midwest, staging jail breaks, robbing banks, and killing 10 men and wounding seven along the way.", "a rocket", "February 12", "Mandi Hamlin", "United Nations", "\"falling space debris,\"", "at a Little Rock military recruiting center", "voluntary manslaughter", "if you don't get to the hospital to have surgery to drain the fluid, \"the deterioration can happen very quickly,\"", "Chris Robinson", "Grease", "Cipro", "34", "E. coli bacteria", "More than 15,000", "a good vegan cupcake has the power to transform everything for the better,\"", "The Sopranos", "the Middle East and North Africa,", "September,", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "South Africa", "Dick Cheney,", "The apartment building collapsed together with two other buildings on March 3.", "Mary Procidano", "injectable vitamin supplements", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "400 years ago", "Gulf of Aden,", "Obama", "31 meters (102 feet) long and 15 meters (49 feet) wide", "Caylee Anthony,", "the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "Wednesday", "managing his time", "not including co-pays or deductibles", "bipartisan", "us to step up.\"", "he acted in self defense in punching businessman Marcus McGhee.", "education about rainforests.", "three", "drug cartels", "Texas Department of Criminal Justice,", "Trevor Rees", "28 passengers,", "Espinoza", "Ed McMahon,", "London Heathrow's Terminal 5", "creation of an Islamic emirate in Gaza,", "Prince George's County Correctional Center", "Genocide Prevention Task Force", "243 days", "Secretary of Homeland Security", "August 5, 1937", "20", "Wisconsin", "Mr Loophole", "Arlo Looking Cloud", "New England", "1694", "the Golden Fleece", "Gustav", "Amish TV", "6teen"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6415831838118022}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2727272727272727, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.4, 0.5, 0.0, 1.0, 1.0, 0.10526315789473685, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-1662", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1801", "mrqa_newsqa-validation-1822", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-242", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-2735", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-5094", "mrqa_triviaqa-validation-1217", "mrqa_searchqa-validation-5235"], "SR": 0.546875, "CSR": 0.545217803030303, "EFR": 0.7931034482758621, "Overall": 0.674304875261233}, {"timecode": 66, "before_eval_results": {"predictions": ["Jeffery Deaver", "r(anging)", "Pete Best", "Robert Taylor", "Vincent Van Gogh", "Spain", "about a mile north of the village of Dunvegan", "ArcelorMittal Orbit", "lodges", "Joseph W(arren)", "Tallinn", "the solar system", "coelacanths", "Belgium", "Dennis Potter", "Calcium", "Eric Coates", "Andrew Gardner, Reginald Bosanquet, Sandy Gall, Anna Ford, Alastair Stewart and Trevor McDonald", "Mel Brooks", "condor", "Ohio", "wind turbines", "Houmidan Grizelda Pugh,", "police drama The Bill", "0 for 7", "Hamlet", "Johannesburg", "Crackerjack", "Bleak House", "Rodgers and Hammerstein", "Spain", "The Sweeney", "special sauce", "Les Dennis", "Kansas City", "Hard Times", "Tuscany", "18 meters", "Singapore", "Scooby-Doo! Mystery Incorporated", "Pakistan International Airlines", "gold, red, blue, black and white", "France", "Tomorrow Never Dies", "his father", "Hong Kong", "Chuck Yeager", "violet- Elizabeth Bott", "Canada", "stamp collecting", "Moby Dick", "the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "12.9 - kilometre ( 8 mi )", "a sociological perspective which developed around the middle of the twentieth century and that continues to be influential in some areas of the discipline", "21 July 2015", "Bern", "28 June 1945", "foreign Terrorist Organization", "25", "Pakistan", "Yves Saint Laurent", "Rush", "Topix", "his business dealings for possible securities violations"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5959821428571429}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333336, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.09523809523809525, 1.0, 0.09523809523809523, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6]}}, "before_error_ids": ["mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-89", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-5331", "mrqa_triviaqa-validation-7476", "mrqa_triviaqa-validation-6792", "mrqa_triviaqa-validation-3319", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-3366", "mrqa_triviaqa-validation-1294", "mrqa_triviaqa-validation-3663", "mrqa_triviaqa-validation-7066", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-4003", "mrqa_triviaqa-validation-613", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-1450", "mrqa_hotpotqa-validation-4788", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1218", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-11790", "mrqa_newsqa-validation-2682"], "SR": 0.53125, "CSR": 0.5450093283582089, "EFR": 0.6666666666666666, "Overall": 0.6489758240049751}, {"timecode": 67, "before_eval_results": {"predictions": ["to convert single - stranded genomic RNA into double - stranded cDNA", "the federal government", "Kevin Corrigan", "August 18, 1945", "after obtaining the consent of the United Kingdom", "a cake", "Olivia Olson", "Los Angeles", "Pyeongchang County, South Korea", "928", "April 7, 2016", "5.7 million customer accounts", "Wembley Stadium", "the President", "David Joseph Madden", "The Fixx", "at night", "Jack Nicklaus", "Plank", "Executive Residence of the White House Complex", "the Royal Air Force ( RAF )", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "the benefits of the US privacy Act to Europeans and gives them access to US courts", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "18th century", "Mariah Carey", "Spektor", "H CO", "seven", "September of that year", "Gertrude Niesen", "October 14, 2017", "Krypton", "November 25, 2002", "IBM", "Soviet Union", "elected", "sport utility vehicles", "Elk and Kanawha Rivers", "The Bellamy Brothers", "the University of Oxford", "Massachusetts", "the plane crash in 1959", "Sir Mix - a-Lot", "June 1991", "July 8, 1997", "New Zealand to New Guinea", "Frank Oz", "1954", "2010", "Missouri River", "\u00c6thelstan", "lute", "Greece", "John Churchill, 1st Duke of Mindelheim", "Gregg Popovich", "Asiana Town building", "Jaime Andrade", "Eleven", "16 Indiana National Guard soldiers", "bees", "Jefferson Davis", "Farsi (Persian)", "hyperbole"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6421770083631538}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true], "QA-F1": [0.5263157894736842, 0.0, 0.0, 0.3333333333333333, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.1904761904761905, 1.0, 1.0, 1.0, 0.0, 0.7368421052631579, 0.5263157894736842, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 0.5, 0.11764705882352941, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8333333333333334, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-5511", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-6383", "mrqa_triviaqa-validation-5828", "mrqa_triviaqa-validation-6897", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-2886"], "SR": 0.53125, "CSR": 0.5448069852941176, "EFR": 0.6333333333333333, "Overall": 0.6422686887254903}, {"timecode": 68, "before_eval_results": {"predictions": ["German state of North Rhine-Westphalia", "Roberta Flack", "sesame seed", "eldest or heir apparent", "September 19", "Barnaby Rudge", "Buddha", "Ethiopia", "1963", "discus", "tabloid", "the royal yacht Britannia", "Chester Racecourse", "California", "Jews of Iberia (in Hebrew, Sepharad) and the Spanish diaspora", "Romanian", "saint Basil's", "Peru", "the keel on the outside", "Evander Holyfield", "a crosse or lacrosse stick.", "Buddhist", "New Orleans", "soda", "a pot or crock and covered with fat.", "Steve Hansen", "brash", "Ken Burns", "Paddy Doherty", "Barry and Yvonne", "phi - the 21st letter of the Greek alphabet", "Hungary", "So Solid Crew", "blues-rock", "Pennsylvania", "caucausus range", "Wales", "morningtown Ride", "Jupiter", "woodentop", "child", "two", "QPR", "Wide Area Augmentation System", "giants", "heart", "B\u00e9la Bart\u00f3k", "Hugh Dowding", "Montpelier", "February", "James IV", "annual income of US $11,770", "318", "Chris Rea", "Andrzej Go\u0142ota", "scholar during the Joseon Dynasty who begins to write erotic novels, and becomes the lover of the King's favorite concubine.", "March 30, 2025", "Knox's parents,", "Dubai", "a rabbit hole, if you will,", "held", "Russia", "American adventure comedy film", "cachemira donde amar no ser un crimen"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5868179563492064}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.19999999999999998, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-7661", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-1786", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-7499", "mrqa_triviaqa-validation-2330", "mrqa_triviaqa-validation-6464", "mrqa_triviaqa-validation-802", "mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-214", "mrqa_triviaqa-validation-1193", "mrqa_triviaqa-validation-4652", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-955", "mrqa_triviaqa-validation-4490", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-412", "mrqa_newsqa-validation-361", "mrqa_searchqa-validation-6026", "mrqa_searchqa-validation-10485", "mrqa_searchqa-validation-7826"], "SR": 0.53125, "CSR": 0.5446105072463768, "EFR": 0.6333333333333333, "Overall": 0.6422293931159421}, {"timecode": 69, "before_eval_results": {"predictions": ["California", "Jeffrey Archer", "Chicago", "California Chrome", "Dar es Salaam", "Sarah Keays", "Miss Marple", "Elkie Brooks", "United Parcel Service", "Djokovic", "piano", "Cambridge", "Bennet", "The Beatles", "glycerol", "Addams", "Doubting Castle", "beetle", "australian", "geoff Hurst", "Harry Shearer", "9-13 years", "pirate day", "penny", "Spice Girls", "The Golden Child", "AFC Wimbledon", "flemish i Gaming", "Tombstone", "Friedrich Nietzsche", "Cambridge", "South Africa", "Bagram", "Pygmalion", "bajan", "blackcurrant", "Dieppe Raid", "dengue fever", "The Left Book Club", "triathlon", "Gabriel Byrne", "dividing of cells into additional cell bodies", "Strictly Come Dancing", "sound and light", "par-5", "jack Russell Terrier", "prairies", "raclette", "Denali", "The Magic Circle", "Yalta Conference", "start fires, hunt, and bury their dead", "London", "Cordelia", "London Luton Airport", "Sarah", "Antigua & Barbuda, Argentina, South Africa, Fernando P\u00f3, S\u00e3o Tom\u00e9, Madagascar, Mauritius, Mayotte, R\u00e9union, Seychelles,", "he said Chaudhary's death should serve as a warning to management,", "\"the evidence and investigatory effort has minimized the likelihood that Haleigh's disappearance is the work of a strangers.\"", "\"remained at the bottom of the hill surviving on leaves and water from a nearby creek,\" the report said.", "Vanilla Ice", "Wordsworth", "Voltaire", "Mars Hill, 150 miles ( 240 km ) to the northeast"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5218229166666667}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.5, 0.6, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.08333333333333333, 0.08, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-4988", "mrqa_triviaqa-validation-4781", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-2639", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-2042", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-5185", "mrqa_triviaqa-validation-6478", "mrqa_triviaqa-validation-2422", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-4970", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-9011", "mrqa_hotpotqa-validation-1017", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1716", "mrqa_newsqa-validation-3566", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3627", "mrqa_searchqa-validation-3002", "mrqa_naturalquestions-validation-6046"], "SR": 0.453125, "CSR": 0.5433035714285714, "EFR": 0.6857142857142857, "Overall": 0.6524441964285714}, {"timecode": 70, "before_eval_results": {"predictions": ["Geraldine Margaret Agnew - Somerville", "A complex sentence", "Australia", "Ashrita Furman", "No Secrets", "1994", "2010", "12 November 2010", "1 October 2006", "1977", "2018", "Missouri", "Swedien and Jones", "1966", "Frederick Chiluba", "4 January 2011", "Dougie MacLean", "the band's logo in gold lettering over black sleeve", "1999", "the Maryland Senate", "Latitude", "anembryonic gestation", "Michael Rosen", "Jos\u00e9 Mart\u00ed", "103", "Steve Lukather", "U.S. dollar banknotes", "referee", "Bonhomme Carnaval", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "If These Dolls Could Talk", "around 2011", "Seton Hall Pirates men's basketball team", "ulnar nerve", "late - 2011", "British Indian Association", "indigenous to many forested parts of the world", "Western Australia", "Carol Worthington", "1830", "exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "thanksgiving for a good harvest", "28", "a contemporary drama in a rural setting", "Justice Harlan", "Bart Howard", "Triple threat", "the birth centenary of Pandit Jawaharlal Nehru", "Anthony Hopkins", "Jesus Christ", "1996", "holography", "Spanish", "far from the madding crowd", "Gillian Leigh Anderson", "direct scattering and inverse scattering", "45th Infantry Division", "\"it should stay that way.\"", "2009", "an open window", "Dutchman", "Samuel Taylor Coleridge", "Pygmalion", "yen"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6742099964985995}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.823529411764706, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7142857142857143, 1.0, 1.0, 0.5882352941176471, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.23529411764705882, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.8333333333333333, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9867", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-2623", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-6113", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-7692", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-5434", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-706", "mrqa_newsqa-validation-2658", "mrqa_searchqa-validation-4464"], "SR": 0.5625, "CSR": 0.5435739436619718, "EFR": 0.6428571428571429, "Overall": 0.6439268423038229}, {"timecode": 71, "before_eval_results": {"predictions": ["John Goodman", "Brooke Wexler", "Lou Rawls", "inner core", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "David Gahan", "the ball is fed into the gap between the two forward packs and they both compete for the ball to win possession", "Total Drama Action", "left of the dinner plate", "red", "off the rez", "either in front or on top of the brainstem", "March 14, 1942", "Agamemnon", "Epithelium", "Erika Mitchell Leonard", "American production duo The Chainsmoker", "Vincent Price", "Pakistan", "Tessa Peake - Jones", "an expected or free or continuously changing behaviour and may have a given individual social status or social position", "United Nations Peacekeeping Operations", "it is part of the normal flora of the human colon and is generally commensal, but can cause infection if displaced into the bloodstream or surrounding tissue following surgery, disease, or trauma", "Noahic Covenant", "Shirley Mae Jones", "heat transfer by thermal radiation may be minimized by silvering flask surfaces facing the gap", "Luke 6 : 67 -- 71", "August 19, 2016", "Sets heart in mediastinum and limits its motion", "scrolls", "Terrell Suggs", "Cecil Lockhart", "August 22, 1980", "the optic disc to the optic chiasma and continues as the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "letter series", "August 21", "along the ridge of the mountains", "The `` Southern Cause ''", "1955", "electrons from electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions, and couples this electron transfer with the transfer of protons ( H ions ) across a membrane", "2", "Montreal", "2002", "3 September", "three levels", "product / market fit", "Wyatt `` Dusty '' Chandler ( George Strait )", "last book accepted into the Christian biblical canon", "In the 1920s", "ice giants", "on what became known to locals as `` Black Monday '', and continued into the mid-1980s", "Camellia sinensis", "European Economic Community (EEC)", "Mumbai", "Frank Fertitta, Jr.", "Coronation Street", "Michael Cremo", "backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry rewards.", "complicated and deeply flawed", "Crandon, Wisconsin", "butterflies", "Rocky Mountain Fever", "11:30 a.m.", "drug labs, markets and convoys"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5646368169701899}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5714285714285715, 0.888888888888889, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.35294117647058826, 0.75, 1.0, 1.0, 1.0, 0.12121212121212122, 0.0, 0.12903225806451613, 1.0, 1.0, 0.7272727272727273, 0.75, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.1904761904761905, 0.0, 0.0, 0.0, 0.0, 1.0, 0.12903225806451613, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.967741935483871, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-7264", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-5114", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-627", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-6049", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-9774", "mrqa_triviaqa-validation-3027", "mrqa_hotpotqa-validation-1747", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2315", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-10588", "mrqa_searchqa-validation-2088", "mrqa_newsqa-validation-2177"], "SR": 0.390625, "CSR": 0.5414496527777778, "EFR": 0.7692307692307693, "Overall": 0.6687767094017094}, {"timecode": 72, "before_eval_results": {"predictions": ["Earth Day", "Afghanistan", "a dark brownish, very thick sauce with many dark bacon chunks swimming in solution.", "suffrage", "barahir", "Las Vegas", "salty", "a cloudy day", "Philip Berrigan", "wheat", "Carole King", "maures", "Pro-Jig Clamp Set", "Christo", "Wichita", "U.S. Department of Agriculture", "Gilligan's Island", "Penelope", "Tom Harkin", "Channel Islands", "Krackel", "Penelope", "pronouns", "bonobos", "Harry's Harbor", "Veep", "aces", "cradle song", "Burma Ruby stone", "Pan's Labyrinth", "barrie", "John Irving", "singular", "The Who", "the Aegean Sea, the Dardanelles-Sea of Marmora-Bosporus", "Xerox", "extra virgin", "Pierre Trudeau", "earned run average", "anxiety disorder", "World War II", "dali", "Lee Harvey Oswald", "George Armstrong Custer", "Newton's", "breath", "Stockholm", "Alaska", "a puff", "Mausolus of Caria in Asia Minor", "qi", "Number 4, Privet Drive, Little Whinging in Surrey, England", "John Smith", "L.K. Advani", "Matthew", "Pentateuch", "Saint Cecilia", "Germany", "1989 until 1994", "600", "80 percent of the woman's face", "Joan Rivers", "three", "Kim Bauer"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5584160052910053}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.14814814814814814, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-383", "mrqa_searchqa-validation-14432", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-10065", "mrqa_searchqa-validation-9796", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-5330", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-11098", "mrqa_searchqa-validation-3351", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-2136", "mrqa_searchqa-validation-2799", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-2902", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-16212", "mrqa_searchqa-validation-11913", "mrqa_searchqa-validation-10453", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-9881", "mrqa_naturalquestions-validation-6720", "mrqa_triviaqa-validation-6286", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-1741", "mrqa_newsqa-validation-1681"], "SR": 0.484375, "CSR": 0.540667808219178, "EFR": 0.48484848484848486, "Overall": 0.6117438836135326}, {"timecode": 73, "before_eval_results": {"predictions": ["Baltimore", "Richie", "Rita Mae Brown", "Bolivia", "New Hampshire", "grasshopper", "the commander", "sure deodorant", "1876", "to protect babies", "onlooker", "Humphrey Bogart", "Maryland", "Lowenbrau", "pen", "Herod", "the Lone Ranger", "Malaysia", "Xavier High School", "Barry Goldwater", "Goofy", "Walter Payton", "Mount Everest", "Marcus Garvey", "Crossword", "the smallest", "Tom Thumb", "Strathearn", "the Mad Hatter", "tryptophan", "Cincinnati", "to aid the athlete's performance", "concert grand", "ketchup", "peanut butter", "Pel", "Tom Petty", "Tuscany", "Tunisia", "Rosa Parks", "inch", "Toulouse", "William Henry Harrison", "Corinthian", "carats", "Bern", "Prada", "the Chicago Civic Center", "the umbilical cord", "Pinta", "possible", "October 22, 2017", "Terrell Owens", "April 21, 2015", "Tipping Point", "Scotland", "the dogger Bank", "NBA 2K16", "ethereal wave", "Ron Goldman", "Mad Men", "fusion teams", "Rolling Stone", "Ren\u00e9 Descartes"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7182291666666667}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-360", "mrqa_searchqa-validation-16935", "mrqa_searchqa-validation-16533", "mrqa_searchqa-validation-13681", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-13507", "mrqa_searchqa-validation-12916", "mrqa_searchqa-validation-8753", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-4837", "mrqa_searchqa-validation-13871", "mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-2988", "mrqa_searchqa-validation-12320", "mrqa_searchqa-validation-15724", "mrqa_naturalquestions-validation-7366", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-5148"], "SR": 0.671875, "CSR": 0.5424408783783784, "EFR": 0.8095238095238095, "Overall": 0.6770335625804376}, {"timecode": 74, "before_eval_results": {"predictions": ["Urban Outfitters", "The Tyger", "Thunder Road", "The Last Supper", "Baccarat", "Rook", "Harlem", "Hertogenbosch", "hulls", "addiction", "cricket", "India", "Children of Men", "Skagway, Alaska", "a petition", "Hippolyta", "a phylum", "John Galt", "Spinach", "milk", "1,000 watts", "Fecund", "World War I", "student loan", "Tony's", "Itzhak Perlman", "Wolfgang Puck", "dachshund", "the Monitor", "Cyprus", "Milwaukee", "a hot latte", "Kevin Costner", "Hot Lips", "Isadora Duncan", "Pig Latin", "Little Debbie", "Donald Rumsfeld", "Speed Racer", "USA", "Aristotle", "ER", "Eagles", "An American Tail", "a bus tour", "the argyle", "The Prius", "a wallaby", "leather", "Mark Twain", "Greg", "30 October 1918", "Mel Tillis", "Michael Moriarty", "James Bolam", "all pieces capture opponent's pieces by moving to the square that the opponent's piece occupies.", "northwest India", "House of Habsburg-Lorraine", "Obergruppenf\u00fchrer", "Kansas\u2013Nebraska Act", "Orbiting Carbon Observatory", "South Africa", "Tuesday", "two"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6875}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-2629", "mrqa_searchqa-validation-1283", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-16232", "mrqa_searchqa-validation-3118", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-13579", "mrqa_searchqa-validation-16121", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-5816", "mrqa_searchqa-validation-10468", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-11419", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-3342", "mrqa_searchqa-validation-7027", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3779", "mrqa_hotpotqa-validation-686"], "SR": 0.65625, "CSR": 0.5439583333333333, "EFR": 0.8181818181818182, "Overall": 0.6790686553030303}, {"timecode": 75, "before_eval_results": {"predictions": ["Muhammad", "following the 2017 season,", "China in modern times", "1908", "DNA at the origin and synthesis of new strands, accommodated by an enzyme known as ligase, results in replication forks growing bi-directionally from the origin", "Javier Fern\u00e1ndez", "Michael Crawford", "silk floss tree", "Hold On", "Gustav Bauer, the head of the new government", "November 2016", "Empiricism", "Identification of alternative plans / policies", "Augustus Waters", "North America, Greenland has been politically and culturally associated with Europe ( specifically Norway and Denmark, the colonial powers, as well as the nearby island of Iceland ) for more than a millennium", "Johnson, a lifelong Democrat and the Republican majority in Congress over how best to deal with the defeated Southern states following the conclusion of the American Civil War", "Song of Songs", "Johnny", "its vast territory was divided into several successor polities", "an abbreviation from the initial components in a phrase or a word, usually individual letters ( as in NATO or laser ) and sometimes syllables", "Sheev Palpatine", "Divyanka Tripathi", "September 24, 2012", "Rick Rude", "Michael Christopher McDowell", "in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "homicidal thoughts of a troubled youth", "Dan Bern and Mike Viola ( of the Candy Butchers )", "Daniel A. Dailey", "Mickey Mantle", "addition", "December 15, 2016", "Kid Creole and the Coconuts", "AD 1 immediately follows the year 1 BC", "2010", "microfilament", "1983", "Edward Douglass White, Charles Evans Hughes, Harlan Fiske Stone, and William Rehnquist", "President of the United States and confirmed by the Senate for staggered 14 - year terms", "1978", "Ravi River", "May 19, 2017", "47 cents", "Podujana Peramuna, led by former president Mahinda Rajapaksa, secured the most seats and local authorities.", "1773", "Buddhism", "Extroverted Thinking ( Te )", "March 16, 2018", "Joseph M. Scriven", "Heat transfer by thermal radiation may be minimized", "Frederik Barth", "peacock", "wakefield", "Thomas Stearns Eliot", "Russell Humphreys", "Lieutenant Colonel Horace Meek Hickam", "Rihanna", "The minister later apologized, telling CNN his comments had been taken out of context.", "fractured pelvis and sacrum", "2001", "loss", "Blackbird", "Callie Torres", "September 25, 2017"], "metric_results": {"EM": 0.5, "QA-F1": 0.5682064539831084}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0909090909090909, 0.0, 0.22222222222222218, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.3636363636363636, 0.17142857142857143, 0.08333333333333333, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.5714285714285715, 0.0, 0.0, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-4308", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-9450", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-8004", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-1368", "mrqa_triviaqa-validation-6929", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-1576", "mrqa_searchqa-validation-5337", "mrqa_searchqa-validation-14889"], "SR": 0.5, "CSR": 0.5433799342105263, "EFR": 0.75, "Overall": 0.6653166118421053}, {"timecode": 76, "before_eval_results": {"predictions": ["bridal shop", "Walter Mondale", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning", "1928", "the team that lost the pre-game coin toss", "21 June 2007", "at least 28", "Theodore Roosevelt, who became president at the age of 42 years, 322 days, following William McKinley's assassination", "once in about 24 hours with respect to the Sun, but once every 23 hours, 56 minutes, and 4 seconds", "Universal Pictures and Focus Features", "multiple origins", "restarting play after a minor infringement", "A footling breech", "( 2012 ) set records for the opening day and the biggest opening weekend for a non-sequel film. The Hunger Games : Catching Fire ( 2013 )", "the President of India", "to signify cunnilingus", "28 %", "Elvis Presley", "N\u0289m\u0289n\u0289\u0289", "JackScanlon", "the eighth episode of Arrow's second season", "Elijah Wood", "head - up display", "Doug Pruzan ( season 2 -- 6 ; played by Alex Skuby )", "1984", "Donna Reed", "in organelles, such as mitochondria or chloroplasts", "pathology", "1986", "introduced and elaborated as early as in 1651 by Thomas Hobbes in his Leviathan, though with a somewhat different meaning ( similar to the meaning used by the British associationists )", "William the Conqueror", "Shawn Wayans", "Wisconsin", "slavery", "ingredients", "Jourdan Miller", "tanks completed in late 1944", "limited period of time", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "New England Patriots ( 5 - 4 )", "40 %", "Janie Crawford's `` ripening from a vibrant, but voiceless, teenage girl into a woman with her finger on the trigger of her own destiny", "the east coast of Queensland", "April 2, 2018", "Lana Del Rey", "Jerry Leiber and Mike Stoller", "Sonu Nigam", "Arnold Schoenberg", "St. Louis Blues ( 3 in 1968 -- 1970 )", "around 1872", "Idaho's Snake River Valley", "starch", "carbon", "on the first Monday of September", "Prudential Center in Newark, New Jersey", "Lisburn Distillery F.C.", "Phelan Beale", "one", "Government Accountability Office report", "journalists and aid workers", "double-breasted", "Heroes", "a lush, plentiful version of the Egypt of the living", "since 1983"], "metric_results": {"EM": 0.375, "QA-F1": 0.5198425455690094}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false], "QA-F1": [0.4444444444444445, 0.0, 0.25, 0.0, 0.8571428571428571, 0.0, 0.26666666666666666, 0.0, 0.2727272727272727, 1.0, 0.0, 1.0, 1.0, 0.4347826086956522, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.7692307692307692, 0.5714285714285715, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.0, 0.32258064516129037, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.30769230769230765, 0.4615384615384615, 0.0, 0.0, 0.09090909090909091, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.2, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1427", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-8452", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9487", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-1459", "mrqa_naturalquestions-validation-8301", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-2782", "mrqa_triviaqa-validation-3040", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-218", "mrqa_hotpotqa-validation-3250", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2043", "mrqa_searchqa-validation-15577", "mrqa_newsqa-validation-377"], "SR": 0.375, "CSR": 0.5411931818181819, "EFR": 0.55, "Overall": 0.6248792613636364}, {"timecode": 77, "before_eval_results": {"predictions": ["herpes virus, (Another name for shingles is herpes zoster.)", "Zork", "Roddy Doyle", "chapter twenty six, \"My guardian was in the room, washing his hands with scented soap.\"", "Prussia", "Rudyard Kipling", "SpongeBob SquarePants", "Exile", "an enclave which is entirely enclosed by another nation.", "South Dakota", "Brian Close", "a caterpillar", "Leeds", "Edinburgh", "a traffic warden", "cricketer", "(Switzerland)", "Neptune", "VimtoVimto", "people or society.", "leicestershire", "carry On Cleo", "Egypt", "sense of taste", "snare drum", "pangea", "sesame seed", "hurdles", "The Centaurs", "tallest building in the world", "American football", "Charlie Chaplin", "Beatrix Potter", "Giglio Island", "Copenhagen", "\"Upper Haight\"", "Geoffrey Rush", "Harry patch", "(Nursery Comics)", "Sight & Sound", "Inigo Jones", "a(nd) r(anging)", "Nelson Mandela", "Today", "trousseau", "Utah", "Mark Darcy", "reptilian", "a northern landmass, still carrying all those bits and pieces of the future southern hemisphere, headed southward after the split.", "Salyut 1", "Puerto Rico", "Evermoist", "62", "Matthew Gregory Wise", "1861", "voice actress", "Limbo", "12.3 million", "July as part of the State Department's Foreign Relations of the United States series.", "Dick Cheney", "New Hampshire", "a skirt", "a fathom", "$163 million (180 million Swiss francs)"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6048633658008659}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false], "QA-F1": [0.3636363636363636, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.5714285714285715, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-3244", "mrqa_triviaqa-validation-5462", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-7005", "mrqa_triviaqa-validation-2907", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-2362", "mrqa_triviaqa-validation-1171", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1663", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-2359", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2432", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-5151", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-520", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-4918", "mrqa_triviaqa-validation-2317", "mrqa_hotpotqa-validation-5278", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-2402", "mrqa_searchqa-validation-2835", "mrqa_newsqa-validation-4029"], "SR": 0.546875, "CSR": 0.5412660256410257, "EFR": 0.5517241379310345, "Overall": 0.625238657714412}, {"timecode": 78, "before_eval_results": {"predictions": ["Three", "he acted in self defense in punching businessman Marcus McGhee.", "1964", "\"It didn't matter if you were 60, 40 or 20 like I am. Michael Jackson's music just spoke to everyone... It was always uplifting and happy music,\"", "\"momentous discovery\"", "al-Aqsa mosque", "Pragyan Ojha two but Dilshan proved a formidable opponent.He continued his superb 2009 with 10 boundaries and two sixes to reach three figures for the 11th time in Tests.", "as soon as 2050,", "Sylt", "media", "Baghdad", "Sen. Barack Obama", "Arnoldo Rueda Medina", "left his indelible fingerprints on the entertainment industry.", "pizza, the other for the drug ketamine.", "Brian David Mitchell", "Defense of Marriage Act", "Joshua", "Brazil forward Ronaldinho", "Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "not susceptible to attack, and that fear of future risks shouldn't impede progress in the field.", "nine o'clock", "start a dialogue of peace based on the conversations she had with Americans along the way.\"", "al Qaeda", "Manmohan Singh's Congress party", "help the convicts find calmness in a prison culture", "J. Crew", "one day", "10,000", "Meira Kumar", "antihistamine and an epinephrine auto-injector", "CNN", "allegations that a dorm parent mistreated students at the school.", "Virgin America", "1940's", "Arthur E. Morgan III,", "million dollars", "bribing other wrestlers to lose bouts, compounding the view that corruption was prevalent in the sport.", "Eleven people died and 36 were wounded", "\"the evidence and investigatory effort has minimized the likelihood that Haleigh's disappearance is the work of a strangers.\"", "10 below", "Steven Gerrard", "$8.8 million", "Karl Eikenberry", "\"The Closer.\"", "Lisa Jackson, former head of New Jersey's environmental agency, to serve as his Environmental Protection Agency administrator, and Nancy Sutley, the Los Angeles deputy mayor for energy and environment, to lead the White House Council on Environmental Quality.", "conviction of Peru's ex-president is a warning to those who deny human rights.", "Brazil", "10", "return of a fallen U.S. service member", "supermodel and philanthropist", "Horseshoe Bartender", "early Christians of Mesopotamia", "16 seasons", "e pluribus unum", "well", "China", "Capture of the Five Boroughs", "Jim Diamond", "pornographicstar", "God spoke to Moses", "Petroleum", "Fannie Farmer", "freedom of choice, other social freedoms, and \"laissez-faire\" capitalism, while also being critical of authority."], "metric_results": {"EM": 0.421875, "QA-F1": 0.5318630906309891}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 0.0, 0.1, 1.0, 0.6666666666666666, 0.13333333333333333, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.5, 0.35294117647058826, 0.25, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2105263157894737]}}, "before_error_ids": ["mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3572", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-1390", "mrqa_naturalquestions-validation-4008", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-3392", "mrqa_searchqa-validation-16904", "mrqa_searchqa-validation-15636", "mrqa_hotpotqa-validation-3140"], "SR": 0.421875, "CSR": 0.5397547468354431, "EFR": 0.5675675675675675, "Overall": 0.6281050878806022}, {"timecode": 79, "before_eval_results": {"predictions": ["1866", "Grant", "Yangtze River", "Genesis", "Queen Anne Stuart", "NY Times I Herman", "Scotland", "Oklahoma", "the Communist Party of China", "1950s", "Sir Humphry Davy", "seoul", "20", "smallpox", "Dead Sea", "the fairway", "Hill Street Blues", "elevation", "Mao Zedong", "Harriet M. Welsch", "Mickey Mouse", "Xerox", "blitz", "Jamaica", "gossip", "an exothermic reaction", "the Chocolate Factory", "Morocco", "Surf's Up", "Yao Ming", "U.S. government", "clothing", "Andrew Marvell", "fruits", "Bollywood", "\"Titanic\"", "Take Me Out to the Ballgame", "parapet", "Joe Lieberman", "a dictionary", "coffee", "Nike", "Margaret Thatcher", "gas masks", "Surinam", "Pearl", "Pirates of the Burning Sea", "Switzerland", "Vestal Virgins", "The Lord of the Rings: The Return of the King", "Barack Obama", "H CO ( equivalently OC (OH ) )", "in a thousand years", "W. Edwards Deming", "Clara", "Douglas Trendle", "Ricky Gervais", "Ricky Marco i Vives", "Edith Cavell", "Forbes", "20 minutes of cardio five days a week.", "22", "Demjanjuk", "1,776"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6637445887445887}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 0.8, 0.28571428571428575, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727272, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.8, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-13597", "mrqa_searchqa-validation-3721", "mrqa_searchqa-validation-889", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-7549", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-8082", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-4390", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-16286", "mrqa_searchqa-validation-11363", "mrqa_searchqa-validation-4161", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16591", "mrqa_searchqa-validation-16014", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-5810", "mrqa_searchqa-validation-8650", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-1871", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-2118", "mrqa_naturalquestions-validation-3561"], "SR": 0.515625, "CSR": 0.539453125, "EFR": 0.7096774193548387, "Overall": 0.6564667338709678}, {"timecode": 80, "before_eval_results": {"predictions": ["Homebrewing", "the German Empire", "Tim Whelan", "Waimea Bay", "Virgin", "The Boeing EA-18G Growler", "George Harrison", "The bald eagle", "1923", "7pm", "Francis Schaeffer", "26,788", "10", "Greek mythology,", "Azeroth", "1,467", "Marco Fu", "Jean- Marc Vall\u00e9e", "Adelaide", "Strange Interlude", "2004", "Hall & Oates", "\"From Here to Eternity\"", "Manor of the More", "England, Scotland, and Ireland", "Workers' Party", "those who work with animals", "his exploration and settlement of what is now Kentucky, which was then part of Virginia but on the other side of the mountains from the settled areas.", "six", "Mauthausen-Gusen", "9 February 1983, Kimberley, Cape Province, South Africa", "Distillery", "Ted Nugent", "New York", "Viscount Cranborne", "The Princess and the Frog", "the Surtees Racing Organisation team", "Them", "Levi Weeks", "Bruce R. Cook", "Mandarin", "Obafemi Martins", "Boulder High School in Boulder, Colorado", "Dutch", "November of that year", "Boston, Massachusetts", "8/7c", "Brendan O'Brien", "Delphine Software International", "University of Kentucky College of Pharmacy", "October 21, 2016", "various locations in Redford's adopted home state of Utah", "31 - member Senate and a 150 - member House of Representatives", "May 31, 2012", "Billie Jean King", "Vienna", "Malaysia", "Akio Toyoda", "Iran's nuclear program.", "alternative-energy", "Bing Crosby", "(two) + kephale head", "Court Jester or Fool", "M&M's"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6853219696969697}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, true], "QA-F1": [0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.7272727272727272, 1.0, 0.19999999999999998, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-3650", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1210", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1809", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-5406", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2930", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-870", "mrqa_searchqa-validation-15999"], "SR": 0.578125, "CSR": 0.5399305555555556, "EFR": 0.7037037037037037, "Overall": 0.6553674768518519}, {"timecode": 81, "before_eval_results": {"predictions": ["Bhaktivedanta Manor", "Ariel Ram\u00edrez", "Potomac River", "four", "1853", "Allies of World War I", "Acid house", "Esteban Ocon", "Lady Frederick Windsor", "Perfume: The Story of a Murderer", "Agent 99", "Razor Ramon", "Birmingham, Alabama", "half of the Nobel Prize in Physics", "Adam Karpel", "rock and roll", "1903", "Windermere, Cumbria", "Sir Frank P. Lowy", "Hermione Baddeley", "Metrolink", "South Australia", "1698", "Tulsa", "Brian A. Miller", "July 25 to August 4", "Restoration Hardware", "John William Henry II", "2009", "McLaren-Honda", "Alexandre Dumas, p\u00e8re, and Paul Meurice", "The Books", "evangelical Christian periodical", "Annales de chimie et de physique", "Dar es Salaam", "English Electric Canberra", "1 September 1864", "Smithsonian", "Chechen Republic", "Harry Robbins \"Bob\" Haldeman", "Cartoon Cartoon Fridays", "The Company", "5", "Eleanor of Aquitaine", "Latium", "April 1, 1949", "Liverpool", "Ericsson", "interstate commerce", "1935", "Michael Redgrave", "Tyrann Devine Mathieu", "1933", "British Columbia, Canada", "Moulin Rouge", "willow grade selection", "Michael Mizrachi", "Kim Clijsters", "Mombasa, Kenya", "hanged in 1979 for the murder of a political opponent two years after he was ousted as prime minister in a military coup.", "elementary", "The New York City Ballet", "voltage", "Willa Cather"], "metric_results": {"EM": 0.5, "QA-F1": 0.595941166321601}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.2608695652173913, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-4891", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-2579", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-1381", "mrqa_hotpotqa-validation-2893", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-1244", "mrqa_hotpotqa-validation-2137", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4314", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-388", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-847", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-7787", "mrqa_searchqa-validation-5952"], "SR": 0.5, "CSR": 0.5394435975609756, "EFR": 0.65625, "Overall": 0.6457793445121951}, {"timecode": 82, "before_eval_results": {"predictions": ["1988", "Dame Harriet Walter", "Algernod Lanier Washington", "Conservative Party", "four", "October 29, 1895", "Turkmenistan", "the Earth", "Standard Oil", "2006", "Norwegian", "The Late Late Show", "Ry\u016bky\u016b people", "Commanding General", "\"50 best cities to live in.\"", "Mike Mills", "Parlophone", "January 15, 2016", "IATA: VNO, ICAO: EYVI", "George Clooney, Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "The Worm", "Herman's Hermits", "Ustad Vilayat Khan", "810", "German", "Vietnam War", "Anatoly Vasilyevich Lunacharsky", "1902", "Richard Price", "Novel", "Gabriel Iglesias", "3,384,569", "Gambaga", "2 March 1972", "the Blue Ridge Parkway", "Teatro Carlo Felice", "every aspect of public and private life", "Gary Ross", "Hanford Nuclear Reservation", "Commissioner", "Sam tick, under the name Metro Sportswear Ltd.", "Estelle Sylvia Pankhurst", "Aaliyah Dana Haughton", "Spain", "Royal Albert Hall", "Leatherheads", "2 May 2015", "England", "King Duncan", "Bourbon County", "Serial (Bad) Weddings", "Epithelium", "Amybeth McNulty", "just after the Super Bowl", "Wichita", "Ringo Starr", "1883", "off the coast of Dubai", "The incident Sunday evening", "not doing more", "hard drives", "polio", "treble clef", "gun charges,"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7288194444444445}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.4444444444444445, 0.8, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-2260", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-438", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-3703", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-1559", "mrqa_hotpotqa-validation-2005", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-1844", "mrqa_triviaqa-validation-3181", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1427", "mrqa_searchqa-validation-882", "mrqa_searchqa-validation-2732"], "SR": 0.640625, "CSR": 0.5406626506024097, "EFR": 0.8695652173913043, "Overall": 0.6886861985987428}, {"timecode": 83, "before_eval_results": {"predictions": ["Massachusetts", "Metacomet", "Washington D.C.", "Leon Trotsky", "a loaf", "a large chart", "The New York Times", "Peter van Schaack", "Ugly Betty", "Winnie-the-Pooh", "Charles Gounod", "Alexander Graham Bell", "(Vijay) Singh", "Lenticular clouds", "a modem", "China", "the Boston Red Sox", "Comedy Central", "Mussolini", "the human breast", "Defence lndustry Defence Weekly Electronic Mission Aircraft Fighting ships", "Christo", "a psychiatrist", "Ichiro Suzuki", "Frank Sinatra", "Africa", "the banjo", "Ulysses S. Grant", "Belle Watling", "Mozart", "American alternative rock band", "Blackwell's Island", "Lord Byron", "meningitis", "Douglas MacArthur", "3M", "The Rolling Stones", "Edie Falco", "USA", "Oneonta College", "1936", "the CN Tower", "The Hurricane", "inheritance", "Baltimore", "rattlesnake", "Japan", "bovine spongiform encephalopathy", "Prince Edward Island", "Hindu", "pronghorn", "January 2, 1971", "San Francisco", "Moscazzano", "Cliff Thorburn", "730,000 people,", "Daedalus", "Conservatorio Verdi", "close range combat", "Paul John Mueller Jr.", "1959", "The son of Gabon's former president", "United States", "in mid November"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6263392857142857}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_searchqa-validation-7412", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-14090", "mrqa_searchqa-validation-16524", "mrqa_searchqa-validation-15607", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-1302", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-12656", "mrqa_searchqa-validation-7948", "mrqa_searchqa-validation-2208", "mrqa_searchqa-validation-9445", "mrqa_searchqa-validation-15675", "mrqa_searchqa-validation-15564", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-9378", "mrqa_searchqa-validation-14625", "mrqa_searchqa-validation-3977", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-15098", "mrqa_searchqa-validation-9230", "mrqa_searchqa-validation-13757", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-12578", "mrqa_triviaqa-validation-2502", "mrqa_hotpotqa-validation-3714", "mrqa_newsqa-validation-3923", "mrqa_naturalquestions-validation-8884"], "SR": 0.546875, "CSR": 0.5407366071428572, "EFR": 0.8275862068965517, "Overall": 0.6803051878078817}, {"timecode": 84, "before_eval_results": {"predictions": ["Shinto", "Blackbeard", "Jabez Stone", "William Howard Taft", "vermouth", "pemmican", "Olivia Newton-John", "Oahu", "Joseph Smith", "phylum Arthropoda", "Harry S. Truman", "Capricorn", "Diane Arbus", "chiles rellenos", "Thomas Jefferson", "legislation", "tofu", "Old School", "the Distant Early Warning Line", "Henry VIII", "Bonn", "mathematical research", "Pope John Paul II", "Variety", "Robert the Bruce", "Zinc", "oxys", "Gargantua", "Elke Sommer", "hoof wall", "Robin Williams", "Philadelphia", "Ivory soap", "Giuseppe Garibaldi", "Morrie Schwartz", "anglerfish", "Jaguar S-Type R", "Thomas Jefferson Family Cemetery", "Mohandas Gandhi", "Brazil", "Jim Thorpe", "comedy", "Dustin Hoffman", "William Shakespeare", "descend", "Leonard Bernstein", "Haunted Mansion", "Rembrandt", "Gilligan's Island", "your timehare resale", "buffalo Bill", "Stanwyck's bedroom window overlooks the night skyline of Manhattan", "Tom Robinson", "Andy Serkis", "South Africa", "a horizontal desire", "Charlie Drake", "Esp\u00edrito Santo Financial Group", "Herman's Hermits", "Punjabi/Pashtun", "Asashoryu", "\"political and religious\" motives", "former U.S. secretary of state", "Newcastle Falcons"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5836805555555555}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.5, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-5032", "mrqa_searchqa-validation-10629", "mrqa_searchqa-validation-6884", "mrqa_searchqa-validation-5260", "mrqa_searchqa-validation-16513", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-3190", "mrqa_searchqa-validation-2225", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-7664", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-7856", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-5873", "mrqa_searchqa-validation-1104", "mrqa_searchqa-validation-11112", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-16304", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-5050", "mrqa_naturalquestions-validation-6465", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3293"], "SR": 0.515625, "CSR": 0.5404411764705883, "EFR": 0.6774193548387096, "Overall": 0.6502127312618595}, {"timecode": 85, "before_eval_results": {"predictions": ["Crime and Punishment", "Postcards from the Edge", "birds", "Virginia", "hot chocolate", "Elementary, my dear Watson", "Ramadan", "\"The play's the thing\"", "The Carol Burnett Show", "a panic", "Gertrude Stein", "pontiff", "love", "Inigo Jones", "Charles Ponzi", "Earhart", "Alfred Hitchcock", "an object oriented programming", "Nova Scotia", "cocoa", "tuna", "Absinthes", "Miles Klee", "marsupials", "quid", "Lincoln", "Anthony Newley", "Swimmer's Ear", "Henry", "5", "Greek", "Jeff Probst", "Hopelessly Devoted to You", "Nasser", "The Moment of Truth", "Laura", "a southern circumpolar sky", "Charles Manson", "Jerusalem", "Xerox", "Billy Crystal", "thyroid", "Hephaestus", "Hurricane Katrina", "pineapple", "Bill & George Clinton", "the Straits of Dardanelles", "May 12, 1907", "Uncle Sam", "Young Frankenstein", "Shout", "to form a higher alkane", "comprehend and formulate language", "Hellenic Polytheism", "Venezuela", "The Shootist", "Sega Saturn", "National Society of Daughters of the American Revolution", "Leonard", "44,300", "it has not intercepted any", "At least 88", "went missing off Catalina Island, near the California coast, following an argument the couple had.", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\" the Boston Police Department said."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6379723837209301}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.33333333333333337, 0.5, 0.0, 0.9302325581395349]}}, "before_error_ids": ["mrqa_searchqa-validation-590", "mrqa_searchqa-validation-13276", "mrqa_searchqa-validation-6865", "mrqa_searchqa-validation-15120", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-1059", "mrqa_searchqa-validation-1681", "mrqa_searchqa-validation-9410", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-738", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-15430", "mrqa_searchqa-validation-7355", "mrqa_searchqa-validation-7969", "mrqa_searchqa-validation-7460", "mrqa_searchqa-validation-15985", "mrqa_searchqa-validation-2305", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-863", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-13742", "mrqa_searchqa-validation-2063", "mrqa_triviaqa-validation-7745", "mrqa_hotpotqa-validation-4024", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-839"], "SR": 0.546875, "CSR": 0.540515988372093, "EFR": 0.7241379310344828, "Overall": 0.6595714088813152}, {"timecode": 86, "before_eval_results": {"predictions": ["John Garfield as Al Schmid", "at a given temperature", "season ten", "October 28, 2007", "seven units of measure", "absorbed the superhuman powers and the psyche of Carol Danvers", "the Isthmus of Suez", "privatized", "Stephen A. Douglas", "abdicated in November 1918", "5 : 7 -- 8", "between the stomach and the large intestine", "Gupta Empire", "Vicente Fox", "Atelier de Construction d' Issy - Les - Moulineaux", "Egypt", "the base of the right ventricle", "Lager", "Justin Timberlake", "Destiny's Child", "position in blackjack relative to the player", "Russell Huxtable", "Husrev Pasha", "Anna Maria Demara", "Donny Osmond", "735 feet ( 224 m )", "it acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Tex - Mex cuisine is characterized by its heavy use of shredded cheese, meat ( particularly beef and pork ), beans, peppers and spices, in addition to flour tortillas", "September 8, 2017", "SURFACE WASA OF ROOTS", "Iowa", "Matt Flinders", "1 October 2006", "Bh\u0101va", "Nucleotides", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Egypt", "nasal septum", "SIP ( Session Initiation Protocol )", "January 1, 1976", "parthenogenesis", "Identification of alternative plans / policies", "Ludacris", "JackScanlon", "Costa Rica, Brazil, and the Philippines", "Welch, West Virginia", "Andy Cole", "ulnar nerve", "retina", "Donna Mills", "Donna", "Annette Crosbie", "Bobby Kennedy", "Minder", "leopard", "Patricia Arquette", "Indian Universities", "Symbionese Liberation Army", "101", "two", "\"Like a Rock\"", "a cat", "King George III", "Norway"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6601868192759254}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true], "QA-F1": [0.5714285714285715, 0.5454545454545454, 0.0, 1.0, 0.4, 0.761904761904762, 0.5, 0.0, 1.0, 0.4210526315789474, 0.0, 0.20000000000000004, 1.0, 1.0, 0.7692307692307692, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.11764705882352942, 0.0, 1.0, 0.0, 0.0, 0.0, 0.72, 0.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-6340", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-5241", "mrqa_searchqa-validation-1808"], "SR": 0.53125, "CSR": 0.5404094827586207, "EFR": 0.6333333333333333, "Overall": 0.6413891882183907}, {"timecode": 87, "before_eval_results": {"predictions": ["Province of Syracuse", "Guardians of the Galaxy Vol.  2", "Arlo Looking Cloud", "Jyothika Sadanah", "Franco-Prussian War", "Hirsch index rating", "Cody Miller", "1951", "Teenitans Go!", "A skerry", "Book of Judges", "torpedoes", "9 February 1971", "San Francisco, California", "Three's Company", "9,984", "Nasim Pedrad", "Marktown, Clayton Mark's planned worker community in Northwest Indiana", "the Rose Theatre", "1 million", "Trey Parker and Matt Stone", "La Scala, Milan", "Fidenza", "237", "Shakespeare's reputation", "Balloon Street, Manchester", "University of Southern California", "6teen", "the port city of Aden, on the southern coast", "Noel Gallagher", "Michael Rispoli", "U2 360\u00b0 Tour", "Michael Jordan", "Scarface", "Austro-Hungarian Army", "St. George", "Ericsson (\"Telefonaktiebolaget L. M. Ericsson\")", "his son Louis, and from 6 September 1724, when he reassumed the throne upon his son's death, to his own death 9 July 1746", "Helsinki, Finland", "Urijah Faber", "four operas", "3 May 1958", "The Thomas Crown Affair", "Bharat Ratna", "November 11, 1901", "Taoiseach of Ireland", "Unbreakable", "The Spiderwick Chronicles", "Sacramento Kings", "Sam Kinison", "Ferdinand Magellan", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "1920s", "Blue laws", "James Hargreaves", "Puff the Magic Dragon", "Hindi", "Samoa", "flooding", "composer of \"Phantom of the Opera\" and \"Cats\" and the \"Harry Potter\" films were recorded there.", "Thesaurus", "Bath", "Winnipeg", "to offer the hope that a happy day being marked would recur many more times"], "metric_results": {"EM": 0.625, "QA-F1": 0.7086516203703703}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.29629629629629634, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407408, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-2747", "mrqa_hotpotqa-validation-4156", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-2145", "mrqa_hotpotqa-validation-871", "mrqa_hotpotqa-validation-4727", "mrqa_hotpotqa-validation-5220", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-4821", "mrqa_hotpotqa-validation-527", "mrqa_naturalquestions-validation-5215", "mrqa_triviaqa-validation-3809", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2094", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-15272"], "SR": 0.625, "CSR": 0.5413707386363636, "EFR": 0.8333333333333334, "Overall": 0.6815814393939394}, {"timecode": 88, "before_eval_results": {"predictions": ["four", "12-hour-plus shifts", "as many as 250,000 unprotected civilians", "Ameneh Bahrami", "40", "state senators", "in an Internet broadband deal with a Chinese firm.", "by text messaging", "Hawaii", "Haiti", "Brazil's", "\"wow.\"", "the Transportation Security Administration", "Her husband and attorney, James Whitehouse,", "helping to plan the September 11, 2001, terror attacks, and believe he may have sought to participate, possibly as the \"20th hijacker.\"", "Empire of the Sun", "the burning World Trade Center", "collaborating with the Colombian government", "Washington Redskins fan and loved to travel, but lived just 10 minutes away from where his mother and stepfather raised him.", "time", "the earthquake's devastation.", "Jason Chaffetz", "summer", "southern port city of Karachi, Pakistan's largest city and the capital of Sindh province.", "stolen the personal credit information of thousands of unsuspecting American and European consumers", "\"If the people closest to him didn't see any indicators or signs that he was going to go off so drastically... how is some public safety officer supposed to recognize this person?\"", "Ricardo Valles de la Rosa", "Islamabad", "Toffelmakaren", "3 p.m. Wednesday", "Microsoft", "1995", "Araceli Valencia", "Casalesi Camorra clan", "Nigeria", "201-262-2800", "South Africa", "very dark and very cold", "Chad", "President Obama", "Tuesday", "contact the insured drivers who have failed to comply", "Mashhad", "Plymouth Rock", "Alina Cho", "Roger Federer", "October 29 and November 5", "exercise equipment", "Anthony Chambers", "10", "second", "Central Germany", "Rust", "gastrocnemius muscle", "Granada", "axes", "Portugal", "June 17, 2007", "England", "Black Elk", "hollandaise", "kwanzaa", "\"to look like\"", "Javan leopard"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6554909066627816}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4615384615384615, 1.0, 0.6153846153846153, 0.125, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5555555555555556, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.5, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-728", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3906", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-1201", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-2749"], "SR": 0.546875, "CSR": 0.5414325842696629, "EFR": 0.4827586206896552, "Overall": 0.6114788659918636}, {"timecode": 89, "before_eval_results": {"predictions": ["through a facility in Salt Lake City, Utah,", "10 below", "Shemsu Sirgaga", "killing massacre.", "HSH Nordbank Arena", "they did not receive a fair trial.", "The federal officers' bodies", "American Bill Haas", "Larry Ellison", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "rising disposable income and an increasing interest in leisure pursuits, a growing number of courses, more television coverage and availability of EU funds,", "Joan Rivers", "The Des Moines Register newspaper.", "Phoenix, Arizona, police", "KBR", "a Muslim and a Coptic family", "Jennifer Aniston, Marta Kauffman, co-creator of the series \"Friends\" and Kristin Hahn, who was the executive producer of \"The Departed.\"", "two years", "body of the aircraft", "the United States, Japan, Russia, South Korea and China,", "will not support the Stop Online Piracy Act,", "pattern matching", "Teen Patti", "almost 9 million", "U.S. senators", "the situation of America wielding a big stick for the last eight years.'\"", "London and Buenos Aires", "she returned to Pakistan in October after President Pervez Musharraf signed an amnesty lifting corruption charges.", "Nazi Germany", "Nafees A. Syed,", "a bank", "two", "illegal immigrants", "At least 38", "Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "The BBC", "\"wipe out\" the United States if provoked.", "Sunday,", "it's historical, inspiring, creative, romantic and beautiful.", "Stella McCartney", "clogs", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Jennifer Aniston, Marta Kauffman, co-creator of the series \"Friends\" and Kristin Hahn, who was the executive producer of \"The Departed.\"", "ALS6", "The EU naval force", "well over 1,000 pounds", "Iran's Green Movement of protesters", "help make the new truck safer, but also could make it more expensive to repair after a collision.", "Friday,", "The supplemental spending bill also contains a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance", "Kathrin Hoelzl of Germany,", "three", "postero - medially towards the optic chiasm", "Hugo Weaving", "aragonite", "Lester", "Seattle", "Bruce R. Cook", "Los Angeles, California", "86,112", "Lewis Carroll", "soap opera", "CPI", "The Cross Foxes Inn"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5698367043316859}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.8, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 0.1, 0.19354838709677416, 1.0, 0.0, 0.5, 0.0, 0.0, 0.1739130434782609, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.19999999999999998, 1.0, 1.0, 1.0, 0.125, 0.5, 1.0, 0.0, 0.1739130434782609, 1.0, 1.0, 0.8571428571428571, 1.0, 0.2222222222222222, 1.0, 0.1, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-436", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1732", "mrqa_naturalquestions-validation-3358", "mrqa_triviaqa-validation-4977", "mrqa_triviaqa-validation-7370", "mrqa_hotpotqa-validation-2460", "mrqa_searchqa-validation-6689", "mrqa_searchqa-validation-11100", "mrqa_triviaqa-validation-2306"], "SR": 0.4375, "CSR": 0.5402777777777779, "EFR": 0.7222222222222222, "Overall": 0.659140625}, {"timecode": 90, "before_eval_results": {"predictions": ["Bessie Smith", "anthrax", "Neptune", "larynx", "the Surgeon", "Ebony", "Chicago", "Sartre", "Wordsworth", "St. Louis", "James K. Polk", "Harpy", "lacrosse", "Naples", "a Dormouse", "Galatians", "a cow pie", "Paradise Lost", "Words", "ice", "Doctor Dolittle", "Graceland", "Mitch Albom", "the beaver", "earthquakes", "Donovan", "Best Supporting Actor", "The Bionic Woman", "5000", "wrinkles", "Narnia", "comet Tempel 1", "Mount Sinai", "Kamehameha", "(Elbert) Gary", "a marker of corporality", "\"Everybody goes there anymore; it's too crowded.\"", "Duke", "Orlans", "Another Brick in the Wall", "Pulp Fiction", "Hester Prynne", "pajamas", "China Airlines", "a bagpipe", "a stork", "BOWLING", "Henry David Thoreau", "Encephalitis", "Iran", "Rookwood", "Hudson Bay", "restored to life", "The long - hair gene is recessive", "the Canaries", "Another Day in Paradise", "The Danelaw", "Donald Wayne Johnson", "Robert A. Iger", "Manchester Airport", "Chicago, Illlinois.", "16", "Japanese officials", "financial gain"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6690104166666666}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-2401", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-9946", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-2011", "mrqa_searchqa-validation-14750", "mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-12389", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-14528", "mrqa_searchqa-validation-10496", "mrqa_searchqa-validation-3443", "mrqa_searchqa-validation-15600", "mrqa_searchqa-validation-12047", "mrqa_searchqa-validation-7404", "mrqa_searchqa-validation-11566", "mrqa_searchqa-validation-14572", "mrqa_naturalquestions-validation-9741", "mrqa_triviaqa-validation-1702", "mrqa_hotpotqa-validation-4724", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1662"], "SR": 0.640625, "CSR": 0.5413804945054945, "EFR": 0.782608695652174, "Overall": 0.6714384630315336}, {"timecode": 91, "before_eval_results": {"predictions": ["Cardiff, Wales", "keirin", "British boxing Middleweight", "Christopher Nolan", "German poet", "highball", "Conan Doyle", "Godfigu", "brain", "six", "Bashir", "dog sport involving jumps, obedience, and bite work", "The Double", "oxygen", "alphabets", "Mickey Mouse", "Clove", "The Welcome Stranger", "recorder", "UAE", "Genesis", "Ladysmith", "californium", "Robert Guerrero", "Arizona Diamondbacks", "Orwell", "Goldie Myerson", "Isambard Kingdom Brunel", "to the tooth,\" meaning that it still has a little bite.", "William Shakespeare", "1960's", "Some Like It Hot", "Beaujolais", "morphine", "the National Council for the Unmarried Mother and her Child", "Sarajevo", "Roger of Montgomery", "the knight, some clergymen, members of the middle class, and a few peasants.", "bullfighting", "Leicestershire", "cycling", "Crimean Tatar", "bedding", "Switzerland", "Shanghai", "Duke Orsino", "Girl Scout Promise", "the Swordfish", "France", "Australia", "France", "17 - year - old Augustus Waters", "1898", "January 1923", "The 1984 South Asian Games", "Ramanaidu Daggubati", "four", "1,500", "13-year-old boy", "38 feet", "Gilbert du Motier", "turquoise", "birds", "2009"], "metric_results": {"EM": 0.5, "QA-F1": 0.5749391233766233}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7142857142857143, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-5840", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-4256", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-3414", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-1651", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-2067", "mrqa_triviaqa-validation-1059", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-1439", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-2053", "mrqa_triviaqa-validation-6589", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-7051", "mrqa_triviaqa-validation-5484", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-64", "mrqa_newsqa-validation-3440", "mrqa_searchqa-validation-6259", "mrqa_searchqa-validation-11134", "mrqa_naturalquestions-validation-8439"], "SR": 0.5, "CSR": 0.5409307065217391, "EFR": 0.6875, "Overall": 0.6523267663043478}, {"timecode": 92, "before_eval_results": {"predictions": ["Tomasz Adamek", "51,271", "Roger Staubach", "from 1979 to 2013", "two", "2001", "Meghan Markle", "2007", "alcoholic drinks for consumption on the premises", "Seoul", "Dutch", "43rd Vice President of the United States from 1981 to 1989", "political correctness", "Russell Humphreys", "Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan to the south", "January 24, 2012", "over 3 million", "Mazda", "Jack St. Clair Kilby", "\"My Father\"", "water", "more than 70", "a type of blood pudding", "Animorphs", "Francis the Talking Mule", "two Nobel Peace Prizes", "Masahiko Takehita", "Apatosaurus", "TD Garden", "the controversial and explicit nature of many of their songs", "Sam Kinison", "Melbourne Storm", "Hawaii", "2007", "Texas", "Prudence Jane Goward", "Vince Guaraldi", "\"What's My Line?", "Kim So-hyun", "seasonal television specials, particularly its work in stop motion animation", "Carol Ann Duffy", "Mathew Sacks", "Charles VI", "17 October 2006", "Kansas Joe McCoy and Memphis Minnie", "29,000", "Dire Straits", "Niger\u2013Congo", "Princess Jessica", "The 2018 Unibet Premier League Darts", "first freshman to finish as the runner-up", "Canada", "defense against rain rather than sun", "the eurozone", "Jane Seymour", "Willie Nelson", "1984", "Argentina", "around 3.5 percent", "Ali Bongo", "Antietam National Battlefield", "your premium", "Princeton", "Miguel Cotto"], "metric_results": {"EM": 0.5, "QA-F1": 0.6670973124098124}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.4444444444444445, 0.4, 0.0, 0.0, 0.8, 0.0, 0.5, 0.5, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5454545454545454, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.3333333333333333, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.8, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-4762", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-5615", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-51", "mrqa_hotpotqa-validation-1662", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-5319", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-1162", "mrqa_newsqa-validation-2508", "mrqa_searchqa-validation-2586", "mrqa_searchqa-validation-15208", "mrqa_searchqa-validation-4038"], "SR": 0.5, "CSR": 0.5404905913978495, "EFR": 0.84375, "Overall": 0.6834887432795699}, {"timecode": 93, "before_eval_results": {"predictions": ["American", "Bill Cosby", "Edward R. Murrow", "Liesl", "Stage Stores", "Suicide Squad", "1998", "World Famous Gold & Silver Pawn Shop", "1972", "Argentina", "High Knob", "Target Corporation", "1 September 1864", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "Iranian-American", "Buck Owens and the Buckaroos", "the Provisional Irish Republican Army (IRA) in Northern Ireland", "Tel Aviv", "Chevy", "the nature of human sexual response", "Overijssel, Netherlands", "great-grandfather", "PEN America", "Love Letter", "2013", "Jericho Union Free School District", "January 15, 1975", "Cartoon Network", "acting", "18.7 miles", "Oracle Corporation", "Titus Lucretius Carus", "water sprite", "Hopeless Records", "August Heckscher", "Isabella Hedgeland", "Pennsylvania's 18th congressional district", "The Five", "anabolic\u2013androgenic steroids", "Dulwich", "Red Dead Redemption", "Sierre", "Buffalo", "Heathrow", "George Martin", "Timo Hildebrand", "Adam Dawes", "Enkare Nairobi", "Rockland", "2009", "Vietnam War", "Toto", "9 February 2018", "Cheap trick", "Gabriel Byrne and Kevin Spacey", "Funchal", "British", "Scudetto", "Mexico's attorney general's office responded with a statement saying that it would investigate the video and any group that tries to take justice into its own hands.", "because a new model is simply out of their reach", "Caroline Ponsonby", "Florida", "Burkina Faso", "Agriculture"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7477994670090258}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.47619047619047616, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.14814814814814814, 0.9411764705882353, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-2879", "mrqa_hotpotqa-validation-5128", "mrqa_hotpotqa-validation-3403", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-961", "mrqa_triviaqa-validation-6478", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2965", "mrqa_searchqa-validation-16694"], "SR": 0.65625, "CSR": 0.5417220744680851, "EFR": 0.6363636363636364, "Overall": 0.6422577671663443}, {"timecode": 94, "before_eval_results": {"predictions": ["\"if a man does not keep pace with his companions, perhaps it is because he hears a different drummer.\"", "(Adolf) Hitler", "Mrs. Miniver", "Simon Cowell", "Eagles", "Packard", "lifejackets", "Ian Fleming", "the Shrew", "Anne Frank's", "Bora Bora's lagoon", "My Name Is Earl", "Nassau", "a geisha", "the former president of France", "the Barbary corsairs", "CIA", "antimicrobial", "the iPhone", "fasting", "Phonetics", "Crosby, Stills, Nash & Young", "Frasier", "paper tickets", "a rocket", "the Court of Cassation", "Chiapas", "Jeopardy", "Afghanistan", "Australia", "water buffalo", "the northeastern landmass and islands, bordering the Pacific", "the College of Dental Medicine", "pitch", "the all-time hits leader", "Esther", "South Africa", "Slim", "GoldenEye", "agriculture", "Dumbo", "Edith Wharton", "Aretha Franklin", "marsupials", "Italian", "The Crow", "Cal Ripken Jr.", "The Magnificent Ambersons", "mongoose", "Russell Crowe", "Ecuador", "first word of the text, written in Koine Greek : apokalypsis", "four", "elected or appointed by means of a commission ( letters patent ) to keep the peace", "Friends", "frankincense", "the solar system", "Ranulf de Gernon, 4th Earl of Chester", "the series \"Runaways\"", "Pieter van Musschenbroek", "Seoul", "\" Fortunately, I've been feeling better every single day since surgery and this weekend my doctors gave me the green light to get back to work.\"", "the Klan experienced a huge resurgence. Its membership was skyrocketing, and its political influence was increasing, so Kennedy went undercover to infiltrate the group.", "The Krankies"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6288194444444444}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true], "QA-F1": [0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.4, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.6666666666666666, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2367", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-13172", "mrqa_searchqa-validation-3231", "mrqa_searchqa-validation-11084", "mrqa_searchqa-validation-2987", "mrqa_searchqa-validation-9617", "mrqa_searchqa-validation-14916", "mrqa_searchqa-validation-11593", "mrqa_searchqa-validation-6734", "mrqa_searchqa-validation-14357", "mrqa_searchqa-validation-15296", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-5897", "mrqa_searchqa-validation-6749", "mrqa_searchqa-validation-4014", "mrqa_searchqa-validation-9760", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-6577", "mrqa_searchqa-validation-1165", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-2476", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-4503", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-108"], "SR": 0.5625, "CSR": 0.5419407894736843, "EFR": 0.7857142857142857, "Overall": 0.672171640037594}, {"timecode": 95, "before_eval_results": {"predictions": ["Clarence Thomas", "Penguin Books", "Kokomo", "Profiles in Courage", "Berkeley", "the summer session", "Lady Godiva", "Beat The Clock", "the Vic-Wells", "Antnio Guterres", "Latin", "King Henry VIII", "San Francisco", "\"The Secrets of a Fire King\"", "Nereid", "Harry Potter and the Chamber of Secrets", "Brutus", "the wild-goose", "Joseph Haydn", "Willa Cather", "Dow Jones", "Aunt Jemima", "fowls", "dynasties", "Homer", "Amanda Bynes", "Danson", "O. Henry", "middle-aged", "B.B. King", "Kennedy", "Donovan", "phytoplankton", "Candlestick Park", "a jointer plane", "just compensation", "Vodka", "pickled", "Adam", "Protestantism", "Ivy Dickens", "dizzy", "thunder", "Ham", "calamine", "Sicily", "Nelson's Column", "Luxor", "Drew Barrymore", "Philip Seymour Hoffman", "Sherlock Holmes", "Caleb", "1957", "Jurchen Aisin Gioro clan", "Wharton's club", "Little arrows", "Rotary", "Craig William Macneill", "William Adelin", "twenty", "Newark's Liberty International Airport", "more than 4,000", "Lifeway Christian Stores", "Pakistan"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6357142857142857}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.28571428571428575, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2232", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-3012", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-5291", "mrqa_searchqa-validation-2224", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-15065", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-7791", "mrqa_searchqa-validation-10119", "mrqa_searchqa-validation-12886", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-12050", "mrqa_searchqa-validation-9043", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-4483", "mrqa_searchqa-validation-2081", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-9639", "mrqa_triviaqa-validation-1852", "mrqa_hotpotqa-validation-1300", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-2278"], "SR": 0.546875, "CSR": 0.5419921875, "EFR": 0.7241379310344828, "Overall": 0.6598666487068965}, {"timecode": 96, "before_eval_results": {"predictions": ["swans", "Andrea del Sarto", "Unbreakable", "Holy Week", "Rosarito", "Wizard", "a bit", "Planned Parenthood", "Jamie Lee Curtis", "Ellen DeGeneres", "saray", "Alexander Graham Bell", "the extreme north-east", "a baffle plate", "corpulent", "Herman", "Erin Go Bragh", "Queen Victoria", "giant", "Medusa", "zoology", "Lucia di Lammermoor", "the largest lakes and rivers", "cricket", "Stephen Hawking", "St. Francis of Assisi", "the luminous intensity", "The Scarlet Letter", "1997", "Drug Rehab & Treatment Center", "pastries", "The Hundred Years' War", "Onassis Cultural Center", "milk and honey", "3", "Stenosis", "The Beatles", "The Bronx", "glucose", "King Kong", "Cubism", "Umbria", "Cottage cheese", "Escher", "Oahu", "the kidney", "F. Scott Fitzgerald", "aria", "Sigourney Weaver", "Marquette University", "the monk", "Fall 1998", "infection, irritation, or allergies", "Bart Howard", "the Netherlands", "Marillion", "Usain Bolt", "Keeper of the Great Seal of Scotland", "J. K. Rowling", "Victorian England", "\"We are a nation of Christians and Muslims, Jews and Hindus -- and nonbelievers.\"", "early 2008", "acid", "number five"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7088541666666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14869", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-16364", "mrqa_searchqa-validation-4803", "mrqa_searchqa-validation-16101", "mrqa_searchqa-validation-12798", "mrqa_searchqa-validation-6016", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14498", "mrqa_searchqa-validation-13015", "mrqa_searchqa-validation-3371", "mrqa_searchqa-validation-15106", "mrqa_searchqa-validation-4441", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-2023", "mrqa_searchqa-validation-10547", "mrqa_naturalquestions-validation-2666", "mrqa_hotpotqa-validation-694", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-2741", "mrqa_newsqa-validation-1645"], "SR": 0.609375, "CSR": 0.5426868556701031, "EFR": 0.72, "Overall": 0.6591779961340206}, {"timecode": 97, "before_eval_results": {"predictions": ["10 cm", "a gift given by \"my true love\"", "Gaston Leroux", "Concorde", "gold", "European Economic Community (EEC)", "Canterbury and Lancaster", "Vietnam", "Florence", "Wanderers", "Emilia Fox", "Amnesty International", "krak\u00f3w", "Shaft", "gal", "Ramadan", "Bizet", "the Count Basie Orchestra", "Pegida", "uranium", "Eva Strong", "Edward Hopper", "Einstein", "Faversham", "Justin Trudeau", "Robin Williams", "Time Team", "Thom Yorke", "OKLAHOMA!", "UNESCO", "Bolivia", "Christian Wulff", "a wish", "river usk", "spider", "Malcolm Turnbull", "Daily Herald", "Nairobi", "Turing", "bone", "the right atrium", "Puck", "Hula-Hoops", "Dubonnet", "Jane Austen's Manuscript Letters", "Rocky Graziano", "sweater", "Today", "Today", "Gene Vincent", "Midgard", "a cascade of events through phosphorylation of intracellular proteins that ultimately transmit ( `` transduce '' ) the extracellular signal to the nucleus", "USS Chesapeake", "Ben Faulks", "as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.", "the right bank of the Gomti River", "Art of Dying", "Jason Bendett", "iPhone 4S", "Christopher Savoie", "Euneos", "Ingenue", "First World War", "Joseph"], "metric_results": {"EM": 0.625, "QA-F1": 0.6833333333333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 0.4, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1962", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-3617", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-6370", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-3074", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-4083", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1240", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-2253", "mrqa_searchqa-validation-1947", "mrqa_searchqa-validation-1427"], "SR": 0.625, "CSR": 0.5435267857142857, "EFR": 0.5833333333333334, "Overall": 0.6320126488095238}, {"timecode": 98, "before_eval_results": {"predictions": ["Queen Victoria", "Edinburgh", "Jerry Mouse", "cirrus uncinus", "Procol Harum", "the River Alt", "cork", "tintagel", "Uganda", "st Pancras", "lactic acid", "villefranche", "Robinson Crusoe", "once a week", "\u201cMy Favorite Martian,\u201d", "Whist", "fear of spiders", "Madagascar", "Wyatt", "March", "One Direction", "The West Wing", "Prince Harry", "1994", "titanium", "car park", "Pegasus", "Alaska", "Tom Sawyer", "Brazil", "the rhizome", "carpentula", "eyes", "Ukrainian", "Bowie knife", "Nile River", "tiger", "Independence Day", "Tinie Tempah", "Portugal", "Greek", "collapsible support assembly", "beard", "Amy", "Oldham, in Greater Manchester, England", "Sunday Post", "Darin", "emirate", "Phil Woolas", "Mansfield Park", "South Africa", "Cam Clarke", "only drivers who were Daytona Pole Award winners", "Florida", "twenty-three", "Mexican War on Drugs", "relationship with Apple co-founder Steve Jobs", "opium", "Basilan", "Tutsis", "The Tempest", "Naples", "Solomon", "heavier than a feather"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6316220238095238}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.28571428571428575]}}, "before_error_ids": ["mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-5217", "mrqa_triviaqa-validation-4266", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-3363", "mrqa_triviaqa-validation-5529", "mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-487", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2257", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-3198", "mrqa_triviaqa-validation-4677", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6822", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-2492", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-9149", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-260", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3659", "mrqa_hotpotqa-validation-3713"], "SR": 0.515625, "CSR": 0.5432449494949495, "EFR": 0.6129032258064516, "Overall": 0.6378702600602801}, {"timecode": 99, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-1203", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-1647", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-1702", "mrqa_hotpotqa-validation-1722", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-1915", "mrqa_hotpotqa-validation-2013", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-209", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-2332", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2832", "mrqa_hotpotqa-validation-2909", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3487", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3911", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4219", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-4367", "mrqa_hotpotqa-validation-4380", "mrqa_hotpotqa-validation-4403", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-4821", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4891", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5138", "mrqa_hotpotqa-validation-5148", "mrqa_hotpotqa-validation-5152", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-79", "mrqa_hotpotqa-validation-851", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-2144", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-2542", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-338", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3510", "mrqa_naturalquestions-validation-3561", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3851", "mrqa_naturalquestions-validation-3950", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-4966", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7029", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-72", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-852", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8870", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9506", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-9722", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-9802", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2039", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-2312", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3129", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-322", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3511", "mrqa_newsqa-validation-3566", "mrqa_newsqa-validation-361", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4120", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-57", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-997", "mrqa_searchqa-validation-10025", "mrqa_searchqa-validation-10101", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10547", "mrqa_searchqa-validation-10578", "mrqa_searchqa-validation-10588", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-11866", "mrqa_searchqa-validation-1195", "mrqa_searchqa-validation-12095", "mrqa_searchqa-validation-12886", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-13411", "mrqa_searchqa-validation-1373", "mrqa_searchqa-validation-13757", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-14133", "mrqa_searchqa-validation-14277", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14604", "mrqa_searchqa-validation-14731", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14916", "mrqa_searchqa-validation-14939", "mrqa_searchqa-validation-15296", "mrqa_searchqa-validation-15850", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-15999", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-16014", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-16533", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2000", "mrqa_searchqa-validation-2021", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2604", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-3052", "mrqa_searchqa-validation-3342", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-3721", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-4161", "mrqa_searchqa-validation-426", "mrqa_searchqa-validation-4613", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-4921", "mrqa_searchqa-validation-50", "mrqa_searchqa-validation-5032", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5525", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5704", "mrqa_searchqa-validation-5813", "mrqa_searchqa-validation-5873", "mrqa_searchqa-validation-6296", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-6863", "mrqa_searchqa-validation-6874", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-7469", "mrqa_searchqa-validation-7512", "mrqa_searchqa-validation-7664", "mrqa_searchqa-validation-7675", "mrqa_searchqa-validation-7784", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8821", "mrqa_searchqa-validation-8894", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9614", "mrqa_searchqa-validation-9881", "mrqa_squad-validation-10180", "mrqa_squad-validation-10185", "mrqa_squad-validation-10337", "mrqa_squad-validation-10399", "mrqa_squad-validation-111", "mrqa_squad-validation-1311", "mrqa_squad-validation-1555", "mrqa_squad-validation-1572", "mrqa_squad-validation-1763", "mrqa_squad-validation-1897", "mrqa_squad-validation-1974", "mrqa_squad-validation-2032", "mrqa_squad-validation-2731", "mrqa_squad-validation-2875", "mrqa_squad-validation-2985", "mrqa_squad-validation-3423", "mrqa_squad-validation-3441", "mrqa_squad-validation-3473", "mrqa_squad-validation-392", "mrqa_squad-validation-4114", "mrqa_squad-validation-4128", "mrqa_squad-validation-4178", "mrqa_squad-validation-4436", "mrqa_squad-validation-4546", "mrqa_squad-validation-4708", "mrqa_squad-validation-479", "mrqa_squad-validation-4927", "mrqa_squad-validation-558", "mrqa_squad-validation-6637", "mrqa_squad-validation-7079", "mrqa_squad-validation-7141", "mrqa_squad-validation-7333", "mrqa_squad-validation-7488", "mrqa_squad-validation-7599", "mrqa_squad-validation-7698", "mrqa_squad-validation-801", "mrqa_squad-validation-8030", "mrqa_squad-validation-8308", "mrqa_squad-validation-8513", "mrqa_squad-validation-9162", "mrqa_squad-validation-9184", "mrqa_squad-validation-9254", "mrqa_squad-validation-9600", "mrqa_squad-validation-9921", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1645", "mrqa_triviaqa-validation-1663", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-2907", "mrqa_triviaqa-validation-3037", "mrqa_triviaqa-validation-3074", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3369", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-3503", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-3771", "mrqa_triviaqa-validation-3779", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-3973", "mrqa_triviaqa-validation-3989", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4139", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4588", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-4884", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5142", "mrqa_triviaqa-validation-5151", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5652", "mrqa_triviaqa-validation-5732", "mrqa_triviaqa-validation-5777", "mrqa_triviaqa-validation-582", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6291", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6669", "mrqa_triviaqa-validation-6747", "mrqa_triviaqa-validation-688", "mrqa_triviaqa-validation-690", "mrqa_triviaqa-validation-692", "mrqa_triviaqa-validation-6945", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7159", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7298", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-772", "mrqa_triviaqa-validation-802", "mrqa_triviaqa-validation-858", "mrqa_triviaqa-validation-972", "mrqa_triviaqa-validation-989"], "OKR": 0.716796875, "KG": 0.52734375, "before_eval_results": {"predictions": ["an angry mob.", "28", "dismissed all charges", "U.S. Defense Department", "11", "inmates", "Kenyan and Somali governments", "prostate cancer,", "Philip Markoff", "crocodile eggs", "Jacob", "crafts poems telling of the pain and suffering of children just like her; girls banned from school, their books burned, as the hard-core Islamic militants spread their reign of terror across parts of Pakistan.", "Red Lines", "The Kirchners", "an African-American woman for the job.", "Arsene Wenger", "Arnold Drummond", "the Carrousel du Louvre", "left his indelible fingerprints on the entertainment industry.", "Revolutionary Armed Forces of Colombia,", "his business dealings", "toxic smoke from burn pits", "not be making any further comments, citing the investigation.No one was inside the apartment at the time of the fire,", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Amado Carrillo Fuentes,\"", "Kerstin", "Amnesty International", "the Dr. Octopus", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "overthrow the socialist government of Salvador Allende in Chile,", "a lump in Henry's nether regions", "reached an agreement late Thursday to form a government of national reconciliation.", "snow, which continued to fall", "Steamboat Bill, Jr.", "dogs who walk on ice in Alaska", "45 minutes, five days a week.", "school.", "Harrison Ford", "Aniston, Demi Moore and Alicia Keys", "police patrol car", "al-Moayad", "\"I wanted to push it up that black a--.\"", "3 thousand", "The Palestinian Islamic Army,", "homicide by undetermined means", "Peruvian Supreme Court", "2,000", "park bench facing Lake Washington", "Cirque du Soleil", "9", "for the rest of the year", "94", "Kevin Spacey", "foreign investors", "neck", "Wrigley", "CBS", "round five of the 2017 season", "North Dakota", "Phil Spector", "Charlie and the Chocolate Factory", "Biff", "Sports Illustrated", "Colonel"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6111289607383357}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5925925925925926, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.25, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.875, 0.5, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-530", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-2952", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-955", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-4326", "mrqa_hotpotqa-validation-2793", "mrqa_searchqa-validation-6438"], "SR": 0.515625, "CSR": 0.54296875, "EFR": 0.5806451612903226, "Overall": 0.6290196572580645}]}