{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=1000.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]_result.json', stream_id=3, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4080, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["The Jones et al. and Briffa reconstructions", "help transfer and dissipate excess energy", "Historical and Critical Dictionary", "human settlement and development of the land", "over three days", "eight", "polynomial algebra", "60 days", "American Association of University Women", "mid-Cambrian period", "1926", "The outcome of most votes can be predicted beforehand", "Newton", "a disaster", "between the 1960s and 1990s", "James Hutton", "British", "growth and investment", "Each step had to be successfully accomplished before the next ones could be performed", "Downtown Riverside", "plague of Athens in 430 BC", "expelled Jews", "interleukin 1", "DuMont Television Network", "The city of Fresno", "1762", "colloblasts", "self", "Alta California", "300 km long and up to 40 km wide", "two-phased system", "On the Councils and the Church", "Super Bowl LI", "1968", "\"right\", \"just\", or \"true\"", "pastors", "meritocracy", "Vistula River", "arrows, swords, and leather shields", "23 November 1963", "since the 1960s", "zero net force", "2012", "CD4", "income inequality", "Tuition Fee Supplement", "April 1523", "the California State Automobile Association and the Automobile Club of Southern California", "Von Miller", "lymphocytes or an antibody-based humoral response", "unsuccessful", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Percy Shelley", "education, sanitation, and traffic control", "six divisions", "six series of theses", "Pi\u0142sudski", "Disney\u2013ABC Television Group", "SAP Center", "Genghis Khan", "materials melted near an impact crater", "William the Silent", "182 million tons", "John G. Trump"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8592347756410257}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4191", "mrqa_squad-validation-9426", "mrqa_squad-validation-368", "mrqa_squad-validation-3776", "mrqa_squad-validation-4769", "mrqa_squad-validation-9194", "mrqa_squad-validation-10305", "mrqa_squad-validation-2812", "mrqa_squad-validation-6559", "mrqa_squad-validation-2153", "mrqa_squad-validation-7246"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["occupational stress", "Sunni Arabs from Iraq and Syria", "The Lower Rhine", "Mike Tolbert", "silt up the lake", "cigarette advertising from all television and radio networks", "Dorothy and Michael Hintze", "climate change", "northwest", "Turkey", "Economist", "The Tyneside flat", "73", "Derek Jacobi", "ten minutes", "Sydney", "Basel", "ideal strings", "unstable molecules", "lasting damage", "December 2014", "other locations throughout Scotland", "eastern", "Newton", "Westwood One", "2008", "the colonies of British America and New France", "early 1546", "lower bounds", "2011", "became the University of Northumbria at Newcastle", "between 1859 and 1865", "8\u20134\u20134 system", "a green algal derived chloroplast", "over 200", "William the Conqueror", "The Service Module was discarded", "18 million volumes", "north", "detention", "collenchyma tissue", "Louis Adamic", "cytokine T IGF-\u03b2", "England", "Yuri Gagarin", "antigenic variation", "Knaurs Lexikon", "Chester, South Carolina", "1992", "three", "9.1 million", "rich and well socially standing", "pharynx", "1969", "Wardenclyffe Tower", "Tower Theatre", "700,000", "Pittsburgh", "jeopardy/2516_Qs.txt at master  jedoublen/jeopardy", "Heroes struggle... Animated, Action, Adventure, Fantasy, Sci-Fi.... Lego Star Wars: The Yoda Ch", "American baseball, until the late 1940s, excluded, with some big exceptions in... The color line was broken for good when Jackie Robinson signed with the Brooklyn", "the hand of cards which he supposedly held at the time of his death... killed by the assassin Jack McCall in Deadwood, Black Hills, August 2, 1876", "His... (initial capital letter) a German-built enciphering machine developed for commercial", "Europe"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8128918650793651}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714285, 0.16, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9752", "mrqa_squad-validation-5549", "mrqa_squad-validation-8718", "mrqa_squad-validation-8891", "mrqa_squad-validation-9744", "mrqa_squad-validation-5337", "mrqa_squad-validation-3885", "mrqa_squad-validation-1568", "mrqa_squad-validation-6523", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-4596"], "SR": 0.78125, "CSR": 0.8046875, "EFR": 1.0, "Overall": 0.90234375}, {"timecode": 2, "before_eval_results": {"predictions": ["John Sutcliffe", "can also concentrate wealth, pass environmental costs on to society, and abuse workers and consumers", "6800", "medical treatment for the sick and wounded French soldiers", "Ticonderoga Point", "October 16, 1973", "1980s", "seven", "books and articles for magazines and journals", "Roger NFL", "the oceans and seas", "60,000 European settlers", "by over 100%", "1350", "North America", "Euclid's fundamental theorem of arithmetic", "Maria de la Queillerie", "to encourage investment", "Julia Butterfly Hill", "farming assets, ranging from mobile phones to productive land and livestock, and is opening pathways for them to move out of poverty", "587,000 square kilometres", "March 1974", "quickly", "end of the 19th century", "DeMarcus Ware", "University of North Florida team", "motivated students", "platyctenids", "elementary school education certificate", "1220", "two", "30.0%", "Raghuram Rajan", "VHF channel 7", "The Pink Triangle", "ideological", "Methodist institutions", "Ticonderoga", "six daughters", "Los Angeles Dodgers", "19th", "a course of study and lesson plan", "a delay costs money", "Funchess", "Watt", "1,000 m3/s (35,000 cu ft/s)", "Thuringia", "rivers", "anti-Semitic policies of the National Socialists", "visor helmet", "Catholic", "Hollywood", "Long Island Sound", "Sweden's", "an invaluable service as usurers in medieval society", "an African American", "first woman governor", "an athlete who plays cricket", "conifer", "Alaska", "an Austrian and American film actress and inventor", "Daniel Defoe", "the Association of American Universities", "50JJB Sports Fitness Clubs"], "metric_results": {"EM": 0.625, "QA-F1": 0.7018229166666666}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.6, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665]}}, "before_error_ids": ["mrqa_squad-validation-7525", "mrqa_squad-validation-10258", "mrqa_squad-validation-1565", "mrqa_squad-validation-85", "mrqa_squad-validation-802", "mrqa_squad-validation-9061", "mrqa_squad-validation-8322", "mrqa_squad-validation-4257", "mrqa_squad-validation-1960", "mrqa_squad-validation-5678", "mrqa_squad-validation-6101", "mrqa_squad-validation-2786", "mrqa_squad-validation-1877", "mrqa_squad-validation-2497", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-13452", "mrqa_searchqa-validation-12341", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-5603"], "SR": 0.625, "CSR": 0.7447916666666667, "EFR": 0.9166666666666666, "Overall": 0.8307291666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["public schools", "connection id in a table", "males", "electrical, water, sewage, phone, and cable facilities", "level of the top tax rate", "\"Wise up or die.\"", "VideoGuard UK", "highly-paid", "pump this into the mesoglea", "Fred Silverman", "atmospheric engine", "force", "ctenophores", "trial and rehabilitation of Joan of Arc", "John Hurt", "an Australian public X.25 network operated by Telstra", "Arizona Cardinals", "Von Miller", "Indianapolis Colts", "42%", "1957", "imprisonment", "orogenic wedges", "one", "Catholic", "Edict of Fontainebleau", "Fort Caroline", "Pittard Sullivan", "wealth", "Niagara Falls", "Hugh Downs", "House of Hohenstaufen", "3D printing technology", "Daniel Burke", "internal strife", "Matthew Murray", "400 AD", "the United States", "Satya Nadella", "the difference between a problem and an instance", "Richard E. Grant", "Inner Mongolia", "cortisol and catecholamines", "a third group of pigments found in cyanobacteria", "isopentenyl pyrophosphate synthesis", "1963", "hotel room", "Italy", "Joan Hughes", "workhouse", "Khartoum", "William Henry Harrison", "Playboy rabbit", "Dutch metaphysicians who try to kill me", "Puerto Rico", "Court TV", "King Tut", "The Prairie Wolf", "Inhospitable Sea", "Moshe Dayan", "Joan Van Dinh", "active athletes", "helicopters and boats", "$17,000"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6802522130647131}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 0.4615384615384615, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7103", "mrqa_squad-validation-7357", "mrqa_squad-validation-4452", "mrqa_squad-validation-3247", "mrqa_squad-validation-230", "mrqa_squad-validation-363", "mrqa_squad-validation-10083", "mrqa_squad-validation-3139", "mrqa_squad-validation-1045", "mrqa_squad-validation-5542", "mrqa_squad-validation-1670", "mrqa_squad-validation-6263", "mrqa_squad-validation-8839", "mrqa_squad-validation-7711", "mrqa_searchqa-validation-8170", "mrqa_searchqa-validation-12199", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-6624", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-3588"], "SR": 0.609375, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 4, "before_eval_results": {"predictions": ["whether he stood by their contents", "1850s", "Troika", "complexity classes", "Parliament of the United Kingdom at Westminster", "teaching", "Desperate Housewives, Lost and Grey's Anatomy", "8 November 2010", "the Y. pestis was spread from fleas on rats", "coastal beaches and the game reserves", "1524", "2p \u2212 1", "horizontal", "Crash the Super Bowl", "collenchyma tissue", "around a billion years ago", "Croatia", "Long Beach", "Edinburgh", "Broncos", "Papin", "Cricket", "William Morris", "T. J. Ward", "the Daleks", "San Diego", "1017", "heat and pressure", "1072", "Chevron", "Africa", "New York City", "Marshall Cohen", "Hypersensitivity", "Business Connect", "Henry Young Darracott Scott", "not break any law", "income inequality", "Bainbridge's", "1294", "chao", "Apollo 17", "Ghirardelli", "Ghirardelli", "Stephen Hawking", "Ghirardelli", "Europe", "Ghirardelli", "Ghirardelli", "Ghirardelli", "Ghirardelli", "Moscow", "crystal anniversary", "prairie crocus", "Detroit River", "Seth Taft", "New Testament", "Dublin", "Ghirardelli", "Ghirardelli", "Albert Einstein", "Doctor Who", "1990", "Zimbabwe"], "metric_results": {"EM": 0.640625, "QA-F1": 0.670217803030303}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5878", "mrqa_squad-validation-5029", "mrqa_squad-validation-2765", "mrqa_squad-validation-821", "mrqa_squad-validation-5344", "mrqa_squad-validation-7615", "mrqa_squad-validation-2679", "mrqa_squad-validation-1061", "mrqa_squad-validation-4147", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-9601", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-4300"], "SR": 0.640625, "CSR": 0.696875, "EFR": 1.0, "Overall": 0.8484375}, {"timecode": 5, "before_eval_results": {"predictions": ["November 2006", "achievement-oriented", "a malfunction in the chameleon circuit", "SAP Center", "March 2011", "above the top of the range", "12th", "1226", "The Late Late Show", "bird", "P", "theNP-complete knapsack problem", "1928", "when the immune system is less active than normal", "disturbances", "Fraud", "1562 to 1598", "Emmerich Rhine Bridge", "ten", "1993", "a primitive intermediate between cyanobacteria and the more evolved chloroplasts", "lung tissue", "US$100,000", "Bakersfield", "Bruno Mars", "patients' prescriptions and patient safety issues", "2009", "the Common Core", "Recognized Student Organizations", "a cubic interpolation formula", "Thomas Edison", "the Swiss Reformation", "phagocytes", "15", "Satyagraha", "two", "microorganisms", "1968", "Aloha \u02bbOe", "1.6 million", "Eric Whitacre", "gourd-bows", "Angus Young", "New York", "the waltz Gunstwerber", "Cherokee River", "Odisha", "January 28, 2016", "138,535", "Adam Rex", "1933", "Sivakumar", "1968", "a Chaplain to the Forces", "astronomer", "Warrington", "1866", "Chattahoochee", "the Provisional Irish Republican Army", "pneumonoultramicroscopicsilicovolcanoconiosis", "Boutros Ghali", "Vertikal-T", "The Wizard of Oz", "he was mad at the U.S. military"], "metric_results": {"EM": 0.59375, "QA-F1": 0.672636217948718}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8531", "mrqa_squad-validation-434", "mrqa_squad-validation-9185", "mrqa_squad-validation-1759", "mrqa_squad-validation-1860", "mrqa_squad-validation-8696", "mrqa_squad-validation-3496", "mrqa_squad-validation-2634", "mrqa_squad-validation-664", "mrqa_squad-validation-3240", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-2158", "mrqa_newsqa-validation-1468", "mrqa_searchqa-validation-13492", "mrqa_newsqa-validation-3290"], "SR": 0.59375, "CSR": 0.6796875, "EFR": 1.0, "Overall": 0.83984375}, {"timecode": 6, "before_eval_results": {"predictions": ["UHF", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "Africa", "1976", "inverse", "non-deterministic time", "60", "Warsaw", "5,000 years", "Edgar Atheling", "Writers Guild of America", "United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO)", "Conservative", "11 million", "a water-cooled undergarment", "the Queen", "3 in 1,000,000", "2009 onwards", "Super Bowl Opening Night", "Fresno Street and Thorne Ave", "southern and central parts of France", "2014", "40%", "John F. Kennedy", "innate immune system", "15\u20131", "history of arms", "Industry and manufacturing", "this contact with nature made him stronger, both physically and mentally.", "1543", "\u015ar\u00f3dmie\u015bcie", "Hmong or Laotian", "Stromules", "oxygen will act as a fuel;", "Johnny Herbert", "\"jus sanguinis\"", "Pharrell Williams", "James Dearden", "J\u00f3zsef Pulitzer", "June 17, 2007", "\"The Frost Report\"", "National Basketball Development League", "Danish", "24 January 76", "Kona coast", "Dave Lee Travis", "Northrop P-61 Black widow", "People v. Turner", "\"Veyyil\"", "Helena Sternlicht", "Highlands Course", "Illinois", "Cartoon Cartoon Fridays", "Timo Hildebrand", "2016", "Ginger Rogers", "Corps of Discovery", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "801,200", "giraffe", "navy", "Ecuador", "Abraham Lincoln", "a final contest"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7204691142191142}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10424", "mrqa_squad-validation-1730", "mrqa_squad-validation-1114", "mrqa_squad-validation-8523", "mrqa_squad-validation-4070", "mrqa_squad-validation-7770", "mrqa_squad-validation-3764", "mrqa_squad-validation-1232", "mrqa_squad-validation-3483", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2183", "mrqa_naturalquestions-validation-4953", "mrqa_newsqa-validation-3782", "mrqa_searchqa-validation-5374"], "SR": 0.671875, "CSR": 0.6785714285714286, "EFR": 1.0, "Overall": 0.8392857142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["convection of the mantle", "Continental Edison Company in France", "South", "T. J. Ward", "25 minutes of transmission length", "1903", "1993", "King George III", "occupancy permit", "Hereford", "a trusted friend, who may hold any office, from Elder to Bishop, or no office at all", "Arts & Entertainment Television (A&E)", "NASA", "Albert Einstein", "Karl von Miltitz", "a matter of custom or expectation,", "Silk Road", "a war", "13 years and 48 days", "Bolshevik leaders", "University of Aberdeen", "transportation, sewer, hazardous waste and water", "during the plague of Athens in 430 BC", "Pedro Men\u00e9ndez de Avil\u00e9s", "expansion", "a deterministic Turing machine", "linear", "when the oxygen concentration is too high", "1290", "17,786,419,", "smallest state on the Australian mainland", "Montreal Montreal", "7000301604928199000", "was an incident on March 5, 1770, in which British Army soldiers shot and killed people while under attack by a mob", "Pakistan, India, and Bangladesh are among the largest individual contributors with around 8,000 units each", "2000", "Christina Calvano", "Moore", "the human hands and face", "Martin Lawrence", "Steve Hale", "a multilayer", "January 2017 patch", "Idaho", "September 6, 2019", "multinational retail corporation", "Joely Richardson", "Jack Gleeson", "claims adjusters", "when the forward reaction proceeds at the same rate as the reverse reaction", "heart", "writ of certiorari", "A standard form contract", "Mr. Feeny", "September 30", "Kelly Osbourne, Ian'' Dicko '' Dickson, Christina Monk and Eddie Perfect", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "a Californio nobleman and master living in Los Angeles during the era of Spanish rule", "\"Household Words\",", "56", "Hindu scriptures", "St. Augustine", "\"isthmi\u201d", "gold"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6797531864937388}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.07692307692307691, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.046511627906976744, 0.0, 0.5, 0.4615384615384615, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1239", "mrqa_squad-validation-2315", "mrqa_squad-validation-6878", "mrqa_squad-validation-1819", "mrqa_squad-validation-2881", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-3491", "mrqa_triviaqa-validation-7579", "mrqa_searchqa-validation-12472", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3744"], "SR": 0.609375, "CSR": 0.669921875, "EFR": 0.96, "Overall": 0.8149609375}, {"timecode": 8, "before_eval_results": {"predictions": ["ivory", "The Dornbirner Ach", "a certain number of teacher's salaries are paid by the State", "non-deterministic time", "five", "December 2014", "an inauspicious typhoon", "four", "Zwickau prophet", "10 July 1856 \u2013 7 January 1943", "1999", "CBS Sports apps on tablets, Windows 10, Xbox One and other digital media players", "3\u20132.7 billion years ago", "New Testament from Greek", "Von Miller", "economists with the Standard & Poor's rating agency", "Germany", "a vestigial red algal derived chloroplast", "two or more teachers working harmoniously to fulfill the needs of every student in the classroom", "a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981", "type III secretion system", "worker, capitalist/business owner, landlord", "tungsten", "the state", "the oxidant", "the signals could come from Mars, Venus, or other planets", "the American Revolution", "Book of Discipline", "Fat Albert", "1943", "\"Big Fucking German\"", "Chelmsford City", "William \"Magic\" Johnson Jr.", "22,500 acres", "1951", "Abu Dhabi, United Arab Emirates", "Oklahoma Memorial Stadium", "American", "the Firth of Forth Site of Special Scientific Interest", "one live album, one compilation album, nineteen singles and fourteen music videos", "Blue Valley Northwest High School", "The Birds", "Battle of the Rosebud", "Homebrewing", "Pablo Escobar", "Cartoon Network Studios", "26 June 2013", "25 million records", "twice", "Geraldine Sue Page", "Rochdale", "Charles Reed Bishop", "motor", "Marco Fu Ka-chun", "2015", "Dusty Springfield", "her translation of and commentary on Isaac Newton's book \"Principia\"", "BeBe Winans", "Henry VIII", "Sunday", "purple", "Dumont d'Urville Station", "Under normal conditions", "a spiritual conversion"], "metric_results": {"EM": 0.578125, "QA-F1": 0.713880119529696}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.125, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.75, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.8, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_squad-validation-1156", "mrqa_squad-validation-531", "mrqa_squad-validation-2272", "mrqa_squad-validation-1912", "mrqa_squad-validation-4849", "mrqa_squad-validation-1529", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-391", "mrqa_triviaqa-validation-4490", "mrqa_newsqa-validation-3405", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-5851"], "SR": 0.578125, "CSR": 0.6597222222222222, "EFR": 1.0, "Overall": 0.8298611111111112}, {"timecode": 9, "before_eval_results": {"predictions": ["1999", "zero", "Mao Zedong", "Chebyshev", "1960", "introduction of Beroe", "1000 CE", "Rollo", "Sunspot, New Mexico", "Sonderungsverbot", "an amending treaty", "the environment in which they lived", "a genetic disease", "C. J. Anderson", "Cadeby", "Warraghiggey", "starts accidentally adding oxygen to sugar precursors", "World Meteorological Organization", "Sunni pan-Islamism", "23\u201316", "yes or no, or alternately either 1 or 0", "white", "the Mongols", "English", "Newton", "1940s and 1950s", "arthur is a 17th-century castle near Muir of Ord and Tore on the Black Isle, in Ross and Cromarty, Scotland", "musical theatre", "Taoiseach", "Duval County", "Bill Ponsford", "$10\u201320 million", "Manasseh Cutler Hall", "Denmark", "Hindi", "the \"Pour le M\u00e9rite\" 1", "Giuseppe Fortunino Francesco Verdi", "Edward Trowbridge Collins Sr.", "1946 and 1947", "Christopher McCulloch", "2016\u201317", "Carson City", "Wembley Stadium", "19th", "Cesar Millan", "Bob Dylan", "Michael Lewis Greenwell", "from 20 March to 1 May 2003", "The Life of Charlotte Bront\u00eb", "2015", "Bill Curry", "Jack White", "Kim Yoon-seok and Ha Jung-woo", "superhero roles", "Brent Robert Barry", "18th congressional district", "the BBC", "2008", "aro Starr", "arthur", "the eastern Afghan province of Logar", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures", "the arthur", "Jamaica"], "metric_results": {"EM": 0.625, "QA-F1": 0.7622478672643146}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7368421052631579, 0.4615384615384615, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.923076923076923, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 0.9714285714285714, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9897", "mrqa_squad-validation-8832", "mrqa_squad-validation-8782", "mrqa_squad-validation-1652", "mrqa_squad-validation-502", "mrqa_hotpotqa-validation-2670", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-4397", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-2645", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_searchqa-validation-13221"], "SR": 0.625, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 10, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3894", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4633", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5145", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5527", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-690", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-599", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9687", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-13863", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14454", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16753", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-4300", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6624", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-9383", "mrqa_squad-validation-10007", "mrqa_squad-validation-10010", "mrqa_squad-validation-10031", "mrqa_squad-validation-10083", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10130", "mrqa_squad-validation-10153", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10234", "mrqa_squad-validation-10249", "mrqa_squad-validation-10258", "mrqa_squad-validation-10305", "mrqa_squad-validation-10345", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10478", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1073", "mrqa_squad-validation-1075", "mrqa_squad-validation-1075", "mrqa_squad-validation-1096", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1232", "mrqa_squad-validation-1239", "mrqa_squad-validation-1254", "mrqa_squad-validation-1296", "mrqa_squad-validation-1372", "mrqa_squad-validation-1529", "mrqa_squad-validation-1543", "mrqa_squad-validation-1586", "mrqa_squad-validation-1632", "mrqa_squad-validation-1652", "mrqa_squad-validation-1681", "mrqa_squad-validation-1723", "mrqa_squad-validation-1730", "mrqa_squad-validation-1731", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-1819", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1877", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-1976", "mrqa_squad-validation-1985", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2111", "mrqa_squad-validation-2153", "mrqa_squad-validation-2181", "mrqa_squad-validation-2189", "mrqa_squad-validation-2246", "mrqa_squad-validation-2247", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2320", "mrqa_squad-validation-2359", "mrqa_squad-validation-2413", "mrqa_squad-validation-2442", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2568", "mrqa_squad-validation-2591", "mrqa_squad-validation-260", "mrqa_squad-validation-2617", "mrqa_squad-validation-2628", "mrqa_squad-validation-2634", "mrqa_squad-validation-2644", "mrqa_squad-validation-2679", "mrqa_squad-validation-2721", "mrqa_squad-validation-2723", "mrqa_squad-validation-2765", "mrqa_squad-validation-2808", "mrqa_squad-validation-2812", "mrqa_squad-validation-2881", "mrqa_squad-validation-2941", "mrqa_squad-validation-2949", "mrqa_squad-validation-2975", "mrqa_squad-validation-2977", "mrqa_squad-validation-30", "mrqa_squad-validation-3111", "mrqa_squad-validation-3139", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3248", "mrqa_squad-validation-3269", "mrqa_squad-validation-3296", "mrqa_squad-validation-3377", "mrqa_squad-validation-3429", "mrqa_squad-validation-3483", "mrqa_squad-validation-3496", "mrqa_squad-validation-3534", "mrqa_squad-validation-3552", "mrqa_squad-validation-360", "mrqa_squad-validation-363", "mrqa_squad-validation-368", "mrqa_squad-validation-3705", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3776", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3885", "mrqa_squad-validation-3952", "mrqa_squad-validation-3981", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4070", "mrqa_squad-validation-4095", "mrqa_squad-validation-4107", "mrqa_squad-validation-4121", "mrqa_squad-validation-4121", "mrqa_squad-validation-4147", "mrqa_squad-validation-4191", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4516", "mrqa_squad-validation-457", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4689", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4758", "mrqa_squad-validation-4797", "mrqa_squad-validation-480", "mrqa_squad-validation-4840", "mrqa_squad-validation-4898", "mrqa_squad-validation-4988", "mrqa_squad-validation-4997", "mrqa_squad-validation-502", "mrqa_squad-validation-5029", "mrqa_squad-validation-5061", "mrqa_squad-validation-5096", "mrqa_squad-validation-5108", "mrqa_squad-validation-5200", "mrqa_squad-validation-5222", "mrqa_squad-validation-5270", "mrqa_squad-validation-5272", "mrqa_squad-validation-5287", "mrqa_squad-validation-531", "mrqa_squad-validation-5337", "mrqa_squad-validation-5344", "mrqa_squad-validation-5347", "mrqa_squad-validation-5382", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5549", "mrqa_squad-validation-5551", "mrqa_squad-validation-556", "mrqa_squad-validation-5621", "mrqa_squad-validation-5670", "mrqa_squad-validation-5741", "mrqa_squad-validation-5794", "mrqa_squad-validation-5839", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6000", "mrqa_squad-validation-6027", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6259", "mrqa_squad-validation-6260", "mrqa_squad-validation-6263", "mrqa_squad-validation-6277", "mrqa_squad-validation-6295", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-65", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6538", "mrqa_squad-validation-6548", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6600", "mrqa_squad-validation-6612", "mrqa_squad-validation-6624", "mrqa_squad-validation-664", "mrqa_squad-validation-6656", "mrqa_squad-validation-6657", "mrqa_squad-validation-6666", "mrqa_squad-validation-6695", "mrqa_squad-validation-6749", "mrqa_squad-validation-6858", "mrqa_squad-validation-6861", "mrqa_squad-validation-6878", "mrqa_squad-validation-6880", "mrqa_squad-validation-6888", "mrqa_squad-validation-689", "mrqa_squad-validation-6898", "mrqa_squad-validation-6900", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-6967", "mrqa_squad-validation-7018", "mrqa_squad-validation-7021", "mrqa_squad-validation-7033", "mrqa_squad-validation-7036", "mrqa_squad-validation-7062", "mrqa_squad-validation-7123", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7246", "mrqa_squad-validation-7268", "mrqa_squad-validation-7302", "mrqa_squad-validation-7312", "mrqa_squad-validation-7323", "mrqa_squad-validation-7357", "mrqa_squad-validation-7362", "mrqa_squad-validation-7373", "mrqa_squad-validation-738", "mrqa_squad-validation-7391", "mrqa_squad-validation-741", "mrqa_squad-validation-7450", "mrqa_squad-validation-7458", "mrqa_squad-validation-7466", "mrqa_squad-validation-7470", "mrqa_squad-validation-755", "mrqa_squad-validation-7562", "mrqa_squad-validation-7603", "mrqa_squad-validation-764", "mrqa_squad-validation-767", "mrqa_squad-validation-7686", "mrqa_squad-validation-7711", "mrqa_squad-validation-7736", "mrqa_squad-validation-7744", "mrqa_squad-validation-7765", "mrqa_squad-validation-7770", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7892", "mrqa_squad-validation-7902", "mrqa_squad-validation-7957", "mrqa_squad-validation-7970", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8232", "mrqa_squad-validation-8236", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8572", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8685", "mrqa_squad-validation-8696", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8743", "mrqa_squad-validation-8760", "mrqa_squad-validation-8763", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8794", "mrqa_squad-validation-8797", "mrqa_squad-validation-8832", "mrqa_squad-validation-8837", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-8965", "mrqa_squad-validation-904", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9236", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-9317", "mrqa_squad-validation-9330", "mrqa_squad-validation-938", "mrqa_squad-validation-9401", "mrqa_squad-validation-9426", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-9579", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9628", "mrqa_squad-validation-9744", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9817", "mrqa_squad-validation-9890", "mrqa_squad-validation-9892", "mrqa_squad-validation-9897", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7579"], "OKR": 0.896484375, "KG": 0.36875, "before_eval_results": {"predictions": ["Thermochemical techniques", "executive producer", "1987", "James O. McKinsey", "North", "one-eighth", "coastal beaches and the game reserves", "Vicodin", "\u00a31.3bn", "Katharina von Bora", "9 March 1508", "Shoushi Li", "his means of seizing power", "ideal strings", "after the Franco-German War", "German", "San Andreas Fault", "NYPD Blue", "Northern Chinese", "Oireachtas funds", "the Marconi Company", "countries with bigger income inequalities", "John Robert Cocker", "King Kelly", "a United States Army Medical Corps psychiatrist", "Michael Manasseri", "1988", "Bergen County", "The Ones Who Walk Away from Omelas", "hiphop", "Lithuania", "historic buildings, arts, and published works", "Disney Parks Christmas Day Parade", "Esp\u00edrito Santo Financial Group", "Guillermo del Toro", "Wolf Creek", "YouTube", "onset and progression of Alzheimer's disease", "San Francisco 49ers", "Lake Placid, New York", "Iron Man 3", "Suffolk, England", "singer, songwriter, actress, and radio and television presenting", "Chicago", "Martin \"Marty\" McCann", "247,597", "Mandalay Entertainment", "actor", "a specialized version of the two-seat F/A-18F Super Hornet", "Barnoldswick", "Pacific War", "A41", "Heather Langenkamp", "Leona Lewis", "The God of Small Things", "BAFTA TV Award Best Actor winner in 1956", "Rodney Crowell", "Andy Serkis", "a specific phobia", "a\u00efda", "Zulfikar Ali Bhutto", "cancer", "a toast made when two people get married", "a dieposition reaction to produce Na(s) and N2(g)"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6719274648962149}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7887", "mrqa_squad-validation-2976", "mrqa_squad-validation-1480", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3822", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-5012", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-1133", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-7417", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-5418"], "SR": 0.59375, "CSR": 0.6505681818181819, "EFR": 1.0, "Overall": 0.7288636363636363}, {"timecode": 11, "before_eval_results": {"predictions": ["April 1887", "semi-arid savanna", "2014", "Levi's Stadium", "misguided", "San Jose Marriott", "1972", "second-largest global producer", "Decision Time", "Victorian Government", "American Revolutionary War", "pep", "human", "the Treaties establishing the European Union", "trans-lunar injection", "Amazonia", "Daniel Andrews", "UNESCO's World Heritage list", "Richard E. Grant", "31\u20130", "I", "Mark Helfrich", "Wal-Mart Canada Corp.", "\"Louie\" Zamperini", "Che Guevara", "Carol Ann Duffy", "Karl-Anthony Towns Jr.", "1978", "Danish", "Ukrainian", "1954", "John John Florence", "\"brainwash\"", "9Lives", "\"valley of the hazels'", "Art Bell", "Lady Frederick Windsor", "Jon Bellion", "Point Place", "Knowlton", "Delilah Rene", "Don Bluth", "Columbus", "Czech Kingdom", "Anne Fletcher", "Sacramento Kings", "South Asian Games", "Tufts College", "Harrods", "Flamingo Las Vegas", "Ben Savage", "\"Suspiria\"", "City of Newcastle", "Japan", "Canada", "in positions 14 - 15, 146 - 147 and 148 - 149", "privatized", "silver", "beetles", "the Iraqi and U.S. soldiers", "Jan Brewer", "\"Annie Get Your Gun\"", "New York", "Hebrew"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7862088585434174}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8447", "mrqa_squad-validation-7288", "mrqa_squad-validation-4015", "mrqa_squad-validation-234", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-447", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-1825", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-953", "mrqa_triviaqa-validation-1604", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-3580", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7583"], "SR": 0.671875, "CSR": 0.65234375, "EFR": 1.0, "Overall": 0.72921875}, {"timecode": 12, "before_eval_results": {"predictions": ["late 1545", "several hundred thousand, some 30%", "five", "every two years", "two-man", "The Hoppings", "Mycobacterium tuberculosis", "C. J. Anderson", "Harvey Martin", "stratigraphic", "environmental determinism", "Wellington", "a problem instance", "the vehicle that is at rest or the outside world that are at rest", "a sin", "for translation initiation in most chloroplasts and prokaryotes", "\"Isel\"", "Edmonton, Canada", "Anthony Stephen Burke", "Eugene O'Neill", "Max Kellerman", "UVM Agriculture Department and the Agricultural Experiment Station", "VfL Wolfsburg", "July 22, 1946", "Shakespeare", "Pendlebury", "McLaren-Honda", "Bismarck", "Comedy Film Nerds", "2016 World Indoor Championships", "MG Cars", "January 18, 1977", "North Greenwich Arena", "The Soloist", "Nikita Khrushchev", "Lipshitz", "fourth President of Pakistan from 1971 to 1973", "February 18, 1965", "automobiles", "Republican", "the afterburner", "Chad", "NBA All-Star Game and All-NBA Team", "Emilia Fox", "Freeform", "St James's Palace", "Law Adam", "American", "Chief Strategy Officer", "November 15, 1903", "De La Soul", "American", "Archbishop of Canterbury", "Via Vai", "1985", "after releasing Xander from the obligation to be Sweet's `` bride '', tells the group how much fun they have been ( `` What You Feel -- Reprise '' )", "when the cell is undergoing the metaphase of cell division", "California", "(multi-user dungeon)", "Dubai", "Iran", "(Halle Berry)", "Sindbad", "Abid Ali Neemuchwala"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7678007822447162}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.380952380952381, 1.0, 0.9411764705882353, 1.0, 1.0, 0.4, 0.0, 1.0, 0.125, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-914", "mrqa_squad-validation-10328", "mrqa_squad-validation-8852", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-191", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-3242", "mrqa_searchqa-validation-13537"], "SR": 0.6875, "CSR": 0.6550480769230769, "EFR": 0.95, "Overall": 0.7197596153846153}, {"timecode": 13, "before_eval_results": {"predictions": ["Tower District", "computational problem", "social and political action", "Duran Duran", "30", "NYPD Blue", "chemical bonds", "John and Benjamin Green", "Lippe", "since the 1970s", "2 million", "a statement to the chamber setting out the Government's legislative programme for the forthcoming year", "40,000", "Citadel Broadcasting", "$45,000", "stream capture", "1,149 feet", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "Robert Remak", "Eddie Murphy", "Audrey II", "Human fertilization", "balance sheet", "The terrestrial biosphere", "Peter Andrew Beardsley MBE", "Leo Arnaud", "lumbar cistern", "not being pushed around by big labels, managers, and agents and being told what to do, and being true to yourself creatively", "1 US dollar worth close to 5,770 guaranies", "digitization of social systems", "Yugoslavia", "Middlesex County", "Sweden had been an active supporter of the League of Nations", "1972", "lightning strike", "Inequality of opportunity was higher", "instant messenger", "George Strait", "alpaca fiber and mohair", "Anakin Skywalker", "Prince James, Duke of York and of Albany", "104 colonists and Discovery", "an enumeration of 7 spiritual gifts originating from patristic authors", "Florida", "Toronto", "Manchuria", "Ben Savage", "at a given temperature", "Max", "Magyarorsz\u00e1g z\u00e1szlaja", "the church at Philippi", "2003", "The Sun", "northern China", "Mackinac Bridge", "Barbarella", "Bergen", "Balvenie Castle", "has been accused of running Zimbabwe's economy into the ground while implementing a draconian crackdown aimed at keeping power.", "repression and dire economic circumstances", "Leon Trotsky", "oregon", "Elizabeth Gaskell", "American radiologist remembered for describing Hampton's hump and Hampton's line"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7080762987012987}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.9777777777777777, 1.0, 1.0, 1.0, 0.1904761904761905, 0.14285714285714285, 0.0, 0.0, 0.7499999999999999, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-3599", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-6020", "mrqa_hotpotqa-validation-877", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-16103", "mrqa_hotpotqa-validation-3149"], "SR": 0.640625, "CSR": 0.6540178571428572, "EFR": 0.9565217391304348, "Overall": 0.7208579192546585}, {"timecode": 14, "before_eval_results": {"predictions": ["reaffirmed Catholicism as the state religion of France", "reached an all-time high between 2005 and 2010", "the Marches", "social", "cartels", "Anglo-Saxon", "Ancient Egypt", "Battle of Olustee", "patrimonial feudalism", "Daniel arap Moi", "near Millingen aan de Rijn, close to the Dutch-German border with the division of the Rhine into Waal and Nederrijn", "an electric lighting company in Tesla's name, Tesla Electric Light & Manufacturing", "1303", "oxygen", "three", "Touma", "1994", "Empire of the Sun", "54 bodies", "Roger Federer", "sodium dichromate", "the two remaining crew members", "July", "citizenship", "40", "18 years to life in prison", "break up ice jams", "Expedia", "Kabul", "\"We've got a long way to go, but we've made progress.\"", "Communist Party of Nepal", "Bob Dole", "Eden Park", "in her home for 12 of the past 18 years", "12.3 million people worldwide", "as soon as 2050", "in all of Lifeway's 100-plus stores nationwide", "in Beijing", "18", "National Park Service", "3-2", "Bob Bogle", "40 militants and six Pakistan soldiers dead", "1959", "Pakistan's High Commission in India", "One of Osama bin Laden's sons", "President Obama's race", "Obama", "Mark Fields", "Ed McMahon", "peace sign", "Muslim festival of Eid al-Adha", "Larry Ellison", "Revolutionary Armed Forces of Colombia", "an unknown recipient", "Jules Shear", "the Soviet Union", "Vienna", "2008\u201309 UEFA Champions League", "310", "New England", "Babylon", "2017", "Algernod Lanier Washington"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6340997544122544}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false], "QA-F1": [0.08333333333333333, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.7499999999999999, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3163", "mrqa_squad-validation-1886", "mrqa_squad-validation-1064", "mrqa_squad-validation-5276", "mrqa_squad-validation-9076", "mrqa_squad-validation-1287", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-1020", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-653", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-324", "mrqa_hotpotqa-validation-2080", "mrqa_searchqa-validation-4684", "mrqa_hotpotqa-validation-5370"], "SR": 0.53125, "CSR": 0.6458333333333333, "EFR": 1.0, "Overall": 0.7279166666666665}, {"timecode": 15, "before_eval_results": {"predictions": ["Robert R. Gilruth", "Arthur Woolf", "Ten", "multiple revisions", "seven", "Ealy", "St. George's United Methodist Church", "1914", "Wales", "more than 70,000", "Nikita Khrushchev", "increasing unemployment", "X reduces to Y", "March 22", "he acted in self defense in punching businessman Marcus McGhee.", "anyone", "Superman brought down the Ku Klux Klan,", "in Spain and at Harvard Law School.", "power-sharing talks to take place in the next few weeks.", "Senate", "15,000", "Kim Jong Un", "Tim Baker", "Philip Markoff", "Democratic", "the IV cafe", "North Korea intends to launch a long-range missile in the near future", "District of Columbia National Guard", "forgery and flying without a valid license", "job is actually digging ditches.", "14", "two-thirds of his body,", "school", "Monday and Tuesday", "27-year-old", "almost 100", "Venus Williams", "peanuts", "more than two years", "Manchester United", "Nafees A. Syed", "Robert Barnett", "military commissions are inherently illegitimate, unconstitutional and incapable, while bringing them in line with the rule of law.", "Alfredo Astiz", "American", "Illness", "procedures", "Araceli Valencia", "56", "Michael Jackson's", "Raymond Thomas", "racially motivated", "Adam Lambert", "placed down flowers.", "California and South Carolina", "along the coast of northern California", "citric acid", "Denali", "The New Yorker", "death", "the MasonDixon line", "high and dry", "capitol building", "New Orleans Saints"], "metric_results": {"EM": 0.59375, "QA-F1": 0.687818605006105}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.888888888888889, 0.2857142857142857, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.30769230769230765, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-698", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-4202", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-6596", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-3685"], "SR": 0.59375, "CSR": 0.642578125, "EFR": 1.0, "Overall": 0.727265625}, {"timecode": 16, "before_eval_results": {"predictions": ["to \"exterminate\" all non-Dalek beings.", "San Diego", "30\u201375%", "the attempt \"to implement Islamic values in all spheres of life.\"", "James Watt", "the Greek \u03ba\u03c4\u03b5\u03af\u03c2 kteis 'comb' and \u03c6\u03ad\u03c1\u03c9 pher\u014d 'carry'; commonly known as comb jellies", "NewcastleGateshead", "1989", "priest", "Denmark, Iceland and Norway", "the need for alliances", "Anderson", "in the 1950s", "Debbie Gibson", "to collect", "at least 18 or 21 years old ( or have a legal guardian present )", "the base 10 logarithm of the molar concentration", "the eighth series of the UK version of The X Factor", "1917", "three", "Montgomery", "Upon closure at birth", "to last four years unless renewed by the Reichstag", "December 1, 2009", "Miami Heat", "Elliot Scheiner", "Alan Shearer", "The 1700 Cascadia earthquake", "the Central and South regions", "Portugal. The Man", "in the blood to the liver", "1960", "two - year terms", "students", "September 19, 2017", "Andy Serkis", "The Abbott and Costello Show", "John Smith", "Idaho", "Holly", "Abraham Gottlob Werner", "Jason Paige", "Jackie Robinson", "Rufus and Chaka Khan", "the body - centered cubic ( BCC ) lattice", "Edward J. Scott", "the merengue", "3", "Olivia Olson", "erosion", "Office of Inspector General", "an integral membrane protein", "Conrad Lewis", "Justice Harlan", "a person employed to write or type what another dictates or to copy what has been written by another,", "Purple Rain", "Oakland, California", "World Famous Gold & Silver Pawn Shop", "Crandon, Wisconsin,", "a racially-tinged remark made by his former caddy,", "Psycho", "diamonds", "Jimmy Carter", "yellow"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6204025689223057}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.2105263157894737, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.4, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.42857142857142855, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9610", "mrqa_squad-validation-4523", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9811", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-3385", "mrqa_triviaqa-validation-1394", "mrqa_hotpotqa-validation-5030", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-3751", "mrqa_triviaqa-validation-262"], "SR": 0.546875, "CSR": 0.6369485294117647, "EFR": 1.0, "Overall": 0.7261397058823529}, {"timecode": 17, "before_eval_results": {"predictions": ["diversity", "\"Provisional Registration\"", "the Tyne and wear Metro", "Highly combustible", "Creon", "QuickBooks", "1892", "The coordinating lead authors", "Six of the seven lines", "They can be combined with ideal pulleys", "\u00d6gedei Khan", "Princes Park", "between the 1950 and 2001 general elections", "Polk", "Mrs. Eastwood & Company", "first train robbery", "Las Vegas", "attorney", "Owsley Stanley", "The visit", "Hong Kong First Division League", "Unbreakable", "rap parts", "\"How to Train Your Dragon\"", "Agra", "1.6 million passengers", "dancer", "Gaius Julius Caesar Augustus Germanicus", "Jeff Van Gundy", "Joachim Trier", "Tamil", "1972", "2013", "Golden Globe Award for Best Actor", "Ronald Lyle \" Ron\" Goldman", "David Allen", "Cleopatra VII Philopator", "political", "Tomorrowland", "16,116", "footballer", "Bishop's Stortford", "Joanna No\u00eblle Levesque", "late eighteenth century", "\"The School Boys\"", "Operation Iceberg", "Texas Longhorn steer", "Hordaland", "1968", "160", "\"Pierement Waltz\" and the \"Amsterdam Polka\"", "October 22, 2012", "Hera Hilmar", "Noah Schnapp", "on about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Neptune", "Lyon, France", "70,000 or so", "Brooklyn, New York,", "They No Man Has Gone Before", "Karan", "oxen", "Aristophanes", "The Killer Angels"], "metric_results": {"EM": 0.5, "QA-F1": 0.6394249307427373}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4444444444444445, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.9811320754716981, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2788", "mrqa_squad-validation-3490", "mrqa_squad-validation-2724", "mrqa_squad-validation-10479", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-4865", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-1308", "mrqa_naturalquestions-validation-7408", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1335", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-3630", "mrqa_searchqa-validation-2489", "mrqa_searchqa-validation-3516"], "SR": 0.5, "CSR": 0.6293402777777778, "EFR": 1.0, "Overall": 0.7246180555555555}, {"timecode": 18, "before_eval_results": {"predictions": ["Lagos and Quiberon Bay", "seven months", "Sava Kosanovi\u0107", "a noble death", "\"Monte Carlo\")", "large compensation pools", "Graz", "5,792", "Lucas\u2013Lehmer", "the Saudi-interpretation", "February 26, 1948", "Hordaland", "the town of El Nacimiento in M\u00fazquiz Municipality", "Stephen Ireland", "Koch Industries", "Washington", "the \"Su\u00f0reyjar\", or \"Southern Isles\"", "2010", "High Falls Brewing Company", "technical director", "Mike Holmgren", "Nathan Bedford Forrest", "Kim Hyun-ah", "green and yellow", "Isobel", "Urijah Faber", "Barack Obama's", "Guthred", "Peel Holdings", "born September 6, 1967", "College Football Scoreboard", "Marko Tapani \" Marco\" Hietala", "In 2017, Pachulia won his first NBA Championship as a member of the Warriors.", "Sarah Hurst", "An invoice, bill or tab", "Johnny Carson", "Clarence Nash", "Golden Valley, Minnesota", "Marco Fu", "Syracuse", "the Durban International Convention Centre (ICC Arena)", "Ryan Babel", "Bob Dylan", "an \"out and back\" wooden roller ride", "Luca Guadagnino", "Jennifer Lynne \" Jennings\" Brown", "11 Grands Prix wins", "National Collegiate Athletic Association", "Bill Cosby", "thirteen", "Argentinian", "Dana Fox", "Sunday, November 2, 2003", "northwest Washington", "who in Dan's semi-autobiographical novel Inside Nate and Eric's literary counterparts were meshed together", "Romanian", "Themes, Motifs & Symbols", "Friday", "Luca di Montezemolo", "the Social Democratic", "The seemingly simple mixture of fruit spread from... the addition of sugar, artificial sweeteners, acid, pectin and thickeners,", "The courts would refuse to even consider them.", "President Obama and Britain's Prince Charles", "Gloria Allred,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5836850649350649}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.5, 0.6666666666666666, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.5, 1.0, 0.7272727272727273, 1.0, 0.6666666666666666, 0.14285714285714285, 0.5714285714285715, 0.4, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-653", "mrqa_squad-validation-9057", "mrqa_squad-validation-8020", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-4757", "mrqa_naturalquestions-validation-6658", "mrqa_triviaqa-validation-4403", "mrqa_triviaqa-validation-3102", "mrqa_newsqa-validation-2163", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-3464", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2844"], "SR": 0.4375, "CSR": 0.6192434210526316, "EFR": 1.0, "Overall": 0.7225986842105263}, {"timecode": 19, "before_eval_results": {"predictions": ["CEPR", "Commission and Council", "14,000", "scoil phr\u00edobh\u00e1ideach", "90 to 95 percent", "R\u00fcdesheim am Rhein and Koblenz", "56.2%", "computational power", "Christ's message and teachings", "Bayern Munich", "fifth", "five times", "A123 Systems, LLC", "\"the backside.\"", "Masaharu Iwata", "the Manhattan Project", "18 December 1975", "3,000", "Leo Richard Howard", "Thom Yorke", "About 200", "Marco Fu", "Orfeo ed Euridice", "\"She of Little Faith\"", "Kristin Scott Thomas, Anne Bancroft, James Fox, Derek Jacobi, and Sean Penn.", "Golden Calf", "Houston Rockets", "Summerlin, Nevada", "Argentinian", "Europe", "Noel", "Savannah River Site", "a family member", "Switzerland", "second largest city", "Frank Lowy", "Fat Man", "Robert Marvin \"Bobby\" Hull, OC", "Nye County, Nevada", "Herman's Hermits", "Marian Delario", "Pendlebury, Lancashire", "300 km north west", "1932", "Arrowhead Stadium", "29 September 1888", "House of Borromeo", "democratic system of \u201crule of the majority\u201d", "Sydney", "Michael Redgrave", "KlingStubbins", "Big 12", "Argand lamp", "2017", "RAF, Fighter Command had achieved a great victory in successfully carrying out Sir Thomas Inskip's 1937 air policy of preventing the Germans from knocking Britain out of the war", "Martina Hingis.", "Matricide", "helicopters and unmanned aerial vehicles", "a one-shot victory in the Bob Hope Classic on the final hole", "\"a system of control\"", "Dune", "fire", "\"teenagers in Versailles.\"", "Jupiter"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6665820868945869}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.15384615384615385, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.07407407407407407, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7001", "mrqa_squad-validation-9093", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-4172", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-32", "mrqa_naturalquestions-validation-954", "mrqa_triviaqa-validation-830", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-2686"], "SR": 0.59375, "CSR": 0.61796875, "EFR": 1.0, "Overall": 0.7223437500000001}, {"timecode": 20, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1582", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2022", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2295", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3571", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3693", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-3881", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-447", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5718", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4155", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-1038", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-402", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-642", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12472", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-1723", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10115", "mrqa_squad-validation-10249", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10328", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1064", "mrqa_squad-validation-1064", "mrqa_squad-validation-1114", "mrqa_squad-validation-1162", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1255", "mrqa_squad-validation-1296", "mrqa_squad-validation-1366", "mrqa_squad-validation-1480", "mrqa_squad-validation-1529", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1597", "mrqa_squad-validation-1723", "mrqa_squad-validation-1748", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-2040", "mrqa_squad-validation-2059", "mrqa_squad-validation-2111", "mrqa_squad-validation-2144", "mrqa_squad-validation-2153", "mrqa_squad-validation-2315", "mrqa_squad-validation-234", "mrqa_squad-validation-2475", "mrqa_squad-validation-2536", "mrqa_squad-validation-2568", "mrqa_squad-validation-2628", "mrqa_squad-validation-2701", "mrqa_squad-validation-2786", "mrqa_squad-validation-287", "mrqa_squad-validation-2898", "mrqa_squad-validation-2976", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3198", "mrqa_squad-validation-3240", "mrqa_squad-validation-3483", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4015", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4259", "mrqa_squad-validation-4263", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-4545", "mrqa_squad-validation-457", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4646", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5200", "mrqa_squad-validation-5276", "mrqa_squad-validation-531", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5542", "mrqa_squad-validation-556", "mrqa_squad-validation-5611", "mrqa_squad-validation-5655", "mrqa_squad-validation-5678", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6263", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-6500", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6523", "mrqa_squad-validation-6538", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6657", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-689", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-7001", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7062", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7152", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7302", "mrqa_squad-validation-7302", "mrqa_squad-validation-7323", "mrqa_squad-validation-7391", "mrqa_squad-validation-7403", "mrqa_squad-validation-7450", "mrqa_squad-validation-7603", "mrqa_squad-validation-7692", "mrqa_squad-validation-7732", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8077", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8219", "mrqa_squad-validation-8225", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8447", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-8612", "mrqa_squad-validation-864", "mrqa_squad-validation-8668", "mrqa_squad-validation-8685", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-876", "mrqa_squad-validation-8836", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9061", "mrqa_squad-validation-9076", "mrqa_squad-validation-9101", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9240", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9652", "mrqa_squad-validation-9716", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9956", "mrqa_squad-validation-9991", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4403", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7642"], "OKR": 0.90234375, "KG": 0.41796875, "before_eval_results": {"predictions": ["Bento de Moura Portugal", "Mongolia", "polynomial-time", "All India Muslim League", "WatchESPN", "1967", "an antigen from a pathogen", "Encoded Archival description (EAD)", "Trey Parker and Matt Stone", "First Family of Competitive eating", "Democratic Unionist Party", "local South Australian and Australian produced content", "\"Naked Killer\"", "7 June 1926 to 17 December 1926", "Olympics were held in Rio de Janeiro, Brazil", "1937", "John Lee Hancock", "Hordaland", "Agent 99", "Bonnie Franklin", "Edmonton, Alberta", "Skyscraper", "Love Actually", "a creek", "Orange County, California", "URO VAMTAC", "The bald eagle", "32", "fifty-word", "Philadelphia Naval Shipyard", "The Books", "Rochdale, North West England", "2013\u201314 Premier League", "Clara Petacci", "Jamie Fraser", "Lionel Brockman Richie Jr.", "\"The Two Noble Kinsmen\"", "Eucritta melanolimnetes", "Jenji Kohan", "Johnson & Johnson", "PewDie mc", "Germanic", "Most of 2017, this is the last Senate election in Minnesota won by a Republican.", "Tamara Ecclestone Rutland", "Thomas Joseph \"T. J.\" Lavin", "Stalybridge Celtic", "Adelaide Lightning", "Kohlberg K Travis Roberts", "\"My Love from the Star\"", "Tottenham", "Sam Bettley", "Ernest Hemingway", "Miss Mulatto, Mani, and Nova", "The Fixx", "The Crossing", "1919", "dynamite", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "head injury", "The Humayun's Tomb", "mammals", "Profit maximization", "electron shells", "1901"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6578125}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.7499999999999999, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1764", "mrqa_squad-validation-525", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-4819", "mrqa_naturalquestions-validation-8584", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-8417"], "SR": 0.578125, "CSR": 0.6160714285714286, "EFR": 1.0, "Overall": 0.7364955357142857}, {"timecode": 21, "before_eval_results": {"predictions": ["an Orthodox priest", "December 12", "Hostmen", "Apollo 8", "Antigone", "a pyrenoid and thylakoids", "Sugarfoot", "\"Crossed: Psychopath\"", "Forbes", "Mitsubishi Motors Corporation", "tabasco", "1 January 1788", "Lowestoft", "Christopher Nolan", "Taylor Swift", "Al Horford", "Johan Leysen", "1854", "American", "New York Giants", "professional footballer", "Logar", "Lauren Alaina", "Ian Fleming", "Mary Bon auto, Susan Murray, and Beth Robinson", "27 November 1956", "a split 7", "Hindi", "United States Auto Club", "The Clash of Triton", "Albany High School", "BAFTA TV Award Best Actor", "\"The Bob Edwards Show\"", "the heaviest album of all", "James Hill", "15,024", "Prime Minister of Denmark 1852\u20131853", "all-time", "Cylon Number Six", "2007", "3,000", "Chinese Coffee", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Mineola", "KWPW", "John Richard Schlesinger", "fourth-ranking", "Blue", "Esperanza Emily Spalding", "the Ruul", "Big & Rich", "1916", "American", "18", "The Golden Gate Bridge", "David Jason", "olea europaea", "Turkey", "\"a total fabrication.\"", "Entourage", "the Vietnamese New Year", "Erica Rivera", "Native Americans", "Madison"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6537833694083695}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.888888888888889, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.8, 0.33333333333333337, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1258", "mrqa_squad-validation-8651", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-5216", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-2367", "mrqa_hotpotqa-validation-1961", "mrqa_newsqa-validation-444", "mrqa_searchqa-validation-15109", "mrqa_naturalquestions-validation-3348"], "SR": 0.53125, "CSR": 0.6122159090909092, "EFR": 0.9333333333333333, "Overall": 0.7223910984848485}, {"timecode": 22, "before_eval_results": {"predictions": ["during the later decades of the 17th century", "WLQP-LP", "Sunni extremist groups", "mid-Eocene", "leftist/communist/nationalist insurgents", "enthusiasm", "the 34th President of the United States", "receive the benefits of the Morrill Acts of 1862 and 1890", "Prussian", "Violet", "German", "Shameless", "Indianola", "the Alien Intelligences", "Nassau County", "What Are Little Boys Made Of", "Andries Jonker", "President John F. Kennedy Jr.", "Mollie Elizabeth King", "1959", "129,007", "San Francisco 49ers", "Big Machine Records", "the Parthian Empire", "almost 3 million people", "Matt Groening", "June 10, 1982", "Philip K. Dick", "John Anthony \"Jack\" White", "Sam Kinison", "Boston", "Lisa", "perjury and obstruction of justice", "Galleria Vittorio Emanuele II", "Paul Avery", "31 October 1783", "Puli Alam", "the Peninsular War", "1838", "Margaret Thatcher", "Plymouth Regional High School", "Manchester Victoria station in air rights space", "east of Ireland", "Estadio de L\u00f3pez Cort\u00e1zar", "Sunday, November 2, 2003", "Jesus", "Agent Carter", "Plies", "Tim \"Ripper\" Owens", "Walt Disney Productions", "chalk quarry", "Rhode Island", "electron donors", "President since creation of the office in 1789", "Sarafina", "Peter Townsend", "the River Thames", "the Sunday Post", "lower house of parliament", "series of abuse", "Stephen Johns reportedly opened the door for the man police say was his killer.", "dugout canoe", "we/wee", "the firebrand Shi'ite cleric"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7504464285714285}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-968", "mrqa_squad-validation-3754", "mrqa_squad-validation-9647", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-1531", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-5288", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-2945", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-6928"], "SR": 0.65625, "CSR": 0.6141304347826086, "EFR": 1.0, "Overall": 0.7361073369565216}, {"timecode": 23, "before_eval_results": {"predictions": ["Jones et al.", "the Moselle", "series of stators (static discs) fixed to the turbine casing", "Pittsburgh Steelers", "14,000", "Scandinavia and northern Europe", "1911", "ESPN's \"SportsCenter\"", "Prince Antoni Radziwi\u0142\u0142", "northern Italy's Lombardy region", "Caesars Palace Grand Prix", "Detroit, Michigan", "Wilton Mall", "the Sun", "Point of Entry", "Wilmette, Illinois", "Malayalam", "Pendlebury, Lancashire", "Leona Lewis", "Laura Dern", "Melbourne", "democracy and personal freedom", "Cool Runnings", "Indian", "Labour", "Thor", "Copa Airlines", "Chiltern Hills", "sex back", "Washington", "Massachusetts", "Edward James Olmos", "a people of mixed Gaelic and Norse ancestry and culture", "Slaughterhouse-Five", "Nashville", "series of 0.500", "Telugu and Tamil", "Peshwa", "Joseph I", "Oracle Corporation", "1999", "William Shakespeare", "January 23, 1898", "1995", "Bergen", "Scribner", "Apprendi v. New Jersey", "Oregon State Beavers", "Jack Elam", "August 17, 1907", "The Design Inference", "R&B vocal group", "in pilgrimages to Jerusalem", "art pottery", "series about restoring someone's faith in love and family relationships", "a filly", "the Spouter Inn", "Christine Keeler", "137", "hundreds of millions of dollars", "Sunday", "Jackie Robinson", "a string of radar stations", "a narrow fellow in the grass"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6956118716809506}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.5, 0.8571428571428571, 0.46153846153846156, 0.0, 0.7368421052631577, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8702", "mrqa_squad-validation-3469", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2129", "mrqa_hotpotqa-validation-994", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-354", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-2866", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-1771", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-1059", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-10472"], "SR": 0.59375, "CSR": 0.61328125, "EFR": 0.9615384615384616, "Overall": 0.7282451923076924}, {"timecode": 24, "before_eval_results": {"predictions": ["Los Angeles", "variously combustion chamber", "13th-century", "spin", "ACL", "Bury Football Club", "Flamingo Hotel & Casino", "Suzuki YZF-R6", "Koninklijke Ahold N.V.", "east", "Gettysburg Address", "Engineering", "Robert \"Bobby\" Germaine, Sr.", "American 3D computer-animated comedy", "the Asia-Pacific War", "Amy Poehler", "manager", "British Labour Party", "USC Marshall School of Business", "Theme Hospital", "1936", "Martin Scorsese", "Maxwell Smart", "The Walking Dead", "2008", "Yasir Hussain", "Let's Make Sure We Kiss Goodbye", "Ronald Ryan", "Elena Verdugo", "soccer", "Peel Holdings", "Chechen Republic", "alcoholic drinks", "Zaire", "Debbie Harry", "Barbara Lee Alexander", "novelist and poet", "1986", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Hilary Duff", "100 million", "political party", "2015 Masters Tournament", "John Schlesinger", "Venice", "Rockstar Games", "A.S. Roma", "Genderqueer", "St Vincents Hall", "Melbourne's City Centre", "South West Peninsula League", "Bury St Edmunds", "1608", "metamorphic rock", "Rugrats in Paris", "auk", "zeny juice", "Rose-Marie", "John McCain", "Colorado Attorney General John Suthers", "Spaniard Carlos Moya", "pink", "calcium", "Si-Tchuan"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6756572420634921}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.8571428571428571, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.3333333333333333, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.5, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3202", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-1741", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-948", "mrqa_hotpotqa-validation-2199", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-3934", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-9626", "mrqa_triviaqa-validation-6356", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-1361", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-14782"], "SR": 0.5625, "CSR": 0.6112500000000001, "EFR": 1.0, "Overall": 0.73553125}, {"timecode": 25, "before_eval_results": {"predictions": ["August 15, 1971", "Geordie", "Peter Howell", "many elements of the old language", "formalize a unified front in trade and negotiations with various Indians", "French", "New York City", "2017", "Logan International Airport", "Pain Language", "Dziga Vertov", "no. 3", "John \"John\" Alexander Florence", "Two Is Better Than One", "July 16, 1971", "Microsoft Office", "Baldwin, Nassau County, New York, United States", "Elton John", "Firestorm", "the Ruul", "March 14, 2000", "David Wells", "Northern Lights", "Chengdu Aircraft Corporation (CAC) of China", "Michael Cremo", "Minnesota", "Oklahoma", "Jim Diamond", "Smithfield, Rhode Island", "Julie Taymor", "29 September\u20132 October 2011", "Columbia Records", "1943", "Maria Brink", "Wild Mountain Thyme", "Cody Miller", "Darkroom", "SOS", "Christopher Nolan", "Undone \u2013 The Sweater Song", "1992", "2016 United States elections", "bushwhackers", "Princes Park", "The Late Late Show", "2012", "1978", "Donald Carl \"Don\" Swayze", "John Morgan", "June", "an organ", "Macau", "49 cents", "Certificate of Release or Discharge from Active Duty", "Jocelyn Flores", "Hyperbole", "Egypt", "a tiger", "1620", "November 1", "A growing percentage of the Somali population has become dependent on humanitarian aid", "Thomas Nast", "ice hockey", "Betty la fea"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7070999313186813}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.46153846153846156, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-3386", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2853", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-1718", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-2092", "mrqa_triviaqa-validation-3925", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-540", "mrqa_searchqa-validation-172"], "SR": 0.640625, "CSR": 0.6123798076923077, "EFR": 0.9565217391304348, "Overall": 0.7270615593645485}, {"timecode": 26, "before_eval_results": {"predictions": ["Italian physicist", "illiberal Islamic regimes", "The Judicial Council", "CBS", "cradle song", "Blue Jean", "Ibex", "Hebridean isle", "prostate", "Vitus Bering", "fuel", "Burundi and Rwanda", "larval", "an unguis", "The Magic Mountain", "the bassoon", "Canada", "Komodo", "fibroids", "15", "Ice Cream", "Gulliver", "Last Summer", "radio waves", "Big brown eyes", "Isis", "Eliza Doolittle", "fibromyalgia", "En banc", "Franklin D. Roosevelt", "six ounces", "Violeta Chamorro", "Top Gun", "Rafael Nadal", "Canberra", "Ich bin ein Berliner", "Caesar's wife", "Good fiction", "Neverbeen Kissed", "Platoon", "Thecher", "The Banana Boat", "Nanjing", "asparagus beetles", "auf Wiedersehen", "blubber", "catalysts", "Stanford-Binet", "Ferdinand von Zeppelin", "Deep Purple", "Jesus", "Grandma Klump", "solve its problem of lack of food self - sufficiency", "Speaker of the House of Representatives", "the Bulgarian 2nd Army", "20", "Time Bandits", "softbill", "Marissa Jaret Winokur", "35", "three centuries", "Consumer Product Safety Commission", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Israel Defense Forces"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49458786231884055}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.08695652173913043, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15646", "mrqa_searchqa-validation-10928", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-2775", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-10174", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-2526", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-4299", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-6143", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-4190", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-7196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-2819", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1748"], "SR": 0.40625, "CSR": 0.6047453703703703, "EFR": 1.0, "Overall": 0.7342303240740741}, {"timecode": 27, "before_eval_results": {"predictions": ["May 18, 1756", "canceled", "modern fashion", "more than half of the global wealth", "Lismore", "STS-51-L", "Paradise", "English", "Newcastle upon Tyne, England", "Colonel", "Cody Miller", "Kentucky", "Hertz Corporation", "Charles Otto Puth Jr.", "Disney California Adventure", "Maria Brink", "The Trapp Family", "G\u00f6tene in Sweden", "Argentine", "novelty songs, comedy, and strange or unusual recordings", "monthly", "6teen", "South America", "Princes Park", "Glenn Close", "the Knight Company", "My Gorgeous Life", "Ashanti", "Republic of Indonesia", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "Northampton, England", "Black Panthers", "Fred Willard", "beer", "Dara Grace Torres", "nine", "1909", "about 5320 km", "3,384,569", "an anvil", "House of Hohenstaufen", "James G. Kiernan", "Johnnie Ray", "Forrest Gump", "2009", "1919", "a skerry", "1620 to 1691", "Charles Edward Stuart", "Mickey Gilley", "Blue Origin", "December 31, 2015", "2017", "Eleanor Roosevelt", "Utah, Arizona, Wyoming, and Oroville, California", "Google", "the lunar phases phase", "the Kinks", "throwing three punches but said only one connected.", "sought Cain's help finding a job", "1975", "Sperm whale", "libraries", "NASA"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7248421717171717}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.8, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7559", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-5471", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-1233", "mrqa_hotpotqa-validation-296", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4650", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-2829", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2586"], "SR": 0.609375, "CSR": 0.6049107142857143, "EFR": 1.0, "Overall": 0.7342633928571429}, {"timecode": 28, "before_eval_results": {"predictions": ["British patrons", "Laverne & Shirley", "South", "Roger Bacon", "apogee", "the fourth rated institution in Pennsylvania", "Pitney Bowes", "The Office", "a proverbs 13:24", "Sofia Loren", "Qubec City", "Edith Piaf", "the Krntnertortheater", "Sappho", "a Fruit Roll-Ups", "Colorado River", "Hershey's", "Timothy Leary", "3800", "Thought Police", "John Grisham", "apogee", "Alice Addertongue", "Doctor", "The Sprint Triathlon", "calcium", "Mikel Arteta", "Wisconsin", "Sandro Botticelli", "To Build a Fire", "auk", "Docu Drama", "transept", "centigrade", "silver", "BBC", "the Jackass penguin", "Clive Staples Lewis", "Blackwater USA", "apogee", "N. Hilton", "December", "Agariste", "Hadrosaurus", "e-T", "Contra", "the C&D Canal", "asthma", "a hongshao sauce", "a trumpet", "Narcissus", "Marion", "in liquids", "the population", "Sarah Silverman", "Anwar Sadat", "armada", "Barings", "Esp\u00edrito Santo Financial Group", "Earvin \"Magic\" Johnson Jr.", "John DeMita", "free laundry service", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.\"", "the shoreline of the city of Quebradillas"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48916666666666664}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.23999999999999996, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-8603", "mrqa_searchqa-validation-4398", "mrqa_searchqa-validation-4388", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-6355", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-11293", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-841", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-6787", "mrqa_searchqa-validation-7159", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-4972", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3961"], "SR": 0.421875, "CSR": 0.5985991379310345, "EFR": 0.972972972972973, "Overall": 0.7275956721808015}, {"timecode": 29, "before_eval_results": {"predictions": ["X Games", "11 million", "GTE", "High school", "John Lee Hancock", "2007", "Westfield Tea Tree Plaza", "Philadelphia", "237", "Gal Gadot", "1860", "Eddie Izzard", "1966 US tour", "Miracle", "James Ellison", "studied Arabic grammar", "Humberside Airport", "The Big Bang Theory", "2012", "Kind Hearts and Coronets", "Centennial Olympic Stadium", "Leatherheads", "Polka", "Sinngedichte", "James G. Kiernan", "Australia", "heavy metal band", "Anah\u00ed", "private", "Tampa Bay Lightning", "tabasco", "Patricia Arquette", "\"Secrets and Lies\"", "William Shand Kydd", "Coca-Cola", "Square Enix", "Geraldine Page", "pornographicstar", "Europe", "179", "three", "Sam the Sham", "pinball", "High Falls Brewery", "Las Vegas Strip in Paradise, Nevada", "PPG Paints Arena", "George I", "J35", "Chief Minister of Tamil Nadu", "Romance", "Bohemia", "Macomb County", "birth", "biochemistry", "provides the public with financial information about a nonprofit organization", "Elgar", "Stockholm syndrome", "magnetism", "Morgan Tsvangirai.", "Empire of the Sun", "Amanda Knox's aunt", "Robert Bruce", "Donna Reed", "gulls"], "metric_results": {"EM": 0.59375, "QA-F1": 0.671875}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5637", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-2004", "mrqa_naturalquestions-validation-9088", "mrqa_triviaqa-validation-5397", "mrqa_triviaqa-validation-3393", "mrqa_newsqa-validation-3391", "mrqa_searchqa-validation-5455", "mrqa_searchqa-validation-9860"], "SR": 0.59375, "CSR": 0.5984375, "EFR": 1.0, "Overall": 0.7329687500000001}, {"timecode": 30, "UKR": 0.685546875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1180", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1342", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1488", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-172", "mrqa_hotpotqa-validation-1726", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2199", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2825", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-3071", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3191", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3402", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4285", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4467", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5247", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-5471", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5561", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5713", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-704", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-98", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1199", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1503", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11434", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-13831", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-2489", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-2686", "mrqa_searchqa-validation-3516", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6045", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-8245", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9601", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10223", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10424", "mrqa_squad-validation-10449", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1064", "mrqa_squad-validation-1096", "mrqa_squad-validation-1114", "mrqa_squad-validation-1177", "mrqa_squad-validation-1232", "mrqa_squad-validation-1255", "mrqa_squad-validation-1258", "mrqa_squad-validation-1296", "mrqa_squad-validation-1480", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1723", "mrqa_squad-validation-1940", "mrqa_squad-validation-1976", "mrqa_squad-validation-2153", "mrqa_squad-validation-2272", "mrqa_squad-validation-2301", "mrqa_squad-validation-234", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2591", "mrqa_squad-validation-287", "mrqa_squad-validation-2976", "mrqa_squad-validation-306", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3296", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3716", "mrqa_squad-validation-3767", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4026", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4259", "mrqa_squad-validation-4286", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4545", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4810", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5276", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5532", "mrqa_squad-validation-556", "mrqa_squad-validation-5578", "mrqa_squad-validation-5611", "mrqa_squad-validation-5779", "mrqa_squad-validation-5839", "mrqa_squad-validation-5881", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6443", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6888", "mrqa_squad-validation-6967", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7125", "mrqa_squad-validation-7152", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7323", "mrqa_squad-validation-7403", "mrqa_squad-validation-7458", "mrqa_squad-validation-7603", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-8219", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-864", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8839", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9061", "mrqa_squad-validation-9191", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9521", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9647", "mrqa_squad-validation-9652", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2654", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-830"], "OKR": 0.841796875, "KG": 0.44453125, "before_eval_results": {"predictions": ["any member of the Scottish Government", "quadratic", "1879", "Xiu Li Dai", "the North Cascades range of, Washington", "northwest Washington", "1924", "The tournament was hosted by England from 18 September to 31 October", "Bud Light", "the status line", "Virginia farmers continued to import during the French and Indian War", "December 2, 2013", "the intermembrane space", "Chinese cooking for over 400 years, most often as bird's nest soup", "Philadelphia", "The United States Secretary of State", "electrical activity produced by skeletal muscles", "thick skin", "a Islamic shrine", "Sylvester Stallone", "Anakin Wars", "September 27, 2017", "convert single - stranded genomic RNA into double - stranded cDNA", "the economy", "Victory gardens", "Paul Hogan", "961", "northern China", "gathering money from the public", "a beach in Malibu, California", "Humpty Alexander Dumpty", "indie pop band Foster the People", "a part of the continent of North America", "Sun Tzu", "18th century in the United Kingdom", "Setsuko Thurlow", "the temporal lobes", "the fictional town of Ramelle", "DNA and other molecules that mediate the function of the genome", "mining", "Keith Thodeaux", "six - hoop game", "Atlantic", "butane", "Julia Roberts", "12 November 2010", "Brazil, China, France, Germany, India, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey, the European Union", "Aaron Harrison", "Panning", "CBS Television City", "Johnny Depp", "James Chadwick", "the Swirral Edge ridge", "eutrophication", "Worcester Cathedral", "Germany", "Rachel, Nevada", "Atlanta", "more than 30 Latin American and Caribbean nations", "two women killed in a stampede at one of his events in Angola on Saturday,", "dogs who walk on ice in Alaska.", "a wave pump", "acker Rudolph Valentino", "Hannibal of Carthage made an alliance with the Gallic Cenomani"], "metric_results": {"EM": 0.359375, "QA-F1": 0.532091632527004}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5263157894736842, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0909090909090909, 1.0, 0.6666666666666665, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.4864864864864865, 0.0, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.6666666666666666, 1.0, 0.0, 0.923076923076923, 1.0, 0.4166666666666667, 0.0, 1.0, 0.7272727272727273, 0.0, 1.0, 0.5, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.25, 1.0, 0.6666666666666666, 0.5454545454545454, 0.13333333333333333, 0.0, 0.0, 0.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_squad-validation-1818", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-6224", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-72", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-2498", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-6465", "mrqa_triviaqa-validation-782", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2873", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-414", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-4098"], "SR": 0.359375, "CSR": 0.5907258064516129, "EFR": 0.975609756097561, "Overall": 0.7076421125098348}, {"timecode": 31, "before_eval_results": {"predictions": ["lighter and seem to lose something in the process", "fertilized eggs", "September 19 - 22, 2017", "social commentary", "John Roberts", "Earth", "between the Eastern Ghats and the Bay of Bengal", "bow bridge with 16 arches shielded by ice guards", "12 to 36 months old", "Tom Brady", "Chelsea", "Darlene Cates", "fascia surrounding skeletal muscle", "Jerry Leiber and Mike Stoller", "a Norwegian town", "Lisa Stelly", "Heroes and Villains", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms and is thus placed in a `` take it or", "Robin", "Jack Barry", "Missouri River", "Randy", "August 23, 1945", "19 June 2018", "Daniel A. Dailey", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "the King James Bible", "many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "International Baccalaureate", "October 28, 2007", "Jaydev Shah", "Mesopotamia", "Action Jackson", "Five years later", "a state or other organizational body that controls the factors of production", "90 \u00b0 N 0 \u00b0 W", "in Ephesus in AD 95 -- 110", "1984", "vehicles solely of sport utility vehicles and off - road vehicles", "Americans", "1997 squad voted atop the final AP Poll", "30 years after Return of the Jedi", "John Joseph Patrick Ryan", "an American musical group founded by Marcus Bowens and Jermaine Fuller", "Rashidun Caliphs", "off the rez", "Woody Paige", "Ren\u00e9 Descartes", "2007 via Valve's Steam content distribution platform", "Asuka", "A Turtle's Tale : Sammy's Adventures and the TV show Suburgatory", "three", "Steve Biko", "Rudolph", "Go", "Anne Fletcher", "October 21, 2016", "Arizona Health Care Cost Containment System", "21,", "Robert Barnett", "Oaxaca, Mexico", "Algeria", "egg in a bottle", "pipa"], "metric_results": {"EM": 0.53125, "QA-F1": 0.672112200285558}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false], "QA-F1": [0.8421052631578948, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.3636363636363636, 0.5714285714285715, 0.0, 0.5, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.961038961038961, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.375, 0.9859154929577464, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6, 0.6666666666666666, 1.0, 0.4615384615384615, 0.4, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3494", "mrqa_squad-validation-4566", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-714", "mrqa_hotpotqa-validation-1803", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-3785", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-4046"], "SR": 0.53125, "CSR": 0.5888671875, "EFR": 0.9, "Overall": 0.6921484375}, {"timecode": 32, "before_eval_results": {"predictions": ["September 1565", "Anglo-Saxon populations who migrated to and conquered much of England after the end of Roman Imperial rule", "Pin the Tail on the Donkey", "the martini", "Wiener Sangerknaben", "cinnamon", "a big bang", "school", "R.E.M.", "Gale Sayers", "French Presidential Power and the Stability of the French Fifth Republic", "Azerbaijan", "Abraham Lincoln", "the Yangtze River", "LANGUAGE LAB", "Ngan Le", "Sharon Epatha Merkerson", "air pressure", "Harold Macmillan", "dinosaurs", "Sebastian Stark", "the Deaf President Now protest", "school", "school", "the orangutan", "anaphylactic shock", "camels", "schoolrene", "school", "Clyde Chestnut Barrow", "John Harvard", "Indo-European language", "school series The Partridge Family", "`` Dorothy Gale", "Guatemala", "school", "Barack Obama on the South Side during his first campaign, for the State Senate", "BOLIVAR", "Albert Einstein", "school", "school at Barnsdall Art Park", "Louisa May Alcott", "tullip", "school will always welcome lovers As time goes by", "Providence", "Tasmanian devil John Quincy", "mother Vineyard", "South Africa", "school famous Bolshoi Ballet and Opera theatre", "carbon dioxide", "school", "tooth", "Gibraltar, a British Overseas Territory, located at the southern tip of the Iberian Peninsula", "1999", "9 February 2018", "Argentina", "Sarah Sawyer", "CGNU plc", "Russian Empire", "1967", "44,300", "228", "July", "``Buying a Prius shows the world that you love the environment and hate using fuel.\""], "metric_results": {"EM": 0.390625, "QA-F1": 0.4727430555555555}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3088", "mrqa_squad-validation-5276", "mrqa_searchqa-validation-10043", "mrqa_searchqa-validation-5846", "mrqa_searchqa-validation-5405", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-16138", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-11993", "mrqa_searchqa-validation-5348", "mrqa_searchqa-validation-12676", "mrqa_searchqa-validation-15919", "mrqa_searchqa-validation-1108", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-3957", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-9970", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-7460", "mrqa_searchqa-validation-4645", "mrqa_searchqa-validation-8650", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-2559", "mrqa_searchqa-validation-10513", "mrqa_searchqa-validation-3246", "mrqa_searchqa-validation-391", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-11899", "mrqa_searchqa-validation-554", "mrqa_naturalquestions-validation-3959", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-5763", "mrqa_newsqa-validation-2395"], "SR": 0.390625, "CSR": 0.5828598484848485, "EFR": 1.0, "Overall": 0.7109469696969697}, {"timecode": 33, "before_eval_results": {"predictions": ["11", "R\u00fcdesheim", "Justin Bieber", "Tennessee Williams", "Ring", "a oats", "John Henry", "Zombies", "Colombia", "belle", "Friday Night Lights", "Halloween", "the Emperor", "a port-wine stain", "the Empire State Building", "Pinta", "Czechoslovakia", "Ferris B Mueller", "Mike Judge", "Unforgiven", "Court TV", "the Galaxy", "Germany", "Gunsmoke", "a lunar scientist", "Frankie Valli", "AT&T", "asthma", "Microsoft", "the blue agave", "Puerto Rico", "24 hours", "flying saucer", "Shakespeare", "a liter", "Edward", "The Silence of the Lambs", "belushi Beldar Conehead", "stuffing", "a fraction", "carbonite", "Spain", "the phi", "an obelisk", "Sam Kinison", "Katharine Hepburn", "(Kansas City, Missouri)", "Kublai Khan", "the Abkhazia", "(DC)", "the bow", "Newfoundland", "538", "a destructive ex-lover", "Peter Hansen", "a googol", "Laos", "Wigan", "1995", "Champion Jockey", "Boeing 757", "the Bush administration", "200", "around 8 p.m. local time"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5793087121212122}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.9090909090909091]}}, "before_error_ids": ["mrqa_squad-validation-9098", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-15910", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-6554", "mrqa_searchqa-validation-3111", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-9672", "mrqa_searchqa-validation-2404", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-227", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11808", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-5234", "mrqa_naturalquestions-validation-6326", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-3445", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-591"], "SR": 0.53125, "CSR": 0.5813419117647058, "EFR": 1.0, "Overall": 0.7106433823529411}, {"timecode": 34, "before_eval_results": {"predictions": ["Hisao Yamada", "Percy Shelley", "3D computer-animated comedy", "aluminum foil", "Montreal, Quebec, Canada", "Lego", "Daniil Borisovich Shafran", "Doc Hollywood", "Richard L. Thompson", "Virgin", "John Christopher Lujack Jr.", "26 June 2013", "Sleepy Brown", "Michael Swango", "Roman Polanski", "322,520", "1754", "Cate Blanchett", "Westfield Marion", "Montana State University", "1961", "Eisenhower Executive Office Building", "137th", "Mohsin Fani", "High Falls Brewery", "2016", "1998", "Sean", "2015", "Mel Blanc", "Corendon Airlines", "Tamil", "number five", "Champion Jockey", "University of Columbia", "Jennifer Aniston", "Larry Eustachy", "Anne Perry", "March 17, 2015", "Julie Taymor", "Mika H\u00e4kkinen", "nine", "a bass", "Buck Owens and the Buckaroos", "January 1788", "Lord Chancellor of England", "MGM Resorts International", "Cleveland, Ohio", "The song also features rap parts from Darryl, RB Djan and Ryan Babel", "Mark Anthony \"Baz\" Luhrmann", "Syracuse University", "\"personal earnings\" (such as salary and wages), \"business income\"", "1969", "georgia", "the referee", "Botticelli", "the Circus World Museum", "Keats", "Dr. Maria Siemionow, the head of plastic surgery research at the Cleveland, Ohio, hospital", "it has not", "Arsene Wenger", "atoms", "Bellerophon", "Monica Lewinsky"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7956845238095238}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-5530", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-421", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-517", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-4302", "mrqa_triviaqa-validation-2977", "mrqa_newsqa-validation-1678", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-7800"], "SR": 0.6875, "CSR": 0.584375, "EFR": 1.0, "Overall": 0.7112499999999999}, {"timecode": 35, "before_eval_results": {"predictions": ["June 4, 2014", "highly diversified", "Walter Pauk", "2018", "a noble gas", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "Peter Hansen", "Ishaan Anirudh Sinha", "the Charlotte Hornets of the National Basketball Association ( NBA )", "one person, whose decisions are subject to neither external legal restraints nor regularized mechanisms of popular control", "Orographic lift", "1945", "6 March 1983", "to cool all the atmosphere by spraying the whole atmosphere as if drawing letters in the air", "Around 1200", "as the second single", "1857", "at the Mount Mannen in Norway", "1937", "from the Kennedy Space Center ( KSC ) in Florida", "Rocky Dzidzornu -- congas", "from 35 to 40 hours per week", "Abbot Suger", "since the early 20th century", "Authority", "a yellow background instead of a white one", "Magnetically soft ( low coercivity ) iron", "southern Anatolia", "1992", "prolonged diarrhea", "1986", "Turducken", "the brain and spinal cord", "abdicated in November 1918", "Massachusetts", "Hans Zimmer, Steve Mazzaro & Missi Hale", "c. 1000 AD", "from Times Square in New York City west to Lincoln Park in San Francisco", "Jerry Leiber and Mike Stoller", "111", "49 cents", "1969", "Central Germany", "1978", "Yuzuru Hanyu", "by observing the magnetic stripe `` anomalies '' on the ocean floor", "speech", "China ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "peninsular mainland jutting out into the Mediterranean Sea", "Santo Domingo", "during season two", "28, 1973", "July 1st", "Frederick William III of Prussia", "Majorca", "National Football League", "Arlo Looking Cloud", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Pervez Musharraf", "spend $60 billion on America's infrastructure.", "Wigan Athletic in northern England", "Ireland", "asteroids", "Colorado"], "metric_results": {"EM": 0.40625, "QA-F1": 0.6142726106130711}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.21052631578947367, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.888888888888889, 0.5, 0.923076923076923, 0.8, 0.923076923076923, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4210526315789474, 1.0, 0.5, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.8, 0.25, 0.0, 0.8, 0.0, 0.0, 0.22222222222222224, 0.0, 0.5, 1.0, 1.0, 0.0, 0.2222222222222222, 0.5714285714285715, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-4646", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3469", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-9165", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-1985", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-6276", "mrqa_hotpotqa-validation-3613", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-9"], "SR": 0.40625, "CSR": 0.5794270833333333, "EFR": 1.0, "Overall": 0.7102604166666666}, {"timecode": 36, "before_eval_results": {"predictions": ["crust and lithosphere", "Nurbanu Sultan", "Skatoony", "number 1", "Satchmo", "San Antonio", "Polish", "Danish", "Milwaukee Bucks", "1908", "glee", "1965", "over 100 million", "Oneida Limited", "Wilmington, North Carolina, United States", "Pieter van Musschenbroek", "Southbank", "London", "Australian", "Rochdale, North West England", "Bardot", "Mario Lemieux", "\"Neptune's Party\"", "The Sun", "2000", "Ferdinand Magellan", "King of France", "1694", "Bobby Sands", "Nanna Popham Britton", "Minette Walters", "leopard", "Moselle", "Anne and Georges", "Bank of China Tower", "American playwright and Nobel laureate in Literature", "Cheshire", "Bob Gibson", "1770", "1974", "Northern Transcon", "Woody Woodpecker", "2", "Eddie Albert", "IFFHS World's Best Goalkeeper", "three members", "1989 until 1994", "Pittsburgh Steelers", "9 venues", "1993 to 2001", "Double Crossed", "economic recession", "needle - like teeth commonly feed on small to medium - sized fish", "Bacon", "human rights lawyer", "constant", "1812", "constant solar powered boat", "Casalesi Camorra clan", "Kingdom City", "Queen Wilhelmina", "greece", "New Mexico", "1992"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6903731684981685}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true], "QA-F1": [0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.28571428571428575, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 0.5, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4952", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-3342", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3417", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-2319", "mrqa_naturalquestions-validation-5119", "mrqa_triviaqa-validation-994", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-743", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-14153"], "SR": 0.578125, "CSR": 0.5793918918918919, "EFR": 0.9629629629629629, "Overall": 0.7028459709709709}, {"timecode": 37, "before_eval_results": {"predictions": ["lasting damage", "Brett Favre", "Geraldine A. Ferraro", "Wool Sack dress", "Billy the Kid", "Oliver Twist", "Hans Christian Andersen", "topaz", "hydrothermal", "Cameroon Pidgin", "x", "Destiny's Child", "x", "California", "Danny Ocean", "Valkyries", "Little Women", "Ich bin ein Berliner", "x", "World War II", "difference", "Gogol", "Malcolm X", "Colorado columbine", "vu", "Michigan", "Sigmund Freud", "red", "T. S. Eliot", "Dumpling", "Alexander Hamilton", "New Zealand", "rum", "theology", "x", "Stephen Decatur", "Castor & Pollux", "Paraguay", "R2-D2", "one magnum", "tense", "Vassar College", "forensic", "National Air and Space Museum", "Vespa", "Warren G. Harding", "Emma Peel", "Tennessee", "Richard I of England", "Will Rogers", "Bee Gees", "Honor\u00e9 Mirabeau", "in 2018", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "UNison", "the first web page", "Scotland", "1980", "1887", "Bonkyll Castle", "more and more suspicious of the way their business books were being handled.", "Top Gun", "Dame Elizabeth,", "Chester Leland Brewer"], "metric_results": {"EM": 0.546875, "QA-F1": 0.645734126984127}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16031", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-7058", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-7080", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-8720", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-13049", "mrqa_searchqa-validation-8295", "mrqa_searchqa-validation-10821", "mrqa_searchqa-validation-2859", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-8890", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-1283", "mrqa_hotpotqa-validation-1884", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-2552", "mrqa_hotpotqa-validation-4203"], "SR": 0.546875, "CSR": 0.5785361842105263, "EFR": 0.9655172413793104, "Overall": 0.7031856851179673}, {"timecode": 38, "before_eval_results": {"predictions": ["the Butcher Market", "the Grito de Dolores", "grommet", "Soundgarden", "a pew", "Russia", "the Penguin", "digitalis glycosides", "Canada", "the stave", "pole vault", "California", "Jordan", "an inked image", "the Battle of Waterloo", "Ukraine", "goombah", "Nuku'alofa", "$1.5 billion", "Exxon Corporation", "Juana", "John Tyler", "an anagram", "La-Z-Boy", "a fire surge", "a phylum", "Narnia", "East Germany", "Linda Keene", "Judges 5", "Vlad Tepes", "Marlee Matlin", "a female bullfrog", "Iraq", "debts", "Lady Jane Grey", "yellow fever", "Days Inn", "Mexico", "Harold Edward \"Red\" Grange", "Peter", "printing", "couscous", "1917", "Colonel (Tom) Parker", "the lilac", "American Pie", "a diamond", "a bowhead whale", "Ohio State", "Sweet Home", "George Strait", "Toledo", "1960", "the Nutcracker", "Dodoma", "the Red Lion", "martial arts action films", "Black Swan", "the Sun", "girls around 11 or 12.", "Vernon Forrest", "President Barack Obama,", "Idaho Falls"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6458333333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-6090", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-14171", "mrqa_searchqa-validation-12204", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-10160", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-4408", "mrqa_searchqa-validation-3640", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7368", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-6757", "mrqa_searchqa-validation-3197", "mrqa_triviaqa-validation-6420", "mrqa_hotpotqa-validation-1192", "mrqa_triviaqa-validation-700"], "SR": 0.578125, "CSR": 0.578525641025641, "EFR": 1.0, "Overall": 0.7100801282051281}, {"timecode": 39, "before_eval_results": {"predictions": ["substantially increased the asking price", "preserved corpses having sex", "\"To My Mother\"", "Lucky Dube", "grocery store", "fallen comrades lost in the heat of battle.", "Samuel Herr", "1918", "participate in Iraq's government.", "body", "Honduran", "Los Angeles", "FBI", "201-262-2800", "stop selling unapproved pain-relief drugs.", "directly involved in an Internet broadband deal with a Chinese firm", "Iraq", "40 militants and six Pakistan soldiers", "1973", "Argentina", "Washington", "burns", "Laura Ling and Euna Lee", "January 24, 2006", "gang rape", "Haleigh Cummings", "Casalesi Camorra", "nine", "a group of 20 similar cars making an annual road trip", "11", "two", "anxious.", "Kurt Cobain", "genocide", "Hartsfield-Jackson International Airport", "barter -- trading goods and services without exchanging money", "\"Dance Your Ass Off.\"", "1994", "Larry Zeiger", "the United States", "Illness", "reached under the counter, grabbed his gun and told the robber to drop the bat and get down on his knees.", "(Charles Reisner) Stunt", "African National Congress", "abuse", "The man ran out of bullets and blew himself up.", "two", "current and historic conflict zones", "14", "11", "British", "Clarence Darrow", "in many plants", "in the attempt to discover first principles", "1768", "chariot", "George Orwell", "\" rated R\"", "1955", "I'm Shipping Up to Boston", "a lock", "Anna Mathilda McNeill", "(St.) George", "electronic junk mail"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5640972222222222}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-1259", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-4211", "mrqa_hotpotqa-validation-1585", "mrqa_searchqa-validation-16932", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-11411", "mrqa_triviaqa-validation-90"], "SR": 0.515625, "CSR": 0.576953125, "EFR": 1.0, "Overall": 0.709765625}, {"timecode": 40, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1348", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2749", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2860", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3171", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-543", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4110", "mrqa_naturalquestions-validation-4211", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6680", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10534", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-2423", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-2883", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7583", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-8890", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-9163", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9967", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-138", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3341", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3787", "mrqa_squad-validation-3813", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4107", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5222", "mrqa_squad-validation-5237", "mrqa_squad-validation-525", "mrqa_squad-validation-5269", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-531", "mrqa_squad-validation-5319", "mrqa_squad-validation-5483", "mrqa_squad-validation-5501", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5741", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-689", "mrqa_squad-validation-7001", "mrqa_squad-validation-7002", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-741", "mrqa_squad-validation-7458", "mrqa_squad-validation-7470", "mrqa_squad-validation-7525", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7736", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9744", "mrqa_squad-validation-9845", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9969", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830"], "OKR": 0.822265625, "KG": 0.4703125, "before_eval_results": {"predictions": ["deflate", "Gov. Jan Brewer.", "Casey Anthony,", "Tehran,", "Britain's", "asked not to be identified as they are not authorized to speak on the information.", "\"I'm extremely gratified at the court's decision. I believe it is legally and factually correct.", "Sheikha Lubna Al Qasimi", "regulators in the agency's Colorado office", "\"Oprah: A Biography,\"", "Vivek Wadhwa,", "\"stand tall, stand firm.\"", "Former Mobile County Circuit Judge Herman Thomas", "eight-day", "a long-range missile on its launch pad,", "he took his talent out of the bedroom, playing with high-school band The Iguanas,", "committed to equality,", "Harry Nicolaides, 41,", "in central Cairo,", "computer security expert Tadayoshi Kohno of the University of Washington.", "opium poppies", "animal products.", "New Haven, Connecticut, firefighter", "at the University of Alabama in Huntsville,", "Miami Beach, Florida,", "secretary of defense", "\"The Real Housewives of Atlanta\"", "dancing", "September,", "Arnold and Klein", "Waterloo Bridge", "six nice golf courses.", "ties, even designer ones,", "a missile", "Nicole", "found unconscious with multiple gunshot wounds at Boston's Copley Marriott Hotel on April 14.", "Sgt. Barbara Jones of the Orlando Police Department.", "filed papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "a man wearing a baseball cap, dark jersey, blue jeans and running shoes entering a store, walking to the back and looking around, then walking out.", "14", "the local political representative", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan:", "delivers a big speech", "Ennis, County Clare", "$17,000", "Prime Minister Fredrik Reinfeldt", "on Anjuna beach in Goa", "The patient,", "South African", "died after shooting himself three times in the head", "a rally", "J. Presper Eckert and John William Mauchly", "Latitude", "long sustained period of inflation", "Saint Cecilia", "One Thousand and One", "George IV", "2011", "Violet", "Kinnairdy Castle", "the hippopotamus", "the boll weevil", "Ivan Denisovich", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5517336071656924}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.2857142857142857, 0.1, 0.25, 0.8, 0.8, 0.08, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6, 0.5, 0.4, 0.4, 0.5, 1.0, 0.058823529411764705, 0.6, 0.9375, 0.0, 1.0, 0.0, 0.17391304347826086, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.5, 0.625, 0.4, 0.2, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-577", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-2729", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-4059", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-10138", "mrqa_searchqa-validation-16464"], "SR": 0.40625, "CSR": 0.5727896341463414, "EFR": 0.9473684210526315, "Overall": 0.7047347360397945}, {"timecode": 41, "before_eval_results": {"predictions": ["Jim Nantz and Phil Simms", "Texas", "Kansas City", "birds", "James Bond", "Taps", "Seal", "Dr. Strangelove", "the longhaired cat", "Atlanta", "\"Ridi Pagliaccio\"", "Ron Shelton", "Coors Field", "Boise State", "Doc Holliday", "Chicken Run", "Hercules", "Stars", "hydrogen", "Svengali", "Magda Gabor", "the Mammoth Cave", "a sousaphone", "S-waves", "Poseidon", "Queen Elizabeth II", "The 39 Steps", "Cynic", "Judges", "Oreo", "St. Lawrence", "the seashore", "Dr. Irina Spalko", "America", "Bill Clinton", "\"Cloverfield,\"", "Paraguay", "Rassendyl", "the Gulf of Tonkin", "to rest or relax, or to rest on something for support", "Resentment", "# Quiz # Question", "a zipper", "Tuesday", "John Farrar", "Robert Cohn", "oil", "South Africa", "De Hooch", "Arnold J. Toynbee", "Lisanne Falk", "January 2004", "Rachel Sarah Bilson", "Pradyumna", "Cheshire", "George H. W. Bush", "blood", "If the citizen's heart was heavier than a feather", "Indooroopilly Shopping Centre", "the first freshman to finish as the runner-up", "opium", "Adam Yahiye Gadahn,", "on an island stronghold of the Islamic militant group Abu Sayyaf,", "Ron Ely"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6205492424242425}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false], "QA-F1": [0.20000000000000004, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-597", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-2509", "mrqa_searchqa-validation-6563", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-2758", "mrqa_searchqa-validation-3701", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-12223", "mrqa_searchqa-validation-2756", "mrqa_searchqa-validation-15316", "mrqa_triviaqa-validation-6163", "mrqa_triviaqa-validation-5356", "mrqa_hotpotqa-validation-3713", "mrqa_newsqa-validation-3404", "mrqa_triviaqa-validation-2080"], "SR": 0.5625, "CSR": 0.5725446428571428, "EFR": 1.0, "Overall": 0.7152120535714286}, {"timecode": 42, "before_eval_results": {"predictions": ["the Divine Right of Kings", "Mussolini", "Cher", "william mckinley", "Tarsus", "Charles Chaplin", "WRITERS", "The Eye Prefer Paris Tour", "Vietnam", "Foil", "The Golden Age of Murder", "Cold Blood", "william mckinley", "Jackie Joyner", "a whale", "Nelson Mandela", "the Taurid", "Cuba Libre", "Thomas Jefferson", "Tanzania", "David Hare", "Puebla", "Pennsylvania", "Borneo", "mckinley", "Walla Walla", "Netflix", "Roger Bannister", "Le Corbusier", "(Scott) Peterson", "an enigma", "Franco", "Bolivia", "rugby", "Ireland", "Vanna White", "Catherine II", "blue", "the ignition coil", "ROE", "Elizabeth Cady Stanton", "the Wetterstein Mountains", "Francis Ford Coppola", "wives and concubines", "meander", "The Wind in the Willows", "\"Honey, I just forgot to duck.\"", "hexadecimal", "The Two Gentlemen of Verona", "the chimpanzee", "the Red Cross", "avian", "August 2012", "Jack Nicklaus", "the Central African Republic", "six", "Yalta", "Emad Hashim", "Brenda Song", "The Krypto Report", "Bayern Munich", "the United States", "cancer", "the Boston Fern"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5578125}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-16519", "mrqa_searchqa-validation-11329", "mrqa_searchqa-validation-11452", "mrqa_searchqa-validation-519", "mrqa_searchqa-validation-2589", "mrqa_searchqa-validation-16809", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-10935", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-16153", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-12644", "mrqa_searchqa-validation-12772", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7323", "mrqa_searchqa-validation-14283", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-7702", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-2135", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-3428", "mrqa_newsqa-validation-3131", "mrqa_triviaqa-validation-6337"], "SR": 0.484375, "CSR": 0.5704941860465116, "EFR": 1.0, "Overall": 0.7148019622093023}, {"timecode": 43, "before_eval_results": {"predictions": ["Orthogonal", "Henry III", "Tomorrow Never Dies", "Liechtenstein", "the gun", "(O) Absalom", "Columbus", "Favre", "South African", "Brian Deane", "Margaret Caroline Fraser", "Argentina", "William Conrad", "1875", "Lloyd Webber", "Iran", "the Fairey Swordfish", "the Isle of Arran", "London County", "Playboy", "a barba beard", "Matalan", "Chesney Wold", "boiston", "the griffin", "red", "the Pussycats", "Popowo", "Judy Cassab", "rings", "Karl Marx and Friedrich Engels", "Utrecht", "Union of Post Office Workers", "(Strangeways)", "Carousel", "36", "Richard Wagner", "the brain and the spinal cord", "\"Garp\"", "(Frederick) William Herschel", "Belgium", "October 31st", "a beetle", "\"Deacon Blues\"", "Pompey", "Denali", "auction", "haddock", "L. P. Hartley", "Italy", "a snake", "Andy Serkis", "2001", "the human hands", "the Distinguished Service Cross", "Shenae Grimes", "five-time", "prostate cancer,", "the U.S. Holocaust Memorial Museum,", "on its final scheduled voyage this week.", "Dale", "a criminal show", "London", "(Oerusalem)"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5625}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10394", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-3005", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-6459", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-4052", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3137", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2244", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-9414"], "SR": 0.453125, "CSR": 0.5678267045454546, "EFR": 1.0, "Overall": 0.7142684659090909}, {"timecode": 44, "before_eval_results": {"predictions": ["co-chair", "the Flying Pickets", "iceland", "Evita", "Victoria", "Sikhism", "dan lloyd", "Sinclair Lewis", "Argentina", "glaze", "Guatemala", "olive", "Munich", "violin", "a window sash", "Paul Nash", "placentals", "first among equals", "a robin", "Indira Priyadarshini Gandhi", "Colombia", "henry harold favours", "Uranus", "Prince Igor", "h2g2", "monaco football", "an electrical component", "The Wicker Man", "henry", "Gorky", "South Africa", "a hovercraft", "john mcEnroe", "white", "Jimmy Boyd", "Tina Turner", "henry", "brash", "honeybees", "Harold Wilson", "The Spice Girls", "john lloyd", "Alan Ladd", "detergent", "Wolfgang Amadeus Mozart", "\"Bubba\" Watson, Jr.", "Utah", "Richard Lester", "December", "peregrines", "steel", "1 October 2006", "Cee - Lo", "Britain", "Marcus Tullius Reynolds", "35,000 members", "Tel Aviv University", "response to a civil disturbance call,", "the BBC's central London offices", "\"Hillbilly Handfishin'\"", "an honest man", "Roosevelt, Churchill", "flanker", "an integral membrane protein that builds up a proton gradient across a biological membrane"], "metric_results": {"EM": 0.609375, "QA-F1": 0.668266369047619}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true], "QA-F1": [0.2857142857142857, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8581", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-7543", "mrqa_triviaqa-validation-5547", "mrqa_naturalquestions-validation-5476", "mrqa_hotpotqa-validation-773", "mrqa_hotpotqa-validation-3313", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-220", "mrqa_searchqa-validation-9143", "mrqa_searchqa-validation-13219", "mrqa_searchqa-validation-9558"], "SR": 0.609375, "CSR": 0.56875, "EFR": 0.96, "Overall": 0.706453125}, {"timecode": 45, "before_eval_results": {"predictions": ["The Apollo spacecraft", "1853", "Daniel A. Dailey", "adrenal medulla", "travis", "Plank", "Ann Gillespie", "near Chesapeake Bay", "a loanword of the Visigothic word guma `` man", "March 26, 1973", "drizzle, rain", "Tommy Shaw", "1858", "Charles Perrault", "John Daly", "1998", "Elizabeth Dean Lail", "March 31, 2017", "Jonathan Breck", "Aristotle", "2007", "plate tectonics", "one complete set", "more than a million", "into the Christian biblical canon", "1995", "Rock Island, Illinois", "1926", "2006 -- 06", "Asia and Australia in the west and the Americas in the east", "on the pelvic floor", "The Osmonds", "the British group Ace", "from the Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s, meaning `` herdsman '' or `` plowman ''", "Teddy Randazzo, Bobby Weinstein, and Lou Stallman", "muscle cells", "Christopher Jones", "to avoid the inconvenienceiences of a pure barter system", "current day", "Neal Dahlen", "liabilities and equity", "London, United Kingdom", "Hellenism", "232", "January 2, 1971", "Newcastle United", "\u20b9 39.50 lakh", "white blood cell", "a crust of potatoes", "the league", "$72", "de Havilland Moth", "12", "bukwus", "the International Hotel", "Gloria Trevi", "postal delivery", "he dropped his children off at a relative's house,", "the army major", "\"Slumdog Millionaire\"", "Clifford Odets", "Margaret Mitchell House", "Microsoft", "travis"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5707433617639819}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 0.5, 0.4615384615384615, 0.8, 1.0, 0.5, 0.6956521739130436, 0.0, 1.0, 0.5, 0.7058823529411764, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-4850", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-10032", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-8596", "mrqa_triviaqa-validation-1421", "mrqa_triviaqa-validation-2110", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-3832", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-13643", "mrqa_searchqa-validation-947"], "SR": 0.453125, "CSR": 0.5662364130434783, "EFR": 0.9428571428571428, "Overall": 0.7025218361801242}, {"timecode": 46, "before_eval_results": {"predictions": ["Gamal Abdul Nasser", "2016", "committed and effective Sultans", "Germany", "contributed military and civilian police personnel to peace operations", "Steve Hale", "King Saud University", "V\u1e5bksayurveda", "John Dalton", "Vienna", "pepsinogen", "Carol Worthington", "pagan custom", "1942", "April 29, 2009", "19 June 2018", "Todd Bridges", "Ben Savage", "the senior-most judge of the supreme court", "multinational retail corporation", "Tex - Mex cuisine", "Andrea Brooks", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "Max Martin", "by October 1986", "232", "forested parts", "about 3.5 mya", "2010", "Confederacy", "mitosis", "johnson", "Texhoma", "marriage officiant", "Nat Finkelstein", "The Royalettes", "\u00b0 C", "Vienna", "Andrew Johnson", "muscle contraction", "four distinct levels", "if the concentration of a compound exceeds its solubility", "Cleveland Indians", "Amanda Leighton", "Kid Creole and the Coconuts", "Stephen Graham", "Anna Maria Demara", "the plane crash", "Tatsumi", "Ernest Hemingway", "14 November 2001", "Venus", "Laos", "nastase", "Bennett Cerf", "Scotty Grainger Jr.", "uncle", "Steve Wozniak.", "motion for a preliminary injunction against a Mississippi school district and high school in federal court Tuesday over the April 2 prom.", "release of the four men", "the poverty line", "the uterus", "panting", "Colonal Sanders"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6198050213675214}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.923076923076923, 0.5, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.7333333333333334, 0.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-3771", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-5936", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-959", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-1475", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-14439", "mrqa_searchqa-validation-15864"], "SR": 0.515625, "CSR": 0.5651595744680851, "EFR": 1.0, "Overall": 0.713735039893617}, {"timecode": 47, "before_eval_results": {"predictions": ["2014", "Ren\u00e9 Descartes", "their bearers", "Dante Pastula", "Havana Harbor", "sedimentary rock", "April 10, 2018", "Tracy McConnell", "the North Atlantic Ocean", "a flood defense system", "111", "2 %", "an Irish feminine name", "the head of the Imperial Family and the traditional head of state of Japan", "November 17, 2017", "an instant messaging client that was first developed and popularized by the Israeli company Mirabilis in 1996", "appellate courts", "her abusive husband", "84", "William Chatterton Dix", "Killer Within", "Broken Hill and Sydney", "appendicular skeleton", "a database maintained by the United States federal government, listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "a couple broken apart by the Iraq War", "the inventor Bi Sheng", "National Industrial Recovery Act ( NIRA )", "Pittsburgh", "the ulnar nerve", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "31 October 1972", "twelve", "Matt Flinders", "The person who has existence in two parallel worlds", "IETF protocols", "Ra\u00fal Eduardo Esparza", "The post translational modification of proinsulin to mature insulin", "4 September 1936", "drivers who meet more exclusive criteria", "Horace Lawson Hunley", "Orangeville, Ontario", "dromedary", "the Jews", "James Hutton", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "decades after its initial release", "the following year", "March 18, 2005", "Joe Young", "in the fovea centralis", "the Russian army", "in bed of sloth, or to die of old age", "Rudyard Kipling", "the Penguin", "Otto Eduard Leopold", "Ukrainian", "237 square miles", "Apple employees", "The secretary started the meeting with an apology to me personally, to the American Legion and to the entire veterans community,\"", "in July", "Margaret Mitchell", "the Crucifix", "Lisa Lisa Lisa", "left-hand or right-hand batsman"], "metric_results": {"EM": 0.5, "QA-F1": 0.6316589201457623}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.33333333333333337, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8108108108108109, 0.0, 0.0, 0.888888888888889, 0.25, 1.0, 0.13333333333333333, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 0.3333333333333333, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.07692307692307693, 0.6666666666666666, 1.0, 1.0, 0.8, 0.4]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-10321", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-3828", "mrqa_hotpotqa-validation-5541", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-272", "mrqa_searchqa-validation-16263", "mrqa_hotpotqa-validation-181"], "SR": 0.5, "CSR": 0.5638020833333333, "EFR": 0.96875, "Overall": 0.7072135416666666}, {"timecode": 48, "before_eval_results": {"predictions": ["Magdalen College", "Rana Daggubati", "13 people", "G. V. Prakash Kumar", "1", "Kristy Lee Cook", "The LA Galaxy", "The Volvo 850", "February 14, 1859", "'Tis the Fifteenth Season", "Biola University", "BAFTA Award for Best Production Design", "2012", "October 13, 1980", "tomato", "Gracie Mansion", "Arsenal Football Club", "Operation Neptune", "Steve Prohm", "a super-regional shopping mall", "Lady Charlotte Elliot", "28th season", "seven", "28 June 1945", "University of California", "Miami Gardens, Florida", "Indian origin", "Paige O'Hara", "Graham Hill", "The Emperor of Japan", "Hillary Clinton", "1896", "Edward Michael \" Mike\"/\"Spanky\" Fincke", "the D\u00e2mbovi\u021ba River", "Philip Mark Quast", "Pierce County", "PPG Paints Arena", "May 10, 1976", "Rodrick Heffley", "Operation Julin", "BAFTA TV Award Best Actor", "First Balkan War", "1587", "Marty Ingels", "Carl David Tolm\u00e9 Runge", "1941", "June 2, 2008", "Charice", "Freddie Jackson", "Waimea", "Ustad Vilayat Khan", "the bottom of the brain immediately below the hypothalamus", "His last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "restoring someone's faith in love and family relationships", "Moose the dog, better known as Eddie in US sitcom Frasier", "gold wedding anniversary", "corvidae", "the GI Bill, unemployment benefits and disaster relief", "out in the woods", "The Sopranos", "Winslow Homer", "Elizabeth II", "the Crimean War", "Alexey Pajitnov,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6287293894830659}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.4, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.823529411764706, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-2480", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-4551", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-4394", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-237", "mrqa_hotpotqa-validation-438", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-5535", "mrqa_newsqa-validation-163", "mrqa_searchqa-validation-14284"], "SR": 0.484375, "CSR": 0.5621811224489797, "EFR": 0.9696969696969697, "Overall": 0.7070787434291899}, {"timecode": 49, "before_eval_results": {"predictions": ["The Yardbirds", "Dubai", "Silverstone", "a Christmas Monkey", "Triumph and Disaster", "1720", "the Battle of Agincourt", "Evan Rachel Wood", "beetles", "nahaserv", "100 years", "Rudyard Kipling", "dragonflies", "Cole Porter", "liriope", "heston", "Big Brother", "motto", "Beaujolais Nouveau", "edchurch", "Paul Dukas", "Tom Watson", "(Roy) Orbison", "otitis externa", "Tokyo", "e", "keeper", "God bless America, My home sweet home.", "Dangerous Minds", "death", "Apollon", "Daft As A Brush", "South Korea", "Boxing Day", "St Pancras International Station", "fish", "wainscot", "staphylococcus", "Scarborough", "Alan Turing", "Isaac Newton", "Calcium Carbonate", "Bombay", "Anna Eleanor Roosevelt", "eddie", "naxos", "plottes", "Hitachi", "wine", "plgrade", "the gizzard", "the St. Louis Cardinals", "counter clockwise direction", "3.5 million years", "Tampa Bay Lightning", "Los Angeles Dance Theater", "Battle of Dresden", "opionville, Haiti,", "$150 billion", "root out terrorists within its borders.", "Latvia", "an alligator", "Jerry Rice", "Jungle Jim"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5925595238095238}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.8571428571428571, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3276", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-2114", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-7216", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-6646", "mrqa_triviaqa-validation-293", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-6612", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-7050", "mrqa_newsqa-validation-93", "mrqa_searchqa-validation-2086", "mrqa_searchqa-validation-5649"], "SR": 0.53125, "CSR": 0.5615625, "EFR": 0.9666666666666667, "Overall": 0.7063489583333333}, {"timecode": 50, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2584", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-438", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-4499", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5405", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3559", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14232", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16791", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7018", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7260", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7002", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-21", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96"], "OKR": 0.8515625, "KG": 0.4625, "before_eval_results": {"predictions": ["Medusa", "Hawaii", "Easy Rider", "Scrabble", "Percy Bysshe Shelley", "Billy Joel", "pardon", "New York City", "Breakfast at Tiffany's", "Roman Polanski", "Dogberry", "Battle Creek", "the Red Sea", "Mary Lincoln", "Mary Poppins", "roosevelt", "phonetics", "The Naked Brothers Band", "Julianne Moore", "saddle bags", "Audrey Hepburn", "quilt", "anemoi", "butyric acid", "the Tagus", "the CIO", "acting", "USS Nautilus", "Bantu", "Denmark", "student loans", "onion", "the flute", "Seattle", "Michael Jordan", "John Quincy Adams", "rosa bonheur", "Louis XIII", "Korea", "December 23, 1777", "chancellor of West Germany", "author", "Crimean Tatar", "Almond Joy", "the White House", "a seashell", "Julius Caesar", "the magi", "Dean Acheson", "Pittsburgh", "jury trials in certain civil cases.", "Neil Patrick Harris", "Sanaa Lathan", "Jonathan Goldstein", "9 imperial gallons", "Ryan Turner", "Spain", "Sean Yseult", "1754", "Robert Harper", "outside the Iranian consulate in Peshawar", "in his native Philippines", "she's in love,", "September 21, 2014"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5565104166666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.25, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6167", "mrqa_searchqa-validation-12468", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-664", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-12477", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-694", "mrqa_searchqa-validation-11463", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-9320", "mrqa_searchqa-validation-4385", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-1251", "mrqa_searchqa-validation-9809", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-15599", "mrqa_searchqa-validation-13662", "mrqa_searchqa-validation-9986", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-9264", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-674", "mrqa_hotpotqa-validation-3612", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-551", "mrqa_hotpotqa-validation-4911"], "SR": 0.4375, "CSR": 0.5591299019607843, "EFR": 1.0, "Overall": 0.7160447303921569}, {"timecode": 51, "before_eval_results": {"predictions": ["Reddi-wip", "the Dachshund", "Saturn", "Brownie Wise", "Risk", "a Bar Mitzvah", "khamsin", "Clark Gable", "Katharine Hepburn", "Metacomet", "surrender", "Tarsus", "the Niagara River", "Hannibal Lecter", "The Man Without A Country", "the Arc de Triomphe", "George Frideric Handel", "the Cologne", "Indonesia", "Florence Henderson", "Linus Pauling", "cocoa", "the English Channel", "the cavallo", "water", "Ohio", "Million Dollar Baby", "vanilla beans", "organ", "Papua New Guinea", "Macy's", "Jeb Bush", "the Arctic Ocean", "enamel", "Port-au-Prince", "the Barbary Coast", "humility", "Aleksandr Vladimirovich Popov", "rice", "gas masks", "\"to look like\"", "\"Cry-Baby\"", "the breast", "a jet of water", "Louis XIV of France", "suspension bridge", "faerie", "trudge", "JetBlue", "Ryan Seacrest", "a key", "Lake Michigan", "Spanish colonies won their independence in the first quarter of the 19th century, in the Spanish American wars of independence", "as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "piscina", "the wren", "Islamabad", "Indianapolis Motor Speedway", "$26 billion", "Deftones", "Adam Lambert and Kris Allen", "27-year-old's", "as spies for more than two years,", "Michael Madhusudan Dutta"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6515677609427608}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.45454545454545453, 0.5185185185185185, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-14737", "mrqa_searchqa-validation-16915", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-13086", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10836", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-10726", "mrqa_searchqa-validation-3994", "mrqa_searchqa-validation-10574", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-10180", "mrqa_searchqa-validation-8660", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-2254", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-2705", "mrqa_newsqa-validation-3145"], "SR": 0.5625, "CSR": 0.5591947115384616, "EFR": 1.0, "Overall": 0.7160576923076923}, {"timecode": 52, "before_eval_results": {"predictions": ["Alfred Preis", "Nip/Tuck", "Adelaide Lightning", "Homeland", "1983", "3.9 mi", "Hilux", "Roc Me Out", "Snowball II", "Anna Clyne", "Flushed Away", "the Elbow River", "Mickey's Christmas Carol", "Ellie Kemper", "Aamir Khan", "Eugene Levy", "25 million", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu", "Julianne Moore", "drummer Seb Rochford", "Samantha Spiro", "David O'Leary", "Los Angeles Galaxy", "Sami Khan", "Total Nonstop Action Wrestling", "Don Hahn", "Nobel Prize in Physics", "Tudor music", "the east of Ireland", "Blue Grass Airport", "Tim Whelan", "the Cleveland Celtics", "Ashley Jensen", "Franc Roddam", "Ben R. Guttery", "Pieter van Musschenbroek", "ABC", "Roseann O'Donnell", "\"media for the 65.8 million,\"", "1902", "the USS \"Enterprise\"", "Las Vegas", "Todd Emmanuel Fisher", "blood", "John M. Dowd", "August 9, 2017", "Beau Rivage", "James Fell", "Clara Petacci", "from 1986 to 2013", "Bill Ponsford", "`` twin", "Alexandra de Rossi", "the Gaget, Gauthier & Co. workshop", "(Thomas) Jefferson", "the gas law", "Vienna", "Harrison Ford", "Flint, Michigan", "27-year-old's", "the Squirrel", "Farsi", "water vapor", "Kitty Kelley"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6268736471861471}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-1584", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-3321", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-2153", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-3391", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-4932", "mrqa_triviaqa-validation-2082", "mrqa_newsqa-validation-2415", "mrqa_searchqa-validation-11381", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-4118"], "SR": 0.53125, "CSR": 0.5586674528301887, "EFR": 1.0, "Overall": 0.7159522405660377}, {"timecode": 53, "before_eval_results": {"predictions": ["from London to Canterbury", "2018", "Welch, West Virginia", "after 800", "2009", "Mark Jackson", "Brazil", "December 24, 1836", "20 November 1989", "BC Jean", "Billy Bishop Toronto City Airport on the Toronto Islands in Toronto", "48,100 km ( 18,016 sq mi )", "Roger Dean Stadium", "Jesse Wesley Williams", "19 June 2018", "Prince William", "Sanchez Navarro", "Nick Kroll", "pigs", "Pittsburgh", "2018", "to the left of the dinner plate", "headdresses", "in a Norwegian town", "1960", "from the 1840s", "heavy tank", "semi-automatic, but not fully automatic", "Jack and Jill", "negative", "halogenated paraffin hydrocarbons", "200 to 500 mg up to 7ml", "Mel Gibson", "November 5, 2017", "blood flow to those organs involved in intense physical activity", "Qutab Ud - Din - Aibak", "M\u00e1ximo Gomez and Antonio Maceo", "muscles", "Robin", "March 26, 1973", "New England Patriots", "New York City", "the Equatorial Counter Current", "31 - member", "Ajay Tyagi", "from the heraldic crest carved in the lintel on St. Ignatius'family home in Azpeitia, Spain", "Brooklyn Heights", "book and architecture", "19 June 2018", "Efren Manalang Reyes", "California", "Barcelona", "molybdenum", "Henri Paul", "January 4, 1976", "1921", "Edmund Allenby", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "glaciers in the European Alps may melt as soon as 2050,", "22", "altitude", "Timberland", "Foo Fighters", "Jennifer Grey"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6566587619896443}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true], "QA-F1": [0.4, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.9090909090909091, 0.0, 0.0, 0.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.11764705882352941, 0.33333333333333337, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-6137", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-3801", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-3837", "mrqa_hotpotqa-validation-5720", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1638", "mrqa_searchqa-validation-6725"], "SR": 0.515625, "CSR": 0.5578703703703703, "EFR": 1.0, "Overall": 0.7157928240740741}, {"timecode": 54, "before_eval_results": {"predictions": ["the martini", "a final contest", "San Francisco", "Yatsenyuk", "Cottage cheese", "Tony Gwynn", "Atonement", "North Carolina", "Collagen", "Just say no", "Typewriter", "Diane Arbus", "Cincinnati", "Cleopatra", "the Suez Canal", "Planet of the Apes", "a room", "Adam Sandler", "the Compass Rose", "To exhort", "a building", "William Shakespeare", "phobias", "Santa Clara", "piano", "the Byzantine Empire", "Dunkirk", "white", "Psalm 115", "pearl", "Gelato", "Jesus", "bacterium", "Balanchine", "Alfred Stieglitz", "\"Don Juan De Marco\"", "Africa", "Gaius", "Applebee's", "the Mercator", "Robin Hood", "Stegosaurus", "Ravel", "Daniel Boone", "William Tecumseh Sherman", "Prison Break", "a hippo", "Donald", "Charles II", "Sinclair Lewis", "Leo", "85 %", "the Colony of Virginia", "Uruguay", "cat", "Mr. Humphries", "Boulder Dam", "Personal History", "Hellenism", "Barnoldswick", "bodies and heads", "the U.S. Holocaust Memorial Museum", "BET", "Havana Harbor"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6145833333333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-12810", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-13725", "mrqa_searchqa-validation-14985", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-1876", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-308", "mrqa_searchqa-validation-7376", "mrqa_searchqa-validation-10591", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-1763", "mrqa_searchqa-validation-15511", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-14679", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-1657", "mrqa_newsqa-validation-23"], "SR": 0.578125, "CSR": 0.5582386363636364, "EFR": 1.0, "Overall": 0.7158664772727272}, {"timecode": 55, "before_eval_results": {"predictions": ["a zebra", "Sarah McLachlan", "dessert", "Japan", "C Daryl Chessman", "Grade Point Average", "grapefruit", "Detroit", "Tiger Woods", "John Paul II", "Blackbeard", "goose", "Jane Goodall", "Big Ben", "Ethiopian", "4", "Stephen Crane", "Luxor", "gng h", "nickel", "Clinton", "Wyoming", "septum", "Nantucket", "Abnormal Psychology", "Elvis Presley", "Gianlorenzo Bernini", "mosquitoes", "Frank Sinatra", "Mark Knopfler", "photons", "National Archives", "low blood pressure", "Mousehunt", "Israeli", "honey", "Rugby Football Union", "Shrew", "a palace", "coffee", "Knott's Berry Farm", "Phaedra", "Carl Linnaeus", "Australia", "Jodie Foster", "Ventricular Tachycardia", "Barbary pirates", "bagels", "a sitcom", "Matilda", "Qike", "1923", "Master Christopher Jones", "T.J. Miller", "7", "Gary Chapman", "Jerry Mouse", "The AVN Adult Entertainment Expo (AEE)", "England and Ireland", "The Beatles", "identity documents", "her decision based on the combination of the interrogation techniques, their duration and the effect on al-Qahtani's health.", "New York City", "Philip Billard Municipal Airport"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6531994047619047}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.2222222222222222, 1.0, 0.0, 0.07142857142857144, 0.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-15701", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-6240", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-12914", "mrqa_searchqa-validation-4022", "mrqa_searchqa-validation-11601", "mrqa_searchqa-validation-14248", "mrqa_searchqa-validation-7849", "mrqa_searchqa-validation-12284", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-7361", "mrqa_hotpotqa-validation-2046", "mrqa_hotpotqa-validation-3169", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2215", "mrqa_hotpotqa-validation-2840"], "SR": 0.546875, "CSR": 0.5580357142857143, "EFR": 1.0, "Overall": 0.7158258928571428}, {"timecode": 56, "before_eval_results": {"predictions": ["Jean Lafitte", "Nymphodorus of Syracuse", "argyle", "the Pacific Ocean", "Easter", "forgive me", "Dalai Lama", "a heptathlon", "a tuba", "\"A Brief History of the 21st Century\"", "tea", "arterial blood vessels", "Nicholas II", "Amerigo Vespucci", "Patrick Henry", "eau de Toilette", "Metallica", "Ho Chi Minh", "the pituitary gland", "Ben Johnson", "Kentucky", "the Byzantine Empire", "9 to 5", "Cambodia", "lupper", "Velvet Revolver", "Sears", "flavonoids", "cherries", "Florence", "Ma Barker", "Joe DiMaggio", "thief", "Naples", "\" Nick and Norah's Infinite Playlist\"", "the Baruch Plan", "a star", "wine", "a bead", "\"The Safety Dance\"", "the Cymric cat", "the Balconies of Lima", "a GPS", "North Carolina", "M&M'S Pe peanut Chocolate Candies", "a cake knife", "Tchaikovsky", "the Tuileries", "a Panama Canal", "Cessna 172", "General McClellan", "Andy Serkis", "Parthenogenesis", "Pangaea", "Germany", "mule", "60", "Silvia Navarro", "26 November", "John Morgan", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "Leo Frank, a northern Jew who'd moved to Atlanta to supervise the National Pencil Company factory.", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "off Somalia's coast."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6471263111888111}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.12121212121212123, 0.0, 0.4615384615384615, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-665", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-3160", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10639", "mrqa_searchqa-validation-9458", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-11186", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-795", "mrqa_searchqa-validation-3454", "mrqa_searchqa-validation-11589", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-13439", "mrqa_searchqa-validation-2151", "mrqa_searchqa-validation-15973", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-3262", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-1022"], "SR": 0.5625, "CSR": 0.5581140350877193, "EFR": 1.0, "Overall": 0.7158415570175438}, {"timecode": 57, "before_eval_results": {"predictions": ["a spectator", "French toast", "Mexico", "plug in", "William Faulkner", "Patty Duke", "Ralph Waldo Emerson", "Hindu", "Brook Busey-Maurio", "Intel", "Hank Williams Jr.", "George C. Wallace", "the United States", "an offensive", "West Virginia", "Edward Hopper", "asteroids", "Mark Twain", "the HST", "a strawberry", "Jimi Hendrix", "Steven Spielberg", "Adam Smith", "Tootsie", "water", "albino", "Bonn", "junk", "chinchillas", "Tennessee", "the No Child Left Behind Act", "William S. Hart", "the Piano Guys", "Francisco Franco", "Tennessee Williams", "a fence", "Douglas Fairbanks, Jr.", "West Point", "Revolver", "Steely Dan", "word", "Norway", "chicken Kiev", "George Clooney", "a diamond", "the Red Sox", "postcards", "Kentucky", "Skateboarding", "Gaul", "blasters", "appropriates ( gives to, sets aside for ) money to specific federal government departments, agencies, and programs", "Havana Harbor", "Melbourne", "Atticus Finch", "a donkey", "Max Planck", "Ella", "Ed O'Neill", "Darkroom", "Dancing With the Stars.", "approximately 600 square miles of south-central Washington, an area roughly half the size of Rhode Island.", "Monday.", "Upstairs Downstairs"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6418913398692812}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.058823529411764705, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-2264", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-10372", "mrqa_searchqa-validation-13915", "mrqa_searchqa-validation-14269", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-7607", "mrqa_searchqa-validation-6688", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-5196", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-8729", "mrqa_searchqa-validation-11120", "mrqa_naturalquestions-validation-10533", "mrqa_triviaqa-validation-3812", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-1161", "mrqa_newsqa-validation-2446"], "SR": 0.59375, "CSR": 0.5587284482758621, "EFR": 1.0, "Overall": 0.7159644396551724}, {"timecode": 58, "before_eval_results": {"predictions": ["Gov. Andrew Jackson", "Sri Lanka", "John Glenn", "Hinduism", "Billie Holiday", "wedlock", "trans fat (i.e.) trans fatty acids", "the Bridge to Terabithia", "Edward III", "Hello, Dolly", "the Mesozoic Era", "Gettysburg", "Martin Lawrence", "plantain", "Heracles", "Fosse", "stem cells", "a cutlass", "the Bodleian Library", "pupil", "a front", "James Franco", "salmon", "The Crow", "goat milk", "James Watt", "1945", "a birthstone", "Ichabod Crane", "Morrie: An Old Man, a Young Man", "Heather Locklear", "noun", "Holden Caulfield", "Chocolate Hazelnut Truffles", "Saudi Arabia", "lamb", "LaDainian Tomlinson", "wheat", "Duke", "a photoelectric cell", "Cape Town", "meiosis", "Austin Powers", "sourdough", "Mo Nissanite", "vice presidential nomination", "Joseph Vissarionovich Stalin", "Fiorello La Guardia", "Chastity", "Turandot", "Texas Rangers", "Henri Biva", "ice giants", "Amenhotep IV", "Zimbabwe", "colony", "phobia", "Saint-Domingue", "a parabolic reflector", "1891", "Vice President Dick Cheney", "1-1", "an upper respiratory infection,", "Russia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6430555555555556}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, false, true, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-11844", "mrqa_searchqa-validation-16405", "mrqa_searchqa-validation-16918", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-4326", "mrqa_searchqa-validation-16956", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5137", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-14926", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-11795", "mrqa_searchqa-validation-13956", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-2685", "mrqa_hotpotqa-validation-119", "mrqa_newsqa-validation-2402", "mrqa_newsqa-validation-2472", "mrqa_naturalquestions-validation-3214"], "SR": 0.546875, "CSR": 0.5585275423728814, "EFR": 1.0, "Overall": 0.7159242584745763}, {"timecode": 59, "before_eval_results": {"predictions": ["Monet Bar", "Mary Magdalene", "The Pillow Book", "General Paulus", "the Grail", "butcher", "The Double", "Dr. Samuel Johnson", "Jessica Simpson", "Zeppelin", "the gallbladder", "Peterloo", "Aaron", "Leo Tolstoy", "Birmingham", "the Penrose triangle", "Magnificent Seven", "the Australian shearers' strike", "Theodore Roosevelt", "raven", "John of Gaunt", "typhoid fever", "ium", "Microsoft", "John Galliano", "the Big Bang", "Willie Nelson", "horseracing", "Stars on 45 Medley", "Lundy", "Guinea", "Kerri Strug", "Belgium", "Del", "Aleister Crowley", "non-Orthodox synagogues", "Amnesio", "Herbert Henry Asquith, 1st Earl of Oxford", "Nirvana and Kiss", "Mr. Humphries", "Paul Gauguin", "Connochaetes", "the Low Countries", "50", "Charlie Harper", "nirvana", "Pet Sounds", "purple", "Brigit Forsyth", "aardvark", "Charles Darwin", "5.7 million", "Missouri", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "8,515", "villanelle", "Awake", "2008", "China, Taiwan, Hong Kong and Mongolia,", "Patrick McGoohan", "a coyote", "heating", "A Tale of Two Cities", "Charlie Wilson"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6704861111111111}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-4273", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-3489", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-6759", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-229", "mrqa_triviaqa-validation-6132", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-577", "mrqa_triviaqa-validation-3208", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-4592", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-4197", "mrqa_newsqa-validation-2061"], "SR": 0.59375, "CSR": 0.5591145833333333, "EFR": 1.0, "Overall": 0.7160416666666667}, {"timecode": 60, "UKR": 0.65625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1121", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-122", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4518", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-5828", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-159", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-1071", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10905", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11198", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-12032", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-12468", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15373", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16915", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3160", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3329", "mrqa_searchqa-validation-3454", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4232", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6187", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8373", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-9809", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4887", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9613", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1078", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3904", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4074", "mrqa_triviaqa-validation-4497", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-5183", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-690", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-998"], "OKR": 0.810546875, "KG": 0.4484375, "before_eval_results": {"predictions": ["15", "G\u00f6tene in Sweden", "Muir Mathieson", "Rensselaer County", "more than 20 principal operations and manufacturing facilities worldwide", "\"Beauty and the Beast\"", "extreme nationalist, and nativist ideologies, as well as authoritarian tendencies.", "penny bun", "Overtime", "south-north motorway", "Hockey Club Davos", "2 April 1940", "Domingo \"Sam\" Samudio", "Sunday", "Taylor Swift", "Asif Kapadia", "\"Game ofGame\" stunt teams", "graffiti artists", "ESPN", "Bangor International Airport", "October 29, 1985", "Point of Entry", "\"Mickey Mouse\" series", "the Harpe brothers", "1940s and 1950s", "Port Clinton", "Totally Tom", "Bharat Ratna", "Ronald Ryan", "made into a TV series for the BBC", "the 2011 Pulitzer Prize in General Nonfiction", "Eliot Cutler", "IT products and services", "American", "1877", "Critics' Choice Television Award", "Jeff Meldrum", "Picric acid", "2002", "1979", "Hannaford", "1968", "post-Roman Republic", "Rigoletto", "Bill Clinton", "\"Twister\"", "94", "horror film", "baa, Baa, Black sheep", "law", "28,776", "a Canaanite god associated with child sacrifice", "by slide clips, slide clamps or a cross-table which is used to achieve precise, remote movement of the slide upon the microscope's stage", "commemorating fealty and filial piety", "God", "video", "charlotte", "new Zealand", "U.N. agencies", "jund Ansar Allah", "Thames", "Pamela Anderson", "Henry Ford", "methane"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5653645833333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5333333333333333, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.4, 0.8, 0.08333333333333334, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-804", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-5589", "mrqa_hotpotqa-validation-2263", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-1621", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-1570", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-182", "mrqa_triviaqa-validation-6429", "mrqa_triviaqa-validation-3314", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-2732"], "SR": 0.453125, "CSR": 0.5573770491803278, "EFR": 0.9714285714285714, "Overall": 0.6888079991217798}, {"timecode": 61, "before_eval_results": {"predictions": ["Home Fires", "cristiano Ronaldo", "Daniel Wroughton Craig", "Magnus Carlsen", "850 saloon", "400 MW", "1991", "Dra\u017een Petrovi\u0107", "Lord's Resistance Army", "Scott Dunlop", "Midtown Manhattan", "IFFHS World's Best Goalkeeper", "David May", "New Jersey", "John Gielgud", "Waimea Bay", "Willie Nelson and Kris Kristofferson", "the German Luftwaffe", "Nickelodeon on Sunset", "Terry the Tomboy", "holy servant of Christ", "Give Up", "Matt Winer", "University of Kentucky", "WB Television Network", "Ice Princess", "on Boxing Day, 2004", "freedom of choice, other social freedoms, and \"laissez-faire\" capitalism", "Australian", "Norse mythology", "Parapsychologist", "Melville", "5,922 at the 2010 census", "You Belong with Me", "Black Abbots", "The Thieves", "Kentucky, Virginia, and Tennessee", "2011", "five", "Veronica Hamel", "literary magazine", "French, English and Spanish", "Edward James Olmos", "John R. Leonetti", "\"Alceste\"", "Perth", "Cersei Jaimeister", "Baltimore", "countdown timer", "Joseph Conrad", "\"The Simpsons\"' thirteenth season", "Ferm\u00edn Francisco de Lasu\u00e9n", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W \ufeff / \ufefb\ufffd 22.000 \u00b0 N 80.000", "Javier Fern\u00e1ndez", "Jose Antonio Reyes", "Jordan", "lulu", "sniff out cell phones.", "18", "paintings", "Prison Break", "Oscar Wilde", "Sicily", "Yukon"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6520236013986014}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [0.0, 0.0, 0.8, 0.5, 0.5, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.25, 0.30769230769230765, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.64, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-175", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-2135", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3542", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-4367", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-3508", "mrqa_hotpotqa-validation-1487", "mrqa_hotpotqa-validation-1670", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-4477", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-4074", "mrqa_newsqa-validation-4032"], "SR": 0.546875, "CSR": 0.5572076612903225, "EFR": 0.9655172413793104, "Overall": 0.6875918555339265}, {"timecode": 62, "before_eval_results": {"predictions": ["at home, attending every soccer game and knowing what his kids like to eat for breakfast", "Pope Benedict XVI refused", "public-television show", "Polo", "punish participants", "Venezuela", "Dore Gold, former Israeli ambassador to the United Nations", "The EU naval force", "a muddy barley field", "India", "the worst snowstorm to hit Britain in 18 years", "A member of the group dubbed the \"Jena 6\"", "illegal immigrants", "the legal right to freedom from tyranny", "Dead Weather's \"Horehound\"", "Cash for Clunkers", "San Diego", "combat veterans", "Robert Park", "Mexican military", "paleontological", "David", "as many as 50,000 members", "Thursday", "her wife's name", "$17,000", "charity, Wheelchair for Iraqi Kids.", "Bob Johnson", "Matthew Fisher", "26", "angel", "$31,000", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "American Civil Liberties Union", "the company's products are roadworthy.", "1994", "High Court Judge Justice Davis", "to provide security as needed.", "$83,03013", "250,000", "Luis Carlos Ameida", "Islamabad", "hundreds of people joined a campus rally to oppose racial intolerance.", "Rodong Sinmun", "Osama", "A judge dismissed all charges Wednesday night and ordered the release of the four men", "\"exceptional circumstances surround these memos and require their release.\"", "United States", "Vernon Forrest", "sexual assault with a minor", "of Columbia National Guard,", "in March 1930", "1961", "Rajendra Prasad", "wigan", "Secretary of State William H. Seward", "the Cascade Range", "Ice Princess", "the Etihad Aldar Spyker F1 Team", "small family car", "delete", "Kansas", "John James Audubon", "Kim Basinger"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4873113524406628}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.33333333333333337, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.08, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.4, 0.06896551724137932, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-1481", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-829", "mrqa_naturalquestions-validation-7628", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6501", "mrqa_hotpotqa-validation-1346", "mrqa_searchqa-validation-5326"], "SR": 0.390625, "CSR": 0.5545634920634921, "EFR": 1.0, "Overall": 0.6939595734126984}, {"timecode": 63, "before_eval_results": {"predictions": ["Israel", "burning World Trade Center", "Saturn", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law.", "Kgalema Motlanthe,", "Ken Choi,", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "travel in cars with tinted windows", "up to $50,000 for her,", "gun charges", "January 24, 2006.", "\"fusion teams,\"", "Philippines", "used-luxury market", "July", "her home", "natural gas", "his Los Angeles, California, courtroom", "jazz", "the Rockies", "beat and binding Andrade, one of the kidnapper put a gun to Valencia's head.", "40 lash for the incident which is said to have taken place in the capital Khartoum on August 21.", "KBR", "Ralph Lauren", "Gulf of Aden,", "Al-Shabaab", "\"Wicked.\"", "269,000", "eight", "Dube, 43, was killed", "North Korea", "Tuesday in Los Angeles.", "Wally", "Alina Cho", "Venus Williams", "the nose, cheeks, upper jaw and facial tissue", "1983", "saying Tuesday the reality he has seen is \"terrifying.\"", "they were not targeting indigenous populations but took the action \"against people who independent of their race, religion, ethnicity, social condition etc. accepted money and put themselves at the service of the army", "three-time road race world champion,", "\"We tortured (Mohammed al-) Qahtani,\"", "Yemen.", "11", "Matthew Fisher", "Afghanistan's restive provinces", "Rob Lehr,", "insect stings,", "Tennessee", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's war zone.", "cancer-causing toxic chemical.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "Sleeping with the Past", "ark of the covenant", "the pachytene stage of prophase I of meiosis", "the Great Chicago Fire", "4", "Edward III", "fourth", "Academy Award for Best Art Direction", "1958", "Peter the Great", "Bronx Zoo", "Percheron", "November"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5840068702094315}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.6153846153846153, 1.0, 0.10526315789473685, 0.4444444444444445, 0.8, 0.2608695652173913, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.10526315789473684, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.8750000000000001, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.2666666666666667, 1.0, 0.9166666666666666, 1.0, 0.0, 0.7777777777777778, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-3049", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-7035", "mrqa_triviaqa-validation-1159", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-3383", "mrqa_searchqa-validation-1794"], "SR": 0.4375, "CSR": 0.552734375, "EFR": 1.0, "Overall": 0.69359375}, {"timecode": 64, "before_eval_results": {"predictions": ["We Found Love", "The 19-year-old woman", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "40", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "helped make the new truck safer,", "19", "Paul McCartney and Ringo Starr", "great jazz music", "a homicide.", "Sodra nongovernmental organization,", "on the family's blog", "computer-generated animated film", "The Screening Room Cannes special on CNN", "on the 12th on the Blue Monster course at Doral", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "signed a power-sharing deal", "collaborating with the Colombian government,", "the Russian air force,", "Rod Blagojevich", "Fiona MacKeown", "50", "the legitimacy of that race.", "Zeyno Baran", "John Lennon and George Harrison,", "Sharon Bialek", "1998.", "45 minutes, five days a week", "Israel and the United States", "Monday.", "Frank Ricci,", "Sixteen", "International Red Cross Committee, the U.N. High Commissioner for Refugees and UNICEF", "The EU naval force", "Al-Shabaab", "Daytime Emmy Lifetime Achievement Award", "since 1983.", "the foyer of the BBC building in Glasgow, Scotland", "The UNHCR", "The EU naval force", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple, with the latest resulting in the arrest of Mesac Damas in January,", "he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted.", "two", "6-2 6-1", "Rio de Janeiro", "John Demjanjuk,", "the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil,", "secure more funds from the region", "Redwood Original", "Drew Barrymore", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "a board that has lines and pads that connect various points together", "Richard Wagner", "H. H. Asquith", "Centers for Medicare and Medicaid Services", "FBI", "Macomb County", "Custer", "Louis XIV", "a waterbed", "Risk"], "metric_results": {"EM": 0.484375, "QA-F1": 0.574883385448401}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.3636363636363636, 0.0, 1.0, 0.4, 0.5714285714285715, 1.0, 0.33333333333333337, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10526315789473684, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352942, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.3, 0.13333333333333333, 0.7692307692307693, 0.23529411764705882, 0.0, 0.14285714285714288, 0.15384615384615385, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-4174", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-4188", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-574", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-5934", "mrqa_hotpotqa-validation-2837"], "SR": 0.484375, "CSR": 0.5516826923076923, "EFR": 1.0, "Overall": 0.6933834134615384}, {"timecode": 65, "before_eval_results": {"predictions": ["North West England", "Carol Ann Duffy", "moth", "Nineteen Eighty-Four", "Battleship", "Hurricane Faith", "the Slavic women accompanying their husbands in the First Balkan War.", "Teutonic Knights", "9", "James Harrison", "Germany", "Jonathan Katz", "Ford Island", "2011", "NCAA Division I", "Tim \"The Toolman\" Taylor", "Latium in central Italy,", "Paul Avery", "Berea College", "Christopher Lloyd Smalling", "January 4, 1976", "riders are turned upside-down and then back upright", "in 1971", "Clovis I", "Tahir \"Tie\" Domi", "in 2007", "poet, and writer", "Quasimodo, the deformed bell-ringer of Notre Dame", "Savin Yeatman-Eiffel of Sav!", "Pieter van Musschenbroek", "23 July 1989", "actress", "Attorney General and as Lord Chancellor of England", "Plato", "the oldest of the four ancient universities", "Henry Mills", "ribosomes", "Ronald Joseph Ryan", "A Hard Day's Night", "Humberside", "Dumfries and Galloway,", "\"Rudolph\"", "from 1989 until 1994", "Cecily Legler Strong", "Polish-Jewish", "Philip Aaberg", "in 2005", "Levon Helm", "Chengdu Aircraft Corporation", "White Knights of the Ku Klux Klan", "Reunited Worlds", "Lou Rawls", "the first year", "Mark Lowry", "Parkinson's", "I Will survive", "Yeats", "Casa de Campo International Airport", "11", "that the four women who Krazy-Glued a cheater's penis to his stomach were way harsh and beyond psycho.", "\"Carmen\"", "New York City", "Massachusetts", "in July"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6668040293040293}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7692307692307693, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-1244", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-2467", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5875", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2872", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1433", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-4711", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-10550", "mrqa_triviaqa-validation-4573", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-1180", "mrqa_searchqa-validation-2844", "mrqa_newsqa-validation-271"], "SR": 0.53125, "CSR": 0.5513731060606061, "EFR": 0.9333333333333333, "Overall": 0.6799881628787878}, {"timecode": 66, "before_eval_results": {"predictions": ["biblical apocrypha", "alligator", "best pediatric centers", "quoit", "Ramona", "Tobacco Road", "M*A*S*H", "Opportunity seldom knocks twice", "Smokey Robinson", "a snake", "Gladiator", "primaries", "trachea", "Cairo", "The Cotton Club", "a sandstorm", "Lord Byron", "neutrino", "Alexandra Rover", "George Eliot", "clouds", "\"The Maracot Deep\"", "San Juan Capistrano", "Auschwitz", "China", "the Nile Trans Africa Tours", "low-calorie", "Edward", "pomegranate", "Bali", "Paris", "decathlon", "Elizabeth II", "kings", "The Hollywood Ten", "take a small boat", "a tieless sleeveless dress", "Jean", "Yogi Berra", "China", "Hawaii", "birds", "peripheral vision", "Espresso", "Delacorte", "head of calves", "Vanessa Williams", "buttercream", "potential energy", "the Byzantine Empire", "Carson", "16 seasons", "photoelectric", "the edges of its known distribution range north to northern Louisiana, west to Colorado, and east to Massachusetts", "Joan Crawford", "Bassenthwaite Lake", "the moon", "Vanilla Air Inc.", "Jack", "Lawrence of Arabia", "\"Wicked.\"", "Six", "attempted burglary", "a long-range missile"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5823835784313726}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.7058823529411765, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-995", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-13562", "mrqa_searchqa-validation-8242", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-14972", "mrqa_searchqa-validation-5294", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-14263", "mrqa_searchqa-validation-10375", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-15880", "mrqa_searchqa-validation-5807", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-1198", "mrqa_searchqa-validation-16731", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-6263", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-15274", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-14831", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-2870", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-2944", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-1661"], "SR": 0.515625, "CSR": 0.550839552238806, "EFR": 1.0, "Overall": 0.6932147854477612}, {"timecode": 67, "before_eval_results": {"predictions": ["Steven Spielberg", "Louisiana", "Wilbur Wright", "Woodrow Wilson", "King George", "Stephen Sondheim", "515 ft", "adding machine", "Bill Wyman", "Department of Health and Human Services", "T.S. Eliot", "lead", "Ke Kele", "the First French Settlements", "gravitational field", "Fisherman's Wharf", "Santa Fe", "Ted Koppel", "Sex Pistols", "chess", "Michael Jordan", "fairground", "doughboy", "Brge Rosenbaum", "Muhammad Ali", "a rabbit", "Secretariat", "the Soup Nazi", "a tooth", "acid", "Gnaeus Naevius", "row", "\"Heaven has no rage like love\"", "John Paul II", "Will Rogers", "Hairspray", "Orlando", "Kease", "the Barbary Wars", "River Phoenix", "Sydney, Australia Harbor", "mutton", "brushes", "Napoleon", "the flag of Mongolia", "Kevens", "barn", "royal punishment", "Missouri", "Sweeney Todd", "Paris", "1956", "Tommy James and the Shondells", "Two Days Before the Day After Tomorrow", "jujitsu", "Salvador Dali", "Robert De Niro", "1993", "October 17, 2017", "from 1986 to 2013", "Afghanistan", "a mammoth", "co-writing credits", "Gary Grimes"], "metric_results": {"EM": 0.5625, "QA-F1": 0.625}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-15468", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-11812", "mrqa_searchqa-validation-7771", "mrqa_searchqa-validation-626", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-7053", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-1250", "mrqa_searchqa-validation-4446", "mrqa_searchqa-validation-2305", "mrqa_searchqa-validation-15211", "mrqa_searchqa-validation-4698", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-15818", "mrqa_naturalquestions-validation-1882", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-512", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2152"], "SR": 0.5625, "CSR": 0.5510110294117647, "EFR": 1.0, "Overall": 0.6932490808823528}, {"timecode": 68, "before_eval_results": {"predictions": ["\"Fisherman's ring\"", "Omaha", "Antwerp", "the Matterhorn", "Loch Lomond", "Alaska", "Frasier", "a temporary need", "Denmark", "\"ball in tube\" or electromechanical crash sensor", "someone", "Pygmalion", "cholera", "\"E.E. Cummings\"", "Wilhelm Roentgen", "Kathleen Kennedy Townsend", "\"Into the lenses\"", "the Green Hornet", "People, people who need", "geolu", "amniotic fluid", "\"300\"", "Diner", "Cleopatra", "shepherd's pie", "St. Petersburg", "Japan", "the Jordan River", "Derek Jeter", "Hans Christian Andersen", "in a sentence", "\"Millions for defense, but not one cent for tribute\"", "\"The Tyger\"", "Percy Shelley", "pearls", "carbon dioxide", "an earthquake", "Jr.", "Citizen Kane", "zero-g", "Mathew Brady", "Clinton", "a spike", "Tasmania", "Wyoming", "Gandalf", "the quick brown fox", "Denmark", "wheat", "\"Sweet Home\"", "a frigate", "the URL of a web page above the page in an address bar", "presidential representative democratic republic", "the Pir Panjal Range", "Barcelona", "China", "Leander", "Lewis and Clark Expedition", "Morocco", "1998", "club managers,", "haute, bandeau-style little numbers", "several weeks", "2011"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6244791666666667}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16502", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-16760", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-12041", "mrqa_searchqa-validation-13013", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-7831", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-4617", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-15210", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-1342", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-3197", "mrqa_searchqa-validation-15704", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-1848", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-3156", "mrqa_newsqa-validation-3500"], "SR": 0.5625, "CSR": 0.551177536231884, "EFR": 1.0, "Overall": 0.6932823822463767}, {"timecode": 69, "before_eval_results": {"predictions": ["KFC", "\"tuxedo\"", "Follies", "Spaghetti", "Andrew Jackson", "Agamemnon", "spurs", "Robert Bartlett", "\"Monkees\"", "cantons", "Louisiana", "a mall", "the percussion", "Richard Nixon", "Artemis", "licorice", "Constellations", "the Crystal Skull", "Fox Network", "20 feet away", "Mendel", "Maria Callas", "Hulk Hogan", "Margaret Tobin", "a palomino", "A Hard Day's Night", "Making the Band", "Judy Garland", "Autumn in New York", "telephone operator", "Franklin D. Roosevelt", "Tranio", "\"I Have No Mouth\"", "La Salle", "lattice", "a penny", "succotash", "the retina", "a prayer", "Lake Coeur d'Alene", "The Sopranos", "Hark", "Huguenots", "the Brooklyn Dodgers", "king", "yellow", "mascara", "Rooster Cogburn", "pine", "Homestead", "Lawrence Wien", "Kyrie Irving", "Mexico", "Aaron Harrison", "Crete", "Miles Morales", "Peter Nichols", "the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "1858", "McComb, Mississippi", "Newcastle", "five years", "\" Unfortunately, this is not an anomaly in Naples and in that neighborhood.\"", "a mermaid"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6270254629629629}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2962962962962963, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1217", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-7946", "mrqa_searchqa-validation-843", "mrqa_searchqa-validation-1777", "mrqa_searchqa-validation-10815", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-185", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-5237", "mrqa_searchqa-validation-12476", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-4606", "mrqa_searchqa-validation-705", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-12330", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-2762", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-2727"], "SR": 0.5625, "CSR": 0.5513392857142857, "EFR": 1.0, "Overall": 0.6933147321428571}, {"timecode": 70, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-947", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16953", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4642", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7212", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7593", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9643", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10254", "mrqa_squad-validation-10328", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1075", "mrqa_squad-validation-1183", "mrqa_squad-validation-1319", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-2975", "mrqa_squad-validation-3139", "mrqa_squad-validation-3168", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4147", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5532", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5826", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6624", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-821", "mrqa_squad-validation-8253", "mrqa_squad-validation-8410", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5488", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6565", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.794921875, "KG": 0.5, "before_eval_results": {"predictions": ["James K. Polk", "stanch", "delta", "barroco", "St. Petersburg", "Australia", "Prohibition", "Lettuce", "The Godfather", "Maria Sharapova", "Dairy Queen", "\"Sonny\"", "11", "\"The Stars and Stripes Forever\"", "Jackie Moon", "Pulp Fiction", "expunge", "the Weser", "a bazooka", "dilithium", "Schwarzenegger", "the Epstein-Barr virus", "hydrogen", "a slow jog", "U.S. Naval Academy", "Iowa", "indirect discourse", "a circle", "Pussycat Dolls", "William Shakespeare", "a lump", "Vin Diesel", "Hitler", "Heath", "the Odyssey", "(Michael) Phelps", "Annapolis", "the Maccabees", "Rolls Royce", "a doses", "the Caucasus Mountains", "Lafayette", "the gopher", "John Burrows", "Coca-Cola", "Warren Burger", "apogee", "Allah", "a mirror", "david archuleta", "Union Carbide", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Ireland", "the accession of the princely state of Hyderabad into the Indian Union on 24 November 1949", "April", "Stockholm syndrome", "Bowness-on-Windermere", "Geographical Indication", "just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "The scarp cuts impact craters", "military veterans", "\"tears good for me to talk about her,\"", "a full garden and pool, a tennis court, or several heli-pads.", "Larry Ellison"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6427765376984127}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.0, 1.0, 0.0, 0.8, 0.888888888888889, 0.0, 0.0, 0.08333333333333333, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8833", "mrqa_searchqa-validation-7803", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-13731", "mrqa_searchqa-validation-15266", "mrqa_searchqa-validation-958", "mrqa_searchqa-validation-13250", "mrqa_searchqa-validation-10678", "mrqa_searchqa-validation-11878", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-5212", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-4050", "mrqa_naturalquestions-validation-6489", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-5511", "mrqa_hotpotqa-validation-2854", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-1701"], "SR": 0.546875, "CSR": 0.5512764084507042, "EFR": 1.0, "Overall": 0.7149427816901408}, {"timecode": 71, "before_eval_results": {"predictions": ["Funki Porcini", "119", "560", "VH1's \"100 Greatest Artists of Hard Rock\"", "Klasky Csupo", "the music genres of electronic rock, electropop and R&B", "the \"Home of the Submarine Force\"", "Luc Besson", "River Shiel", "\"Vera Cruz\"", "the Lommel differential equation", "Harry Booth", "Westland", "1.23 million", "Boston, Massachusetts", "281", "Northern Ireland", "1916 Easter Rising", "Park Hyung-Sik", "Conservative Party", "quantum mechanics", "gamecock", "Theo James Walcott", "April 8, 1943", "\"Traceless\" or \"Without a Trace\"", "their unusual behavior", "11", "Victoria Peak", "\"Back to December\"", "Volvo 850", "Hindi", "Statutory List of Buildings of Special Architectural or Historic Interest", "Genesee Brewing Company", "one child, Lisa Brennan-Jobs", "Fort Frederick", "Autopia", "Hindi", "Art Deco-style skyscraper", "Jon Walker", "\"Bad Blood\"", "Melesha O'Garro", "Green Chair", "Walker Smith Jr.", "Waimea Bay", "Juan Manuel Mata Garc\u00eda", "Umina Beach, New South Wales", "Mickey Mouse cup", "Kinnairdy Castle", "the Falkland Islands, and Peru", "Stephen James Ireland", "Lola Dee", "1924", "chromosomes", "the inner core and growing bud", "Helen of Troy", "Vince Cable", "3", "Jasmin Singer found herself face to face with a scientist who conducts animal testing,", "Sporting Lisbon", "Illness", "programming", "bowling", "Gin Rummy", "teeth"], "metric_results": {"EM": 0.59375, "QA-F1": 0.674671710762064}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.923076923076923, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-1397", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-1799", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-2678", "mrqa_hotpotqa-validation-1716", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5242", "mrqa_triviaqa-validation-2441", "mrqa_newsqa-validation-3127", "mrqa_searchqa-validation-12240", "mrqa_searchqa-validation-3082"], "SR": 0.59375, "CSR": 0.5518663194444444, "EFR": 0.9615384615384616, "Overall": 0.7073684561965812}, {"timecode": 72, "before_eval_results": {"predictions": ["U2 360\u00b0 Tour", "Brent Wilson", "Peter Kay's Car Share", "Michael Crawford", "Brazilian Jiu-Jitsu", "2012 Olympic bronze medalist", "CMYKOG process", "Colonel", "the first month of World War I.", "Germany", "the River Clyde", "George Washington Bridge", "Argand lamp", "Lowestoft, Suffolk", "The Ethics of Ambiguity", "Bishop's Stortford", "Rural Electrification Act", "Vitor Vieira Belfort", "Carlos Coy", "Hawai\u02bb i state District of Ko\u02bb olaupoko", "of Cuban descent", "35,000", "24 NCAA sports", "the Bahamian island of Great Exuma", "musical research", "Bonnie Franklin", "the Cheshire League Premier Division", "Kelly Bundy", "Canada's first train robbery", "Carson City", "Ben R. Guttery", "director-General", "Montreal", "New Zealand", "February 14, 1859", "Peel Holdings", "1692", "film", "Teddy Riley", "672 km2", "140 million", "Lamar Hunt", "Vienna", "Alemannic", "17", "SpongeBob SquarePants 4-D", "Seti I", "\"Agent Vinod\"", "Joseph E. Grosberg", "January 15, 1975", "2015", "the true horrors of human history derive not from orc and Dark Lords, but from ourselves", "energy loss", "Gautamiputra Satakarni", "Alexandrina", "Audi", "Valletta", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "Heshmatollah Attarzadeh", "U.S.-Mexico border", "New York City", "Turandot", "Champagne", "seabirds"], "metric_results": {"EM": 0.625, "QA-F1": 0.7308035714285714}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.5, 1.0, 0.5, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-1320", "mrqa_hotpotqa-validation-4088", "mrqa_hotpotqa-validation-1695", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-1260", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4489", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-6234", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-3555", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3199"], "SR": 0.625, "CSR": 0.5528681506849316, "EFR": 1.0, "Overall": 0.7152611301369862}, {"timecode": 73, "before_eval_results": {"predictions": ["Adonijah", "Poland", "Hillary Clinton", "Hannibal", "Elysium", "Birmingham", "syndicates", "Hansel", "J.M.W. Turner", "the Heisenberg Uncertainty Principle", "astronaut", "glockenspiel", "David Hockney", "Kyoto Protocol", "She has slept with every star in Hollywood, except Lassie", "Syrian", "Kansas City", "South Carolina was the first state to vote to secede", "Survivor Series", "taxis", "bell peppers", "piscinae", "Edward III", "Bruce Wayne", "lighting", "Tesco", "Cologne", "the northern prawn", "New York", "Nikola Tesla", "smartphones and similar devices to establish radio communication with each other by touching them together or bringing them into close proximity,", "Tennessee", "Grimbsy", "Delamere Way", "Robert Guerrero", "Virginia Plain", "Columbia", "Scotland", "Freema Agyeman", "Spanish", "plushing", "Medusa", "31 million men", "keninsky", "heavyweight", "ArcelorMittal Orbit", "Madness", "Mao Zedong", "Sweet Home Alabama", "Greece", "St Helens", "a valuable way to feed the poor, and would relieve some pressure of the land redistribution process", "required many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the", "a cake", "High Knob", "the Sacramento Kings", "200 Indians were killed", "African National Congress Deputy President Kgalema Motlanthe,", "British Prime Minister Gordon Brown's wife, Sarah, wore an outfit from designer", "near the village of Dara Bazar in the Bajaur Agency,", "campanile", "Bret Maverick", "Ronald McDonald House Charities", "#364"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5059862012987013}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0909090909090909, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9714285714285714, 0.19999999999999998, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4166", "mrqa_triviaqa-validation-3099", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-5315", "mrqa_triviaqa-validation-5341", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-6835", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-5672", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-6598", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-1673", "mrqa_triviaqa-validation-4776", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-5719", "mrqa_triviaqa-validation-614", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-4414", "mrqa_hotpotqa-validation-5878", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-3358", "mrqa_searchqa-validation-6760", "mrqa_searchqa-validation-1361"], "SR": 0.421875, "CSR": 0.551097972972973, "EFR": 1.0, "Overall": 0.7149070945945946}, {"timecode": 74, "before_eval_results": {"predictions": ["iPod Classic or... Shuffle.", "1-0 victory", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "Citizens", "Simon Cowell", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith", "1983", "bread", "journalists and aid workers", "are \"active athletes,\" far from couch potatoes,,", "the FBI.", "10,000", "his business dealings for possible securities violations", "Procol Harum", "California, Texas and Florida,", "morphine sulfate", "\"one of the most magnificent expressions of freedom and free enterprise in history\"", "Iran", "the Arab world to use the Internet for fun and not interfere with government and serious issues,", "Rawalpindi", "killed Lauterbach on December 14, 2007, and used her ATM card 10 days later before fleeing to Mexico.", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "peanuts", "North Korea", "Samoans", "syrian forest-firefighters", "Six", "the Irish capital.", "20-something woman at the tenteki 10 Caf\u00e9", "At least 13", "Whitney Houston", "Ferraris, a Lamborghini and an Acura NSX", "free fixes for the consumer.", "nine-wicket", "10 below", "Madhav Kumar Nepal", "has to move out of her rental house because it is facing foreclosure", "black is beautiful", "fifth", "Iran's parliament speaker has criticized U.S. President-elect Barack Obama for saying that Iran's development of a nuclear weapon is unacceptable.", "the Australian government", "JBS Swift Beef Company", "Hurricane Gustav", "U.S. troops", "murder", "Manny Pacquiao returned home to a hero's welcome in his native Philippines", "The American Civil Liberties Union", "1,073 immigration detainees", "flying", "Alfredo Astiz", "Los Angeles", "Super Bowl VIII", "3", "Jason Momoa", "sharpening steels", "Edinburgh", "true", "Norbertine", "Anheuser-Busch", "Beauty and the Beast", "the Chiefs", "Cleopatra", "Philip", "Sparafucile"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5715001007764167}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 0.9473684210526316, 1.0, 0.0, 0.3636363636363636, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.33333333333333337, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1904761904761905, 0.0, 0.7272727272727273, 1.0, 0.0, 0.33333333333333337, 0.3076923076923077, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-2101", "mrqa_naturalquestions-validation-288", "mrqa_triviaqa-validation-3067", "mrqa_searchqa-validation-14806"], "SR": 0.453125, "CSR": 0.5497916666666667, "EFR": 1.0, "Overall": 0.7146458333333333}, {"timecode": 75, "before_eval_results": {"predictions": ["1-1 draw", "Ten South African ministers and the deputy president", "Herman Cain", "demolishing American third seed Venus Williams in the final of the Sony Ericsson Open in Miami", "voluntary involuntary after witnesses identified him and he was interviewed by police.", "The Kirchners", "Sharon Bialek", "Roy Foster", "March 24,", "African-Americans", "Bill Haas", "David Beckham", "Rwanda", "Caster Semenya", "nuclear weapon.", "$627,", "Turkish President Abdullah Gul.", "People Against Switching Sides (PASS)", "sedative.", "26", "Bob Dole,", "White Hills, Arizona,", "the assassination of President Mohamed Anwar al-Sadat", "Australian Environment Minister Peter Garrett", "\"nation-building\" projects.", "Josef Fritzl", "Veracruz, Mexico,", "Sharon Bialek", "About 100,000 workers", "was to have spent a 30-day adaptation period in the United States before his father gained full custody.", "San Simeon, California,", "don't walk -- run -- to your nearest mental health professional.", "Fiona Mac Keown said she did not believe he was the man who killed her daughter.", "2001", "\"Toy Story\" in 1995,", "a violent government crackdown seeped out.\"", "43 percent", "Jezebel.com's Crap E-mail From A Dude", "Kenner, Louisiana", "Saturday", "that the deadly attack on India's financial capital last month was planned inside Pakistan,", "in 2006 and 2007", "Hong Kong's Victoria Harbor", "Marco Polo", "seemed to have a smile on her face when her kids were around.\"", "71 percent of Americans consider China an economic threat to the United States,", "\"Oprah: A Biography,\"", "the Bronx.", "Idriss Deby hopes the journalists and the flight crew will be freed,", "in 1994.", "murder in the beating death of", "February 7, 2018", "1439", "Blue laws", "Siddhartha", "Prince Bumpo", "Nitrogen", "Araminta Ross", "ABC1 and ABC2", "G\u00f6tene in Sweden", "the kitchen sink", "whey", "to the Young Men of Italy", "roosevelt"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5695421918767507}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.14285714285714285, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8235294117647058, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-801", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-4014", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-406", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-177", "mrqa_searchqa-validation-7856"], "SR": 0.484375, "CSR": 0.5489309210526316, "EFR": 1.0, "Overall": 0.7144736842105264}, {"timecode": 76, "before_eval_results": {"predictions": ["cricket", "Alan Greenspan", "Bolivia", "Matalan", "Ub Iwerks", "Macbeth Soliloquy", "German Chancellor Angela Merkel", "Monopoly", "transsexual", "yellow", "the chest", "doubles", "Paul Gauguin", "Ben Jonson", "a parallelogram", "Willy Lott", "19 years and 28 days old", "Abu Dhabi", "lamas", "14", "lice", "palladium", "NASA's Hubble Space Telescope", "James Van Allen", "Rawalpindi", "Mexico", "Philip Glenister", "Miss Prism", "Sir William Hamilton", "Beethoven", "Haystacks", "in late 2016", "Margaret Thatcher", "Mauricio Pochettino", "the Instrument of Surrender", "Sensurround", "lamas", "Olympic Games", "Blue Ivy Carter", "Rihanna", "Tripoli", "asia", "Eva Per\u00f3n", "Doctor Who", "pink", "lincoln Center Theater's Vivian Beaumont.", "Illinois", "The Magnificent Seven", "Uranus", "typhoid fever", "Ross MacManus", "September 9, 2012", "Santa Fe, New Mexico, USA", "seven", "Ricky Marco", "American pharmaceutical company", "Queens, New York", "1-1 draw", "Alwin Landry's supply vessel Damon Bankston", "intricate Flemish tapestries in an east-facing sitting room called the Morning Room.", "The Big Sleep", "Dairy Queen", "Paul", "Confederate victory"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6815273268398269}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-2144", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-4855", "mrqa_triviaqa-validation-1504", "mrqa_triviaqa-validation-2828", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-4242", "mrqa_triviaqa-validation-2047", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-6179", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-2333", "mrqa_naturalquestions-validation-4746", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2631", "mrqa_naturalquestions-validation-767"], "SR": 0.609375, "CSR": 0.5497159090909092, "EFR": 1.0, "Overall": 0.7146306818181818}, {"timecode": 77, "before_eval_results": {"predictions": ["nearly $162 billion in war funding", "poor.", "183 people, including 137 children,", "\"I reject this course because it sets goals that are beyond what we can achieve at a reasonable cost, and what we need to forge a lasting friendship in which America is your partner, and never your patron.\"", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Transport Workers Union leaders", "said the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "President Bush", "November 26,", "Wednesday.", "change course", "gun charges", "famous for its celebrity clientele.", "off the coast of Dubai", "his past and his future", "Spc. Megan Lynn Touma,", "Hussein's Revolutionary Command Council.", "fast-food chain", "$1,500", "Black History Month", "tenement in the Mumbai suburb of Chembur,", "Monday.", "weren't taking it well.", "The island's dining scene", "22", "foyer of the BBC building in Glasgow, Scotland", "Gov. Mark Sanford,", "President Sheikh Sheikh Ahmed", "Graham's wife", "don Draper", "two", "ambassadors", "fritter his cash away on fast cars, drink and celebrity parties.", "Wigan Athletic", "criminals", "The Ministry of Defense", "civilians,", "21-year-old", "Ignazio La Russa", "Passers-by", "Pixar's", "NATO fighters", "New York City Mayor Michael Bloomberg", "A Colorado prosecutor", "16 Indiana National Guard soldiers", "1940's", "The Maraachlis' daughter, Zeina,", "a member of the self-styled revolutionary Symbionese Liberation Army", "South Africa", "\"I am sick of life -- what can I say to you?\"", "a tanker", "The onset of rigor mortis and its resolution partially determine the tenderness of meat", "Walter Pauk", "Louis Prima", "Alanis Morissette", "usus", "madonna", "February 16, 1944", "Fort Saint Anthony", "Fall Out Boy", "fractions", "Vermont", "Kiribati", "Jay Van Andel"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6473435418747919}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true], "QA-F1": [0.1818181818181818, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.9714285714285714, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.3076923076923077, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-162", "mrqa_newsqa-validation-780", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1145", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-6088", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-1874", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-13106"], "SR": 0.5625, "CSR": 0.5498798076923077, "EFR": 1.0, "Overall": 0.7146634615384615}, {"timecode": 78, "before_eval_results": {"predictions": ["the test results by the medical examiner's office,", "the bombers on their 13-hour flight over the Arctic Ocean and the Atlantic.", "\"We miss having his love and compassion in our home,\"", "isabella", "5 1/2-year-old son, Ryder Russell,", "finance", "gun charges", "forgery and flying without a valid license,", "in the Willamette Valley to the Pacific coast.", "no", "the peace with Israel", "two", "70,000", "rural California,", "Sabina Guzzanti", "650", "on Expedia.", "i report", "Virgin America", "Italian government", "54,", "sandbags lines", "four decades", "Damon Bankston", "The e-mails", "papillomavirus", "the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "two tickets to Italy", "Dr. Maria Siemionow,", "Hillary Clinton", "Barack Obama:", "NATO member states, Russia and India,", "assassination of", "kill members of the Zetas,", "June 6, 1944,", "Orbiting Carbon Observatory,", "black civil rights leaders and prominent Democrats have largely bitten their tongues,", "the Airbus A330-200", "divorced Goldman and married a Brazilian lawyer.", "poems telling of the pain and suffering of children", "be silent.", "Tamil", "Adidas", "2,700-acre sanctuary in rural Tennessee.", "a drug lord with ties to paramilitary groups,", "can indeed help people with irritable bowel syndrome,", "martial arts,", "a student who admitted to hanging a noose in a campus library,", "543 elected members, of which 58 are women.", "Barack Obama", "The Louvre", "Bay of Plenty, Taupo and Wellington,", "Rihanna", "Ute name for them, k\u0268mantsi ( enemy )", "johnson", "the Count Basie Orchestra", "Billy Cox", "wooden", "Yoruba gods and goddesses", "boxer", "chrysanthemums", "john Wesley", "Colonel Sanders", "depicting multiple alternative realities rather than a novel"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5356553564278148}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false], "QA-F1": [0.5, 0.18181818181818182, 0.11764705882352941, 1.0, 0.0, 0.5, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.5, 0.64, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.6956521739130436, 1.0, 0.5, 0.2222222222222222, 0.6666666666666666, 0.5, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3030", "mrqa_newsqa-validation-3240", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2609", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-5101", "mrqa_hotpotqa-validation-5784", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12910", "mrqa_naturalquestions-validation-2729"], "SR": 0.390625, "CSR": 0.5478639240506329, "EFR": 1.0, "Overall": 0.7142602848101266}, {"timecode": 79, "before_eval_results": {"predictions": ["one of popular music's most poignant anthems of sorrow regarding the environment", "in 1989", "Bill Irwin", "Hon July Moyo", "glycine and arginine", "38 - 7", "12951 / 52 Mumbai Rajdhani Express", "the Rolling Stones", "A Christmas Story", "In 2010", "2018", "in 1975", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "David Tennant", "the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "the Canadian Rockies continental divide east to central Saskatchewan", "Deuteronomy 5 : 4 -- 25", "the President", "John Musker", "Andy Cole", "each team", "an armed conflict without the consent of the U.S. Congress", "Sauron", "1775", "Supplemental oxygen", "the level of the third lumbar vertebra, or L3, at birth", "have had sovereignty over some or all of the current territory of the U.S. state of Texas", "103", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Reba McEntire and Linda Davis", "New Mexico", "Karen Gillan", "1832", "SI joint", "cut off close by the hip, and under the left shoulder", "Arkansas", "Mickey Rourke", "A rotation is a circular movement of an object around a center ( or point ) of rotation", "Ron Harper", "Number 4, Privet Drive, Little Whinging in Surrey, England", "a blend of ground beef and other ingredients and is usually served with gravy or brown sauce", "Sunday evenings", "Santa Clara Pueblo, New Mexico, USA", "a major fall in stock prices", "April 1979", "Reverend J. Long", "2005", "`` save, rescue, savior ''", "as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Mike Alstott", "Nicole Gale Anderson", "10", "At Kineton, near Banbury, in Oxfordshire", "James Taylor", "as the eastern border of the re-emerging sovereign Republic following the century of partitions", "goalkeeper", "Emancipation Proclamation", "4.6 million", "executive director of the Americas Division of Human Rights Watch,", "Pope Benedict XVI", "Afghanistan", "Erie Canal", "blackbuck antelope,", "the Black Sea"], "metric_results": {"EM": 0.5, "QA-F1": 0.6405805154689451}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [0.42857142857142855, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6451612903225806, 1.0, 0.7692307692307692, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6086956521739131, 1.0, 1.0, 0.0, 0.16666666666666666, 0.8571428571428571, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5, 0.16666666666666666, 1.0, 1.0, 0.125, 0.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.12500000000000003, 1.0, 1.0, 1.0, 0.5, 0.6956521739130435, 0.0, 1.0, 0.0, 0.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-4108", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-2813", "mrqa_triviaqa-validation-752", "mrqa_triviaqa-validation-5693", "mrqa_hotpotqa-validation-4599", "mrqa_searchqa-validation-10525"], "SR": 0.5, "CSR": 0.547265625, "EFR": 0.9375, "Overall": 0.701640625}, {"timecode": 80, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12910", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3139", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1535", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.8046875, "KG": 0.50703125, "before_eval_results": {"predictions": ["prevent any contaminants in the sink from flowing into the potable water system by siphonage", "all - female", "late - September", "Moscazzano", "Bachendri Pal", "the Finch family's African - American housekeeper", "1994", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "Joe Pizzulo and Leeza Miller", "19th - century India", "British", "Jenny Slate", "Gunpei Yokoi", "January 15, 2010", "One Son", "leaves of the plant species Stevia rebaudiana", "Julie Deborah Kavner", "Vincent Price", "infection", "Jewel Akens", "the surname Watson ( `` Wat's son '' )", "March 11, 2018", "Sauron", "12", "Frank Langella", "Number 4, Privet Drive, Little Whinging in Surrey, England", "shared", "Southport, North Carolina", "John C. Reilly", "Wednesday, September 21, 2016", "Washington metropolitan area", "in the UK", "1885", "Kevin Kline", "the Vital Records Office of the states, capital district, territories and former territories", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "Monk's Caf\u00e9", "20 locations all within the Pittsburgh metropolitan area", "Spanish", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "butch or Killer", "Norway", "elected", "1997", "Brooks & Dunn", "12 November 2010", "the homicidal thoughts of a troubled youth", "W. Edwards Deming", "convergent plate boundary", "Sylvester Stallone", "1967", "Harrods, London", "Ottorino Respighi", "Steve Coogan", "Boston University", "the Bay of Fundy", "Lincoln Riley", "The Rosie Show,\"", "Tomas Olsson, the journalists' Swedish attorney.", "2,000 euros ($2,963)", "dishwasher", "letter", "(Albert) Einstein", "wheezing"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6560621879800308}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.4, 1.0, 0.625, 0.5, 0.4, 1.0, 1.0, 0.967741935483871, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2842", "mrqa_triviaqa-validation-435", "mrqa_hotpotqa-validation-4160", "mrqa_newsqa-validation-2045", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-9418"], "SR": 0.5625, "CSR": 0.5474537037037037, "EFR": 0.9642857142857143, "Overall": 0.7143010085978837}, {"timecode": 81, "before_eval_results": {"predictions": ["Jason Momoa", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "1599", "by June 1945", "Lafayette", "April 3, 1973", "5 liters", "Diane", "from 13 to 22 June 2012", "2.5 %", "22", "late November or early December", "Guwahati", "Nala", "1979 / 80", "2017", "Julius Caesar", "Leslie", "electron shells", "drizzle, rain, sleet, snow, graupel and hail", "compasses", "production -- possibility frontier ( PPF ) or production possibility curve ( PPC )", "1987", "in the eye", "Eagle Ridge Outdoor pool in Coquitlam, BC", "gave the German Cabinet -- in effect, Chancellor Adolf Hitler -- the power to enact laws without the involvement of the Reichstag", "Charlton Heston", "the fourth ventricle", "October 27, 2017", "John Cooper Clarke", "Saturday", "Cheryl Campbell", "Spanish", "~ 0.058 - 0.072 mm", "February 16, 2016", "Jeff Bezos", "Erica Rivera", "warm and short with an average high of 23 \u00b0 C ( 73 \u00b0 F ) and overnight lows of 14 \u00b0 C", "January 2, 1971", "Tracy McConnell", "gastrocnemius", "Parker's pregnancy at the time of filming", "4", "the Turco - Mongol Timurid dynasty of Central Asia", "1830", "Frank Oz", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Celtic", "Guant\u00e1namo or GTMO", "Morgan Freeman", "The Lightning Thieves", "in the muscle tissue", "Throw-Darts", "Jewish Learning", "Katarina Witt", "Bhaktivedanta Manor", "Lowe's Companies, Inc.", "Honduran", "Al-Shabaab,", "Columbian mammoth fossil \"Zed.\"", "flee", "hateful", "churrasco", "refrigerator"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5490321744227995}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.375, 0.0, 0.0, 1.0, 0.5, 0.2857142857142857, 0.0, 0.9090909090909091, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666667, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.8, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-2144", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-5214", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5185", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-2449", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-1512", "mrqa_searchqa-validation-7442", "mrqa_searchqa-validation-12256"], "SR": 0.421875, "CSR": 0.545922256097561, "EFR": 0.972972972972973, "Overall": 0.7157321708141067}, {"timecode": 82, "before_eval_results": {"predictions": ["whittling", "Hans Christian Andersen", "swing", "Charles Lindbergh", "sucrose", "T.S. Eliot", "Superman Returns", "Nokomis", "Yale", "tidal streams", "The Nutcracker", "Over the hifls", "circumnavigate", "Maine", "tarzan", "L.I.", "manx", "rum", "Baroque", "pterodactyl", "lollipop", "Dracula", "skating", "Sweden", "War and Peace", "Hannah Montana", "Van Allen", "Mitch McConnell", "bravery or valor", "the gallbladder", "\"Invisibility\"", "Himalaya", "Chile", "Sri Lanka", "the St. Valentine's Day Massacre", "Saturday Night Live", "Sayonara", "Oakland", "The Taming of the Shrew", "E", "Andrew Johnson", "the knee", "NASA", "Gavin MacLeod", "a snake", "The Count of Monte Cristo", "American sitcom", "Equus", "Wyandotte", "denton True Young", "sondheim", "December 27, 2015", "digital transmission modes", "Andy Cole", "yellow", "Galileo", "harry", "Afro-American", "Midtown Manhattan in New York City", "Awake", "Ferraris,", "not a project for commercial gain.", "South Africa.", "U.S. state of Washington"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5510416666666667}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-5014", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-13546", "mrqa_searchqa-validation-11022", "mrqa_searchqa-validation-8553", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-3227", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-10329", "mrqa_searchqa-validation-9510", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-14521", "mrqa_searchqa-validation-14081", "mrqa_searchqa-validation-8439", "mrqa_searchqa-validation-6144", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-2222", "mrqa_triviaqa-validation-1023", "mrqa_hotpotqa-validation-5173", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3226", "mrqa_naturalquestions-validation-3281"], "SR": 0.484375, "CSR": 0.5451807228915663, "EFR": 1.0, "Overall": 0.7209892695783132}, {"timecode": 83, "before_eval_results": {"predictions": ["the Coca-Cola Company", "Oklahoma State", "spoiled", "Pippin", "Georgia", "the Chesapeake Bay", "a dugout", "cement", "underfloor heating", "(John) Cusack", "James Fenimore Cooper", "Out of Africa", "( Leon Uris) Uris", "potato chip", "the Bay of Bengal", "the Clark bar", "laxis", "Dresden", "John Ashcroft", "Phil of the Future", "Newman", "Death Valley", "rings", "to pick up and hold until released", "George Eliot", "Fly Like an Eagle", "the Twinkle Tales", "Waylon Jennings", "jaded", "Sgt. Pepper's Lonely Hearts Club Band", "palindrome", "a trapezoid", "Scrubs", "Henrik Ibsen", "Chastity", "Canticle", "Friedrich Nietzsche", "(Rodney) King", "Halloween", "Henry Higgins", "Tanzania", "Coca-Cola", "(Deepak) Chopra", "Rudolf Hess", "Siberia", "Rings Twice", "Sylvester Stallone", "(Edna Ferber) Rodgers", "Etch A Sketch", "safari", "3.69-Carat diamond", "During his epic battle with Frieza", "1997", "The stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "(Carl Wilhelm) Scheele", "Granada", "independent music", "1940s and 1950s", "Pulitzer Prize for Drama", "York County", "six", "Michelle Rounds", "Derek Mears", "Adidas"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6682324016563147}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.34782608695652173, 0.0, 1.0, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2363", "mrqa_searchqa-validation-12304", "mrqa_searchqa-validation-11933", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-16183", "mrqa_searchqa-validation-2282", "mrqa_searchqa-validation-8962", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-15025", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-16081", "mrqa_searchqa-validation-39", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-16469", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-15780", "mrqa_searchqa-validation-15171", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-11096", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-2381", "mrqa_hotpotqa-validation-5309"], "SR": 0.578125, "CSR": 0.5455729166666667, "EFR": 1.0, "Overall": 0.7210677083333333}, {"timecode": 84, "before_eval_results": {"predictions": ["Portugal", "nor\u00f0rvegr", "daisy", "John Galsworthy", "Belfast", "W W Jacobs", "the Andaman & Nicobar Islands", "the Trasks", "John Buchan", "Doncaster Rovers", "Anne", "Yokohama", "9", "Supertramp", "Sky", "Joanne Harris", "abacus", "Eriksson", "buffalo", "acceleration", "James Valentine", "Grittar", "a moon", "joseph", "White spirit", "aglet", "lemurs", "Ontario", "Fabio Capello", "Mickey Mouse", "cricket", "1973", "William Neil Connor", "Azerbaijan", "logic", "Spain", "Ferdinand Volkswagen", "Chief Inspector of Prisons", "Moulin Rouge", "golf", "a dog", "Robert Devereux", "Hamelin", "joseph", "George Osborne", "oxygen", "Toyota", "a snakes", "Amethyst", "hairdresser", "Antony", "Jethalal Gada", "Spektor", "San Francisco", "F\u00fchrer", "4 km", "Anaheim, California", "a depth of about 1,300 meters in the Mediterranean Sea.", "Afghanistan,", "Deutschneudorf,", "a chimp", "tin", "The Usual Suspects", "repel bullets"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5975183823529412}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.5, 0.5, 0.6666666666666666, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-4027", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-6937", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-5457", "mrqa_triviaqa-validation-6666", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4166", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2685", "mrqa_newsqa-validation-796", "mrqa_searchqa-validation-1820", "mrqa_naturalquestions-validation-2309"], "SR": 0.515625, "CSR": 0.5452205882352941, "EFR": 0.967741935483871, "Overall": 0.714545629743833}, {"timecode": 85, "before_eval_results": {"predictions": ["the Grand Harbour", "Eurasia", "Tom Ewell", "American", "9,000", "My Beautiful Dark Twisted Fantasy", "secondary school study", "Shut Up", "30.9%", "William Shakespeare", "Nic Cester", "the Kingdom of Morocco", "Rocky Mountain goat", "Prince George's County", "Ariel Ram\u00edrez", "The Apple iPod+HP", "four months in jail", "Objectivism", "Vixen", "Rogue One: A Star Wars Story", "Stephen Crawford Young", "Michael Stipe", "League of the Three Emperors, or \"Dreikaiserbund\"", "Baron Cherwell", "Sunflower County", "Alexandre Dimitri Song Billong", "Dutch", "Rabies", "Malayalam", "Sim Theme Park", "Outside", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present.", "Sri Lanka Freedom Party", "Jennifer Aniston", "the North Atlantic Conference", "Sarah Newlin", "Downtown", "Rockstar San Diego", "Strato of Lampsacus", "Columbus Crew SC", "Beno\u00eet Jacquot", "the Manor of More", "Anita Dobson", "500-room", "Rajmund Roman Thierry Pola\u0144ski", "\"Discovery\" on STS-51-C.", "\"Orchard County\"", "Saint Michael, Barbados", "Championnat National 3", "Boston, Massachusetts", "The King of Chutzpah", "one person", "anion CH CO, or CH COO", "Gibraltar", "The Seine", "jimmy australia", "daltonism", "Virgin America", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Zelaya", "a judgment", "Japan", "jimmy Stalin", "Rupert\\'s Land"], "metric_results": {"EM": 0.703125, "QA-F1": 0.770811795112782}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-2506", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-2372", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-1202", "mrqa_triviaqa-validation-5716", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-2592", "mrqa_searchqa-validation-5450"], "SR": 0.703125, "CSR": 0.5470566860465116, "EFR": 1.0, "Overall": 0.7213644622093023}, {"timecode": 86, "before_eval_results": {"predictions": ["California", "the image", "any unfavourable and unintended sign ( including an abnormal laboratory finding ), symptom, or disease temporally associated with the use of a medicinal ( investigational ) product", "18 - season", "December 24, 1836", "the bank", "Carroll O'Connor", "20 years from the filing date", "Italian mezzadria", "the 1940s", "American production duo The Chainsmokers", "Steve Russell", "the President pro tempore", "Donna", "the Dutch", "turlough", "Oahu, Hawaii", "Elected Emperor of the Romans", "9 February 2018", "1933", "the eighth episode of Arrow's second season", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "supervillains who pose catastrophic challenges to the world", "two", "the Charbagh structure", "Elizabeth Dean Lail", "password recovery tool for Microsoft Windows", "semi-autonomous", "May 1979", "13 to 22 June 2012", "60", "electron donors", "in the pouring rain", "monitor lizards", "Abbot Suger", "the Mahalangur Himal sub-range of the Himalayas", "The Royalettes", "the winter solstice", "State Bar of Arizona", "Per Gessle", "Geothermal gradient", "2001", "Hellenismos", "Urge Overkill", "2000", "blue", "the Mishnah", "1078", "April 1979", "around 1872", "Atlanta, Georgia", "glagolitic", "California", "\"Little Red Rented Rowboat\"", "France", "American rapper", "November 10, 2017", "jobs", "Buenos Aires", "France's famous Louvre", "arbutus", "Dragnet", "Harry Potter", "Fairfax County"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5421317740382866}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 0.9387755102040816, 1.0, 0.6666666666666666, 0.0, 1.0, 0.058823529411764705, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-916", "mrqa_triviaqa-validation-3891", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3197", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3536", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2614", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-1911"], "SR": 0.453125, "CSR": 0.5459770114942528, "EFR": 0.9142857142857143, "Overall": 0.7040056701559934}, {"timecode": 87, "before_eval_results": {"predictions": ["About $10 billion have gone into the accelerator's construction, the particle detectors and the computers,", "don Draper", "President Obama", "Two pages -- usually high school juniors who serve Congress as messengers", "Marcell J Hansen", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "nuclear warheads", "California-based Current TV", "riders a 19th-century experience on cruises complete with the carnival-like sounds of the steam-whistle calliope.", "a city of romance, of incredible architecture and history.", "peanuts, nuts, shellfish and fish", "Piers Morgan Tonight", "Dubai", "in the St. Louis suburb of Columbia, Illinois,", "jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "free services.", "Steven Chu", "Mark Obama Ndesandjo", "Hayden", "30,000", "the Southern California glam-rocker Adam Lambert", "Ashura.", "$17,000", "250,000 unprotected civilians", "Theikini rocketed to fame in 1960 with Brian Hyland's hit single, \"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "the Juarez drug cartel.", "Muslim festival", "the Indian Ocean waters near the Gulf of Aden,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Marcus Schrenker,", "150", "cancer", "the U.S. Chamber of Commerce", "a Muslim with Lebanese heritage,", "South Korea's new president", "an open window", "Jiverly Wong,", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "bronze", "98", "President Obama", "Stratfor, a global intelligence company,", "Cologne, Germany,", "that former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq and that a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "Arkansas", "five minutes before commandos descended from ropes that dangled from helicopters,", "intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "Mary Procidano", "The son of Gabon's former president", "Samoa", "Derek Mears", "January to May 2014", "Frederick County", "RMS Titanic", "calcium carbonate", "The Great Leap", "India", "Eli Manning", "CBS", "\"Dr. Gr\u00e4sler, Badearzt\"", "pinniped", "glaucoma", "Carl Sandburg", "holly"], "metric_results": {"EM": 0.515625, "QA-F1": 0.588586266504465}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false], "QA-F1": [0.2857142857142857, 1.0, 0.0, 0.3076923076923077, 0.4, 0.6153846153846153, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4000000000000001, 1.0, 1.0, 0.0, 0.5925925925925926, 0.5, 1.0, 0.0, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.17647058823529413, 0.0, 0.625, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-343", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-931", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-5141", "mrqa_searchqa-validation-16173", "mrqa_searchqa-validation-5382"], "SR": 0.515625, "CSR": 0.5456321022727273, "EFR": 1.0, "Overall": 0.7210795454545454}, {"timecode": 88, "before_eval_results": {"predictions": ["Caylee,", "at least 18 federal agents and two soldiers", "$22 million", "The federal officers' bodies", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "the area of the 11th century Preah Vihear temple", "police to question people if there's reason to suspect they're in the United States illegally.", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "and Jquante Crews,", "Chad", "poems", "social media networks", "Cleve Landsberg,", "Saturday", "a Columbian mammoth", "Afghan security forces and government.", "genocide, crimes against humanity, and war crimes.", "in July 1999,", "in the county jail in Spanishfork,", "192,000", "forged credit cards and identity theft", "seven", "an independent homeland since 1983.", "Miguel Cotto", "South Africa", "seven", "to best your own fuel economy achievements,\"", "returning combat veterans", "scientific reasons.", "dismissed all charges", "United States, NATO member states, Russia and India", "75", "Ameneh Bahrami", "will not support the Stop Online Piracy Act,", "Ma Khin Khin Leh,", "12 brutal rounds", "The Rosie Show,\"", "The station", "Larry Zeiger", "The son of Gabon's former president", "United States is providing weapons and ammunition to Somalia's transitional government as it fights al Qaeda-linked Islamist militants.", "Seoul.", "part of the proceeds", "longest domestic torch relay in the games' history,", "vitamin injections that promise to improve health and beauty.", "Donald Trump.", "cartel members blocked roads with hijacked vehicles Thursday and Friday to prevent military reinforcements from arriving.", "tabloid Web site TMZ.", "Indonesian", "Robert Park", "At least 14 bodies", "late 2018 or early 2019", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "2,050 metres ( 6,730 ft )", "cutis anserina", "the Battle of Agincourt", "the Esmeralda's Barn night  club", "a Belgian former footballer", "Geelong Football Club", "Richard L. Thompson", "a fisheye lens", "a clavichord", "orchids", "John Knox"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5822288976209212}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true], "QA-F1": [0.6666666666666666, 0.888888888888889, 1.0, 1.0, 0.1111111111111111, 0.0, 0.08, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.8, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 0.16666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.4347826086956522, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-999", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-3468", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-306", "mrqa_newsqa-validation-3162", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-1586", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-798", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-8794", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-277", "mrqa_hotpotqa-validation-2982", "mrqa_searchqa-validation-14968", "mrqa_searchqa-validation-2827"], "SR": 0.453125, "CSR": 0.5445926966292135, "EFR": 1.0, "Overall": 0.7208716643258427}, {"timecode": 89, "before_eval_results": {"predictions": ["Ariel Binns", "Ignazio La Russa", "gathering information about the rebels to give to the Colombian military.", "collaborating with the Colombian government,", "ownership of the Falklands.", "2000", "$24.1 million,", "\"Gas Cities LLC,\"", "Chadian President Idriss Deby", "Maersk Line Ltd.", "two hunters -- one of whom witnessed the attack", "U.S. State Department and British Foreign Office", "the hunt for Nazi Gold and possibly the legendary Amber Room", "U.S. President-elect Barack Obama", "control and censorship", "News of the World tabloid.", "Too many glass shards left by beer drinkers in the city center,", "not for sale,", "blind Majid Movahedi,", "fear of losing their licenses to fly.", "Jenny Sanford,", "Kurdish militant group", "a vast settlement of people left without loved ones, without homes, without life's belongings.", "\"falling space debris,\"", "Wilhelmina Kids,", "France's", "581 points", "Robert Barnett,", "Mexican military", "14", "Venezuela", "41,", "Wednesday at the age of 95.", "Idriss Deby hopes the journalists and the flight crew will be freed,", "the report should spur U.S. diplomacy to prevent Iran from developing nuclear weapons.", "\"illegitimate.\"", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "2-1", "Haeftling,", "Saturday.", "five minutes before commandos descended", "a severely disfigured woman", "cell phones are valuable contraband, fetching a greater asking price from convicts than some shipments of illegal drugs.", "homicide by undetermined means,", "forcibly drugging", "for the rest of the year", "July", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Brazil", "Glasgow, Scotland", "Casey Anthony,", "2006 -- 06", "member", "John Vincent Calipari", "New Zealand", "Jessica Simpson", "fox hunting", "Los Angeles", "Samsung Galaxy S7", "Hilux pickup truck", "eggshells", "Manchester", "Gertrude Stein", "Dick Van Dyke"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5803381539824555}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.15384615384615383, 0.125, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.6153846153846153, 1.0, 0.0, 0.05714285714285715, 1.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.8235294117647058, 0.07142857142857144, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19047619047619047, 0.4, 0.25, 1.0, 0.6666666666666666, 0.42857142857142855, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1499", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-299", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-2011", "mrqa_naturalquestions-validation-5602", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5380"], "SR": 0.453125, "CSR": 0.5435763888888889, "EFR": 1.0, "Overall": 0.7206684027777778}, {"timecode": 90, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-342", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8553", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.826171875, "KG": 0.51328125, "before_eval_results": {"predictions": ["eels", "greece", "Stevie Wonder", "Sir Tom Finney", "Jackson Pollock", "the Black Sea", "Jumping Jack Flash", "jimmy priestley", "Dumbo", "New Zealand", "Call for the Dead", "Bill Bryson", "Hans Christian Andersen", "Christian Louboutin", "Vespa", "naked", "Laputa", "a Hungarian Horntail", "Jumanji", "Flo Rida", "at", "The Princess bride", "a cursor", "a pig", "Dancing With the Stars", "Australia", "Leicester", "a whelner", "Andr\u00e9s Iniesta", "bATH", "in 1924", "a band of light formed by stars in the cosmic plane", "Duty Free", "Mark Twain", "fruit", "carbon", "Caernarfon", "khalifa Abdullah", "Johnny Mathis", "Sergio Garcia", "Chad", "Arthur", "Yulia Tymochenko", "E. Nesbit", "Charles of Leiningen", "Henry Cabot Lodge, Jr.", "Sheree Murphy", "Louis", "His Majesty\u2019s Airship R34", "Yukon", "\"our MUTUAL FRIend\"", "Chlorofluorocarbons", "The tower has three levels for visitors, with restaurants on the first and second levels", "the coffee shop Monk's", "Mani", "DreamWorks Animation", "Melbourne", "338", "Pakistan", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "the U.S. Marine Band", "John Adams", "Charles Feuilles d'automnes", "Austria"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6149003623188405}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.8571428571428571, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.08695652173913043, 0.8, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-2698", "mrqa_triviaqa-validation-5650", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3335", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-2111", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-4011", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1997", "mrqa_triviaqa-validation-2857", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-339", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-724", "mrqa_searchqa-validation-6163", "mrqa_searchqa-validation-8979", "mrqa_hotpotqa-validation-5203"], "SR": 0.53125, "CSR": 0.5434409340659341, "EFR": 0.9333333333333333, "Overall": 0.7054329784798534}, {"timecode": 91, "before_eval_results": {"predictions": ["dumbo", "Ernest Hemingway", "Switzerland", "Mexican orange blossom", "perry Mason", "england", "James I", "trapezium", "Canada", "my little 1937 Austin Seven Ruby Open Top Tourer", "seven", "Vancouver,", "Nigeria", "Switzerland", "the Union Gap", "england", "punky brewster", "european", "gin", "guitar", "gaffe", "New Zealand's north-west corner of the central business district", "Virginia", "dysmenorrhea", "pasta", "witch trials", "sailor", "my Favorite Martian", "plutocracy", "Bahrain", "Bosnia and Herzegovina", "Ace of Spades", "constantine communism", "China", "eyelids", "Venice", "New Zealand", "1973", "carol thatcher", "Brighton", "c.offical", "(Plautius) Lateranus", "Jimmy Carter", "joseph peter", "Argentina", "george", "sauce", "khrushchev", "arsenic", "joseph european", "htsborough", "May 2017", "mitosis", "Morgan Freeman", "Mike Fiers", "coca wine", "politician", "$41.1 million", "to stop the Afghan opium trade", "Trevor Rees,", "edvard gRIEG", "Buddhism", "(Thomas) Francis Eagleton", "water"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5732599431818182}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.12500000000000003, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6589", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-3942", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3775", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-7584", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-3062", "mrqa_triviaqa-validation-2032", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-7028", "mrqa_triviaqa-validation-1046", "mrqa_hotpotqa-validation-2210", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2183", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-11019"], "SR": 0.546875, "CSR": 0.5434782608695652, "EFR": 0.9655172413793104, "Overall": 0.711877225449775}, {"timecode": 92, "before_eval_results": {"predictions": ["murder, rape, conspiracy", "Karen Floyd", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "Krishna Rajaram,", "U.S. President-elect Barack Obama", "Kurt Cobain", "a delegation of American Muslim and Christian leaders", "a treadmill", "Current TV", "gun", "\"E! News\"", "1983", "nude beaches.", "tennis", "President Barack Obama,", "19", "Max Foster,", "military trials", "Tehran plans to use its program to build nuclear weapons.", "a lion Among Men.", "any indicators or signs that he was going to go off so drastically... how is some public safety officer supposed to recognize this person?\"", "surge", "to clean up Washington State's decommissioned Hanford nuclear site,", "delivers a big speech", "island's dining scene", "Saturday,", "warns business owners to close their shops during daily prayers, or they will be temporarily shut down,", "ice jam", "gasoline", "they did not receive a fair trial.", "2.5 million", "abducting each other for ransoms or retribution.", "Egypt", "about three minutes after launch early Tuesday,", "Barack Obama's", "iTunes Music Store,", "president Robert Mugabe", "her boyfriend,", "12.3 million", "ties", "three", "tote bag", "75 percent", "Hu Jintao", "1,500", "usually hot and humid", "two counts of murder.", "young self-styled anarchists", "first grand Slam,", "at a construction site in the heart of Los Angeles.", "Alberto Espinoza Barron's", "2013", "Vincenzo Peruggia", "April 1979", "Pelham One Two Three", "Hercule Poirot", "1997", "Debbie Reynolds", "43rd", "USS Essex", "Daylight Saving Time", "Amelia Earhart", "citric acid", "Pakistan"], "metric_results": {"EM": 0.53125, "QA-F1": 0.7054884453781513}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 0.0, 0.3529411764705882, 1.0, 0.0, 0.0, 0.9411764705882353, 1.0, 1.0, 0.6666666666666666, 0.5833333333333334, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.25, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.9333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2160", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2917", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1516", "mrqa_newsqa-validation-2476", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-4531", "mrqa_hotpotqa-validation-1316", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-13027"], "SR": 0.53125, "CSR": 0.5433467741935484, "EFR": 1.0, "Overall": 0.7187474798387097}, {"timecode": 93, "before_eval_results": {"predictions": ["David Beckham", "to reach beyond their individual capabilities and build a practical framework that could help the U.S. government better respond to threats of genocide", "March 8", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "Kearny, New Jersey.", "her husband had knocked her down, held a loaded gun to her head", "five minutes before commandos descended", "Democratic", "Kim Il Sung", "Roy Foster's", "South African's", "managing his time.", "U.S. Consulate in Rio de Janeiro,", "Majid Movahedi,", "Molotov cocktails,", "Facebook and Google,", "four university students and a safety officer", "customers are lining up for vitamin injections that promise to improve health and beauty.", "Oxbow,", "British Prime Minister Gordon Brown's", "those missing", "summer", "a vast settlement of people left without loved ones, without homes, without life's belongings.", "Friday,", "\"a whole new treasure trove of fossils\" on Wednesday.", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "producing rock music with a country influence.", "more than 78,000 parents", "\"You people don't make good CEO.\"", "Republican", "the underprivileged.", "March 24,", "North Korea,", "Mexican military", "Bill Haas", "devoted to federal ocean planning.", "killing of a 15-year-old boy", "Somalia's piracy problem was fueled by environmental and political events.", "Swat Valley,", "Australian Environment Minister Peter Garrett", "a crew of Grayback forest-firefighters", "Steven Gerrard", "three", "a bronze medal", "keeping our children from just this type of public exposure.\"", "the Beatles", "88", "Illinois Reform Commission", "use of torture and indefinite detention", "No. 4", "severe flooding", "Pastoral farming", "Kaley Christine Cuoco", "Hermann Ebbinghaus", "Route 66", "wagner", "Black Wednesday", "Cambridge University", "early 20th-century Europe", "The Beatles' 1966 US tour", "Mickey Mouse", "The daiquiri", "Israel", "UNESCO / ILO Recommendation concerning the Status of Teachers"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6659000231350776}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false], "QA-F1": [0.0, 0.07142857142857142, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.782608695652174, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.21428571428571427, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.33333333333333337, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 0.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_newsqa-validation-1910", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-4092", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3631", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-1437", "mrqa_searchqa-validation-6", "mrqa_naturalquestions-validation-7261"], "SR": 0.546875, "CSR": 0.5433843085106382, "EFR": 1.0, "Overall": 0.7187549867021276}, {"timecode": 94, "before_eval_results": {"predictions": ["Fourteen gunmen snatch Lunsmann in July while she was vacationing with her family on the island of Tictabon,", "23-year-old", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr,", "Thirty to 40 ships", "Larry Ellison,", "California, Texas and Florida,", "Alberto Espinoza Barron,", "review their emergency plans and consider additional security measures", "4,000", "Whitney Houston", "2nd Platoon, A Company, 2nd Light Armored Reconnaissance Battalion,", "Lillo Brancato Jr.", "United States, NATO member states, Russia", "publicist had no comment on his plans.\"", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "many riders say it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "$10 billion", "opium poppies,", "Casa de Campo International Airport in the Dominican Republic", "Pakistan's", "her dancing against a stripper's pole.", "homeless veterans and their entire family,\"", "CNN's Campbell Brown", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Iraqi Prime Minister Nouri al-Maliki", "Alfredo Astiz,", "full Senate Sotomayor,", "Majid Movahedi,", "Reid's dismissal,", "three", "1960.", "education and energy, innovation and infrastructure,", "students to engage in learning differently, enjoy a customized approach", "the first", "Tom Baer.", "Charlotte Gainsbourg and Willem Dafoe", "The public endorsement", "ties", "Barack Obama sent a message that fight against terror will once against honor some of the most cherished ideals of our republic:", "2005", "Two UH-60 Blackhawk helicopters", "Cirque du Soleil", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Ghana", "Christopher Savoie", "246 passengers and most of the crew aboard the MS Columbus, currently at the start of an around-the-world cruise,", "London's Waterloo Bridge", "Italian Serie A title", "in the last few months,", "84-year-old", "a nuclear weapon", "2020 National Football League ( NFL ) season", "John Cooper Clarke", "late 1980s", "Chicago", "kolkata", "painter", "MGM Grand Garden Special Events Center", "The Royal Navy (RN)", "Manchester United", "Bangkok", "roof", "Venice", "KXII"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6056047901751027}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true], "QA-F1": [0.125, 1.0, 0.625, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5625000000000001, 1.0, 1.0, 0.0, 0.1212121212121212, 1.0, 1.0, 0.125, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 0.9090909090909091, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-640", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-1131", "mrqa_naturalquestions-validation-8685", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-565", "mrqa_hotpotqa-validation-4069", "mrqa_searchqa-validation-13225"], "SR": 0.46875, "CSR": 0.5425986842105264, "EFR": 0.9705882352941176, "Overall": 0.7127155089009288}, {"timecode": 95, "before_eval_results": {"predictions": ["Aksel Sandemose", "pubs, bars and restaurants", "Retina display", "Chick tracts", "Julia Verdin", "uncle of Prince Philip, Duke of Edinburgh,", "to prevent the opposing team from scoring goals", "Graffiti", "ARY Digital Network", "Drifting", "Larry Wayne Gatlin", "9 October 19408 December 1980", "Key West, Florida", "its riverside location,", "Scottish", "Skipton", "Atlas ICBM", "PPG Paints Arena", "Kelly Bundy", "John Joseph Travolta", "Love Letter", "SBS", "AVN Adult Entertainment Expo", "119 minutes", "50 million", "Intelligent Design", "Sinngedichte", "Hidden America with Jonah Ray", "Irish", "Metro-Goldwyn-Mayer", "RAF Tangmere, West Sussex", "The Summer Olympic Games", "1692", "Art Deco-style skyscraper", "American professional baseball left fielder for the Miami Marlins", "25 November 2015", "Hyuna", "Mathieu Kassovitz", "Kentucky Music Hall of Fame", "Louis King", "Danish", "Indian", "two", "McComb, Mississippi", "D\u00e2mbovi\u021ba River", "24800 mi", "saint", "143,007", "Michael Edward \" Mike\" Mills", "American", "fourth", "2001", "a set of related data,", "to develop alternative approaches to the environmental determinism dominant at that time in ecological studies", "Roberta Flack", "Sir Walter Scott", "Braille", "Caster Semenya", "Heshmatollah Attarzadeh", "London's", "\"heroes\"", "Wizard", "the Erie Canal", "Andy Warhol"], "metric_results": {"EM": 0.671875, "QA-F1": 0.767263986013986}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.4, 1.0, 0.0, 0.33333333333333337, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-2013", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-8669", "mrqa_newsqa-validation-3651", "mrqa_searchqa-validation-9300"], "SR": 0.671875, "CSR": 0.5439453125, "EFR": 1.0, "Overall": 0.7188671875}, {"timecode": 96, "before_eval_results": {"predictions": ["\"Black Abbots\"", "Audrey Mossom", "Nicolas Winding Refn", "PBS", "Ed Lee", "James Victor Chesnutt", "Lu\u00eds Carlos Almeida da Cunha", "South Asia and the Middle East", "Martin Ingerman", "Rice University", "I", "Richard Price", "Reich Chancellery", "Kim So-hyun", "pubs, bars and restaurants", "Amal Clooney", "Lombardy", "1885", "Pac-12 Conference", "Lamar Wyatt", "Homer Hickam, Jr.", "Straits of Gibraltar", "Port Clinton", "Patrick Dempsey", "Issaquah", "Taylor Swift", "Appleby-in-Westmorland", "2000", "Mickey's Christmas Carol", "The American relay of Michael Phelps, Ryan Lochte, Peter Vanderkaay", "Orson Welles", "1902", "Brittany Snow", "northernmost province, Lapland", "Elvis' Christmas Album", "CD Castell\u00f3n", "Coronation Street", "Zimbabwe", "residential", "William Scott Elam", "1998", "Johnny Cash, Waylon Jennings", "Brad Silberling", "ITV", "1982", "the Marx Brothers film", "Tennessee", "Raabta", "James Worthy", "Harrods", "2007", "January 2004", "trees", "left - sided heart failure", "Karl Marx", "Laurence Olivier", "Indian Ocean", "Rodong Sinmun", "two-day", "Arizona", "Vatican City", "a porch", "Eric Knight", "to conclude a set of simple instructions or when a result is reached"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7057066197691197}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4444444444444445, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2033", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-4948", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1656", "mrqa_hotpotqa-validation-1006", "mrqa_hotpotqa-validation-3381", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7760", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-776", "mrqa_searchqa-validation-69", "mrqa_naturalquestions-validation-9071"], "SR": 0.609375, "CSR": 0.5446198453608248, "EFR": 1.0, "Overall": 0.719002094072165}, {"timecode": 97, "before_eval_results": {"predictions": ["New Orleans", "poker", "Budapest", "hoppin' John", "birds", "bass", "El Cid", "Vestal Virgins", "contract", "Akihito", "lead", "Israel", "Bach", "Nancy Astor", "Imperative mood", "the bald eagle", "high altitude", "Oslo", "a leap year", "Little Miss Muffet", "The Gila monster", "The Hague", "Zyrtec", "Buddhism", "Carson City", "Syria", "I AM", "the Council of Better Business Bureaus", "Linda Tripp", "a stationwagon", "Aqua Teen Hunger Force", "a moderate to maybe-just-a-little-bit-conservative man", "economics", "Korean War", "diseases", "Rocky Mountain Fever", "euros", "Lebanon", "typewriters", "Isadora Duncan", "Jaws 2", "Custer", "nag", "Homer", "Motor Trend", "United States", "Manhattan", "Naxos", "steel", "Xaymaca", "had 2 kids, Lizzie & Tommie", "Lord Banquo", "Theest train in India : 12951 / 52 Mumbai Rajdhani Express", "Paul Lynde", "John Part", "March 10, 1997", "Hubble Space Telescope", "Household Words", "Rockland County", "Trilochanapala", "Col. Sansern Kaewkumnerd said the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Turkey", "The first line of law and order", "Quintero"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6939980158730159}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.7142857142857143, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6111111111111112, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-10502", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-1155", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-16033", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-16243", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-12404", "mrqa_searchqa-validation-2730", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-10588", "mrqa_searchqa-validation-9423", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-2927", "mrqa_searchqa-validation-15833", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-3416", "mrqa_triviaqa-validation-3516", "mrqa_hotpotqa-validation-2278", "mrqa_newsqa-validation-310", "mrqa_triviaqa-validation-5852"], "SR": 0.609375, "CSR": 0.5452806122448979, "EFR": 1.0, "Overall": 0.7191342474489796}, {"timecode": 98, "before_eval_results": {"predictions": ["Scary Movie 2", "Mall", "godliness", "Time", "the Annunciation of Our Lady", "the Thames", "Alyssa Milano", "drowsiness", "lilies", "Alaska", "Yellowstone", "Duchamp", "Little Red Riding Hood", "Vaduz", "the tongue", "the English Channel", "Michelin", "a celebration", "Simple Simon", "hot chocolate", "vibrations", "a metronome", "data", "the Phillie Phanatic", "GILBERT & SULLIVAN", "Pringles", "Gentlemen Prefer Blondes", "Stratocaster", "anchors", "Romeo", "a mirror", "Pamela Anderson", "trampolining", "King of the Hill", "the Bahamas", "Tiger", "dark places", "Elton John", "the Sphinx", "Toy Story", "lump", "density", "hockey", "Heather Locklear", "the Explorer", "wheels", "the Holy Grail", "a crone", "John", "the weem", "Target", "Left Behind", "71 -- 74 \u00b0 C ( 160 -- 165 \u00b0 F )", "the Devastator", "cotton", "Mary Decker", "(Donatello)", "1939", "Leafcutter John", "University of Vienna", "Ferraris,", "80 percent", "Michael Jackson", "north"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7412202380952381}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true], "QA-F1": [0.8, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9200", "mrqa_searchqa-validation-8408", "mrqa_searchqa-validation-3541", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11345", "mrqa_searchqa-validation-13276", "mrqa_searchqa-validation-10245", "mrqa_searchqa-validation-8717", "mrqa_searchqa-validation-8288", "mrqa_searchqa-validation-15535", "mrqa_searchqa-validation-10022", "mrqa_searchqa-validation-15136", "mrqa_searchqa-validation-5946", "mrqa_searchqa-validation-416", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-13668", "mrqa_searchqa-validation-381", "mrqa_triviaqa-validation-6743", "mrqa_hotpotqa-validation-537", "mrqa_newsqa-validation-1683"], "SR": 0.671875, "CSR": 0.5465593434343434, "EFR": 0.9523809523809523, "Overall": 0.7098661841630591}, {"timecode": 99, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1673", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12845", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14027", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6260", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8234", "mrqa_searchqa-validation-8339", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_searchqa-validation-999", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6448", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7028", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-959"], "OKR": 0.8203125, "KG": 0.49375, "before_eval_results": {"predictions": ["file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\" Teen Patti\"", "Miami Beach, Florida,", "Kevin Evans", "sixth world title", "Sri Lankan", "said.Flights at Atlanta's Hartsfield-Jackson International Airport were delayed Tuesday afternoon.", "Addis Ababa,", "Saturn", "piano", "China and Japan.", "two years,", "shut down, and desperately needed aid cannot be unloaded quickly.", "$17,000", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Haleigh, a Florida girl", "The sailboat, named Cynthia Woods,", "saying Chaudhary's death was warning to management.", "violent separatist campaign", "the \" Michoacan Family,\"", "two", "exotic sports", "Bryant Purvis", "role as a bride in the 2007 movie \"License to Wed\"", "Kurt Cobain's", "Department of Homeland Security Secretary Janet Napolitano", "Gary Brooker", "E. coli bacteria", "in July", "engineering and construction", "India", "a one-shot victory in the Bob Hope Classic", "Dubai", "U.S. Vice President Dick Cheney", "Rwanda", "Tehran,", "France's", "Daytime Emmy Lifetime Achievement Award.", "money or other discreet aid for the effort if it could be made available,", "checkposts and military camps in the Mohmand agency,", "Somalia's piracy problem was fueled by environmental and political events.", "five", "December Monday", "start a dialogue of peace based on the conversations she had with Americans along the way.", "would compromise the public broadcaster's appearance of impartiality.", "President Obama", "The Ministry of Defense", "Wednesday.", "his father", "Michael Arrington,", "BMW 3-Series", "1973", "1975", "1546", "Kaiser Chiefs", "rivers", "Ecuador", "Afghanistan", "University College of North Staffordshire", "Nelson County", "the bull-running festival", "Jean-Michel Basquiat", "the Chinese Coast Guard", "silicon oxide"], "metric_results": {"EM": 0.625, "QA-F1": 0.725667735042735}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 0.4, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5555555555555556, 0.0, 1.0, 1.0, 0.0, 0.1, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1822", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-2609", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2968", "mrqa_searchqa-validation-14651", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-13028"], "SR": 0.625, "CSR": 0.54734375, "EFR": 1.0, "Overall": 0.71759375}]}