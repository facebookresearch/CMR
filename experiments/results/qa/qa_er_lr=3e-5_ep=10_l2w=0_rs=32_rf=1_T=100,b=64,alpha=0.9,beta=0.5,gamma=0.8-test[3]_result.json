{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=1, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]_result.json', stream_id=3, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 7860, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["The Jones et al. and Briffa reconstructions", "help transfer and dissipate excess energy", "Historical and Critical Dictionary", "human settlement and development of the land", "over three days", "eight", "polynomial algebra", "60 days", "American Association of University Women", "mid-Cambrian period", "1926", "The outcome of most votes can be predicted beforehand", "Newton", "a disaster", "between the 1960s and 1990s", "James Hutton", "British", "growth and investment", "Each step had to be successfully accomplished before the next ones could be performed", "Downtown Riverside", "plague of Athens in 430 BC", "expelled Jews", "interleukin 1", "DuMont Television Network", "The city of Fresno", "1762", "colloblasts", "self", "Alta California", "300 km long and up to 40 km wide", "two-phased system", "On the Councils and the Church", "Super Bowl LI", "1968", "\"right\", \"just\", or \"true\"", "pastors", "meritocracy", "Vistula River", "arrows, swords, and leather shields", "23 November 1963", "since the 1960s", "zero net force", "2012", "CD4", "income inequality", "Tuition Fee Supplement", "April 1523", "the California State Automobile Association and the Automobile Club of Southern California", "Von Miller", "lymphocytes or an antibody-based humoral response", "unsuccessful", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Percy Shelley", "education, sanitation, and traffic control", "six divisions", "six series of theses", "Pi\u0142sudski", "Disney\u2013ABC Television Group", "SAP Center", "Genghis Khan", "materials melted near an impact crater", "William the Silent", "182 million tons", "John G. Trump"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8592347756410257}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4191", "mrqa_squad-validation-9426", "mrqa_squad-validation-368", "mrqa_squad-validation-3776", "mrqa_squad-validation-4769", "mrqa_squad-validation-9194", "mrqa_squad-validation-10305", "mrqa_squad-validation-2812", "mrqa_squad-validation-6559", "mrqa_squad-validation-2153", "mrqa_squad-validation-7246"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["occupational stress among teachers", "Sunni Arabs from Iraq and Syria", "The Lower Rhine", "Mike Tolbert", "will silt up the lake", "cigarette advertising from all television and radio networks", "Dorothy and Michael Hintze", "global regulation based on the Montreal Protocol", "northwest", "Turkey", "Economist", "The Tyneside flat", "73", "Derek Jacobi", "US President John F. Kennedy", "Sydney", "Basel", "ideal strings", "pass on their signal to an unknown second messenger molecule", "lasting damage", "December 2014", "other locations throughout Scotland", "eastern", "Newton", "Westwood One", "2008", "the colonies of British America and New France", "early 1546", "lower bounds", "2011", "became the University of Northumbria at Newcastle", "between 1859 and 1865", "8\u20134\u20134 system", "a green algal derived chloroplast", "over 200", "William the Conqueror", "The Service Module was discarded", "18 million volumes", "the north", "detention", "collenchyma tissue", "Louis Adamic", "cytokine T IGF-\u03b2", "England", "Yuri Gagarin", "antigenic variation", "Knaurs Lexikon", "Chester, South Carolina", "1992", "three", "9.1 million", "rich and well socially standing", "pharynx", "1969", "Wardenclyffe Tower", "Tower Theatre", "700,000", "Pittsburgh", "\"The Vision of Adamn\u00e1n\" is one of the oldest prose works of this Atlantic island nation", "Heroes struggle... Animated, Action, Adventure, Fantasy, Sci-Fi.... Lego Star Wars: The Yoda Ch", "the Civil Rights Movement owe baseball", "The hand of cards which he supposedly held at the time of his death... killed by the assassin Jack McCall in Deadwood, Black Hills, August 2, 1876", "His... (initial capital letter) a German-built enciphering machine developed for commercial", "Europe"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8237847222222222}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9752", "mrqa_squad-validation-9236", "mrqa_squad-validation-5549", "mrqa_squad-validation-7744", "mrqa_squad-validation-9744", "mrqa_squad-validation-5337", "mrqa_squad-validation-3885", "mrqa_squad-validation-1568", "mrqa_squad-validation-6523", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-4596"], "SR": 0.78125, "CSR": 0.8046875, "retrieved_ids": ["mrqa_squad-train-54264", "mrqa_squad-train-84197", "mrqa_squad-train-57815", "mrqa_squad-train-37173", "mrqa_squad-train-14563", "mrqa_squad-train-14829", "mrqa_squad-train-4202", "mrqa_squad-train-31543", "mrqa_squad-train-44061", "mrqa_squad-train-52932", "mrqa_squad-train-11113", "mrqa_squad-train-47041", "mrqa_squad-train-15599", "mrqa_squad-train-22530", "mrqa_squad-train-4310", "mrqa_squad-train-58367", "mrqa_squad-validation-9194", "mrqa_squad-validation-10305", "mrqa_squad-validation-2153", "mrqa_squad-validation-3776", "mrqa_squad-validation-2812", "mrqa_squad-validation-368", "mrqa_squad-validation-7246", "mrqa_squad-validation-4191", "mrqa_squad-validation-6559", "mrqa_squad-validation-9426", "mrqa_squad-validation-4769"], "EFR": 1.0, "Overall": 0.90234375}, {"timecode": 2, "before_eval_results": {"predictions": ["John Sutcliffe", "their business is perpetually understaffed", "6800", "The British provided medical treatment for the sick and wounded French soldiers", "Ticonderoga Point", "October 16, 1973", "1980s", "seven", "books and articles for magazines and journals", "Roger Goodell", "the depths of the oceans and seas", "60,000", "by over 100%", "1350", "North America", "Euclid's fundamental theorem of arithmetic", "Maria de la Queillerie", "to encourage investment", "Julia Butterfly Hill", "drought resistant", "587,000 square kilometres", "March 1974", "quickly", "end of the 19th century", "DeMarcus Ware", "University of North Florida team", "motivated students", "platyctenids", "elementary school education certificate", "1220", "two", "30.0%", "Raghuram Rajan", "low-band VHF frequencies", "The Pink Triangle", "ideological", "Methodist institutions", "Ticonderoga", "at least six daughters", "Los Angeles Dodgers", "19th", "a course of study", "a delay costs money", "Funchess", "Watt", "more than doubles the Rhine's water discharge", "Thuringia", "rivers", "hostile publications towards the Jews and their Jewish religion", "visor helmet", "the Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East", "Hollywood", "Long Island Sound", "Sweden's flag features a blue background and a yellow Scandinavian cross", "there were 3 different British units called this to measure the volume of wine", "a Mexican-American physicist", "Wyoming", "an athlete who plays cricket", "crested caracara", "Alaska", "an Austrian and American film actress and inventor", "an English castaway", "the Association of American Universities", "50JJB Sports Fitness Clubs"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7375983391608392}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.8000000000000002, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6153846153846154, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665]}}, "before_error_ids": ["mrqa_squad-validation-7525", "mrqa_squad-validation-10258", "mrqa_squad-validation-1565", "mrqa_squad-validation-9061", "mrqa_squad-validation-4257", "mrqa_squad-validation-1960", "mrqa_squad-validation-5678", "mrqa_squad-validation-6101", "mrqa_squad-validation-2786", "mrqa_squad-validation-1877", "mrqa_squad-validation-9268", "mrqa_squad-validation-2497", "mrqa_squad-validation-1075", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-13452", "mrqa_searchqa-validation-12341", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-5603"], "SR": 0.65625, "CSR": 0.7552083333333334, "retrieved_ids": ["mrqa_squad-train-19453", "mrqa_squad-train-2604", "mrqa_squad-train-40209", "mrqa_squad-train-5925", "mrqa_squad-train-35353", "mrqa_squad-train-13142", "mrqa_squad-train-67930", "mrqa_squad-train-63325", "mrqa_squad-train-45769", "mrqa_squad-train-3484", "mrqa_squad-train-46970", "mrqa_squad-train-72230", "mrqa_squad-train-6327", "mrqa_squad-train-84814", "mrqa_squad-train-80831", "mrqa_squad-train-84254", "mrqa_squad-validation-6559", "mrqa_squad-validation-2812", "mrqa_squad-validation-2153", "mrqa_squad-validation-7246", "mrqa_searchqa-validation-12594", "mrqa_squad-validation-1568", "mrqa_squad-validation-7744", "mrqa_searchqa-validation-14178", "mrqa_squad-validation-368", "mrqa_squad-validation-9744", "mrqa_squad-validation-9236", "mrqa_squad-validation-3776", "mrqa_squad-validation-4191", "mrqa_squad-validation-5549", "mrqa_squad-validation-9752", "mrqa_squad-validation-5337"], "EFR": 1.0, "Overall": 0.8776041666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["public schools", "connection id in a table", "males", "electrical, water, sewage, phone, and cable facilities", "the level of the top tax rate", "\"Wise up or die.\"", "VideoGuard UK", "highly-paid professions", "pump this into the mesoglea", "Fred Silverman", "atmospheric engine", "force", "ctenophores", "the trial and rehabilitation of Joan of Arc, Rouen", "John Hurt", "an Australian public X.25 network operated by Telstra", "Seattle Seahawks", "Von Miller", "San Diego Chargers", "42%", "1957", "The majority may be powerful but it is not necessarily right", "orogenic wedges", "one", "Catholic orthodoxy", "Edict of Fontainebleau", "Fort Caroline", "Pittard Sullivan", "wealth", "Niagara Falls Cataract Construction Company", "Hugh Downs", "Robert Guiscard", "3D printing technology", "Daniel Burke", "internal strife", "Matthew Murray", "post-classical European sculpture", "United States", "Satya Nadella", "the difference between a problem and an instance", "Richard E. Grant, Jim Broadbent, Hugh Grant and Joanna Lumley", "the People's Republic of China", "cortisol and catecholamines", "a third group of pigments found in cyanobacteria", "isopentenyl pyrophosphate synthesis", "1963", "his hotel room", "San Francisco", "Ted Hughes", "the parish beadle", "Khartoum", "William Henry Harrison", "Playboy rabbit", "Dutch metaphysicians", "Puerto Rico", "Court TV", "Howard Carter", "The Prairie Wolf", "Inhospitable Sea", "Moshe Dayan", "Kim", "active athletes", "helicopters and boats, as well as vessels from other agencies", "$17,000"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6860557844932844}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7103", "mrqa_squad-validation-7357", "mrqa_squad-validation-4452", "mrqa_squad-validation-3247", "mrqa_squad-validation-5347", "mrqa_squad-validation-363", "mrqa_squad-validation-618", "mrqa_squad-validation-6967", "mrqa_squad-validation-10083", "mrqa_squad-validation-3139", "mrqa_squad-validation-1366", "mrqa_squad-validation-1045", "mrqa_squad-validation-1670", "mrqa_squad-validation-7885", "mrqa_squad-validation-6263", "mrqa_squad-validation-8839", "mrqa_squad-validation-7711", "mrqa_searchqa-validation-8170", "mrqa_searchqa-validation-12199", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-6624", "mrqa_newsqa-validation-142"], "SR": 0.59375, "CSR": 0.71484375, "retrieved_ids": ["mrqa_squad-train-29255", "mrqa_squad-train-12713", "mrqa_squad-train-68728", "mrqa_squad-train-17560", "mrqa_squad-train-19046", "mrqa_squad-train-31908", "mrqa_squad-train-2594", "mrqa_squad-train-23305", "mrqa_squad-train-30078", "mrqa_squad-train-42051", "mrqa_squad-train-50827", "mrqa_squad-train-48314", "mrqa_squad-train-24174", "mrqa_squad-train-85849", "mrqa_squad-train-40135", "mrqa_squad-train-46687", "mrqa_squad-validation-9194", "mrqa_squad-validation-10305", "mrqa_searchqa-validation-13452", "mrqa_squad-validation-9236", "mrqa_squad-validation-9061", "mrqa_squad-validation-1075", "mrqa_squad-validation-9268", "mrqa_squad-validation-7744", "mrqa_squad-validation-9426", "mrqa_squad-validation-6523", "mrqa_searchqa-validation-9383", "mrqa_squad-validation-4257", "mrqa_squad-validation-10258", "mrqa_squad-validation-2153", "mrqa_searchqa-validation-8844", "mrqa_squad-validation-5337"], "EFR": 1.0, "Overall": 0.857421875}, {"timecode": 4, "before_eval_results": {"predictions": ["whether he stood by their contents", "1850s", "Troika", "complexity classes", "Parliament of the United Kingdom at Westminster", "teaching", "Dancing with the Stars", "8 November 2010", "the Y. pestis was spread from fleas on rats", "coastal beaches and the game reserves", "1524", "2p \u2212 1", "horizontal", "Crash the Super Bowl", "collenchyma tissue", "a billion years ago", "Croatia", "Port of Long Beach", "Edinburgh", "Brad Nortman", "Papin", "Cricket", "William Morris", "T. J. Ward", "the Daleks", "San Diego", "1017", "heat and pressure", "1072", "Chevron", "Africa", "New York City", "Marshall Cohen", "Hypersensitivity", "Business Connect", "Henry Young Darracott Scott", "European Court of Justice held that a Commissioner giving her dentist a job, for which he was clearly unqualified, did in fact not break any law", "income inequality", "Bainbridge's", "1294", "chao", "Apollo 17", "8 Mile Island", "Betty Ford", "Stephen Hawking", "\"The Old Haney Place\"", "Europe", "helium-4", "San Martn", "Baby'S Birthstone, Precious Stones, Ruby Gemstone, Gem Stones, Passion Moving,", "Panatela", "Moscow", "crystal anniversary", "Prairie Lily", "Detroit River", "Cleveland", "the earliest New Testament manuscripts were written on papyrus, made from a reed that grew", "Dublin", "\"suits\" attractive, but, damn...  Tibet's beacon of peace maintains a calm compassion  even as Beijing cracks down on his people", "Ben E18 Physical Thing.... objects, and museums for all the collections that they hold, seen as a continuum from highly standardised... and museum objects can represent events or characters found in books", "Albert Einstein", "Doctor Who", "1990", "Zimbabwe"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7322280898704789}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0625, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5878", "mrqa_squad-validation-5029", "mrqa_squad-validation-5344", "mrqa_squad-validation-7615", "mrqa_squad-validation-2679", "mrqa_squad-validation-1061", "mrqa_squad-validation-4147", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-4300"], "SR": 0.703125, "CSR": 0.7125, "retrieved_ids": ["mrqa_squad-train-49762", "mrqa_squad-train-11543", "mrqa_squad-train-1590", "mrqa_squad-train-13899", "mrqa_squad-train-28529", "mrqa_squad-train-21703", "mrqa_squad-train-85803", "mrqa_squad-train-24218", "mrqa_squad-train-50911", "mrqa_squad-train-34465", "mrqa_squad-train-38633", "mrqa_squad-train-12174", "mrqa_squad-train-58111", "mrqa_squad-train-72723", "mrqa_squad-train-81484", "mrqa_squad-train-26756", "mrqa_squad-validation-618", "mrqa_squad-validation-5347", "mrqa_squad-validation-4191", "mrqa_squad-validation-9236", "mrqa_squad-validation-9061", "mrqa_searchqa-validation-12199", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-16899", "mrqa_squad-validation-3776", "mrqa_squad-validation-7246", "mrqa_squad-validation-1075", "mrqa_searchqa-validation-9383", "mrqa_squad-validation-3247", "mrqa_squad-validation-6967", "mrqa_squad-validation-1960"], "EFR": 1.0, "Overall": 0.85625}, {"timecode": 5, "before_eval_results": {"predictions": ["November 2006", "achievement-oriented motivations (\"pull\")", "a blue British Police box", "SAP Center", "March 2011", "the actual sea level rise was above the top of the range", "12th", "1226", "The Late Show", "sediments", "P", "theNP-complete knapsack problem", "1928", "when the immune system is less active than normal", "disturbances", "Fraud", "1562 to 1598", "Emmerich Rhine Bridge", "ten", "1993", "a primitive intermediate between cyanobacteria and the more evolved chloroplasts", "lung tissue", "US$100,000", "Downtown Los Angeles", "Bruno Mars", "patients' prescriptions and patient safety issues", "2009", "Common Core", "Recognized Student Organizations", "a cubic interpolation formula", "Thomas Edison", "the Swiss Reformation", "phagocytes", "15", "Satyagraha", "two", "microorganisms", "1968", "Aloha \u02bbOe", "1.6 million", "Eric Whitacre", "mouthbows", "Angus Young", "New York City", "the waltz Gunstwerber", "Cherokee River", "Odisha", "January 28, 2016", "138,535", "Adam Rex", "1933", "Sivakumar, S. V. Subbaiah, Jayachitra, Srividya, Shubha, Kamal Haasan and Jayasudha", "1968", "Archdeacon", "the Military Band of Hanover", "Warrington", "1866", "Chattahoochee", "the Provisional Irish Republican Army", "pneumonoultramicroscopicsilicovolcanoconiosis", "Boutros Ghali", "Vertikal-T", "the Wizard of Oz", "he was mad at the U.S. military because of what they had done to Muslims"], "metric_results": {"EM": 0.625, "QA-F1": 0.676161858974359}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7373", "mrqa_squad-validation-434", "mrqa_squad-validation-9185", "mrqa_squad-validation-1759", "mrqa_squad-validation-1860", "mrqa_squad-validation-8696", "mrqa_squad-validation-3496", "mrqa_squad-validation-2634", "mrqa_squad-validation-664", "mrqa_squad-validation-3240", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-2158", "mrqa_newsqa-validation-1468", "mrqa_searchqa-validation-13492", "mrqa_newsqa-validation-3290"], "SR": 0.625, "CSR": 0.6979166666666667, "retrieved_ids": ["mrqa_squad-train-37993", "mrqa_squad-train-28956", "mrqa_squad-train-46094", "mrqa_squad-train-50321", "mrqa_squad-train-21120", "mrqa_squad-train-20594", "mrqa_squad-train-9160", "mrqa_squad-train-16811", "mrqa_squad-train-50600", "mrqa_squad-train-27847", "mrqa_squad-train-69410", "mrqa_squad-train-35478", "mrqa_squad-train-44614", "mrqa_squad-train-68268", "mrqa_squad-train-70604", "mrqa_squad-train-60518", "mrqa_squad-validation-5337", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-4199", "mrqa_squad-validation-4452", "mrqa_squad-validation-6559", "mrqa_squad-validation-9744", "mrqa_searchqa-validation-6624", "mrqa_squad-validation-10083", "mrqa_searchqa-validation-12199", "mrqa_squad-validation-368", "mrqa_squad-validation-3776", "mrqa_squad-validation-2679", "mrqa_squad-validation-9061", "mrqa_squad-validation-4191", "mrqa_squad-validation-5344", "mrqa_searchqa-validation-15812"], "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["UHF", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "Africa", "1976", "inverse", "non-deterministic time", "60", "Warsaw", "5,000 years", "Edgar Atheling", "Writers Guild of America", "IPCC Trust Fund", "Conservative", "11 million members", "visor helmet", "the Queen", "3 in 1,000,000", "1970 story Spearhead from Space, released in July 2013", "Super Bowl Opening Night", "Fresno Street and Thorne Ave", "southern and central parts of France", "in 2014", "40%", "Dwight D. Eisenhower", "innate immune system", "15\u20131", "history of arms", "Industry and manufacturing", "this contact with nature made him stronger, both physically and mentally.", "1543", "\u015ar\u00f3dmie\u015bcie", "Hmong or Laotian", "Stromules", "ignition sources are minimized", "Johnny Herbert", "Diego Rub\u00e9n Tonetto", "Blake Shelton", "Adrian Lyne", "J\u00f3zsef Pulitzer", "June 17, 2007", "\"The Frost Report\"", "National Basketball Development League", "Danish", "24 January 76", "Kealakekua Bay", "David Patrick Griffin", "Northrop P-61 Black widow", "People v. Turner", "\"Veyyil\" (2006)", "Helena Sternlicht", "Highlands Course", "Illinois", "Cartoon Cartoon Fridays", "Timo Hildebrand", "2016", "Ginger Rogers", "Corps of Discovery", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "801,200 people", "giraffe", "one-of-a-kind navy dress with red lining", "Ecuador", "Joseph Barnes", "Surface Runoff"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7330528846153845}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-10424", "mrqa_squad-validation-1730", "mrqa_squad-validation-1114", "mrqa_squad-validation-8523", "mrqa_squad-validation-10107", "mrqa_squad-validation-7770", "mrqa_squad-validation-6900", "mrqa_squad-validation-1232", "mrqa_squad-validation-3483", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2183", "mrqa_naturalquestions-validation-4953", "mrqa_newsqa-validation-3782", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-5374"], "SR": 0.65625, "CSR": 0.6919642857142857, "retrieved_ids": ["mrqa_squad-train-50023", "mrqa_squad-train-51887", "mrqa_squad-train-55125", "mrqa_squad-train-43158", "mrqa_squad-train-61879", "mrqa_squad-train-40472", "mrqa_squad-train-68431", "mrqa_squad-train-48442", "mrqa_squad-train-74571", "mrqa_squad-train-69809", "mrqa_squad-train-31019", "mrqa_squad-train-20507", "mrqa_squad-train-11828", "mrqa_squad-train-40174", "mrqa_squad-train-69863", "mrqa_squad-train-76", "mrqa_squad-validation-9744", "mrqa_searchqa-validation-15640", "mrqa_newsqa-validation-142", "mrqa_squad-validation-664", "mrqa_squad-validation-3776", "mrqa_squad-validation-9426", "mrqa_squad-validation-1670", "mrqa_searchqa-validation-15579", "mrqa_hotpotqa-validation-4318", "mrqa_squad-validation-1565", "mrqa_hotpotqa-validation-2120", "mrqa_squad-validation-3885", "mrqa_searchqa-validation-4199", "mrqa_squad-validation-4191", "mrqa_squad-validation-1075", "mrqa_searchqa-validation-4300"], "EFR": 1.0, "Overall": 0.8459821428571428}, {"timecode": 7, "before_eval_results": {"predictions": ["convection of the mantle", "Continental Edison Company in France", "South", "T. J. Ward", "25 minutes", "1903", "1993", "King George III", "an occupancy permit", "Hereford", "Elder to Bishop", "Arts & Entertainment Television", "NASA", "Albert Einstein", "Karl von Miltitz", "a matter of custom or expectation", "Silk Road", "a war erupted in the Philippines", "26", "Bolshevik leaders", "University of Aberdeen", "transportation, sewer, hazardous waste and water", "430 BC", "Pedro Men\u00e9ndez de Avil\u00e9s", "expansion", "single-tape", "linear", "oxygen concentration is too high", "1290", "17,786,419", "second-largest", "Montreal", "7000301604928199000", "was an incident on March 5, 1770, in which British Army soldiers shot and killed people while under attack by a mob", "member states", "2000", "Anna Faris", "Samantha Jo", "the central sulcus", "Aldis Hodge", "Steve Hale", "a multilayer", "January 2017 patch", "Idaho", "September 6, 2019", "multinational", "Glenn Close", "Jack Gleeson", "Claims adjuster", "when the forward reaction proceeds at the same rate as the reverse reaction", "heart", "writ of certiorari", "A standard form contract", "Shawn", "September 30", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophia Monk and Eddie Perfect", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot.", "Zorro", "\"Household Words\"", "56", "Hindu scriptures", "Agnes", "A", "gold"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6924200335355859}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.07692307692307691, 0.4444444444444445, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.046511627906976744, 0.0, 0.5, 0.4615384615384615, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1239", "mrqa_squad-validation-2315", "mrqa_squad-validation-6000", "mrqa_squad-validation-6878", "mrqa_squad-validation-10007", "mrqa_squad-validation-360", "mrqa_squad-validation-6500", "mrqa_squad-validation-8923", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-3491", "mrqa_searchqa-validation-12472", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3744"], "SR": 0.578125, "CSR": 0.677734375, "retrieved_ids": ["mrqa_squad-train-26071", "mrqa_squad-train-25828", "mrqa_squad-train-45496", "mrqa_squad-train-71602", "mrqa_squad-train-62376", "mrqa_squad-train-31003", "mrqa_squad-train-58007", "mrqa_squad-train-65745", "mrqa_squad-train-55296", "mrqa_squad-train-20236", "mrqa_squad-train-52383", "mrqa_squad-train-37900", "mrqa_squad-train-28999", "mrqa_squad-train-14974", "mrqa_squad-train-46039", "mrqa_squad-train-25843", "mrqa_searchqa-validation-11348", "mrqa_squad-validation-1730", "mrqa_squad-validation-6900", "mrqa_hotpotqa-validation-4719", "mrqa_squad-validation-3240", "mrqa_squad-validation-6523", "mrqa_squad-validation-7885", "mrqa_searchqa-validation-5374", "mrqa_hotpotqa-validation-114", "mrqa_searchqa-validation-7713", "mrqa_squad-validation-4769", "mrqa_squad-validation-6559", "mrqa_searchqa-validation-14178", "mrqa_squad-validation-9185", "mrqa_searchqa-validation-5247", "mrqa_hotpotqa-validation-1526"], "EFR": 0.9629629629629629, "Overall": 0.8203486689814814}, {"timecode": 8, "before_eval_results": {"predictions": ["ivory", "The Dornbirner Ach", "a certain number of teacher's salaries are paid by the State", "non-deterministic time", "five", "December 2014", "an inauspicious typhoon", "four", "Zwickau prophet", "10 July 1856 \u2013 7 January 1943", "In 1999", "digital streams of the game via CBS Sports.com", "3\u20132.7 billion years ago", "New Testament from Greek", "Von Miller", "economists with the Standard & Poor's rating agency", "Germany", "a vestigial red algal derived chloroplast", "two or more teachers working harmoniously to fulfill the needs of every student in the classroom", "a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981", "type III secretion system", "worker, capitalist/business owner, landlord", "tungsten", "Leonard Hubert Hoffmann", "oxidant", "the signals could come from Mars, Venus, or other planets", "the American Revolution", "The Book of Discipline", "Fat Albert", "1943", "\"Big Fucking German\"", "Chelmsford", "William Novak", "22,500 acres", "1951", "Al Jazira Club", "86,112", "American", "Firth of Forth Site of Special Scientific Interest", "one live album", "Blue Valley Northwest High School", "\" Cleopatra\"", "Battle of the Rosebud", "Homebrewing", "Pablo Escobar", "Brian A. Miller", "26 June 2013", "25 million", "320 years", "Geraldine Sue Page", "Rochdale", "Charles Reed Bishop", "a plastic sheet and film material for book binding and case covering for speakers and amplifiers", "Marco Fu", "2015", "Dusty Springfield", "her translation of and commentary on Isaac Newton's book \"Principia\" containing basic laws of physics.", "BeBe Winans", "Henry VIII", "Sunday", "\"Tucker\"", "Dumont d'Urville Station", "Under normal conditions", "atransformiation, change of mind, repentance, and atonement"], "metric_results": {"EM": 0.625, "QA-F1": 0.710207793591745}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.6666666666666666, 0.7272727272727273, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.782608695652174, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_squad-validation-1156", "mrqa_squad-validation-4121", "mrqa_squad-validation-531", "mrqa_squad-validation-2272", "mrqa_squad-validation-1912", "mrqa_squad-validation-4849", "mrqa_squad-validation-6873", "mrqa_squad-validation-1529", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-391", "mrqa_triviaqa-validation-4490", "mrqa_newsqa-validation-3405", "mrqa_searchqa-validation-703", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-5851"], "SR": 0.625, "CSR": 0.671875, "retrieved_ids": ["mrqa_squad-train-38152", "mrqa_squad-train-81533", "mrqa_squad-train-76864", "mrqa_squad-train-81820", "mrqa_squad-train-36310", "mrqa_squad-train-79589", "mrqa_squad-train-60214", "mrqa_squad-train-63427", "mrqa_squad-train-41947", "mrqa_squad-train-45462", "mrqa_squad-train-15537", "mrqa_squad-train-58027", "mrqa_squad-train-25625", "mrqa_squad-train-79295", "mrqa_squad-train-69926", "mrqa_squad-train-70621", "mrqa_naturalquestions-validation-3491", "mrqa_searchqa-validation-15305", "mrqa_squad-validation-9752", "mrqa_naturalquestions-validation-4953", "mrqa_squad-validation-9194", "mrqa_searchqa-validation-7076", "mrqa_squad-validation-5878", "mrqa_squad-validation-4257", "mrqa_squad-validation-4769", "mrqa_searchqa-validation-9383", "mrqa_squad-validation-9185", "mrqa_squad-validation-9744", "mrqa_squad-validation-8523", "mrqa_squad-validation-2679", "mrqa_naturalquestions-validation-10004", "mrqa_squad-validation-8839"], "EFR": 1.0, "Overall": 0.8359375}, {"timecode": 9, "before_eval_results": {"predictions": ["1999", "zero", "Mao Zedong", "Chebyshev", "1960", "introduction of Beroe", "1000 CE", "the 880s", "Sunspot, New Mexico", "Sonderungsverbot", "amending", "the environment in which they lived", "a genetic disease such as severe combined immunodeficiency", "C. J. Anderson", "Cadeby", "Warraghiggey", "it has trouble distinguishing between carbon dioxide and oxygen, so at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors", "World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)", "Sunni pan-Islamism", "11 points", "yes or no, or alternately either 1 or 0", "red", "the Mongols", "English", "Newton", "1940s and 1950s", "a \u2018Z\u2019 plan fortalice dating from no later than 1618 and possibly founded as early as 1580", "Broadway musicals", "Taoiseach", "Duval County", "Bill Ponsford", "$10\u201320 million", "Manasseh Cutler Hall", "Denmark", "Hindi", "\"Pour le M\u00e9rite\"", "Giuseppe Verdi", "Edward Trowbridge Collins Sr.", "1946 and 1947", "Christopher McCulloch", "100th season of operation", "Carson City", "Wembley Stadium", "19th", "Cesar Millan", "Robert Allen Zimmerman", "Michael Lewis Greenwell", "20 March to 1 May 2003", "The Life of Charlotte Bront\u00eb", "2015", "Bill Curry", "Jack White", "Kim Yoon-seok and Ha Jung-woo", "superhero roles", "Richard Barry", "18th congressional district", "the BBC", "2008", "Ringo Starr", "Aida", "Logar province", "has been ongoing work to update the labeling of the fluoroquinolone drug products", "the Necklace", "Jamaica"], "metric_results": {"EM": 0.609375, "QA-F1": 0.730250641923436}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.7058823529411764, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 0.4, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.07142857142857142, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9897", "mrqa_squad-validation-1025", "mrqa_squad-validation-3981", "mrqa_squad-validation-6443", "mrqa_squad-validation-8832", "mrqa_squad-validation-260", "mrqa_squad-validation-1652", "mrqa_squad-validation-502", "mrqa_hotpotqa-validation-2670", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_triviaqa-validation-464", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809"], "SR": 0.609375, "CSR": 0.665625, "retrieved_ids": ["mrqa_squad-train-34849", "mrqa_squad-train-38202", "mrqa_squad-train-59386", "mrqa_squad-train-18811", "mrqa_squad-train-5882", "mrqa_squad-train-10306", "mrqa_squad-train-76158", "mrqa_squad-train-19012", "mrqa_squad-train-30735", "mrqa_squad-train-74315", "mrqa_squad-train-31574", "mrqa_squad-train-21267", "mrqa_squad-train-84796", "mrqa_squad-train-83197", "mrqa_squad-train-29907", "mrqa_squad-train-15374", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-8374", "mrqa_squad-validation-7711", "mrqa_searchqa-validation-7713", "mrqa_squad-validation-1239", "mrqa_triviaqa-validation-4490", "mrqa_squad-validation-7246", "mrqa_searchqa-validation-16899", "mrqa_squad-validation-10107", "mrqa_squad-validation-5337", "mrqa_squad-validation-10007", "mrqa_hotpotqa-validation-2294", "mrqa_squad-validation-2812", "mrqa_squad-validation-6559", "mrqa_squad-validation-2679"], "EFR": 1.0, "Overall": 0.8328125}, {"timecode": 10, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3894", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4633", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5145", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5527", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-690", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-599", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9687", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-13863", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14454", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16753", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-4300", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6624", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-9383", "mrqa_squad-validation-10007", "mrqa_squad-validation-10010", "mrqa_squad-validation-10031", "mrqa_squad-validation-10083", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10130", "mrqa_squad-validation-10153", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10234", "mrqa_squad-validation-10249", "mrqa_squad-validation-10258", "mrqa_squad-validation-10305", "mrqa_squad-validation-10345", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10478", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1073", "mrqa_squad-validation-1075", "mrqa_squad-validation-1075", "mrqa_squad-validation-1096", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1232", "mrqa_squad-validation-1239", "mrqa_squad-validation-1254", "mrqa_squad-validation-1296", "mrqa_squad-validation-1372", "mrqa_squad-validation-1529", "mrqa_squad-validation-1543", "mrqa_squad-validation-1586", "mrqa_squad-validation-1632", "mrqa_squad-validation-1652", "mrqa_squad-validation-1681", "mrqa_squad-validation-1723", "mrqa_squad-validation-1730", "mrqa_squad-validation-1731", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-1819", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1877", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-1976", "mrqa_squad-validation-1985", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2111", "mrqa_squad-validation-2153", "mrqa_squad-validation-2181", "mrqa_squad-validation-2189", "mrqa_squad-validation-2246", "mrqa_squad-validation-2247", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2320", "mrqa_squad-validation-2359", "mrqa_squad-validation-2413", "mrqa_squad-validation-2442", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2568", "mrqa_squad-validation-2591", "mrqa_squad-validation-260", "mrqa_squad-validation-2617", "mrqa_squad-validation-2628", "mrqa_squad-validation-2634", "mrqa_squad-validation-2644", "mrqa_squad-validation-2679", "mrqa_squad-validation-2721", "mrqa_squad-validation-2723", "mrqa_squad-validation-2765", "mrqa_squad-validation-2808", "mrqa_squad-validation-2812", "mrqa_squad-validation-2881", "mrqa_squad-validation-2941", "mrqa_squad-validation-2949", "mrqa_squad-validation-2975", "mrqa_squad-validation-2977", "mrqa_squad-validation-30", "mrqa_squad-validation-3111", "mrqa_squad-validation-3139", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3248", "mrqa_squad-validation-3269", "mrqa_squad-validation-3296", "mrqa_squad-validation-3377", "mrqa_squad-validation-3429", "mrqa_squad-validation-3483", "mrqa_squad-validation-3496", "mrqa_squad-validation-3534", "mrqa_squad-validation-3552", "mrqa_squad-validation-360", "mrqa_squad-validation-363", "mrqa_squad-validation-368", "mrqa_squad-validation-3705", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3776", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3885", "mrqa_squad-validation-3952", "mrqa_squad-validation-3981", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4070", "mrqa_squad-validation-4095", "mrqa_squad-validation-4107", "mrqa_squad-validation-4121", "mrqa_squad-validation-4121", "mrqa_squad-validation-4147", "mrqa_squad-validation-4191", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4516", "mrqa_squad-validation-457", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4689", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4758", "mrqa_squad-validation-4797", "mrqa_squad-validation-480", "mrqa_squad-validation-4840", "mrqa_squad-validation-4898", "mrqa_squad-validation-4988", "mrqa_squad-validation-4997", "mrqa_squad-validation-502", "mrqa_squad-validation-5029", "mrqa_squad-validation-5061", "mrqa_squad-validation-5096", "mrqa_squad-validation-5108", "mrqa_squad-validation-5200", "mrqa_squad-validation-5222", "mrqa_squad-validation-5270", "mrqa_squad-validation-5272", "mrqa_squad-validation-5287", "mrqa_squad-validation-531", "mrqa_squad-validation-5337", "mrqa_squad-validation-5344", "mrqa_squad-validation-5347", "mrqa_squad-validation-5382", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5549", "mrqa_squad-validation-5551", "mrqa_squad-validation-556", "mrqa_squad-validation-5621", "mrqa_squad-validation-5670", "mrqa_squad-validation-5741", "mrqa_squad-validation-5794", "mrqa_squad-validation-5839", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6000", "mrqa_squad-validation-6027", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6259", "mrqa_squad-validation-6260", "mrqa_squad-validation-6263", "mrqa_squad-validation-6277", "mrqa_squad-validation-6295", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-65", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6538", "mrqa_squad-validation-6548", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6600", "mrqa_squad-validation-6612", "mrqa_squad-validation-6624", "mrqa_squad-validation-664", "mrqa_squad-validation-6656", "mrqa_squad-validation-6657", "mrqa_squad-validation-6666", "mrqa_squad-validation-6695", "mrqa_squad-validation-6749", "mrqa_squad-validation-6858", "mrqa_squad-validation-6861", "mrqa_squad-validation-6878", "mrqa_squad-validation-6880", "mrqa_squad-validation-6888", "mrqa_squad-validation-689", "mrqa_squad-validation-6898", "mrqa_squad-validation-6900", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-6967", "mrqa_squad-validation-7018", "mrqa_squad-validation-7021", "mrqa_squad-validation-7033", "mrqa_squad-validation-7036", "mrqa_squad-validation-7062", "mrqa_squad-validation-7123", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7246", "mrqa_squad-validation-7268", "mrqa_squad-validation-7302", "mrqa_squad-validation-7312", "mrqa_squad-validation-7323", "mrqa_squad-validation-7357", "mrqa_squad-validation-7362", "mrqa_squad-validation-7373", "mrqa_squad-validation-738", "mrqa_squad-validation-7391", "mrqa_squad-validation-741", "mrqa_squad-validation-7450", "mrqa_squad-validation-7458", "mrqa_squad-validation-7466", "mrqa_squad-validation-7470", "mrqa_squad-validation-755", "mrqa_squad-validation-7562", "mrqa_squad-validation-7603", "mrqa_squad-validation-764", "mrqa_squad-validation-767", "mrqa_squad-validation-7686", "mrqa_squad-validation-7711", "mrqa_squad-validation-7736", "mrqa_squad-validation-7744", "mrqa_squad-validation-7765", "mrqa_squad-validation-7770", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7892", "mrqa_squad-validation-7902", "mrqa_squad-validation-7957", "mrqa_squad-validation-7970", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8232", "mrqa_squad-validation-8236", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8572", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8685", "mrqa_squad-validation-8696", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8743", "mrqa_squad-validation-8760", "mrqa_squad-validation-8763", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8794", "mrqa_squad-validation-8797", "mrqa_squad-validation-8832", "mrqa_squad-validation-8837", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-8965", "mrqa_squad-validation-904", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9236", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-9317", "mrqa_squad-validation-9330", "mrqa_squad-validation-938", "mrqa_squad-validation-9401", "mrqa_squad-validation-9426", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-9579", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9628", "mrqa_squad-validation-9744", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9817", "mrqa_squad-validation-9890", "mrqa_squad-validation-9892", "mrqa_squad-validation-9897", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7579"], "OKR": 0.927734375, "KG": 0.4234375, "before_eval_results": {"predictions": ["Thermochemical techniques", "executive producer", "1987", "James O. McKinsey", "North", "one-eighth", "coastal beaches and the game reserves", "Vicodin", "\u00a31bn", "Katharina von Bora", "9 March 1508", "Shoushi Li", "his means of seizing power", "ideal strings", "after the Franco-German War", "German", "San Andreas Fault", "NYPD Blue", "Northern Chinese", "Oireachtas funds", "Supreme Court of the United States", "countries with bigger income inequalities", "John Robert Cocker", "King Kelly", "Nidal Malik Hasan", "Richard Masur", "1988", "Bergen County", "The Ones Who Walk Away from Omelas", "hiphop", "Lithuanian national team", "historic buildings, arts, and published works", "Disney Parks Christmas Day Parade", "Esp\u00edrito Santo Financial Group", "Guillermo del Toro", "Wolf Creek", "YouTube", "the onset and progression of Alzheimer's disease", "National Football League", "Lake Placid, New York", "Iron Man 3", "Suffolk, England", "singer, songwriter, actress", "Chicago", "Martin \"Marty\" McCann", "247,597", "Mandalay Entertainment", "actor", "Boeing EA-18G Growler", "Barnoldswick", "Pacific War", "A41", "Heather Elizabeth Langenkamp", "Leona Lewis", "The Ministry of Utmost Happiness", "BAFTA TV Award Best Actor", "Rodney Crowell", "Andy Serkis", "a specific phobia, which is discussed on the home page", "a pure breed Siamese cats with other varieties like the British and American Shorthair", "Zulfikar Ali Bhutto", "cancer", "a small and unique home that will provide you with a Jewish home living environment geared to those who are in need of assisted living", "a decomposition reaction to produce Na(s) and N2(g)"], "metric_results": {"EM": 0.625, "QA-F1": 0.726808608058608}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7887", "mrqa_squad-validation-2976", "mrqa_squad-validation-2701", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3822", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-2639", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-7417", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-5418"], "SR": 0.625, "CSR": 0.6619318181818181, "retrieved_ids": ["mrqa_squad-train-17016", "mrqa_squad-train-24920", "mrqa_squad-train-66883", "mrqa_squad-train-48846", "mrqa_squad-train-21341", "mrqa_squad-train-85062", "mrqa_squad-train-32687", "mrqa_squad-train-50811", "mrqa_squad-train-72170", "mrqa_squad-train-32106", "mrqa_squad-train-3349", "mrqa_squad-train-31537", "mrqa_squad-train-51050", "mrqa_squad-train-47375", "mrqa_squad-train-78774", "mrqa_squad-train-54666", "mrqa_squad-validation-6523", "mrqa_squad-validation-5347", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1272", "mrqa_squad-validation-7103", "mrqa_squad-validation-3139", "mrqa_squad-validation-7885", "mrqa_squad-validation-1912", "mrqa_newsqa-validation-3405", "mrqa_squad-validation-1075", "mrqa_naturalquestions-validation-9687", "mrqa_hotpotqa-validation-2325", "mrqa_squad-validation-6559", "mrqa_squad-validation-5029", "mrqa_hotpotqa-validation-2120", "mrqa_squad-validation-4191"], "EFR": 1.0, "Overall": 0.7576988636363636}, {"timecode": 11, "before_eval_results": {"predictions": ["April 1887", "semi-arid savanna", "2014", "Levi's Stadium", "misguided", "San Jose Marriott", "1972", "second-largest", "Decision Time", "Victorian Government", "the American Revolutionary War", "pep", "human", "the Treaties establishing the European Union", "trans-lunar injection", "Amazonia", "Daniel Andrews", "UNESCO's World Heritage list", "Rowan Atkinson", "31\u20130", "NCAA's Division I", "Mark Helfrich", "Wal-Mart Canada", "Louis Zamperini", "Che Guevara", "Carol Ann Duffy", "University of Kentucky", "1978", "Danish", "Ukrainian", "1954", "\"John\" Alexander Florence", "Simon Baron-Cohen", "9Lives", "'valley of the hazels'", "Art Bell", "Sophia Winkleman", "Eminem", "Point Place", "Knowlton School", "Delilah Rene", "Don Bluth", "Columbus", "Czech Kingdom", "Jon M. Chu", "Sacramento Kings", "South Asian Games", "Tufts College", "Harrods", "Flamingo Las Vegas", "Ben Savage", "Suspiria", "City of Newcastle", "Japan", "Canada", "in positions 14 - 15, 146 - 147 and 148 - 149", "privatized", "sestertius", "beetles", "was attacked by small-arms, machine-gun and RPG fire", "Republican Gov. Jan Brewer", "Irving Berlin", "New York City", "Night"], "metric_results": {"EM": 0.671875, "QA-F1": 0.8006433823529412}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.0, 0.0, 1.0, 0.7000000000000001, 0.8571428571428571, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8447", "mrqa_squad-validation-7288", "mrqa_squad-validation-4015", "mrqa_squad-validation-234", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3571", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-1825", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-953", "mrqa_triviaqa-validation-1604", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-3580", "mrqa_searchqa-validation-7583"], "SR": 0.671875, "CSR": 0.6627604166666667, "retrieved_ids": ["mrqa_squad-train-25369", "mrqa_squad-train-45635", "mrqa_squad-train-74621", "mrqa_squad-train-32160", "mrqa_squad-train-76423", "mrqa_squad-train-72186", "mrqa_squad-train-13434", "mrqa_squad-train-77237", "mrqa_squad-train-14914", "mrqa_squad-train-63954", "mrqa_squad-train-61783", "mrqa_squad-train-49035", "mrqa_squad-train-60372", "mrqa_squad-train-86020", "mrqa_squad-train-21770", "mrqa_squad-train-35887", "mrqa_squad-validation-363", "mrqa_squad-validation-3776", "mrqa_squad-validation-260", "mrqa_squad-validation-10258", "mrqa_hotpotqa-validation-807", "mrqa_searchqa-validation-14178", "mrqa_hotpotqa-validation-4927", "mrqa_squad-validation-10007", "mrqa_hotpotqa-validation-2183", "mrqa_squad-validation-10305", "mrqa_squad-validation-4147", "mrqa_hotpotqa-validation-2670", "mrqa_squad-validation-8832", "mrqa_newsqa-validation-1793", "mrqa_squad-validation-3885", "mrqa_hotpotqa-validation-3020"], "EFR": 1.0, "Overall": 0.7578645833333334}, {"timecode": 12, "before_eval_results": {"predictions": ["late 1545", "several hundred thousand", "five", "every two years", "two-man", "The Hoppings", "Mycobacterium tuberculosis", "C. J. Anderson", "Harvey Martin", "stratigraphic", "environmental determinism", "Wellington", "a problem instance", "inertia of the ball associated with its constant velocity in the direction of the vehicle's motion that ensures the ball continues to move forward even as it is thrown up and falls back down", "a sin", "small subunit ribosomes, which they use to synthesize a small fraction of their proteins", "\"Isel\"", "Edmonton, Canada", "Tony Burke", "Eugene O'Neill", "Max Kellerman", "authored the Morrill Land-Grant Acts of 1862 and 1890", "VfL Wolfsburg", "July 22, 1946", "Julia Verdin", "Pendlebury", "McLaren-Honda", "Bismarck", "Comedy Film Nerds", "2016 World Indoor Championships", "MG", "January 18, 1977", "North Greenwich Arena", "The Soloist", "Nikita Khrushchev", "Hal Linden", "fourth President of Pakistan", "February 18, 1965", "automobiles", "Republican", "afterburner", "Chad", "NBA All-Star Game and All-NBA Team", "Emilia Fox", "Freeform", "Mark Masons' Hall", "Law Adam", "American", "Chief Strategy Officer", "November 15, 1903", "De La Soul", "American", "Archbishop of Canterbury", "Via Vai", "1985", "After releasing Xander from the obligation to be Sweet's `` bride '', tells the group how much fun they have been ( `` What You Feel -- Reprise '' ) and disappears", "when the cell is undergoing the metaphase of cell division", "California", "multi-user dungeon", "Dubai", "Iran", "Halle Berry", "Sindbad", "Abid Ali Neemuchwala"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7832986934089876}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20512820512820512, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7058823529411765, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407408, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-914", "mrqa_squad-validation-10328", "mrqa_squad-validation-8852", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-4362", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-3242", "mrqa_searchqa-validation-13537"], "SR": 0.71875, "CSR": 0.6670673076923077, "retrieved_ids": ["mrqa_squad-train-66269", "mrqa_squad-train-25826", "mrqa_squad-train-16806", "mrqa_squad-train-59235", "mrqa_squad-train-84383", "mrqa_squad-train-30428", "mrqa_squad-train-27744", "mrqa_squad-train-38738", "mrqa_squad-train-83554", "mrqa_squad-train-40054", "mrqa_squad-train-41051", "mrqa_squad-train-6908", "mrqa_squad-train-47436", "mrqa_squad-train-67338", "mrqa_squad-train-49207", "mrqa_squad-train-6940", "mrqa_naturalquestions-validation-9687", "mrqa_squad-validation-7373", "mrqa_searchqa-validation-7583", "mrqa_naturalquestions-validation-7710", "mrqa_squad-validation-1960", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-3884", "mrqa_squad-validation-6523", "mrqa_searchqa-validation-6624", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-3073", "mrqa_squad-validation-1565", "mrqa_hotpotqa-validation-5191", "mrqa_squad-validation-2701", "mrqa_squad-validation-1114"], "EFR": 1.0, "Overall": 0.7587259615384615}, {"timecode": 13, "before_eval_results": {"predictions": ["Tower District", "a computational problem where a single output (of a total function) is expected for every input", "social and political action", "Duran Duran", "30", "NYPD Blue", "chemical bonds", "John and Benjamin Green", "the Lippe", "between AD 0\u20131250", "2 million", "a statement to the chamber setting out the Government's legislative programme for the forthcoming year", "40,000", "Citadel Broadcasting", "$45,000", "stream capture", "735 feet", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "Robert Remak", "Eddie Murphy", "Audrey II", "Human fertilization", "assets = Liabilities + Equity", "The terrestrial biosphere", "Peter Andrew Beardsley MBE", "Leo Arnaud", "lumbar cistern", "not being pushed around by big labels, managers, and agents and being told what to do, and being true to yourself creatively", "1 US dollar worth close to 5,770 guaranies", "digitization of social systems", "Yugoslav model of state organization, as well as a `` middle way '' between planned and liberal economy", "Middlesex County", "Sweden had been an active supporter of the League of Nations", "1980s", "Qutab Ud - Din - Aibak", "they were weaker when it came to training and tertiary education", "a fully centralized service with individual user accounts", "George Strait", "silk, hair / fur ( including wool ) and feathers", "Anakin Wars", "Prince James", "Paspahegh Indians", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Spain", "Toronto, Ontario, Canada", "Manchuria", "Ben Savage", "higher the vapor pressure of a liquid at a given temperature", "Max", "Magyarorsz\u00e1g z\u00e1szlaja", "the church at Philippi", "2003", "The Sun", "northern China", "Mackinac Bridge", "Barbarella", "Bergen", "Balvenie Castle", "some money he made from trading sugar bought at a discount from the supermarket where he worked, and funds borrowed from friends to secure a visitor's visa and bus ticket to Johannesburg.", "South Africa, seeking a better life.", "Frida Kahlo", "insincere", "Elizabeth Gaskell", "Walter Reed Army Medical Center (WRAMC)"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6266887626262626}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.9777777777777777, 1.0, 1.0, 0.0, 0.1904761904761905, 0.14285714285714285, 0.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.25, 0.5, 0.0, 0.0, 0.18181818181818182, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1600", "mrqa_squad-validation-3599", "mrqa_squad-validation-4304", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-6020", "mrqa_hotpotqa-validation-877", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-16103", "mrqa_hotpotqa-validation-3149"], "SR": 0.578125, "CSR": 0.6607142857142857, "retrieved_ids": ["mrqa_squad-train-69367", "mrqa_squad-train-34793", "mrqa_squad-train-34796", "mrqa_squad-train-11465", "mrqa_squad-train-71635", "mrqa_squad-train-17143", "mrqa_squad-train-28287", "mrqa_squad-train-54164", "mrqa_squad-train-64842", "mrqa_squad-train-66837", "mrqa_squad-train-9883", "mrqa_squad-train-10417", "mrqa_squad-train-3812", "mrqa_squad-train-59455", "mrqa_squad-train-32926", "mrqa_squad-train-37896", "mrqa_searchqa-validation-4281", "mrqa_squad-validation-3885", "mrqa_newsqa-validation-3782", "mrqa_squad-validation-5347", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2423", "mrqa_squad-validation-531", "mrqa_hotpotqa-validation-2158", "mrqa_squad-validation-8852", "mrqa_naturalquestions-validation-10495", "mrqa_squad-validation-3496", "mrqa_squad-validation-2812", "mrqa_hotpotqa-validation-4266", "mrqa_squad-validation-2497", "mrqa_naturalquestions-validation-8096", "mrqa_hotpotqa-validation-4397"], "EFR": 1.0, "Overall": 0.757455357142857}, {"timecode": 14, "before_eval_results": {"predictions": ["reaffirmed Catholicism as the state religion", "an all-time high between 2005 and 2010", "Marches", "public policy", "cartels", "Anglo-Saxon populations", "Ancient Egypt", "Battle of Olustee", "patrimonial feudalism", "Daniel arap Moi", "near Millingen aan de Rijn", "an electric lighting company in Tesla's name, Tesla Electric Light & Manufacturing", "1303", "Oxygen", "three", "Lance Cpl. Maria Lauterbach", "December", "Empire of the Sun", "54 bodies", "Roger Federer", "sodium dichromate", "the two remaining crew members from the helicopter", "July", "citizenship", "40", "18", "break up ice jams", "Expedia", "Kabul", "\"We've got a long way to go, but we've made progress.\"", "Communist Party of Nepal", "Bob Dole", "Eden Park", "her home", "12.3 million", "as soon as 2050", "behind the counter", "Shanghai", "18", "National Park Service", "3-2", "Bob Bogle", "40", "1959", "Pakistan's High Commission in India", "his father", "Obama's race", "Steven Chu", "the Obama administration", "Johnny Carson", "a peace sign", "Muslim festival", "Larry Ellison", "Revolutionary Armed Forces of Colombia", "letters about his life", "Jules Shear", "Soviet Union", "Vienna", "2008\u201309 UEFA Champions League", "310", "White River Valley Chamber of Commerce", "Persia", "2017", "Algernod Lanier Washington"], "metric_results": {"EM": 0.53125, "QA-F1": 0.612016369047619}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3163", "mrqa_squad-validation-1886", "mrqa_squad-validation-1064", "mrqa_squad-validation-7017", "mrqa_squad-validation-1287", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-1020", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-680", "mrqa_naturalquestions-validation-7056", "mrqa_hotpotqa-validation-2080", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-1723", "mrqa_hotpotqa-validation-5370"], "SR": 0.53125, "CSR": 0.6520833333333333, "retrieved_ids": ["mrqa_squad-train-78397", "mrqa_squad-train-84778", "mrqa_squad-train-47729", "mrqa_squad-train-74598", "mrqa_squad-train-32703", "mrqa_squad-train-37505", "mrqa_squad-train-48853", "mrqa_squad-train-54927", "mrqa_squad-train-47661", "mrqa_squad-train-78879", "mrqa_squad-train-60334", "mrqa_squad-train-6706", "mrqa_squad-train-16100", "mrqa_squad-train-7697", "mrqa_squad-train-48934", "mrqa_squad-train-77365", "mrqa_hotpotqa-validation-4002", "mrqa_searchqa-validation-12472", "mrqa_squad-validation-7615", "mrqa_squad-validation-4147", "mrqa_squad-validation-9752", "mrqa_naturalquestions-validation-7710", "mrqa_squad-validation-1860", "mrqa_triviaqa-validation-3242", "mrqa_naturalquestions-validation-5738", "mrqa_hotpotqa-validation-4940", "mrqa_squad-validation-5678", "mrqa_squad-validation-8523", "mrqa_squad-validation-4769", "mrqa_searchqa-validation-15579", "mrqa_hotpotqa-validation-2126", "mrqa_squad-validation-363"], "EFR": 1.0, "Overall": 0.7557291666666667}, {"timecode": 15, "before_eval_results": {"predictions": ["Robert R. Gilruth", "Arthur Woolf", "Ten", "Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "seven", "Ealy", "St. George's United Methodist Church", "1914", "Wales", "more than 70,000", "Nikita Khrushchev", "in proportion to capital inputs, increasing unemployment (the \"reserve army of labour\")", "X is no more difficult than Y", "March 22", "he acted in self defense in punching businessman Marcus McGhee.", "Morgan Tsvangirai", "the Klan experienced a huge resurgence. Its membership was skyrocketing, and its political influence was increasing, so Kennedy went undercover to infiltrate the group.", "Peru's national court system", "sanctions 17 entities, including three government-owned or controlled companies used by Mugabe and his government \"to illegally siphon revenue and foreign exchange from the Zimbabwean people,\" as well as one individual.\"", "Senate Democrats", "15,000", "Kim Jong Un", "Pfc. Bowe Bergdahl", "Philip Markoff", "Democratic", "the IV cafe", "North Korea intends to launch a long-range missile in the near future", "the District of Columbia National Guard", "forgery and flying without a valid license", "work of Grayback Forestry in Medford, Oregon", "14", "All three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "school", "Monday and Tuesday", "27-year-old", "almost 100 vessels", "Venus Williams", "allergies to peanuts, nuts, shellfish, peanuts, tree nuts, wheat and soy", "as spies for more than two years,", "Manchester United", "President Obama", "Robert Barnett", "\"a striking blow to due process and the rule of law\"", "Alfredo Astiz", "is a Muslim with Lebanese heritage, but her family is \"not defined by religion,\"", "Illness", "the procedures", "Jaime Andrade", "56", "Michael Jackson", "Raymond Thomas", "racially motivated", "Adam Lambert", "in Section 60", "Georgia", "along the coast of northern California", "citric acid", "Mount Kilimanjaro", "Cond\u00e9 Nast", "capital crimes", "the Ohio River", "Hold the line, don't slack or heave around", "capitol building", "New Orleans Saints"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6030115717615718}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.36363636363636365, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.05128205128205128, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.888888888888889, 0.2857142857142857, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.7272727272727273, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-698", "mrqa_squad-validation-7183", "mrqa_squad-validation-1748", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-6596", "mrqa_triviaqa-validation-1346", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-4897", "mrqa_searchqa-validation-3948", "mrqa_searchqa-validation-10794", "mrqa_hotpotqa-validation-3685"], "SR": 0.484375, "CSR": 0.6416015625, "retrieved_ids": ["mrqa_squad-train-28368", "mrqa_squad-train-32746", "mrqa_squad-train-50617", "mrqa_squad-train-31123", "mrqa_squad-train-46779", "mrqa_squad-train-18835", "mrqa_squad-train-8874", "mrqa_squad-train-74687", "mrqa_squad-train-76470", "mrqa_squad-train-75990", "mrqa_squad-train-17378", "mrqa_squad-train-42878", "mrqa_squad-train-10037", "mrqa_squad-train-7595", "mrqa_squad-train-8575", "mrqa_squad-train-85026", "mrqa_searchqa-validation-15812", "mrqa_squad-validation-6263", "mrqa_newsqa-validation-142", "mrqa_searchqa-validation-5374", "mrqa_squad-validation-4769", "mrqa_searchqa-validation-12594", "mrqa_hotpotqa-validation-4655", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-278", "mrqa_squad-validation-1239", "mrqa_squad-validation-7357", "mrqa_hotpotqa-validation-4266", "mrqa_naturalquestions-validation-8096", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-3127", "mrqa_searchqa-validation-15930"], "EFR": 0.9696969696969697, "Overall": 0.7475722064393939}, {"timecode": 16, "before_eval_results": {"predictions": ["\"exterminate\" all non-Dalek beings", "San Diego", "30\u201375%", "\"to implement Islamic values in all spheres of life.\"", "James Watt", "comb jellies", "NewcastleGateshead", "1989", "priest", "Denmark, Iceland and Norway", "the need for alliances", "Anderson", "1969", "Debbie Gibson", "collect menstrual flow", "at least 18 or 21 years old ( or have a legal guardian present )", "10 logarithm", "Syco Music", "April 6, 1917", "The Fellowship of the Ring ( 2001 ), The Two Towers ( 2002 ) and The Return of the King ( 2003 )", "Montgomery", "at birth", "last four years unless renewed by the Reichstag", "December 1, 2009", "Miami Heat", "Timothy B. Schmit", "Alan Shearer", "Cascadia earthquake", "throughout Mexico, in particular the Central and South regions, and by people of Mexican ancestry living in other places, especially the United States", "Portugal. The Man", "blood to the liver", "1936", "Resident Commissioner", "students", "September 19, 2017", "Andy Serkis", "The Abbott and Costello Show", "John Smith", "Idaho", "Ali", "Abraham Gottlob Werner", "Jason Paige", "Jackie Robinson", "Rufus and Chaka Khan", "the body - centered cubic ( BCC ) lattice", "Kimberlin Brown", "merengue", "3", "Olivia Olson", "erosion", "Office of Inspector General", "an integral membrane protein", "Conrad Lewis", "Justice Harlan", "a person employed to write or type what another dictates or to copy what has been written by another,", "his devotion to a girl,", "Oakland, California", "World Famous Gold & Silver Pawn Shop", "three times in the head", "an outstanding caddy and a friend, and has been instrumental in many of my accomplishments.", "Psycho", "greenish-blue color", "Jimmy Carter", "yellow"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6548702583232193}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false], "QA-F1": [0.888888888888889, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.08695652173913043, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5517241379310345, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.42857142857142855, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7778", "mrqa_squad-validation-9610", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9811", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-3385", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-7642", "mrqa_hotpotqa-validation-5030", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-3751", "mrqa_triviaqa-validation-262"], "SR": 0.578125, "CSR": 0.6378676470588236, "retrieved_ids": ["mrqa_squad-train-54758", "mrqa_squad-train-61448", "mrqa_squad-train-4343", "mrqa_squad-train-17688", "mrqa_squad-train-8193", "mrqa_squad-train-40914", "mrqa_squad-train-31743", "mrqa_squad-train-75702", "mrqa_squad-train-7095", "mrqa_squad-train-3897", "mrqa_squad-train-82276", "mrqa_squad-train-42372", "mrqa_squad-train-63665", "mrqa_squad-train-71220", "mrqa_squad-train-68831", "mrqa_squad-train-25551", "mrqa_naturalquestions-validation-5586", "mrqa_hotpotqa-validation-5091", "mrqa_searchqa-validation-16899", "mrqa_newsqa-validation-80", "mrqa_squad-validation-9194", "mrqa_squad-validation-7525", "mrqa_squad-validation-8852", "mrqa_naturalquestions-validation-6466", "mrqa_newsqa-validation-3585", "mrqa_squad-validation-1529", "mrqa_naturalquestions-validation-5017", "mrqa_squad-validation-9268", "mrqa_squad-validation-6873", "mrqa_triviaqa-validation-1604", "mrqa_newsqa-validation-3435", "mrqa_searchqa-validation-4300"], "EFR": 1.0, "Overall": 0.7528860294117646}, {"timecode": 17, "before_eval_results": {"predictions": ["natural ecosystem", "Provisional Registration", "Tyne and wear Metro", "Highly combustible", "Creon", "QuickBooks", "1892", "The coordinating lead authors", "Six", "ideal pulleys", "\u00d6gedei Khan", "Princes Park", "47", "Polk County", "Mrs. Eastwood & Company", "Canada's first train robbery", "Las Vegas", "attorney", "Owsley Stanley III", "The visit", "Hong Kong First Division League", "Unbreakable", "plays for Turkish club Be\u015fikta\u015f", "The Maze Runner", "Agra", "1.6 million passengers", "actress", "Germanicus", "Jeff Van Gundy", "Joachim Trier", "Tamil", "1972", "2013", "Golden Globe Award", "Ron Goldman", "1", "Cleopatra VII Philopator", "political", "Tomorrowland", "16,116", "footballer", "Bishop's Stortford", "JoJo", "eighteenth", "The School Boys", "Operation Iceberg", "Texas Longhorn", "Hordaland", "1968", "30", "Polka", "October 22, 2012", "Soldier", "Noah Schnapp", "Paradise, Nevada", "Neptune", "Canada", "70,000", "Miami Beach, Florida", "boldly go where no man has gone before", "Esperanto", "Copenhagen", "the life of ancient Athens", "Civil War"], "metric_results": {"EM": 0.625, "QA-F1": 0.7160199175824176}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.5, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3490", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-4865", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1308", "mrqa_naturalquestions-validation-7408", "mrqa_triviaqa-validation-4029", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-3630", "mrqa_searchqa-validation-6753"], "SR": 0.625, "CSR": 0.6371527777777778, "retrieved_ids": ["mrqa_squad-train-25334", "mrqa_squad-train-33212", "mrqa_squad-train-36764", "mrqa_squad-train-59883", "mrqa_squad-train-20154", "mrqa_squad-train-12751", "mrqa_squad-train-56702", "mrqa_squad-train-17643", "mrqa_squad-train-34087", "mrqa_squad-train-61515", "mrqa_squad-train-27279", "mrqa_squad-train-52716", "mrqa_squad-train-48643", "mrqa_squad-train-17434", "mrqa_squad-train-8380", "mrqa_squad-train-27354", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2325", "mrqa_searchqa-validation-9383", "mrqa_hotpotqa-validation-5304", "mrqa_naturalquestions-validation-3616", "mrqa_squad-validation-10083", "mrqa_squad-validation-3163", "mrqa_hotpotqa-validation-4005", "mrqa_newsqa-validation-1420", "mrqa_naturalquestions-validation-953", "mrqa_newsqa-validation-3290", "mrqa_searchqa-validation-278", "mrqa_squad-validation-9236", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-5723", "mrqa_squad-validation-10305"], "EFR": 1.0, "Overall": 0.7527430555555555}, {"timecode": 18, "before_eval_results": {"predictions": ["Lagos and Quiberon Bay", "seven months old", "Sava Kosanovi\u0107", "a noble death", "probabilistic (or \"Monte Carlo\")", "large compensation pools", "Graz", "5,792 students", "Lucas\u2013Lehmer", "Wahhabism", "February 26, 1948", "Hordaland", "El Nacimiento in M\u00fazquiz Municipality", "Stephen Ireland", "Koch Industries", "Washington", "\"Su\u00f0reyjar\"", "Children's Mercy Park", "Genesee Brewing Company", "a co-founder of Metaweb", "Mike Holmgren", "Nathan Bedford Forrest", "Kim Hyun-ah", "green and yellow", "Ranulf de Gernon, 4th Earl of Chester", "Urijah Faber", "Rachel Anne Maddow", "Guthred", "Peel Holdings", "1 million copies worldwide", "ESPN College Football Friday Primetime", "Marko Tapani \" Marco\" Hietala", "In 2017, Pachulia won his first NBA Championship as a member of the Warriors.", "Sarah Hurst", "an invoice, bill or tab", "Shep Meyers", "Clarence Nash", "Golden Valley, Minnesota", "Marco Fu", "Syracuse", "Durban International Convention Centre (ICC Arena)", "Ryan Babel", "Bob Dylan", "a wooden roller ride located at Lakemont Park in Altoona, Pennsylvania", "Luca Guadagnino", "Kristine Leahy", "11 Grands Prix wins", "National Collegiate Athletic Association (NCAA)", "stand-up comedian", "thirteen", "Argentinian", "Dana Fox", "Sunday, November 2, 2003", "northwest Washington", "Jenny and Eric", "Nadia Comaneci", "The Mayor of Casterbridge", "Friday", "Luca di Montezemolo", "the Social Democratic", "a jam or preserve with no added sugar.", "accusations of improper or criminal conduct", "President Obama and Britain's Prince Charles", "\"Mad Men's\" Don Draper"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6070819805194805}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.8, 0.5, 0.5, 1.0, 0.0, 0.5, 0.6666666666666666, 0.14285714285714285, 0.5714285714285715, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9057", "mrqa_squad-validation-8020", "mrqa_squad-validation-9592", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-4757", "mrqa_naturalquestions-validation-6658", "mrqa_newsqa-validation-2163", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-3464", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2844"], "SR": 0.46875, "CSR": 0.6282894736842105, "retrieved_ids": ["mrqa_squad-train-20309", "mrqa_squad-train-66127", "mrqa_squad-train-9753", "mrqa_squad-train-31549", "mrqa_squad-train-14190", "mrqa_squad-train-81391", "mrqa_squad-train-51444", "mrqa_squad-train-1921", "mrqa_squad-train-18320", "mrqa_squad-train-57112", "mrqa_squad-train-73978", "mrqa_squad-train-1142", "mrqa_squad-train-82170", "mrqa_squad-train-26384", "mrqa_squad-train-33480", "mrqa_squad-train-8551", "mrqa_naturalquestions-validation-4401", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-1526", "mrqa_squad-validation-698", "mrqa_squad-validation-1652", "mrqa_hotpotqa-validation-1891", "mrqa_triviaqa-validation-464", "mrqa_squad-validation-234", "mrqa_newsqa-validation-2884", "mrqa_squad-validation-3981", "mrqa_hotpotqa-validation-1549", "mrqa_newsqa-validation-1879", "mrqa_squad-validation-502", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-1794"], "EFR": 1.0, "Overall": 0.7509703947368421}, {"timecode": 19, "before_eval_results": {"predictions": ["CEPR", "the Council of the European Union", "7,200", "scoil phr\u00edobh\u00e1ideach", "30\u201375%", "R\u00fcdesheim am Rhein and Koblenz", "56.2%", "computational power", "Christ's message and teachings", "Bayern Munich", "the Stade Didier Deschamps", "five", "A123 Systems, LLC", "the backside", "Basiscape", "the VHF Global Lightning and Severe Storm Monitor (V-GLASS) system.", "1975", "3,000", "youngest TV director ever", "Thom Yorke", "200", "Marco Fu", "Orfeo ed Euridice", "Fifteenth Season", "Kristin Scott Thomas", "Golden Calf", "Houston Rockets", "Summerlin, Clark County, Nevada", "Argentinian", "Europe", "Noel Gallagher.", "Savannah River Site", "a family member", "Switzerland", "second largest", "Frank Lowy", "Fat Man", "Robert Marvin \"Bobby\" Hull,", "Nye County", "Herman's Hermits", "Mani", "Pendlebury, Lancashire", "300 km north west", "1932", "Arrowhead Stadium", "1910s", "House of Borromeo", "power directly or elect representatives from among themselves to form a governing body, such as a parliament.", "Sydney", "Michael Redgrave", "KlingStubbins", "Big 12", "Argand", "2017", "Fighter Command", "John McEnroe", "Matricide", "helicopters and unmanned aerial vehicles", "a one-shot victory in the Bob Hope Classic", "Israel's", "Dune", "fire", "Marie Antoinette", "Jupiter"], "metric_results": {"EM": 0.625, "QA-F1": 0.7125236742424242}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.7272727272727273, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4545", "mrqa_squad-validation-4813", "mrqa_squad-validation-9093", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-119", "mrqa_naturalquestions-validation-954", "mrqa_newsqa-validation-2858", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-13145"], "SR": 0.625, "CSR": 0.628125, "retrieved_ids": ["mrqa_squad-train-21890", "mrqa_squad-train-29175", "mrqa_squad-train-3163", "mrqa_squad-train-9441", "mrqa_squad-train-39968", "mrqa_squad-train-83865", "mrqa_squad-train-66219", "mrqa_squad-train-23500", "mrqa_squad-train-27701", "mrqa_squad-train-56511", "mrqa_squad-train-41615", "mrqa_squad-train-5778", "mrqa_squad-train-42144", "mrqa_squad-train-57115", "mrqa_squad-train-75323", "mrqa_squad-train-45591", "mrqa_hotpotqa-validation-4535", "mrqa_naturalquestions-validation-9230", "mrqa_squad-validation-9268", "mrqa_searchqa-validation-4199", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-2030", "mrqa_squad-validation-4769", "mrqa_hotpotqa-validation-5228", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5738", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1225", "mrqa_naturalquestions-validation-3658", "mrqa_hotpotqa-validation-3884", "mrqa_searchqa-validation-16198", "mrqa_squad-validation-1287"], "EFR": 1.0, "Overall": 0.7509374999999999}, {"timecode": 20, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1582", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2022", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2295", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3571", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3693", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-3881", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-447", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5718", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4155", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-1038", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-402", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-642", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12472", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-1723", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10115", "mrqa_squad-validation-10249", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10328", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1064", "mrqa_squad-validation-1064", "mrqa_squad-validation-1114", "mrqa_squad-validation-1162", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1255", "mrqa_squad-validation-1296", "mrqa_squad-validation-1366", "mrqa_squad-validation-1480", "mrqa_squad-validation-1529", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1597", "mrqa_squad-validation-1723", "mrqa_squad-validation-1748", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-2040", "mrqa_squad-validation-2059", "mrqa_squad-validation-2111", "mrqa_squad-validation-2144", "mrqa_squad-validation-2153", "mrqa_squad-validation-2315", "mrqa_squad-validation-234", "mrqa_squad-validation-2475", "mrqa_squad-validation-2536", "mrqa_squad-validation-2568", "mrqa_squad-validation-2628", "mrqa_squad-validation-2701", "mrqa_squad-validation-2786", "mrqa_squad-validation-287", "mrqa_squad-validation-2898", "mrqa_squad-validation-2976", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3198", "mrqa_squad-validation-3240", "mrqa_squad-validation-3483", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4015", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4259", "mrqa_squad-validation-4263", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-4545", "mrqa_squad-validation-457", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4646", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5200", "mrqa_squad-validation-5276", "mrqa_squad-validation-531", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5542", "mrqa_squad-validation-556", "mrqa_squad-validation-5611", "mrqa_squad-validation-5655", "mrqa_squad-validation-5678", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6263", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-6500", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6523", "mrqa_squad-validation-6538", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6657", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-689", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-7001", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7062", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7152", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7302", "mrqa_squad-validation-7302", "mrqa_squad-validation-7323", "mrqa_squad-validation-7391", "mrqa_squad-validation-7403", "mrqa_squad-validation-7450", "mrqa_squad-validation-7603", "mrqa_squad-validation-7692", "mrqa_squad-validation-7732", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8077", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8219", "mrqa_squad-validation-8225", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8447", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-8612", "mrqa_squad-validation-864", "mrqa_squad-validation-8668", "mrqa_squad-validation-8685", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-876", "mrqa_squad-validation-8836", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9061", "mrqa_squad-validation-9076", "mrqa_squad-validation-9101", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9240", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9652", "mrqa_squad-validation-9716", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9956", "mrqa_squad-validation-9991", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4403", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7642"], "OKR": 0.91796875, "KG": 0.4390625, "before_eval_results": {"predictions": ["Bento de Moura Portugal", "Mongolia", "polynomial-time", "All India Muslim League", "CBS Sports.com", "1967", "an antigen from a pathogen", "Encoded Archival description (EAD)", "Trey Parker and Matt Stone", "First Family of Competitive eating", "the Democratic Unionist Party (DUP)", "local South Australian and Australian produced content", "Naked Soldier", "7 June 1926 to 17 December 1926", "1900", "2000", "John Lee Hancock", "Hordaland", "Agent 99", "Bonnie Franklin", "Edmonton, Alberta", "Skyscraper", "Love Actually", "body of water", "Orange County, California", "High Mobility Multipurpose Wheeled Vehicle (HMMWV)", "The bald eagle", "32 people", "Tauti\u0161ka giesm\u0117", "Philadelphia Naval Shipyard", "The Books", "Rochdale, North West England", "2013\u201314 Premier League", "Clara Petacci", "Claire Fraser", "Lionel Brockman Richie Jr.", "The Two Noble Kinsmen", "Eucritta melanolimnetes", "Jenji Kohan", "Johnson & Johnson", "PewDie Pie", "Germanic", "November 5, 2002", "Tamara Ecclestone Rutland", "Thomas Joseph \"T. J. Lavin", "Stalybridge Celtic", "Adelaide Lightning", "Kohlberg K Travis Roberts", "\"My Love from the Star\"", "Tottenham Hotspur F.C.", "Sam Bettley", "Don DeLillo", "Nia Kay", "The Fixx", "The Crossing", "1876", "dynamite", "the chaos and horrified reactions after the July 7, 2005, London transit bombings", "a head injury", "The Humayun's Tomb", "teeth", "Profit maximization", "electron shells", "1901"], "metric_results": {"EM": 0.53125, "QA-F1": 0.626076388888889}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.7499999999999999, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.08, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1764", "mrqa_squad-validation-525", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-4819", "mrqa_naturalquestions-validation-8584", "mrqa_triviaqa-validation-7266", "mrqa_newsqa-validation-894", "mrqa_searchqa-validation-9565"], "SR": 0.53125, "CSR": 0.6235119047619048, "retrieved_ids": ["mrqa_squad-train-16532", "mrqa_squad-train-19834", "mrqa_squad-train-68345", "mrqa_squad-train-79003", "mrqa_squad-train-44420", "mrqa_squad-train-30537", "mrqa_squad-train-26734", "mrqa_squad-train-4266", "mrqa_squad-train-1090", "mrqa_squad-train-8180", "mrqa_squad-train-24631", "mrqa_squad-train-2843", "mrqa_squad-train-39458", "mrqa_squad-train-8650", "mrqa_squad-train-50405", "mrqa_squad-train-34743", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-5370", "mrqa_triviaqa-validation-7642", "mrqa_naturalquestions-validation-4953", "mrqa_newsqa-validation-686", "mrqa_hotpotqa-validation-5485", "mrqa_newsqa-validation-3290", "mrqa_searchqa-validation-5540", "mrqa_naturalquestions-validation-5531", "mrqa_squad-validation-4015", "mrqa_hotpotqa-validation-4771", "mrqa_squad-validation-10328", "mrqa_hotpotqa-validation-5477", "mrqa_triviaqa-validation-7417", "mrqa_naturalquestions-validation-5938", "mrqa_squad-validation-8020"], "EFR": 1.0, "Overall": 0.7472805059523809}, {"timecode": 21, "before_eval_results": {"predictions": ["an Orthodox priest", "December 12", "the Hostmen", "Apollo 8", "Antigone", "a pyrenoid and thylakoids stacked in groups of three", "Sugarfoot", "\"Crossed: Badlands\"", "Forbes", "Mitsubishi Motors Corporation", "Tabasco", "1 January 1788", "Lowestoft", "Christopher Nolan", "Taylor Swift", "Al Horford", "Johan Leysen", "1854", "Swiss", "Pittsburgh Steelers", "professional footballer", "Logar", "Lauren Alaina", "Ian Fleming", "Mary Bonauto, Susan Murray, and Beth Robinson", "27 November 1956", "\"Om / Six Organs of Admittance\"", "Hindi", "United States Auto Club", "\"The Clash of Triton\"", "Albany High School", "BAFTA TV Award", "\"The Bob Edwards Show\"", "\"the heaviest album of all\"", "Field Marshal Lord Gort", "15,024", "Prime Minister of Denmark 1852\u20131853 as head of the Cabinet of Bluhme I (the \"January Cabinet\")", "the most awarded female act of all-time", "\"Alberta\"", "2007", "3,000", "Chinese Coffee", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Mineola", "KWPW", "John Richard Schlesinger", "Deputy Majority Whip", "Atomic Kitten", "Esperanza Spalding", "the Ruul", "Lonestar", "1916", "American", "16", "The Golden Gate Bridge", "David Jason", "Olive", "Turkey", "that the 50-year-old King of Pop has agreed to a series of summer concerts at the O2", "Entourage", "the Tet Offensive", "Erica Rivera", "Spanish missionaries", "Madison"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7170195256132756}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1258", "mrqa_squad-validation-8651", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-347", "mrqa_naturalquestions-validation-1640", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-3348"], "SR": 0.578125, "CSR": 0.6214488636363636, "retrieved_ids": ["mrqa_squad-train-30045", "mrqa_squad-train-2670", "mrqa_squad-train-5749", "mrqa_squad-train-62115", "mrqa_squad-train-32412", "mrqa_squad-train-23017", "mrqa_squad-train-57258", "mrqa_squad-train-948", "mrqa_squad-train-30966", "mrqa_squad-train-30028", "mrqa_squad-train-88", "mrqa_squad-train-85863", "mrqa_squad-train-40318", "mrqa_squad-train-65651", "mrqa_squad-train-9284", "mrqa_squad-train-15718", "mrqa_newsqa-validation-733", "mrqa_squad-validation-360", "mrqa_searchqa-validation-13452", "mrqa_squad-validation-5029", "mrqa_hotpotqa-validation-3073", "mrqa_squad-validation-3496", "mrqa_searchqa-validation-15579", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-1618", "mrqa_squad-validation-1064", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1508", "mrqa_searchqa-validation-15640", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-5113", "mrqa_searchqa-validation-9565"], "EFR": 0.9629629629629629, "Overall": 0.7394604903198653}, {"timecode": 22, "before_eval_results": {"predictions": ["1688\u20131692", "WLQP-LP", "Sunni extremist groups", "mid-Eocene", "leftist/communist/nationalist insurgents/opposition", "enthusiasm", "the 34th President of the United States", "receive the benefits of the Morrill Acts of 1862 and 1890", "Prussian", "Violet", "German", "Shameless", "Indianola", "the gods themselves", "Nassau County", "What Are Little Boys Made Of", "Andries Jonker", "President John F. Kennedy and First Lady Jacqueline Kennedy", "Mollie Elizabeth King", "1959", "129,007", "San Francisco 49ers", "Big Machine Records", "the Parthian Empire", "3 million people", "Matt Groening", "June 10, 1982", "Philip K. Dick", "John Anthony \"Jack\" White", "Sam Kinison", "Boston", "Lisa", "four months in jail", "Galleria Vittorio Emanuele II", "Paul Avery", "31 October 1783", "Puli Alam", "Civil War", "1838", "John Major", "Ashland is home to Scribner-Fellows State Forest", "Manchester Victoria station in air rights space", "Leinster", "Estadio de L\u00f3pez Cort\u00e1zar", "November 2, 2003", "Victor Garber", "Agent Carter", "Plies", "Tim \"Ripper\" Owens", "Walt Disney Productions", "chalk quarry", "Rhode Island", "electron donors", "President since creation of the office in 1789", "Nala", "Peter Townsend.", "the River Thames", "The Sunday Post", "the lower house of parliament", "Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Stephen Johns reportedly opened the door for the man police say was his killer.\"", "dugout canoe", "we/wee", "Tim Russert"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7856534090909091}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3754", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-4826", "mrqa_hotpotqa-validation-1531", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-3736", "mrqa_hotpotqa-validation-2737", "mrqa_naturalquestions-validation-4370", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-2945", "mrqa_searchqa-validation-11779"], "SR": 0.6875, "CSR": 0.6243206521739131, "retrieved_ids": ["mrqa_squad-train-71737", "mrqa_squad-train-46645", "mrqa_squad-train-74546", "mrqa_squad-train-48720", "mrqa_squad-train-57646", "mrqa_squad-train-2744", "mrqa_squad-train-15608", "mrqa_squad-train-10559", "mrqa_squad-train-49299", "mrqa_squad-train-5884", "mrqa_squad-train-60764", "mrqa_squad-train-4291", "mrqa_squad-train-76019", "mrqa_squad-train-85194", "mrqa_squad-train-53826", "mrqa_squad-train-61476", "mrqa_newsqa-validation-3405", "mrqa_hotpotqa-validation-1247", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-1818", "mrqa_searchqa-validation-13145", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4940", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-10039", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-12472", "mrqa_naturalquestions-validation-10004", "mrqa_newsqa-validation-1992", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-2044"], "EFR": 1.0, "Overall": 0.7474422554347826}, {"timecode": 23, "before_eval_results": {"predictions": ["Jones et al. 1998, Pollack, Huang & Shen 1998, Crowley & Lowery 2000 and Briffa 2000", "the Moselle", "static discs", "Pittsburgh Steelers", "14,000", "Scandinavia and northern Europe", "1911", "ESPN's \"SportsCenter\"", "Prince Antoni Radziwi\u0142\u0142", "northern Italy's Lombardy region", "Caesars Palace Grand Prix", "Detroit, Michigan", "Wilton Mall at Saratoga (or simply The Wilton mall)", "the Sun", "Point of Entry", "Chicago", "Malayalam cinema", "Pendlebury, Lancashire", "Leona Louise Lewis", "Laura Dern", "Melbourne", "democracy and personal freedom", "Cool Runnings", "Indian", "Labour Party", "Thor", "Copa Airlines", "Chiltern Hills", "\"SexyBack\"", "Washington", "Big Mamie", "Edward James Olmos", "Norse ancestry and culture", "\"Slaughterhouse-Five\"", "Nashville", "simple language", "Telugu and Tamil film industries", "\"Peshwa\"", "Joseph I", "Oracle Corporation", "1999", "William Shakespeare", "1898", "1953", "Bergen", "Scribner", "Apprendi v. New Jersey", "the Philadelphia Eagles", "William Scott Elam", "2003", "The Bridge Between Science and Theology", "R&B", "pilgrimages to Jerusalem", "art pottery", "restoring someone's faith in love and family relationships", "California Chrome", "Moby Dick", "Christine Keeler", "137", "The park bench facing Lake Washington", "Sunday", "Jackie Robinson", "the DEW Line", "snake"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7146905637254901}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.5, 0.0, 0.823529411764706, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-2129", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-354", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-1656", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-2866", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-3186", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-1059"], "SR": 0.609375, "CSR": 0.6236979166666667, "retrieved_ids": ["mrqa_squad-train-8378", "mrqa_squad-train-45824", "mrqa_squad-train-36465", "mrqa_squad-train-79522", "mrqa_squad-train-5235", "mrqa_squad-train-63469", "mrqa_squad-train-16832", "mrqa_squad-train-20346", "mrqa_squad-train-52967", "mrqa_squad-train-1018", "mrqa_squad-train-86581", "mrqa_squad-train-48978", "mrqa_squad-train-83063", "mrqa_squad-train-25191", "mrqa_squad-train-59508", "mrqa_squad-train-83437", "mrqa_hotpotqa-validation-31", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-5274", "mrqa_squad-validation-1025", "mrqa_searchqa-validation-11779", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2737", "mrqa_newsqa-validation-1992", "mrqa_hotpotqa-validation-5601", "mrqa_naturalquestions-validation-4094", "mrqa_squad-validation-1886", "mrqa_hotpotqa-validation-5843", "mrqa_naturalquestions-validation-5938", "mrqa_searchqa-validation-6929", "mrqa_hotpotqa-validation-4655"], "EFR": 0.96, "Overall": 0.7393177083333333}, {"timecode": 24, "before_eval_results": {"predictions": ["Los Angeles", "combustion chamber", "13th", "spinat", "ACL tears", "Bury Football Club", "Las Vegas", "Suzuki YZF-R6", "Koninklijke Ahold N.V.", "the east of Scotland", "The Gettysburg Address", "Science", "Lufthansa heist", "American 3D computer-animated comedy", "the Asia-Pacific War", "Amy Poehler", "footballer", "British Labour Party", "USC Marshall School of Business", "Sim Theme Park", "1937", "The Wolf of Wall Street", "Maxwell Smart", "The Walking Dead", "2008", "Yasir Hussain", "Let's Make Sure We Kiss Goodbye", "Ronald Ryan", "James Brolin", "soccer", "Peel Holdings", "the Chechen Republic", "alcoholic drinks", "Zaire", "Debbie Harry", "Barbara Niven", "novelist and poet", "1986", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Hilary Duff", "100 million", "Albanian political party", "2015 Masters Tournament", "John Schlesinger", "Venice", "Rockstar San Diego", "A.S. Roma", "Genderqueer", "Gothic Revival", "Melbourne's City Centre", "South West Peninsula League", "Bury St Edmunds, Suffolk, England", "1608", "Sedimentary rock", "Rugrats in Paris", "auk", "a cocktail traditionally made with cognac, orange liqueur (Cointreau, Grand Marnier or another triple sec), and lemon juice.", "Rose-Marie", "John McCain", "Colorado Attorney General John Suthers", "Spanish Davis Cup hero Fernando Verdasco", "pink", "calcium", "Si-Tchuan"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6744295634920635}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.6, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.7499999999999999, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10449", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-1741", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-234", "mrqa_naturalquestions-validation-9626", "mrqa_triviaqa-validation-6356", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-1361", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-14782"], "SR": 0.59375, "CSR": 0.6225, "retrieved_ids": ["mrqa_squad-train-2958", "mrqa_squad-train-43799", "mrqa_squad-train-77345", "mrqa_squad-train-82497", "mrqa_squad-train-44981", "mrqa_squad-train-58511", "mrqa_squad-train-49406", "mrqa_squad-train-7281", "mrqa_squad-train-82190", "mrqa_squad-train-25572", "mrqa_squad-train-57715", "mrqa_squad-train-7759", "mrqa_squad-train-73394", "mrqa_squad-train-25689", "mrqa_squad-train-55255", "mrqa_squad-train-23656", "mrqa_hotpotqa-validation-5352", "mrqa_squad-validation-1258", "mrqa_hotpotqa-validation-550", "mrqa_squad-validation-1877", "mrqa_hotpotqa-validation-2737", "mrqa_newsqa-validation-655", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-6730", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3307", "mrqa_naturalquestions-validation-8617", "mrqa_squad-validation-6873", "mrqa_hotpotqa-validation-347", "mrqa_squad-validation-3490", "mrqa_naturalquestions-validation-10205", "mrqa_hotpotqa-validation-1047"], "EFR": 1.0, "Overall": 0.747078125}, {"timecode": 25, "before_eval_results": {"predictions": ["August 15, 1971", "Geordie", "Peter Howell", "many elements", "formalize a unified front in trade and negotiations with various Indians", "French", "Upper Manhattan, New York City", "in 2017", "Logan International Airport", "GZA", "Paradzhanov", "no. 3", "John John Florence", "Two Is Better Than One", "July 16, 1971", "Microsoft Office", "Baldwin is a hamlet and census-designated place (CDP) located in the town of Hempstead in Nassau County, New York, United States", "Elton John", "Firestorm", "the Ruul", "March 14, 2000", "David Wells", "Philip Pullman's", "Chengdu Aircraft Corporation (CAC) of China", "Michael A. Cremo", "Minnesota", "Oklahoma", "I Should Have Known Better", "Smithfield, Rhode Island, U.S.", "Julie Taymor", "29 September\u20132 October 2011", "Columbia Records", "1943", "Maria Brink", "\"The Braes o' Bowhether\"", "Cody Miller", "Darkroom", "\"Tainted Love\"", "Christopher Nolan", "The Blue Album", "1992", "2016 United States elections", "pro-Confederate partisan rangers (\"bushwhackers\")", "Princes Park in Melbourne", "The Late Late Show", "2012", "1978", "Donald Carl \"Don\" Swayze", "John Morgan", "May 1801", "an organ", "Macau", "49 cents", "Certificate of Release or Discharge from Active Duty", "Jocelyn Flores", "Hyperbole", "Egypt", "tiger", "1831", "June 2002", "A growing percentage of the Somali population has become dependent on humanitarian aid", "Thomas Nast", "ice hockey", "Betty la fea"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6914749313186813}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.46153846153846156, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-5504", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2081", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-3386", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1718", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-2092", "mrqa_triviaqa-validation-3925", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-540", "mrqa_searchqa-validation-172"], "SR": 0.578125, "CSR": 0.6207932692307692, "retrieved_ids": ["mrqa_squad-train-34795", "mrqa_squad-train-78554", "mrqa_squad-train-23973", "mrqa_squad-train-25186", "mrqa_squad-train-68509", "mrqa_squad-train-68164", "mrqa_squad-train-17846", "mrqa_squad-train-28944", "mrqa_squad-train-9196", "mrqa_squad-train-13162", "mrqa_squad-train-2967", "mrqa_squad-train-3891", "mrqa_squad-train-1681", "mrqa_squad-train-34152", "mrqa_squad-train-54184", "mrqa_squad-train-63841", "mrqa_searchqa-validation-16725", "mrqa_squad-validation-7246", "mrqa_hotpotqa-validation-4015", "mrqa_triviaqa-validation-3242", "mrqa_newsqa-validation-2884", "mrqa_naturalquestions-validation-5468", "mrqa_squad-validation-8696", "mrqa_naturalquestions-validation-6020", "mrqa_hotpotqa-validation-1825", "mrqa_newsqa-validation-1059", "mrqa_naturalquestions-validation-2106", "mrqa_hotpotqa-validation-4740", "mrqa_squad-validation-4545", "mrqa_newsqa-validation-1879", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-5635"], "EFR": 0.9629629629629629, "Overall": 0.7393293714387463}, {"timecode": 26, "before_eval_results": {"predictions": ["Italian physicist", "illiberal Islamic regimes", "The Judicial Council", "CBS", "berceuse", "Blue Jean", "the Markhor", "Hebridean isle", "prostate", "Vitus Bering", "fuel", "Rwanda", "larva", "a claws", "Der Zauberberg", "the bassoon", "Canada", "Komodo Dragon", "a fiber-optic viewing scope", "Billy Bob Thornton", "brush", "what you did", "Last Summer", "radio waves", "a tiger", "Isis", "Eliza Doolittle", "fibrous connective tissue that attaches a muscle to a bone", "En banc", "Franklin D. Roosevelt", "mercury", "Violeta Chamorro", "Take My Breath Away", "Rafael Nadal", "Canberra", "Ich bin ein Berliner", "caesar's wife", "Quiz", "Josie Geller", "Platoon", "William Augustus, Duke of Cumberland", "Banana Boat Song", "Nanjing", "asparagus beetles", "goodbye", "blubber", "catalysts", "the Stanford-Binet Intelligence test", "Germany", "Deep Purple", "moses", "Ernie Klump", "transform agricultural productivity", "Speaker of the House of Representatives", "the Bulgarian 1st Army", "20", "Kevin ( Craig Warnock)", "pet bird", "Kerry Marie Butler", "17", "three centuries", "Consumer Product Safety Commission", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the United Nations"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49635416666666665}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15646", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-10928", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-2775", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-2526", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-4299", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-4190", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-7196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-2819", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-484", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1748"], "SR": 0.40625, "CSR": 0.6128472222222222, "retrieved_ids": ["mrqa_squad-train-85715", "mrqa_squad-train-85258", "mrqa_squad-train-76316", "mrqa_squad-train-24246", "mrqa_squad-train-39215", "mrqa_squad-train-63103", "mrqa_squad-train-10636", "mrqa_squad-train-74038", "mrqa_squad-train-36595", "mrqa_squad-train-19116", "mrqa_squad-train-10316", "mrqa_squad-train-65435", "mrqa_squad-train-30675", "mrqa_squad-train-19750", "mrqa_squad-train-50908", "mrqa_squad-train-14372", "mrqa_newsqa-validation-1793", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-5688", "mrqa_squad-validation-8447", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-7076", "mrqa_squad-validation-6443", "mrqa_hotpotqa-validation-337", "mrqa_naturalquestions-validation-7056", "mrqa_hotpotqa-validation-132", "mrqa_naturalquestions-validation-7962", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4819", "mrqa_squad-validation-9610", "mrqa_squad-validation-360", "mrqa_squad-validation-4545"], "EFR": 0.9736842105263158, "Overall": 0.7398844115497076}, {"timecode": 27, "before_eval_results": {"predictions": ["May 18, 1756", "canceled", "modern fashion", "more than half", "Lismore", "the Space Shuttle \"Discovery\"", "Paradise, Nevada", "European culture", "Newcastle upon Tyne, England", "Colonel", "Cody Miller", "Virginia", "Thrifty Automotive Group", "Wiz Khalifa", "Disney California Adventure", "Maria Brink", "The Sound of Music", "G\u00f6tene", "Italian", "novelty songs, comedy, and strange or unusual recordings", "Perfect 10", "6teen", "South America", "Princes Park", "Alan Blessed, Lance Henriksen, Wayne Knight, and Nigel Hawthorne", "the Knight Company", "My Gorgeous Life", "Ashanti Region", "the Dutch Empire", "Culiac\u00e1n, Sinaloa", "Northampton, England", "Black Panthers", "Fred Willard", "beer", "Dara Grace Torres", "nine", "1909", "the E22", "3,384,569", "an anvil firing", "House of Hohenstaufen", "James G. Kiernan", "Johnnie Ray", "Forrest Gump", "2009", "1919", "a skerry", "1620 to 1691", "the exiled House of Stuart", "Gilley's Club", "Blue Origin", "2015 Orange Bowl", "2017", "Eleanor Roosevelt", "Utah, Arizona, Wyoming", "Google", "the moon", "the Kinks", "throwing three punches but said only one connected.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997", "1975", "the sperm whale", "libraries", "NASA"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6687161796536796}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.28571428571428575, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-878", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-1233", "mrqa_hotpotqa-validation-296", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4650", "mrqa_hotpotqa-validation-1965", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-4645", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-2829", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2586"], "SR": 0.578125, "CSR": 0.6116071428571428, "retrieved_ids": ["mrqa_squad-train-18864", "mrqa_squad-train-41644", "mrqa_squad-train-41491", "mrqa_squad-train-52203", "mrqa_squad-train-66012", "mrqa_squad-train-62808", "mrqa_squad-train-68163", "mrqa_squad-train-70869", "mrqa_squad-train-51847", "mrqa_squad-train-51712", "mrqa_squad-train-82952", "mrqa_squad-train-20024", "mrqa_squad-train-70309", "mrqa_squad-train-40908", "mrqa_squad-train-7868", "mrqa_squad-train-20017", "mrqa_searchqa-validation-8453", "mrqa_hotpotqa-validation-4757", "mrqa_squad-validation-6878", "mrqa_hotpotqa-validation-4357", "mrqa_squad-validation-6443", "mrqa_searchqa-validation-13492", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1004", "mrqa_searchqa-validation-14178", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-5232", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-4281", "mrqa_hotpotqa-validation-3013"], "EFR": 1.0, "Overall": 0.7448995535714286}, {"timecode": 28, "before_eval_results": {"predictions": ["British patrons", "Laverne & Shirley", "the U.S. South", "Roger Bacon", "bacteria", "the fourth rated institution in Pennsylvania", "Arthur Pitney", "unicorns to leprechauns", "a rod", "Sofia Scicolone", "Qubec", "Edith Piaf", "the Krntnertor", "Sappho", "a fruit snack made by General Mills (GM) in the brand line", "Colorado", "Hershey", "Timothy Leary", "3800", "HIV/AIDS", "John Grisham", "a ruby", "Alice Addertongue", "Doctor", "a 1.5 km swim", "calcium", "Mikel Arteta", "Wisconsin", "Sandro Botticelli", "To Build a Fire", "auk", "Docu Drama", "a nave", "centigrade", "silver", "the BBC", "penguins", "Jack Lewis", "Blackwater", "apogee", "Conrad N. Hilton", "October", "Joan Baez", "Hadrosaurus", "\"E-T\"", "an ambitious set menu, featuring interesting local and seasonal ingredients", "the C&D Canal", "asthma", "a duck", "a trumpet", "Narcissus", "Marion", "11,843 mph", "its population", "Sarah Silverman", "Anwar Sadat", "apogee", "Barings", "Esp\u00edrito Santo", "Earvin \"Magic\" Johnson Jr.", "Elliot Fletcher", "a free laundry service", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4764407384740922}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.23255813953488372, 0.5263157894736842]}}, "before_error_ids": ["mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-11819", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-8603", "mrqa_searchqa-validation-4398", "mrqa_searchqa-validation-4388", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-6355", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-13129", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-841", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-7159", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-3402", "mrqa_hotpotqa-validation-4972", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3961"], "SR": 0.40625, "CSR": 0.6045258620689655, "retrieved_ids": ["mrqa_squad-train-4863", "mrqa_squad-train-27159", "mrqa_squad-train-48931", "mrqa_squad-train-47231", "mrqa_squad-train-70324", "mrqa_squad-train-56667", "mrqa_squad-train-4768", "mrqa_squad-train-23375", "mrqa_squad-train-57215", "mrqa_squad-train-80826", "mrqa_squad-train-2743", "mrqa_squad-train-25770", "mrqa_squad-train-8603", "mrqa_squad-train-18072", "mrqa_squad-train-26952", "mrqa_squad-train-20650", "mrqa_naturalquestions-validation-6897", "mrqa_hotpotqa-validation-3151", "mrqa_squad-validation-3139", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-1549", "mrqa_naturalquestions-validation-3962", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4740", "mrqa_searchqa-validation-4190", "mrqa_naturalquestions-validation-3916", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4300", "mrqa_squad-validation-1748", "mrqa_newsqa-validation-2324", "mrqa_naturalquestions-validation-1640"], "EFR": 0.9736842105263158, "Overall": 0.7382201395190562}, {"timecode": 29, "before_eval_results": {"predictions": ["ESPN Sports", "11 million", "GTE", "secondary school study", "John Lee Hancock", "2007", "Westfield Tea Tree Plaza", "Philadelphia", "237", "Gal Gadot", "1860", "Eddie Izzard", "1966", "Miracle", "James Ellison", "poet, and writer", "Humberside Airport", "8 Simple Rules", "2015", "Kind Hearts and Coronets", "Centennial Olympic Stadium", "Leatherheads", "Polka", "Sinngedichte", "James G. Kiernan", "Australia", "heavy metal band", "Anah\u00ed", "publicly", "Tampa Bay Lightning", "tabasco peppers", "Patricia Arquette", "Secrets and Lies", "William Shand Kydd", "non-alcoholic", "Crystal dynamics", "Geraldine Page", "pornographicstar", "Europe", "179", "five-time", "Sam the Sham", "Frank Edward Thomas Jr.", "High Falls Brewery", "Las Vegas", "Pittsburgh Steelers organization of the National Hockey League", "George I of Great Britain", "J35", "Engirundho Vandhaal", "Romance", "Bohemia", "Macomb County", "birth", "biochemistry", "provides the public with financial information about a nonprofit organization", "Elgar's Enigma Variations", "Stockholm syndrome", "magnetism", "Morgan Tsvangirai", "Empire of the Sun", "Amanda Knox's aunt", "Robert Bruce", "Dallas", "gulls"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6500744047619047}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 1.0, 0.5, 0.0, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5826", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-2004", "mrqa_naturalquestions-validation-9088", "mrqa_triviaqa-validation-5397", "mrqa_triviaqa-validation-3393", "mrqa_newsqa-validation-3391", "mrqa_searchqa-validation-5455", "mrqa_searchqa-validation-9860"], "SR": 0.578125, "CSR": 0.6036458333333333, "retrieved_ids": ["mrqa_squad-train-30497", "mrqa_squad-train-14417", "mrqa_squad-train-19901", "mrqa_squad-train-17701", "mrqa_squad-train-46598", "mrqa_squad-train-37666", "mrqa_squad-train-58291", "mrqa_squad-train-49320", "mrqa_squad-train-67647", "mrqa_squad-train-22646", "mrqa_squad-train-20022", "mrqa_squad-train-22219", "mrqa_squad-train-72359", "mrqa_squad-train-34459", "mrqa_squad-train-40219", "mrqa_squad-train-54928", "mrqa_squad-validation-3885", "mrqa_hotpotqa-validation-76", "mrqa_squad-validation-7373", "mrqa_hotpotqa-validation-881", "mrqa_squad-validation-3754", "mrqa_hotpotqa-validation-2342", "mrqa_squad-validation-260", "mrqa_hotpotqa-validation-5274", "mrqa_searchqa-validation-10794", "mrqa_squad-validation-2786", "mrqa_newsqa-validation-1020", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5851", "mrqa_squad-validation-9057", "mrqa_searchqa-validation-9130", "mrqa_hotpotqa-validation-1816"], "EFR": 1.0, "Overall": 0.7433072916666666}, {"timecode": 30, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1180", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1342", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1488", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-172", "mrqa_hotpotqa-validation-1726", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2199", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2825", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-3071", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3191", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3402", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4285", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4467", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5247", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-5471", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5561", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5713", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-704", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-98", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1199", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1503", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11434", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-13831", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-2489", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-2686", "mrqa_searchqa-validation-3516", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6045", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-8245", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9601", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10223", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10424", "mrqa_squad-validation-10449", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1064", "mrqa_squad-validation-1096", "mrqa_squad-validation-1114", "mrqa_squad-validation-1177", "mrqa_squad-validation-1232", "mrqa_squad-validation-1255", "mrqa_squad-validation-1258", "mrqa_squad-validation-1296", "mrqa_squad-validation-1480", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1723", "mrqa_squad-validation-1940", "mrqa_squad-validation-1976", "mrqa_squad-validation-2153", "mrqa_squad-validation-2272", "mrqa_squad-validation-2301", "mrqa_squad-validation-234", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2591", "mrqa_squad-validation-287", "mrqa_squad-validation-2976", "mrqa_squad-validation-306", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3296", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3716", "mrqa_squad-validation-3767", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4026", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4259", "mrqa_squad-validation-4286", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4545", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4810", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5276", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5532", "mrqa_squad-validation-556", "mrqa_squad-validation-5578", "mrqa_squad-validation-5611", "mrqa_squad-validation-5779", "mrqa_squad-validation-5839", "mrqa_squad-validation-5881", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6443", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6888", "mrqa_squad-validation-6967", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7125", "mrqa_squad-validation-7152", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7323", "mrqa_squad-validation-7403", "mrqa_squad-validation-7458", "mrqa_squad-validation-7603", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-8219", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-864", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8839", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9061", "mrqa_squad-validation-9191", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9521", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9647", "mrqa_squad-validation-9652", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2654", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-830"], "OKR": 0.931640625, "KG": 0.4796875, "before_eval_results": {"predictions": ["any member of the Scottish Government", "quadratic", "1879", "Xiu Li Dai", "the North Cascades range of, Washington", "northwest Washington", "1924", "England", "Millerlite", "the status line", "Benjamin Franklin", "late - night", "into the intermembrane space", "Chinese", "Laodicea", "The United States Secretary of State", "electric potential generated by muscle cells when these cells are electrically or neurologically activated", "thick skin", "a Islamic shrine", "Sylvester Stallone", "Anakin Skywalker", "September 27, 2017", "convert single - stranded genomic RNA into double - stranded cDNA", "the economy", "Victory gardens", "Paul Hogan", "961", "northern China", "gathering money from the public", "a beach in Malibu", "Humpty Alexander Dumpty", "homicidal thoughts of a troubled youth", "North America", "Sun Tzu", "18th century", "Setsuko Thurlow", "temporal lobes", "Ramelle", "DNA replication", "mining", "Keith Timberwolvesodeaux", "six", "Atlantic", "butane", "Julia Roberts", "12 November 2010", "Brazil, China, France, Germany, India, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey, the United Kingdom,", "Aaron Harrison", "Panning", "CBS Television City", "Mike Higham", "James Chadwick", "Red Tarn", "eutrophication", "Worcester Cathedral", "Germany", "Rachel, Nevada", "Atlanta, Georgia", "more than 30 Latin American and Caribbean nations", "two women", "police dogs", "a Observatory", "Akram Farid", "Hannibal"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6126809166089352}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665, 0.35294117647058826, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4864864864864865, 0.0, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.4, 1.0, 1.0, 0.4444444444444445, 1.0, 0.1904761904761905, 0.0, 1.0, 0.25, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 0.5454545454545454, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1818", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-6224", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-2498", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-6465", "mrqa_triviaqa-validation-782", "mrqa_hotpotqa-validation-4986", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-414", "mrqa_searchqa-validation-5042"], "SR": 0.484375, "CSR": 0.5997983870967742, "retrieved_ids": ["mrqa_squad-train-72056", "mrqa_squad-train-73340", "mrqa_squad-train-47873", "mrqa_squad-train-46301", "mrqa_squad-train-591", "mrqa_squad-train-32768", "mrqa_squad-train-19457", "mrqa_squad-train-81369", "mrqa_squad-train-57056", "mrqa_squad-train-28916", "mrqa_squad-train-46395", "mrqa_squad-train-54879", "mrqa_squad-train-46969", "mrqa_squad-train-30635", "mrqa_squad-train-51763", "mrqa_squad-train-68261", "mrqa_newsqa-validation-1020", "mrqa_hotpotqa-validation-3084", "mrqa_searchqa-validation-12341", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-833", "mrqa_squad-validation-5878", "mrqa_squad-validation-5344", "mrqa_squad-validation-4545", "mrqa_squad-validation-9744", "mrqa_naturalquestions-validation-3253", "mrqa_hotpotqa-validation-5120", "mrqa_squad-validation-7778", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-1557", "mrqa_searchqa-validation-8453", "mrqa_hotpotqa-validation-220"], "EFR": 1.0, "Overall": 0.7592565524193547}, {"timecode": 31, "before_eval_results": {"predictions": ["weight in burning was hidden by the buoyancy of the gaseous combustion products", "genus Beroe", "September 19 - 22, 2017", "The Deserted Village", "John Roberts", "contemporary Earth", "between the Eastern Ghats and the Bay of Bengal", "621 metres ( 2,037 ft )", "12 to 36 months old", "Tom Brady", "Chelsea", "Darlene Cates", "hyperirritable spots in the fascia surrounding skeletal muscle", "Jerry Leiber", "a Norwegian town", "Gloria", "Horses", "contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms and is thus placed in a `` take it or leave", "Robin", "Jack Barry", "Missouri River", "Randy", "August 18, 1945", "19 June 2018", "Daniel A. Dailey", "minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "King James Bible", "to provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "International Baccalaureate", "October 28, 2007", "Jaydev Shah", "domestication of the wild mouflon in ancient Mesopotamia", "Action Jackson", "Five years later", "state or other organizational body that controls the factors of production", "90 \u00b0 N 0 \u00b0 W", "in Ephesus in AD 95 -- 110", "2006", "sport utility vehicles", "The North Bridge skirmish", "1879", "30 years after Return of the Wars franchise", "John Joseph Patrick Ryan", "American musical group founded by Marcus Bowens and Jermaine Fuller,", "the Rashidun Caliphs", "off the rez", "Woody Paige", "Ren\u00e9 Descartes", "in 2007 via Valve's Steam content distribution platform", "Asuka", "A Turtle's Tale : Sammy's Adventures", "three", "Steve Biko", "Rudolph", "Go", "Anne Fletcher", "October 21, 2016", "Arizona Health Care Cost Containment System", "21", "Robert", "the Oaxacan countryside of southern Mexico", "Algeria", "a vacuum effect", "a pear-shaped instrument"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5956427834154838}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false], "QA-F1": [0.09090909090909091, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.5, 1.0, 0.8333333333333333, 0.5714285714285715, 1.0, 0.0, 0.0, 0.9743589743589743, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.375, 0.6415094339622641, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.7692307692307692, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7272727272727272, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3494", "mrqa_squad-validation-4566", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-853", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-714", "mrqa_hotpotqa-validation-1803", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-3785", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-4046"], "SR": 0.453125, "CSR": 0.59521484375, "retrieved_ids": ["mrqa_squad-train-3522", "mrqa_squad-train-83323", "mrqa_squad-train-72116", "mrqa_squad-train-51285", "mrqa_squad-train-49925", "mrqa_squad-train-29390", "mrqa_squad-train-63338", "mrqa_squad-train-28732", "mrqa_squad-train-65577", "mrqa_squad-train-21482", "mrqa_squad-train-50882", "mrqa_squad-train-77889", "mrqa_squad-train-4590", "mrqa_squad-train-76964", "mrqa_squad-train-9146", "mrqa_squad-train-12377", "mrqa_newsqa-validation-35", "mrqa_searchqa-validation-11252", "mrqa_newsqa-validation-296", "mrqa_hotpotqa-validation-2019", "mrqa_naturalquestions-validation-3385", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-32", "mrqa_newsqa-validation-1166", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-3127", "mrqa_searchqa-validation-12782", "mrqa_triviaqa-validation-4029", "mrqa_newsqa-validation-1020", "mrqa_hotpotqa-validation-4005"], "EFR": 0.8857142857142857, "Overall": 0.735482700892857}, {"timecode": 32, "before_eval_results": {"predictions": ["September 1565", "Anglo-Saxon populations", "Pin the Tail on the Donkey", "the martini", "Wiener Sangerknaben", "cinnamon", "the universe started", "All Soul's Day", "R.E.M.", "Gale Sayers", "French Presidential Power and the Stability of the French Fifth Republic", "Azerbaijan", "Abraham Lincoln", "the Yellow River", "LANGUAGE LAB", "Billy Bob Thornton", "Sharon Epatha Merkerson", "air pressure", "Maurice Harold Macmillan, 1st Earl of Stockton, OM, PC, FRS", "skulls", "shark", "the Deaf President Now protest", "AUGUSTA", "the tongue", "the orangutan", "anaphylaxis", "the Bisha'a", "a critically insufficient blood supply", "frarly", "Bonnie & Clyde", "John Harvard", "the Roman branch of the Indo-European language family", "\"David Cassidy: Man Undercover\"", "Dorothy Gale", "Guatemala", "\"just joking\"", "Raul Castro", "Nemzeti himnusz", "Albert Einstein", "school", "studiotour.com", "Little Women", "Tulipa", "rely", "Providence", "Tasmanian devil John Quincy", "Mother Vineyard", "Clinton", "Swan Lake", "anhydride", "schoolbell Essays and Papers", "FOR Your Heart", "Gibraltar", "1999", "9 February 2018", "Argentina", "Robin McKinley", "france", "Russian anarchist avant-garde artist, art theorist and graphic designer", "1967", "44,300", "228", "after she was released", "\"You love the environment and hate using fuel,\""], "metric_results": {"EM": 0.390625, "QA-F1": 0.46690340909090905}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3088", "mrqa_searchqa-validation-10043", "mrqa_searchqa-validation-608", "mrqa_searchqa-validation-5846", "mrqa_searchqa-validation-5405", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-16138", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-11993", "mrqa_searchqa-validation-5348", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-12676", "mrqa_searchqa-validation-1108", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-15681", "mrqa_searchqa-validation-3957", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-9970", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-7460", "mrqa_searchqa-validation-4645", "mrqa_searchqa-validation-8650", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-2559", "mrqa_searchqa-validation-3246", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-2883", "mrqa_searchqa-validation-11899", "mrqa_searchqa-validation-554", "mrqa_searchqa-validation-93", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-5763", "mrqa_hotpotqa-validation-1070", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-2395"], "SR": 0.390625, "CSR": 0.5890151515151515, "retrieved_ids": ["mrqa_squad-train-11174", "mrqa_squad-train-69938", "mrqa_squad-train-50976", "mrqa_squad-train-13953", "mrqa_squad-train-55097", "mrqa_squad-train-4909", "mrqa_squad-train-24930", "mrqa_squad-train-84391", "mrqa_squad-train-31507", "mrqa_squad-train-70922", "mrqa_squad-train-55651", "mrqa_squad-train-84256", "mrqa_squad-train-46258", "mrqa_squad-train-64366", "mrqa_squad-train-8271", "mrqa_squad-train-31779", "mrqa_triviaqa-validation-1346", "mrqa_naturalquestions-validation-1834", "mrqa_newsqa-validation-1361", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-5120", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2031", "mrqa_hotpotqa-validation-4005", "mrqa_triviaqa-validation-1120", "mrqa_naturalquestions-validation-4094", "mrqa_triviaqa-validation-7417", "mrqa_squad-validation-234", "mrqa_hotpotqa-validation-4235", "mrqa_naturalquestions-validation-2092"], "EFR": 1.0, "Overall": 0.7570999053030303}, {"timecode": 33, "before_eval_results": {"predictions": ["11", "R\u00fcdesheim am Rhein", "American singer Michael Jackson", "Tennessee Williams", "the Ring", "Cherrybomb", "John Henry", "Zombies", "Colombia", "Judges", "Friday Night Lights", "Halloween", "the Mummy", "a port-wine stain", "the Empire State Building", "Pinta", "the Czech Republic", "Matthew Broderick", "Mike Judge", "(Clint) Eastwood", "Court TV", "the Galaxy", "Germany", "the \"adult Western\"", "the moon", "candy girl", "AT&T", "asthma", "Microsoft", "blue", "Puerto Rico", "18", "flying saucer", "Shakespeare", "the liter", "Edward", "The Silence of the Lambs", "Dan Aykroyd", "stuffing", "a fraction", "carbonite", "Spain", "the phi phenomenon", "an obelisk", "Sam Kinison", "Katharine Hepburn", "Brian C. Wimes", "Kublai Khan", "the South Ossetia", "New York City", "a bow", "Newfoundland", "538", "a kiss - off to a Narc ex-lover who did the protagonist wrong", "Peter Hansen", "a googol", "Laos", "Wigan", "1995", "Champion Jockey", "Donald J. Trump", "the Obama administration.", "200", "around 8 p.m. local time"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6574337121212122}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.9090909090909091]}}, "before_error_ids": ["mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-15910", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-6554", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-1445", "mrqa_searchqa-validation-9672", "mrqa_searchqa-validation-2404", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-227", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-5234", "mrqa_naturalquestions-validation-6326", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-3445", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-591"], "SR": 0.609375, "CSR": 0.5896139705882353, "retrieved_ids": ["mrqa_squad-train-54612", "mrqa_squad-train-12401", "mrqa_squad-train-34354", "mrqa_squad-train-42625", "mrqa_squad-train-28125", "mrqa_squad-train-69777", "mrqa_squad-train-31007", "mrqa_squad-train-24141", "mrqa_squad-train-72118", "mrqa_squad-train-28646", "mrqa_squad-train-26929", "mrqa_squad-train-11923", "mrqa_squad-train-1801", "mrqa_squad-train-32452", "mrqa_squad-train-31541", "mrqa_squad-train-26935", "mrqa_squad-validation-360", "mrqa_hotpotqa-validation-4569", "mrqa_naturalquestions-validation-1407", "mrqa_searchqa-validation-15930", "mrqa_squad-validation-2497", "mrqa_hotpotqa-validation-2169", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-5348", "mrqa_newsqa-validation-3407", "mrqa_naturalquestions-validation-2334", "mrqa_searchqa-validation-172", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2698", "mrqa_newsqa-validation-680", "mrqa_naturalquestions-validation-4569"], "EFR": 0.96, "Overall": 0.749219669117647}, {"timecode": 34, "before_eval_results": {"predictions": ["Hisao Yamada", "Percy Shelley", "3D computer-animated comedy", "aluminum foil", "Montreal, Quebec, Canada", "Lego", "Daniil Shafran", "Doc Hollywood", "Richard L. Thompson", "Virgin label", "John Christopher Lujack Jr.", "26 June 2013", "Ms. Jackson", "Michael Swango", "Roman Polanski", "322,520", "1754", "Cate Blanchett", "Westfield Marion", "Montana State University", "1961", "Eisenhower Executive Office Building", "137th", "Mohsin Fani", "High Falls Brewery", "2016", "1998", "Harry Hook in Disney's \"Descendants 2\"", "2015", "Mel Blanc", "Corendon Airlines", "Tamil", "number five", "Champion Jockey", "University of the District of Columbia", "Jennifer Aniston", "Larry Eustachy", "Marion Hulme", "March 17, 2015", "Julie Taymor", "Mika H\u00e4kkinen", "nine", "A bass", "Ringo Starr", "January 1788", "Lord Chancellor of England", "MGM Resorts International", "Washington, D.C.", "The song also features rap parts from Darryl, RB Djan and Ryan Babel.", "Mark Anthony \"Baz\" Luhrmann", "Syracuse University", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "1969", "georgia", "the referee", "Botticelli", "the Great Circus Parade", "John Keats", "Dr. Maria Siemionow", "it has not intercepted any Haitianians attempting illegal crossings into U.S. waters", "Arsene Wenger", "the Basic Building Blocks of Matter", "Perseus", "Monica Lewinsky"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7112580128205128}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5530", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-421", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-2989", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-4302", "mrqa_triviaqa-validation-2977", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-7800", "mrqa_searchqa-validation-13037"], "SR": 0.640625, "CSR": 0.5910714285714286, "retrieved_ids": ["mrqa_squad-train-30316", "mrqa_squad-train-29501", "mrqa_squad-train-56743", "mrqa_squad-train-65690", "mrqa_squad-train-34429", "mrqa_squad-train-26901", "mrqa_squad-train-20258", "mrqa_squad-train-10256", "mrqa_squad-train-8152", "mrqa_squad-train-61493", "mrqa_squad-train-420", "mrqa_squad-train-59553", "mrqa_squad-train-34972", "mrqa_squad-train-33450", "mrqa_squad-train-19632", "mrqa_squad-train-67417", "mrqa_naturalquestions-validation-954", "mrqa_searchqa-validation-1212", "mrqa_squad-validation-7183", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-4015", "mrqa_newsqa-validation-51", "mrqa_searchqa-validation-103", "mrqa_hotpotqa-validation-849", "mrqa_searchqa-validation-12782", "mrqa_hotpotqa-validation-2430", "mrqa_naturalquestions-validation-7084", "mrqa_newsqa-validation-1961", "mrqa_hotpotqa-validation-3821", "mrqa_searchqa-validation-4866", "mrqa_hotpotqa-validation-1618"], "EFR": 1.0, "Overall": 0.7575111607142857}, {"timecode": 35, "before_eval_results": {"predictions": ["June 4, 2014", "highly diversified", "Walter Pauk", "2018", "noble gas", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "Peter Hansen", "Ishaan Anirudh Sinha", "Charlotte Hornets of the National Basketball Association ( NBA )", "one person", "Orographic lift", "17 August 1945", "6 March 1983", "an alternative is to cool all the atmosphere by spraying the whole atmosphere as if drawing letters in the air", "Around 1200", "April 21, 2015", "1857", "Mount Mannen in Norway", "August 5, 1937", "Kennedy Space Center ( KSC ) in Florida", "Rocky Dzidzornu", "35 to 40 hours per week", "Abbot Suger", "early 20th century", "Authority", "a standalone variation on the standard speed limit sign, with a yellow background instead of a white one", "by Magnetically soft ( low coercivity ) iron", "near the mouth of the Pinarus River", "1992", "prolonged diarrhea", "September 1995", "Turducken", "the brain and spinal cord", "abdicated in November 1918", "Massachusetts", "Hans Zimmer", "c. 1000 AD", "Times Square in New York City west to Lincoln Park in San Francisco", "Jerry Leiber and Mike Stoller", "111", "49 cents", "December 1, 1969", "Central Germany", "1978", "Javier Fern\u00e1ndez", "plate tectonics", "`` something that is to be expressed through some medium, as speech, writing or any of various arts", "China ( formerly the Republic of China ), Russia (formerly the Soviet Union ), France, the United Kingdom, and the United States", "peninsular mainland", "Santo Domingo", "during season two", "Gina Tognoni / to\u028an\u02c8jo\u028ani /", "Battle of Somme", "Queen Alexandra", "Majorca", "National Football Conference", "Arlo Looking Cloud", "Parliamentarians (\" roundsheads\") and Royalists (\"Cavaliers\")", "President Pervez Musharraf", "spending billions to revitalize the nation's economy", "Wigan Athletic", "Ireland", "asteroids", "Colorado"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6554577850877192}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4210526315789474, 1.0, 1.0, 1.0, 0.631578947368421, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.8, 0.5, 0.0, 0.0, 0.7499999999999999, 1.0, 0.8000000000000002, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-1985", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-6276", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-2959", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1970"], "SR": 0.546875, "CSR": 0.58984375, "retrieved_ids": ["mrqa_squad-train-55306", "mrqa_squad-train-28606", "mrqa_squad-train-54746", "mrqa_squad-train-53729", "mrqa_squad-train-31115", "mrqa_squad-train-61396", "mrqa_squad-train-50343", "mrqa_squad-train-15452", "mrqa_squad-train-38008", "mrqa_squad-train-42378", "mrqa_squad-train-44297", "mrqa_squad-train-65837", "mrqa_squad-train-85994", "mrqa_squad-train-21990", "mrqa_squad-train-37138", "mrqa_squad-train-1564", "mrqa_squad-validation-1960", "mrqa_squad-validation-363", "mrqa_hotpotqa-validation-571", "mrqa_searchqa-validation-5455", "mrqa_hotpotqa-validation-3760", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-10459", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-2746", "mrqa_squad-validation-3776", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-2208", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-2781", "mrqa_hotpotqa-validation-4956", "mrqa_squad-validation-7615"], "EFR": 1.0, "Overall": 0.7572656249999999}, {"timecode": 36, "before_eval_results": {"predictions": ["a crust and lithosphere", "Suleiman the Magnificent", "Skatoony", "number 1", "Satchmo, Satch or Pops", "San Antonio", "Polish", "Danish", "Milwaukee Bucks", "1952", "glee", "1965", "100 million", "Oneida Limited", "Wilmington, North Carolina", "Pieter van Musschenbroek", "Southbank in Victoria, Australia", "London", "Australian", "Rochdale, North West England", "Bardot", "Mario Lemieux", "To SquarePants or Not to SquarePant", "The Sun", "Sydney, New South Wales, Australia", "Ferdinand Magellan", "King of France", "1694", "Liam Cunningham", "Nanna Popham Britton", "Minette Walters", "leopard", "Moselle", "Anne and Georges", "Bank of China Tower", "American playwright and Nobel laureate in Literature", "Cheshire County", "Robert Gibson", "1770", "1974", "Great Northern Railway", "Woody Woodpecker", "2", "Eddie Albert", "IFFHS World's Best Goalkeeper", "seven", "1989 until 1994", "Pittsburgh Steelers", "9", "1993 to 2001", "Double Crossed", "economic recession", "needle - like teeth", "Bacon", "human rights lawyer", "by increasing the number of arcs", "1812", "energy propels the boat that travels between 5 and 10 knots an hour", "Camorra", "Kingdom City", "Wilhelmina", "hives", "Santa Fe", "1992"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7493675595238095}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true], "QA-F1": [0.25, 0.5, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4952", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-2883", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-893", "mrqa_naturalquestions-validation-5119", "mrqa_triviaqa-validation-994", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-743"], "SR": 0.640625, "CSR": 0.5912162162162162, "retrieved_ids": ["mrqa_squad-train-29326", "mrqa_squad-train-17346", "mrqa_squad-train-78514", "mrqa_squad-train-57246", "mrqa_squad-train-80117", "mrqa_squad-train-45638", "mrqa_squad-train-15364", "mrqa_squad-train-27653", "mrqa_squad-train-54454", "mrqa_squad-train-72831", "mrqa_squad-train-74839", "mrqa_squad-train-22170", "mrqa_squad-train-62343", "mrqa_squad-train-67595", "mrqa_squad-train-52039", "mrqa_squad-train-85914", "mrqa_hotpotqa-validation-4648", "mrqa_naturalquestions-validation-3348", "mrqa_searchqa-validation-12341", "mrqa_newsqa-validation-2031", "mrqa_searchqa-validation-14281", "mrqa_squad-validation-1366", "mrqa_hotpotqa-validation-5228", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-1166", "mrqa_triviaqa-validation-2977", "mrqa_searchqa-validation-7583", "mrqa_searchqa-validation-8844", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-5608", "mrqa_newsqa-validation-3405"], "EFR": 0.9565217391304348, "Overall": 0.7488444660693302}, {"timecode": 37, "before_eval_results": {"predictions": ["lasting damage", "Brett Favre", "Bob Turner", "Wool Sack dress", "the Kid", "Oliver Twist", "Hans Christian Andersen", "Topaz", "magma", "Cameroon", "an acre", "Destiny's Child", "the Patriarch of Constantinople", "Kentucky", "Ocean's Eleven", "the Valkyries", "Little Women", "\"Ich bin ein Berliner\"", "x", "the largest naval battle of World War II", "difference", "Nikolay Vasilyevich", "Malcolm X", "Rocky Mountain Columbine", "French", "Michigan", "Sigmund Freud", "phineas taylor barnum", "T. S. Eliot", "a Dumpling", "Alexander Hamilton", "New Zealand", "molasses", "Theology of God", "phineas taylor barnum", "Stephen Decatur", "the Hernicians", "Paraguay", "R2-D2", "3 standard bottles", "tense", "Vassar College", "forensic medicine", "the Air and Space Museum", "Vespa", "Warren G. Harding", "Emma Peel", "Tennessee", "Richard I", "Jesse Alexander Helms", "Bee Gees", "Honor\u00e9 Mirabeau", "in 2018", "Hanna Alstr\u00f6m", "NUPE", "the first web browser", "Scotland", "1898", "1887", "Bonkyll Castle", "more and more suspicious of the way their business books were being handled.", "the Southern California shoot of the 1986 hit movie.", "Dame Elizabeth", "1875"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5671875}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-16031", "mrqa_searchqa-validation-16215", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-3599", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-7080", "mrqa_searchqa-validation-3714", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-8720", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-10345", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-13049", "mrqa_searchqa-validation-8295", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-2859", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-8890", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-7359", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-1283", "mrqa_hotpotqa-validation-1884", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-2552", "mrqa_hotpotqa-validation-4203"], "SR": 0.453125, "CSR": 0.5875822368421053, "retrieved_ids": ["mrqa_squad-train-46199", "mrqa_squad-train-56226", "mrqa_squad-train-79401", "mrqa_squad-train-51235", "mrqa_squad-train-58077", "mrqa_squad-train-56803", "mrqa_squad-train-79509", "mrqa_squad-train-29940", "mrqa_squad-train-45595", "mrqa_squad-train-80805", "mrqa_squad-train-18282", "mrqa_squad-train-43529", "mrqa_squad-train-17119", "mrqa_squad-train-761", "mrqa_squad-train-13036", "mrqa_squad-train-20805", "mrqa_naturalquestions-validation-10205", "mrqa_newsqa-validation-2945", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3445", "mrqa_naturalquestions-validation-5651", "mrqa_squad-validation-1818", "mrqa_hotpotqa-validation-3307", "mrqa_newsqa-validation-2031", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-14049", "mrqa_squad-validation-2786", "mrqa_searchqa-validation-10759", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-414"], "EFR": 1.0, "Overall": 0.756813322368421}, {"timecode": 38, "before_eval_results": {"predictions": ["the Butcher Market", "the Grito de Dolores", "grommet", "Soundgarden", "a church", "Russian", "the Penguin", "Digitalis", "Canada", "sopra", "pole vault", "California", "Jordan", "the inked image", "the Battle of Waterloo", "Ukraine", "goombah", "Nuku'alofa", "#49 David Geffen", "Hallmark Cards", "Joan of Arc", "John Tyler", "Jane", "La-Z-Boy", "a fire", "a phylum", "Narnia", "East Germany", "Ginger Rogers", "Judges", "Dracula", "Marlee Matlin", "frogs", "TIL Qatar", "debts", "Lady Jane Grey", "yellow fever", "Days Inn", "Mexico", "Harold Edward \"Red\" Grange", "Peter", "indirect printing", "couscous", "1917", "Colonel (Tom) Parker", "the lilac", "Bonanza", "a diamond", "a whale", "Ohio State", "Sweet Home", "George Strait", "Toledo", "1960", "\"The Nutcracker\"", "Dodoma", "the Red Lion", "martial arts action", "Black Swan", "the Sun", "girls", "Vernon Forrest", "President Barack Obama", "Lewiston"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6769345238095237}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-6090", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-14171", "mrqa_searchqa-validation-12204", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-4408", "mrqa_searchqa-validation-3640", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7368", "mrqa_searchqa-validation-11312", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-3197", "mrqa_triviaqa-validation-6420", "mrqa_hotpotqa-validation-770", "mrqa_hotpotqa-validation-1192", "mrqa_newsqa-validation-1371", "mrqa_triviaqa-validation-700"], "SR": 0.578125, "CSR": 0.5873397435897436, "retrieved_ids": ["mrqa_squad-train-40869", "mrqa_squad-train-14516", "mrqa_squad-train-39115", "mrqa_squad-train-49954", "mrqa_squad-train-45865", "mrqa_squad-train-60392", "mrqa_squad-train-13938", "mrqa_squad-train-71735", "mrqa_squad-train-66645", "mrqa_squad-train-44264", "mrqa_squad-train-57386", "mrqa_squad-train-2947", "mrqa_squad-train-13723", "mrqa_squad-train-28193", "mrqa_squad-train-50304", "mrqa_squad-train-42028", "mrqa_searchqa-validation-1011", "mrqa_newsqa-validation-476", "mrqa_naturalquestions-validation-7359", "mrqa_searchqa-validation-15579", "mrqa_newsqa-validation-2513", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-11779", "mrqa_hotpotqa-validation-5120", "mrqa_squad-validation-5878", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-7366", "mrqa_hotpotqa-validation-1272", "mrqa_newsqa-validation-960", "mrqa_naturalquestions-validation-4593", "mrqa_searchqa-validation-9860", "mrqa_hotpotqa-validation-4266"], "EFR": 1.0, "Overall": 0.7567648237179487}, {"timecode": 39, "before_eval_results": {"predictions": ["substantially increased the asking price", "preserved corpses having sex", "\"To My Mother\"", "Lucky Dube", "Festival Foods", "fallen comrades lost in the heat of battle.", "Juri Kibuishi", "1918-1919", "participate in Iraq's government.", "internal bleeding in the chest cavity.", "Honduras", "Hearst Castle.", "FBI", "201-262-2800", "concern that taking the product off the market would result in hardship for terminally ill patients and their caregivers", "were directly involved in an Internet broadband deal with a Chinese firm.", "Iraq", "40 militants and six Pakistan soldiers", "1973", "Argentina", "Washington", "his injuries", "Laura Ling and Euna Lee", "January 24, 2006", "gang rape", "Haleigh Cummings", "The Casalesi Camorra clan", "four", "cars making an annual road trip,", "two", "two", "bankruptcy becomes a reality", "Kurt Cobain", "genocide", "Salt Lake City, Utah", "barter -- trading goods and services without exchanging money", "\"Dance Your Ass Off.\"", "1994", "Larry Zeiger", "United States", "upper respiratory infection", "\"He's crying like a baby,\"", "an open window that fits neatly around him", "Time's Most Influential People", "abuse", "The man ran out of bullets", "two", "the world's poorest children.", "At least 14", "11", "British", "Clarence Darrow", "in honey, tree and vine fruits, flowers, berries, and most root vegetables", "in the attempt to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything '", "1768", "chariots", "George Orwell", "\"Rated R\"", "1955", "\"I'm Shipping Up to Boston\"", "a lock", "(Whistler's Mother)", "a war bond", "electronic junk mail or junk newsgroup posting"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5494122927511086}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.9714285714285714, 0.9473684210526316, 0.0, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 1.0, 0.75, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2946", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1813", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-1259", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-4211", "mrqa_hotpotqa-validation-1585", "mrqa_searchqa-validation-16932", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-11411", "mrqa_triviaqa-validation-90"], "SR": 0.453125, "CSR": 0.583984375, "retrieved_ids": ["mrqa_squad-train-28569", "mrqa_squad-train-39823", "mrqa_squad-train-48537", "mrqa_squad-train-36954", "mrqa_squad-train-10581", "mrqa_squad-train-44653", "mrqa_squad-train-84717", "mrqa_squad-train-17456", "mrqa_squad-train-51075", "mrqa_squad-train-60391", "mrqa_squad-train-59583", "mrqa_squad-train-85140", "mrqa_squad-train-4554", "mrqa_squad-train-85564", "mrqa_squad-train-82637", "mrqa_squad-train-25018", "mrqa_hotpotqa-validation-2228", "mrqa_searchqa-validation-15083", "mrqa_newsqa-validation-1225", "mrqa_searchqa-validation-7485", "mrqa_squad-validation-7885", "mrqa_hotpotqa-validation-4952", "mrqa_newsqa-validation-1468", "mrqa_hotpotqa-validation-4645", "mrqa_squad-validation-4147", "mrqa_squad-validation-9592", "mrqa_searchqa-validation-10579", "mrqa_newsqa-validation-2813", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-3822"], "EFR": 1.0, "Overall": 0.75609375}, {"timecode": 40, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1348", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2749", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2860", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3171", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-543", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4110", "mrqa_naturalquestions-validation-4211", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6680", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10534", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-2423", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-2883", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7583", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-8890", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-9163", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9967", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-138", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3341", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3787", "mrqa_squad-validation-3813", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4107", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5222", "mrqa_squad-validation-5237", "mrqa_squad-validation-525", "mrqa_squad-validation-5269", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-531", "mrqa_squad-validation-5319", "mrqa_squad-validation-5483", "mrqa_squad-validation-5501", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5741", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-689", "mrqa_squad-validation-7001", "mrqa_squad-validation-7002", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-741", "mrqa_squad-validation-7458", "mrqa_squad-validation-7470", "mrqa_squad-validation-7525", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7736", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9744", "mrqa_squad-validation-9845", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9969", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830"], "OKR": 0.884765625, "KG": 0.49453125, "before_eval_results": {"predictions": ["deflate", "Gov. Jan Brewer.", "Casey Anthony,", "Tehran,", "Britain's", "\"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "I'm extremely gratified at the court's decision. I believe it is legally and factually correct.", "Gordon Brown", "regulators in the agency's Colorado office", "Oprah: A Biography", "Vivek Wadhwa", "\"stand tall, stand firm.\"", "former Mobile County Circuit Judge Herman Thomas", "eight-day", "a long-range missile", "Iggy Pop split again.", "equality", "Harry Nicolaides,", "central", "Tadayoshi Kohno of the University of Washington.", "opium poppies", "flavorful foods they can eat.", "New Haven firefighter", "at the University of Alabama in Huntsville,", "1959,", "diplomat's", "The Real Housewives of Atlanta", "dancing", "September,", "because of the challenges a pregnancy could pose to her health.", "Waterloo Bridge", "two", "ties", "Korean regime intends to fire a missile toward Hawaii", "Nicole", "was arrested earlier in the afternoon after a traffic stop south of the city,", "David Russ", "filed papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the execution.", "14", "the local political representative", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan:", "delivers a big speech", "Ennis,", "$17,000", "Tomas Olsson", "Goa", "the patient,", "South Africa", "\"The three gunshot wounds to the head included two nonfatal rounds with entry points below the chin, and one fatal shot that entered Peterson through the right side of the head,\"", "a rally at the State House next week.", "J. Presper Eckert and John William Mauchly", "Latitude", "excessive growth", "Cecilia", "One Thousand and One", "George IV", "2011", "Violet", "Kinnairdy Castle", "the hippopotamus", "the boll weevil", "Alexander Solzhenitsyn", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5925548991936227}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8125000000000001, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5454545454545454, 1.0, 0.19354838709677416, 0.0, 0.9375, 1.0, 1.0, 0.0, 0.17391304347826086, 1.0, 0.5, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.1764705882352941, 0.8, 0.2, 0.0, 0.22222222222222224, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-4059", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-4653", "mrqa_searchqa-validation-16464"], "SR": 0.4375, "CSR": 0.5804115853658536, "retrieved_ids": ["mrqa_squad-train-63210", "mrqa_squad-train-35874", "mrqa_squad-train-53736", "mrqa_squad-train-66997", "mrqa_squad-train-38729", "mrqa_squad-train-3852", "mrqa_squad-train-36413", "mrqa_squad-train-44172", "mrqa_squad-train-18188", "mrqa_squad-train-19610", "mrqa_squad-train-11041", "mrqa_squad-train-29622", "mrqa_squad-train-80988", "mrqa_squad-train-9964", "mrqa_squad-train-69753", "mrqa_squad-train-27905", "mrqa_searchqa-validation-3569", "mrqa_hotpotqa-validation-2044", "mrqa_squad-validation-4566", "mrqa_hotpotqa-validation-5274", "mrqa_searchqa-validation-841", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-1784", "mrqa_triviaqa-validation-3925", "mrqa_naturalquestions-validation-7278", "mrqa_hotpotqa-validation-119", "mrqa_newsqa-validation-3391", "mrqa_hotpotqa-validation-1233", "mrqa_hotpotqa-validation-1247", "mrqa_squad-validation-9268", "mrqa_searchqa-validation-15910", "mrqa_naturalquestions-validation-6764"], "EFR": 0.9444444444444444, "Overall": 0.7370805809620596}, {"timecode": 41, "before_eval_results": {"predictions": ["Tracy Wolfson and Evan Washburn", "Texas", "Kansas City, Missouri", "birds", "James Bond", "Taps", "Batman Forever", "Dr. Strangelove", "a cat", "Atlanta", "sul tuo amore", "a baseball movie", "Coors Field", "boise State", "Doc Holliday", "Chicken Run", "Hercules", "American science fiction writer, science writer and futurist, inventor, undersea", "hydrogen", "Svengali", "Magdolna \"Magda\" Gabor", "Mammoth Cave", "a sousaphone", "S-waves", "Poseidon", "Queen Elizabeth II", "The 39 Steps", "Sinope", "Judges", "oreo", "St. Lawrence", "the seashore", "Indiana Jones", "America", "Bill Clinton", "Cloverfield", "Paraguay", "King of Ruritania", "Gulf of Tonkin", "to rest or relax, or to rest on something for support", "be someone else that", "each phrase in # Quiz # Question", "a zipper", "Tuesday", "Olivia Newton-John", "Robert Cohn", "oil", "South Africa", "De Hooch", "Arnold J. Toynbee", "Heathers", "January 2004", "Rachel Sarah Bilson", "Pradyumna", "Wirral", "Franklin D. Roosevelt", "blood left at crime scenes", "heavier than a feather", "Indooroopilly Shopping Centre", "freshman", "opium", "Adam Yahiye Gadahn,", "A Filipino-American woman", "ape-man"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6210813492063492}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.28571428571428575, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-7018", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-2509", "mrqa_searchqa-validation-6563", "mrqa_searchqa-validation-4782", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-2758", "mrqa_searchqa-validation-3701", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-12223", "mrqa_searchqa-validation-2756", "mrqa_triviaqa-validation-6163", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7596", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-294", "mrqa_newsqa-validation-3404", "mrqa_triviaqa-validation-2080"], "SR": 0.53125, "CSR": 0.5792410714285714, "retrieved_ids": ["mrqa_squad-train-24131", "mrqa_squad-train-2585", "mrqa_squad-train-11127", "mrqa_squad-train-70491", "mrqa_squad-train-79340", "mrqa_squad-train-21124", "mrqa_squad-train-4292", "mrqa_squad-train-60983", "mrqa_squad-train-67050", "mrqa_squad-train-7319", "mrqa_squad-train-50750", "mrqa_squad-train-55301", "mrqa_squad-train-9794", "mrqa_squad-train-9142", "mrqa_squad-train-65182", "mrqa_squad-train-10672", "mrqa_searchqa-validation-15483", "mrqa_triviaqa-validation-3186", "mrqa_searchqa-validation-2943", "mrqa_naturalquestions-validation-346", "mrqa_hotpotqa-validation-3386", "mrqa_squad-validation-9610", "mrqa_searchqa-validation-2617", "mrqa_squad-validation-6967", "mrqa_squad-validation-7017", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-3736", "mrqa_hotpotqa-validation-4362", "mrqa_squad-validation-7246", "mrqa_triviaqa-validation-3744", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-1825"], "EFR": 1.0, "Overall": 0.7479575892857142}, {"timecode": 42, "before_eval_results": {"predictions": ["the Divine Right of Kings", "Mussolini", "Cher", "deuce", "Tarsus", "Charles Chaplin", "Jayne Torvill and Christopher Dean", "France", "Vietnam", "Foil", "Agatha Christie", "Cold Blood", "Social Media and Social Change", "Jackie Joyner-Kersee", "a bowhead whale", "Nelson Mandela", "the Perseid", "Cuba Libre", "Thomas Jefferson", "Tanzania", "Oscar Wilde", "Puebla", "Pennsylvania", "Borneo", "pumpkins", "Walla Walla", "Netflix", "Roger Bannister", "Le Corbusier", "(Scott) Peterson", "I cannot forecast to you the action of Russia", "Franco", "Bolivia", "rugby", "Ireland", "Vanna White", "Catherine the Great", "blue", "the distributor cap", "ROE", "Elizabeth Cady Stanton", "the Wetterstein Mountains", "Francis Ford Coppola", "wives and concubines", "to meander", "The Wind in the Willows", "Jack Dempsey", "hexadecimal", "Ahab", "a chimpanzee", "the Red Cross", "pigs", "August 2012", "Jack Nicklaus", "the Central African Republic", "six", "Yalta", "Emad Hashim", "Lexy Gold", "The Daily Stormer", "Bayern Munich", "the United States", "cancer", "a fern"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6375}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-16519", "mrqa_searchqa-validation-11329", "mrqa_searchqa-validation-11452", "mrqa_searchqa-validation-519", "mrqa_searchqa-validation-2589", "mrqa_searchqa-validation-16809", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-6757", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-10935", "mrqa_searchqa-validation-16153", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-12644", "mrqa_searchqa-validation-12772", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-7323", "mrqa_searchqa-validation-14283", "mrqa_naturalquestions-validation-7702", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-2135", "mrqa_hotpotqa-validation-5307", "mrqa_newsqa-validation-3131", "mrqa_triviaqa-validation-6337"], "SR": 0.5625, "CSR": 0.5788517441860466, "retrieved_ids": ["mrqa_squad-train-14859", "mrqa_squad-train-23254", "mrqa_squad-train-66972", "mrqa_squad-train-53553", "mrqa_squad-train-37206", "mrqa_squad-train-35217", "mrqa_squad-train-35162", "mrqa_squad-train-20019", "mrqa_squad-train-16745", "mrqa_squad-train-30890", "mrqa_squad-train-85255", "mrqa_squad-train-68712", "mrqa_squad-train-46315", "mrqa_squad-train-67811", "mrqa_squad-train-51107", "mrqa_squad-train-47085", "mrqa_searchqa-validation-7982", "mrqa_hotpotqa-validation-1516", "mrqa_naturalquestions-validation-3348", "mrqa_hotpotqa-validation-526", "mrqa_searchqa-validation-15579", "mrqa_naturalquestions-validation-4821", "mrqa_hotpotqa-validation-5660", "mrqa_searchqa-validation-4388", "mrqa_hotpotqa-validation-2169", "mrqa_squad-validation-531", "mrqa_hotpotqa-validation-217", "mrqa_searchqa-validation-15640", "mrqa_hotpotqa-validation-3381", "mrqa_newsqa-validation-1661", "mrqa_hotpotqa-validation-4648", "mrqa_newsqa-validation-3785"], "EFR": 1.0, "Overall": 0.7478797238372092}, {"timecode": 43, "before_eval_results": {"predictions": ["Orthogonal components", "Henry II", "Tomorrow Never Dies", "Liechtenstein", "the word of John Landy", "Absalom", "Columbus", "Brett Favre", "South African", "Brian Deane", "Pinter", "Argentina", "William Conrad", "1875", "Andrew Lloyd Webber", "Iran", "Fairey Swordfish", "Arran", "London", "Playboy", "a person whose occupation is mainly to cut, dress, groom, style and shave", "Matalan", "Chesney Wold", "boise", "a griffin", "red", "Top Cat", "Jersey City", "Judy Cassab", "gold rings", "Karl Marx and Friedrich Engels", "Utrecht", "Union of Post Office Workers", "Strangeways", "Carousel", "14", "Richard Wagner", "the brain", "Adrian Cronauer", "(Frederick) William Herschel", "Belgium", "October", "beetles", "Steely Dan", "Pompey", "denali", "auction", "haddock", "L. P. Hartley", "Italy", "snakes", "Devyn Dalton", "2001", "the human hands", "Distinguished Service Cross", "Shenae Grimes", "five-time", "prostate cancer", "the U.S. Holocaust Memorial Museum", "1927", "Dale", "Theory", "London", "Uru-Salim"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6359375}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, false, true, false, false, true, false], "QA-F1": [0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10394", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-4052", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-6442", "mrqa_naturalquestions-validation-8177", "mrqa_hotpotqa-validation-3137", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-2943", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-9414"], "SR": 0.546875, "CSR": 0.578125, "retrieved_ids": ["mrqa_squad-train-74111", "mrqa_squad-train-22010", "mrqa_squad-train-65921", "mrqa_squad-train-54545", "mrqa_squad-train-28151", "mrqa_squad-train-21419", "mrqa_squad-train-62792", "mrqa_squad-train-50411", "mrqa_squad-train-49462", "mrqa_squad-train-65785", "mrqa_squad-train-35905", "mrqa_squad-train-33085", "mrqa_squad-train-17227", "mrqa_squad-train-12189", "mrqa_squad-train-21379", "mrqa_squad-train-45999", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-4917", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-10043", "mrqa_squad-validation-9057", "mrqa_newsqa-validation-1748", "mrqa_hotpotqa-validation-4222", "mrqa_searchqa-validation-5842", "mrqa_newsqa-validation-4015", "mrqa_naturalquestions-validation-4370", "mrqa_searchqa-validation-6753", "mrqa_naturalquestions-validation-2092", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-1884", "mrqa_triviaqa-validation-4784"], "EFR": 1.0, "Overall": 0.7477343750000001}, {"timecode": 44, "before_eval_results": {"predictions": ["co-chair", "the Flying Pickets", "iceland", "Evita", "Victoria", "Sikhism", "Dick Turpin", "Sinclair Lewis", "Argentina", "glaze", "Guatemala", "olive", "Munich", "violin", "a sash", "Paul Nash", "mammals", "first among equals", "a robin", "Indira Gandhi", "Colombia", "jean Baptiste Say", "Uranus", "Prince Igor", "November", "an indoor football league", "an electrical component", "The Wicker Man", "nathaniel Hawkeye", "Gorky", "South Africa", "hovercraft", "Pete Sampras", "white", "Jimmy Boyd", "Tina Turner", "henry vases", "brash", "honeybees", "Harold Wilson", "the Spice Girls", "mmorpg", "adrian ladd", "hair", "Wolfgang Amadeus Mozart", "bubba", "Utah", "Richard Lester", "December", "peregrines", "steel", "7 June 2005", "Billboard magazine", "Britain", "Marcus T. Reynolds", "35,000", "Tel Aviv University", "response to a civil disturbance call,", "the BBC building in Glasgow, Scotland", "\"Hillbilly Handfishin'\"", "Diogenes", "henry vii", "a flanker", "an integral membrane protein that builds up a proton gradient across a biological membrane"], "metric_results": {"EM": 0.640625, "QA-F1": 0.715141369047619}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.2857142857142857, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.7499999999999999, 0.8333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8581", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-7543", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-5476", "mrqa_hotpotqa-validation-773", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-220", "mrqa_searchqa-validation-13219", "mrqa_searchqa-validation-9558"], "SR": 0.640625, "CSR": 0.5795138888888889, "retrieved_ids": ["mrqa_squad-train-31471", "mrqa_squad-train-59146", "mrqa_squad-train-12390", "mrqa_squad-train-35062", "mrqa_squad-train-49763", "mrqa_squad-train-79106", "mrqa_squad-train-43170", "mrqa_squad-train-62814", "mrqa_squad-train-5776", "mrqa_squad-train-12228", "mrqa_squad-train-7035", "mrqa_squad-train-76649", "mrqa_squad-train-60446", "mrqa_squad-train-23631", "mrqa_squad-train-8139", "mrqa_squad-train-35247", "mrqa_searchqa-validation-15483", "mrqa_newsqa-validation-848", "mrqa_naturalquestions-validation-6019", "mrqa_hotpotqa-validation-3084", "mrqa_searchqa-validation-9383", "mrqa_newsqa-validation-1537", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-636", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1034", "mrqa_searchqa-validation-12426", "mrqa_hotpotqa-validation-1101", "mrqa_triviaqa-validation-1394", "mrqa_newsqa-validation-655", "mrqa_hotpotqa-validation-4648", "mrqa_naturalquestions-validation-4433"], "EFR": 0.9565217391304348, "Overall": 0.7393165006038648}, {"timecode": 45, "before_eval_results": {"predictions": ["Apollo", "1902", "Daniel A. Dailey", "hyperarousal, or the acute stress response", "Anakin Skywalker", "Plank", "Ann Gillespie", "near Chesapeake Bay", "gomes", "March 26, 1973", "drizzle, rain, sleet, snow, graupel and hail", "Tommy Shaw", "1858", "Charles Perrault", "John Daly", "1998", "Elizabeth Dean Lail", "March 31, 2017", "Jonathan Breck", "Aristotle", "2018 and 2019", "plate tectonics", "eight", "more than a million", "Revelation was the last book accepted into the Christian biblical canon", "2004", "Rock Island, Illinois", "1926", "2016", "the Southern Ocean", "on the vaginal floor", "The Osmonds", "Ace", "from the Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s", "Teddy Randazzo", "pancreas", "Captain Jones", "tradeable entity used to avoid the inconvenienceiences of a pure barter system", "the last Ice Age", "Neal Dahlen", "The balance sheet", "London, United Kingdom", "Hellenism", "232", "January 2, 1971", "Andy Cole", "\u20b9 39.50 lakh", "natural killer cells", "a crust of potatoes", "team", "4.5 pounds or 2.04 kg", "de Havilland Moth", "12", "bukwus", "Las Vegas Hilton", "Gloria Trevi", "postal delivery", "he dropped his children off at a relative's house,", "the leader of a drug cartel", "\"Slumdog Millionaire\"", "Clifford Odets", "Margaret Mitchell", "Microsoft", "Sherlock Holmes"], "metric_results": {"EM": 0.546875, "QA-F1": 0.648369058299157}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5454545454545454, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 0.125, 0.4, 1.0, 1.0, 0.4210526315789474, 0.0, 1.0, 1.0, 0.9, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-7509", "mrqa_triviaqa-validation-1421", "mrqa_triviaqa-validation-2110", "mrqa_hotpotqa-validation-3832", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-947"], "SR": 0.546875, "CSR": 0.5788043478260869, "retrieved_ids": ["mrqa_squad-train-2122", "mrqa_squad-train-18043", "mrqa_squad-train-60694", "mrqa_squad-train-22074", "mrqa_squad-train-10896", "mrqa_squad-train-69269", "mrqa_squad-train-2270", "mrqa_squad-train-53296", "mrqa_squad-train-64540", "mrqa_squad-train-76705", "mrqa_squad-train-76140", "mrqa_squad-train-76907", "mrqa_squad-train-59765", "mrqa_squad-train-6974", "mrqa_squad-train-42315", "mrqa_squad-train-86498", "mrqa_newsqa-validation-3857", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-1179", "mrqa_searchqa-validation-10490", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-5021", "mrqa_naturalquestions-validation-4302", "mrqa_newsqa-validation-2586", "mrqa_hotpotqa-validation-992", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-7739", "mrqa_squad-validation-1877", "mrqa_newsqa-validation-3435", "mrqa_naturalquestions-validation-853"], "EFR": 0.9310344827586207, "Overall": 0.7340771411169416}, {"timecode": 46, "before_eval_results": {"predictions": ["Gamal Abdul Nasser", "2016", "a line of committed and effective Sultans", "the U.S.", "United Nations Peacekeeping Operations", "Steve Hale", "King Saud University", "Parashara", "John Dalton", "Vienna", "pepsinogen", "Carol Worthington", "ancient Rome", "1928", "April 29, 2009", "19 June 2018", "Todd Bridges", "Ben Savage", "the senior-most judge of the supreme court", "multinational retail corporation", "Texas - style chili con carne", "Andrea Brooks", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "Max Martin", "by October 1986", "232", "many forested parts of the world", "about 3.5 mya", "1998", "Each side", "mitosis", "an Aldabra giant tortoise", "Texhoma", "A marriage officiant", "Andy Warhol", "The Royalettes", "Latin centum", "Treaty of Paris", "Andrew Johnson", "microfilament", "four", "if the concentration of a compound exceeds its solubility", "the National League ( NL ) champion Chicago Cubs", "Gary Cole", "Kid Creole and the Coconuts", "Stephen Graham", "Anna Maria Demara", "the plane crash", "Tatsumi", "Ernest Hemingway", "14 November 2001", "Venus", "Vietnam", "nastase", "\"Famous Ghost Stories\"", "Scotty Grainger", "uncle", "Steve Jobs", "a preliminary injunction against a Mississippi school district and high school", "released of the four men", "poverty line", "the uterus", "sweat glands", "(John) Brown Jr."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6346882284382284}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8333333333333333, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.9, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3771", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-5936", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-3418", "mrqa_triviaqa-validation-959", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-1475", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-15864"], "SR": 0.546875, "CSR": 0.578125, "retrieved_ids": ["mrqa_squad-train-58241", "mrqa_squad-train-39257", "mrqa_squad-train-50672", "mrqa_squad-train-5893", "mrqa_squad-train-45535", "mrqa_squad-train-12631", "mrqa_squad-train-55642", "mrqa_squad-train-72999", "mrqa_squad-train-66990", "mrqa_squad-train-37863", "mrqa_squad-train-56373", "mrqa_squad-train-7585", "mrqa_squad-train-79549", "mrqa_squad-train-20779", "mrqa_squad-train-5099", "mrqa_squad-train-4191", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5579", "mrqa_naturalquestions-validation-8763", "mrqa_hotpotqa-validation-2342", "mrqa_squad-validation-10007", "mrqa_triviaqa-validation-4653", "mrqa_searchqa-validation-4408", "mrqa_searchqa-validation-1723", "mrqa_naturalquestions-validation-3658", "mrqa_squad-validation-3490", "mrqa_squad-validation-10107", "mrqa_squad-validation-234", "mrqa_hotpotqa-validation-1312", "mrqa_triviaqa-validation-5038", "mrqa_squad-validation-4952", "mrqa_squad-validation-2679"], "EFR": 1.0, "Overall": 0.7477343750000001}, {"timecode": 47, "before_eval_results": {"predictions": ["2014", "Ren\u00e9 Descartes", "their bearers", "Dante Pastula", "Havana Harbor", "sedimentary rock", "April 10, 2018", "Ted Mosby", "North Atlantic Ocean", "self - closing flood barrier", "111", "2 %", "an Irish feminine name", "the head of the Imperial Family and the traditional head of state of Japan", "November 17, 2017", "an instant messaging client that was first developed and popularized by the Israeli company Mirabilis", "appellate courts", "her abusive husband", "84", "William Chatterton Dix", "`` Killer Within ''", "Broken Hill and Sydney", "the appendicular skeleton", "a database maintained by the United States federal government", "a couple broken apart by the Iraq War", "Johannes Gutenberg", "National Industrial Recovery Act ( NIRA )", "all within the Pittsburgh metropolitan area", "the ulnar nerve", "the Nationalists", "31 October 1972", "twelve", "Matt Flinders", "The person who has existence in two paradise", "IMS uses IETF protocols", "Ra\u00fal Eduardo Esparza", "the beta cells of the islets of Langerhans", "4 September 1936", "drivers who meet more exclusive criteria", "H.L. Hunley", "Orangeville, Ontario", "Bactrian", "the Gentiles", "James Hutton", "to address the historic oppression, inequality and discrimination faced by those communities", "decades after its initial release", "2005", "March 18, 2005", "Fats Waller", "in the fovea centralis", "the Russian army", "in bed of sloth", "Rudyard Kipling", "the Penguin", "Otto Eduard Leopold", "Ukrainian", "237 square miles", "Apple employees", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "July", "Margaret Mitchell", "a crucifix", "Lisa Lisa", "left-hand or right-hand batsman"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6715553461325414}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.375, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.8333333333333333, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.7999999999999999, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.4, 0.8, 0.25, 0.0, 1.0, 0.7857142857142858, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-10321", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-6040", "mrqa_triviaqa-validation-3828", "mrqa_hotpotqa-validation-5541", "mrqa_newsqa-validation-4151", "mrqa_hotpotqa-validation-181"], "SR": 0.53125, "CSR": 0.5771484375, "retrieved_ids": ["mrqa_squad-train-63460", "mrqa_squad-train-42225", "mrqa_squad-train-14323", "mrqa_squad-train-74973", "mrqa_squad-train-58349", "mrqa_squad-train-66255", "mrqa_squad-train-49051", "mrqa_squad-train-24776", "mrqa_squad-train-40113", "mrqa_squad-train-63658", "mrqa_squad-train-17431", "mrqa_squad-train-58812", "mrqa_squad-train-46408", "mrqa_squad-train-79886", "mrqa_squad-train-21085", "mrqa_squad-train-42749", "mrqa_hotpotqa-validation-2423", "mrqa_searchqa-validation-7224", "mrqa_newsqa-validation-1259", "mrqa_searchqa-validation-9672", "mrqa_newsqa-validation-839", "mrqa_hotpotqa-validation-1211", "mrqa_triviaqa-validation-1217", "mrqa_hotpotqa-validation-5117", "mrqa_searchqa-validation-7018", "mrqa_newsqa-validation-1793", "mrqa_searchqa-validation-16198", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-5034", "mrqa_newsqa-validation-1065", "mrqa_hotpotqa-validation-893", "mrqa_triviaqa-validation-464"], "EFR": 0.9666666666666667, "Overall": 0.7408723958333334}, {"timecode": 48, "before_eval_results": {"predictions": ["Magdalen College", "Rana Daggubati", "13 people and injuring 145.", "S Pictures' \"Veyyil\"", "1 draw", "Kristy Lee Cook", "LA Galaxy", "850 saloon", "February 14, 1859", "\"The Simpsons\"' thirteenth season", "Biola University", "BAFTA Award for Best Production Design", "2012 NBA draft", "October 13, 1980", "tomato", "Gracie Mansion", "Arsenal Football Club", "Battle of Normandy", "Steve Prohm", "super-regional shopping mall", "Charlotte Carnegie", "1990", "seven players have had 50\u201340\u201390 seasons.", "28 June 1945", "University of California", "Miami Gardens", "Indian", "Paige O'Hara", "Graham Hill", "Masahiko Takehita", "Hillary Clinton", "1896", "Edward Michael \" Mike\"/\"Spanky\" Fincke", "D\u00e2mbovi\u021ba River", "Philip Quast", "Pierce County", "PPG Paints Arena", "May 10, 1976", "Brent", "Operation Sculpin", "BAFTA TV Award Best Actor", "First Balkan War", "1587", "Marty Ingels", "mathematician, physicist, and spectroscopist", "1941", "June 2, 2008", "Charice", "Sleepy Brown", "Waimea Bay", "Nikhil Banerjee", "brain", "last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "restoring someone's faith in love and family relationships", "Moose the dog, better known as Eddie", "gold anniversary", "gull", "The supplemental spending bill also contains a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance", "Bailey, Colorado,", "The Sopranos", "Winslow Homer", "Elizabeth II", "the Crimean War", "Alexey Pajitnov"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6328081232492997}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.5, 0.5714285714285715, 0.5, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.823529411764706, 0.2857142857142857, 0.0, 0.0, 0.1, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-2480", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-1757", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-5373", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-5535", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1997", "mrqa_searchqa-validation-14284"], "SR": 0.484375, "CSR": 0.5752551020408163, "retrieved_ids": ["mrqa_squad-train-39912", "mrqa_squad-train-83998", "mrqa_squad-train-54774", "mrqa_squad-train-24938", "mrqa_squad-train-38012", "mrqa_squad-train-50352", "mrqa_squad-train-184", "mrqa_squad-train-3846", "mrqa_squad-train-76939", "mrqa_squad-train-31119", "mrqa_squad-train-5534", "mrqa_squad-train-16090", "mrqa_squad-train-73171", "mrqa_squad-train-10278", "mrqa_squad-train-39609", "mrqa_squad-train-67552", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-4002", "mrqa_squad-validation-1258", "mrqa_newsqa-validation-4060", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-6040", "mrqa_hotpotqa-validation-5120", "mrqa_squad-validation-4257", "mrqa_newsqa-validation-1371", "mrqa_squad-validation-618", "mrqa_naturalquestions-validation-2092", "mrqa_newsqa-validation-1541", "mrqa_hotpotqa-validation-1549", "mrqa_searchqa-validation-15910", "mrqa_triviaqa-validation-312", "mrqa_newsqa-validation-3407"], "EFR": 0.9696969696969697, "Overall": 0.7410997893475572}, {"timecode": 49, "before_eval_results": {"predictions": ["The Yardbirds", "Dubai", "Silverstone", "\" Ted\" Shakleford", "Triumph and Disaster", "1720", "Henry V", "Randy \"The Ram\" Robinson", "beetle", "cuticle", "100 years", "Rudyard Kipling", "dragonflies", "Cole Porter", "liriope", "Chancellor of the Exchequer", "Big Brother", "nippori", "Beaujolais", "Auckland", "Paul Dukas", "Tom Watson", "$3000", "ear", "Tokyo", "low-E", "keeper of the Longstone (Fame Islands) lighthouse", "God bless America", "Dangerous Minds", "dying", "Apollo", "Daft As A Brush", "South Korea", "Boxing Day", "St Pancras", "fish", "wainscoting", "streptococcus", "Scarborough", "Alan Turing", "Isaac Newton", "Calcium carbonate", "Bombay", "Eleanor Roosevelt", "Roy Rogers", "Cyclades", "arthur", "Hitachi", "plutarch", "Serbia", "the gizzard", "the Detroit Tigers", "in a counter clockwise direction", "3.5 million years old", "Tampa Bay Lightning", "Los Angeles Dance Theater", "Battle of Dresden", "U.S. Agency for International Development", "$150 billion", "root out terrorists within its borders.", "Latvia", "an alligator", "Jerry Rice", "tarzan"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6427083333333333}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7216", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-293", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-6612", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-4647", "mrqa_newsqa-validation-93", "mrqa_searchqa-validation-2086", "mrqa_searchqa-validation-5649"], "SR": 0.578125, "CSR": 0.5753125, "retrieved_ids": ["mrqa_squad-train-64392", "mrqa_squad-train-22524", "mrqa_squad-train-9682", "mrqa_squad-train-27100", "mrqa_squad-train-47430", "mrqa_squad-train-33658", "mrqa_squad-train-18824", "mrqa_squad-train-68614", "mrqa_squad-train-83578", "mrqa_squad-train-75347", "mrqa_squad-train-34438", "mrqa_squad-train-60406", "mrqa_squad-train-12194", "mrqa_squad-train-22684", "mrqa_squad-train-53283", "mrqa_squad-train-62438", "mrqa_squad-validation-10305", "mrqa_searchqa-validation-3701", "mrqa_newsqa-validation-2843", "mrqa_squad-validation-8923", "mrqa_searchqa-validation-9239", "mrqa_newsqa-validation-35", "mrqa_squad-validation-6900", "mrqa_searchqa-validation-4299", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-7351", "mrqa_newsqa-validation-80", "mrqa_triviaqa-validation-4466", "mrqa_searchqa-validation-11681", "mrqa_hotpotqa-validation-2319", "mrqa_naturalquestions-validation-3199", "mrqa_hotpotqa-validation-4373"], "EFR": 1.0, "Overall": 0.747171875}, {"timecode": 50, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2584", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-438", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-4499", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5405", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3559", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14232", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16791", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7018", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7260", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7002", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-21", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96"], "OKR": 0.865234375, "KG": 0.49453125, "before_eval_results": {"predictions": ["Medusa", "Hawaii", "Easy Rider", "Scrabble", "Percy Bysshe Shelley", "Billy Joel", "pardon", "New York City", "Breakfast at Tiffany's", "Roman Polanski", "Dogberry", "Battle Creek", "the Red Sea", "Mary Todd Lincoln", "Mary Poppins", "Elizabeth, empress of Russia", "phonetics", "The Naked Brothers Band", "Julianne Moore", "saddle bags", "Holly Golightly", "quilt", "anemoi", "butter", "the Tagus river", "the CIO", "improv", "USS Nautilus", "xylophone", "Denmark", "student loans", "Latin", "the flute", "Seattle", "Michael Jordan", "John Quincy Adams", "the French Legion of Honour", "Louis XIII", "Korean", "1777", "Hitler", "Washington Irving", "Crimean Tatar", "candy bar", "the White House", "a gastropod shell", "Julius Caesar", "Della", "Dean G. Acheson", "Pittsburgh Steelers", "jury", "Malina Weissman", "Kyla Pratt", "Jonathan Goldstein", "9", "not", "Spain", "Sean Yseult", "1754", "Tommy Cannon", "outside the Iranian consulate in Peshawar", "in his native Philippines", "she's in love, thinks maybe it's a good thing she thought Rounds was straight.", "September 21, 2014"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6441558441558441}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.5, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.12121212121212123, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6167", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-664", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-9320", "mrqa_searchqa-validation-4385", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-9809", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-15599", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-9986", "mrqa_triviaqa-validation-674", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-551", "mrqa_hotpotqa-validation-4911"], "SR": 0.546875, "CSR": 0.5747549019607843, "retrieved_ids": ["mrqa_squad-train-72193", "mrqa_squad-train-34534", "mrqa_squad-train-18188", "mrqa_squad-train-18347", "mrqa_squad-train-53770", "mrqa_squad-train-64425", "mrqa_squad-train-47323", "mrqa_squad-train-39452", "mrqa_squad-train-45054", "mrqa_squad-train-54042", "mrqa_squad-train-27228", "mrqa_squad-train-6144", "mrqa_squad-train-25217", "mrqa_squad-train-33122", "mrqa_squad-train-21416", "mrqa_squad-train-18101", "mrqa_hotpotqa-validation-2342", "mrqa_squad-validation-7373", "mrqa_hotpotqa-validation-5872", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-12782", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-5030", "mrqa_triviaqa-validation-5424", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5477", "mrqa_squad-validation-3496", "mrqa_searchqa-validation-2360", "mrqa_squad-validation-5549", "mrqa_squad-validation-2497", "mrqa_searchqa-validation-2589", "mrqa_naturalquestions-validation-9428"], "EFR": 1.0, "Overall": 0.7388572303921569}, {"timecode": 51, "before_eval_results": {"predictions": ["Reddi-wip", "the Dachshund", "Saturn", "Dagny Taggart", "Risk", "a Bar Mitzvah", "the cauliflower ear", "Clark Gable", "Katharine Hepburn", "Metacomet", "surrender", "Tarsus", "Niagara Falls", "Hannibal Lecter", "The Man Without A Country", "the Arc de Triomphe", "George Frideric Handel", "the cologne", "Indonesia", "Florence Henderson", "Linus Pauling", "cocoa", "Niagara Falls", "the horse", "hydrothermal", "Ohio", "Million Dollar Baby", "the vanilla extract", "an organ", "Papua New Guinea", "Macy's", "\"Charlie\" Crist", "the Arctic Ocean", "enamel", "Port-au-Prince", "the Barbary Coast", "humility", "Alexander Popov", "rice", "gas masks", "\"to look like\"", "\"The Jinx\"", "the breast", "a jet of water", "Marie de Bourbon", "a suspension bridge", "faerie", "the trudge", "JetBlue", "Ryan Seacrest", "roller", "Lake Michigan", "Spanish colonies", "as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "piscinae", "the house sparrows", "Islamabad", "Indianapolis Motor Speedway", "$26 billion", "Deftones", "Adam Lambert and Kris Allen", "27-year-old's", "more than two years,", "Dinabandhu Mitra"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5914351851851851}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5185185185185185, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-14737", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-13086", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10836", "mrqa_searchqa-validation-16862", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10726", "mrqa_searchqa-validation-3994", "mrqa_searchqa-validation-10574", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-10180", "mrqa_searchqa-validation-8660", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-8155", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-2705", "mrqa_naturalquestions-validation-8099"], "SR": 0.53125, "CSR": 0.5739182692307692, "retrieved_ids": ["mrqa_squad-train-53893", "mrqa_squad-train-72028", "mrqa_squad-train-50944", "mrqa_squad-train-36596", "mrqa_squad-train-38051", "mrqa_squad-train-24977", "mrqa_squad-train-48664", "mrqa_squad-train-20804", "mrqa_squad-train-21149", "mrqa_squad-train-81334", "mrqa_squad-train-56280", "mrqa_squad-train-2764", "mrqa_squad-train-65838", "mrqa_squad-train-8021", "mrqa_squad-train-73880", "mrqa_squad-train-5694", "mrqa_newsqa-validation-490", "mrqa_squad-validation-3885", "mrqa_hotpotqa-validation-1492", "mrqa_searchqa-validation-10759", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-1014", "mrqa_searchqa-validation-4866", "mrqa_naturalquestions-validation-5586", "mrqa_hotpotqa-validation-3613", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-15812", "mrqa_hotpotqa-validation-5790", "mrqa_searchqa-validation-10579", "mrqa_newsqa-validation-1711", "mrqa_naturalquestions-validation-4433"], "EFR": 0.9666666666666667, "Overall": 0.7320232371794873}, {"timecode": 52, "before_eval_results": {"predictions": ["Alfred Preis", "Nip/Tuck", "Adelaide Lightning", "Homeland", "1983", "3.9 mi", "Hilux", "her fifth studio album", "Snowball II", "Anna Clyne", "Flushed Away", "Elbow River", "Mickey's Christmas Carol", "Ellie Kemper", "Aamir Khan", "Noah Levenstein", "25 million", "M\u0101noa in Honolulu CDP", "Julianne Moore", "electronic musical instrument", "Samantha Spiro", "Martin O'Neill", "Los Angeles Galaxy", "Aamina Sheikh", "Total Nonstop Action Wrestling", "Walt Disney Feature Animation", "Nobel Prize in Physics", "Tudor music", "the east of Ireland", "Blue Grass Airport", "Tim Whelan", "Cleveland Celtics", "Ashley Jensen", "Franc Roddam", "Ben R. Guttery", "Pieter van Musschenbroek", "ABC", "Roseann O'Donnell", "\"online hub for Clinton backers so that they can find easy-to-share facts, stats and other information you can take out to social media when you\u2019re having debates on key issues people are discussing\"", "1902", "USS \"Enterprise\"", "Las Vegas", "Todd Emmanuel Fisher", "supply chain management", "John M. Dowd", "August 9, 2017", "MGM Grand Las Vegas", "James Fell", "Clara Petacci", "from 1986 to 2013", "Bill Ponsford", "one of Jesus'disciples", "Grace Zabriskie", "Gaget, Gauthier & Co.", "Martin Van Buren", "Robert Boyle", "Vienna", "Harrison Ford", "the first day of June", "27-year-old's", "Ernest", "the right hand", "condensation", "Kitty Kelley"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6309302606177606}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10810810810810813, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-1584", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-3321", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-617", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-3286", "mrqa_hotpotqa-validation-2153", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-3391", "mrqa_triviaqa-validation-2082", "mrqa_newsqa-validation-2415", "mrqa_searchqa-validation-12184"], "SR": 0.546875, "CSR": 0.5734080188679245, "retrieved_ids": ["mrqa_squad-train-21555", "mrqa_squad-train-77691", "mrqa_squad-train-57482", "mrqa_squad-train-4346", "mrqa_squad-train-35812", "mrqa_squad-train-49260", "mrqa_squad-train-78706", "mrqa_squad-train-2823", "mrqa_squad-train-52334", "mrqa_squad-train-48150", "mrqa_squad-train-76213", "mrqa_squad-train-80073", "mrqa_squad-train-45104", "mrqa_squad-train-30977", "mrqa_squad-train-51855", "mrqa_squad-train-68193", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-5113", "mrqa_squad-validation-4849", "mrqa_triviaqa-validation-1120", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-13129", "mrqa_naturalquestions-validation-6844", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1748", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-2421", "mrqa_searchqa-validation-2859", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-2708", "mrqa_triviaqa-validation-5763", "mrqa_newsqa-validation-3391"], "EFR": 1.0, "Overall": 0.7385878537735849}, {"timecode": 53, "before_eval_results": {"predictions": ["London", "2018", "Welch, West Virginia", "1 BC", "2009", "Mark Jackson", "Brazil", "December 24, 1836", "2 September 1990", "BC Jean and Toby Gad", "Billy Bishop Toronto City Airport", "off the southernmost tip of the South American mainland", "Roger Dean Stadium", "Jesse Wesley Williams", "19 June 2018", "Prince William", "Sanchez Navarro", "Nick Kroll", "pigs", "in Pittsburgh", "2018", "to the left of the dinner plate", "headdresses", "in a Norwegian town circa 1879", "1960", "1840s", "AMX - 30", "semi-automatic, but not fully automatic", "Humpty Dumpty and Kitty Softpaws", "displacement", "halogenated paraffin hydrocarbons", "200 to 500 mg up to 7ml", "Alan Menken", "November 5, 2017", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys", "lightning strike", "1868 war veterans, such as Polish internationalist General Carlos Roloff and Seraf\u00edn S\u00e1nchez in Las Villas", "in muscles", "Robin", "March 26, 1973", "New England Patriots", "New York City", "S - shaped", "31 - member Senate and a 150 - member House of Representatives", "Ajay Tyagi", "from the heraldic crest carved in the lintel on St. Ignatius'family home in Azpeitia, Spain", "Brooklyn Heights", "book and architecture", "19 June 2018", "Efren Manalang Reyes", "California", "Barcelona", "molybdenum", "Dodi Fayed", "January 4, 1976", "1921", "General Allenby", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "water and leave behind whitened skeletons.", "22", "a triangle", "Timbaland", "Foo Fighters", "Jennifer Grey"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6288475974597297}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7692307692307693, 0.923076923076923, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.9090909090909091, 0.5714285714285715, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.19999999999999998, 1.0, 0.11764705882352941, 0.33333333333333337, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6137", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8873", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-7013", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-3801", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-3837", "mrqa_triviaqa-validation-6364", "mrqa_hotpotqa-validation-5720", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1638", "mrqa_searchqa-validation-16366"], "SR": 0.484375, "CSR": 0.5717592592592593, "retrieved_ids": ["mrqa_squad-train-65486", "mrqa_squad-train-22832", "mrqa_squad-train-57070", "mrqa_squad-train-56654", "mrqa_squad-train-370", "mrqa_squad-train-83911", "mrqa_squad-train-27045", "mrqa_squad-train-67927", "mrqa_squad-train-62606", "mrqa_squad-train-9", "mrqa_squad-train-81458", "mrqa_squad-train-77529", "mrqa_squad-train-8509", "mrqa_squad-train-83365", "mrqa_squad-train-18053", "mrqa_squad-train-10374", "mrqa_squad-validation-4769", "mrqa_hotpotqa-validation-1169", "mrqa_triviaqa-validation-2080", "mrqa_searchqa-validation-9320", "mrqa_hotpotqa-validation-4826", "mrqa_naturalquestions-validation-104", "mrqa_squad-validation-2976", "mrqa_searchqa-validation-15681", "mrqa_newsqa-validation-540", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4926", "mrqa_naturalquestions-validation-8429", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-11329"], "EFR": 1.0, "Overall": 0.7382581018518519}, {"timecode": 54, "before_eval_results": {"predictions": ["the martini", "water", "San Francisco", "Volodymyr Groysman", "hoop cheese", "Gwynn", "Atonement", "Kentucky", "collagen", "\"Just say no\"", "Typewriter", "Diane Arbus", "Cincinnati", "Cleopatra", "Suez Canal", "Planet of the Apes", "garret", "Adam Sandler", "north", "to do something", "member", "\"Unco Guid\"", "phobias", "San Jose", "pianissimo", "the Byzantine Empire", "Dunkirk", "white", "Psalms", "a pearl", "gelato", "Jesus", "viruses", "(George) Balanchine", "Alfred Stieglitz", "(Don Juan De Marco)", "Africa", "Gaius", "Applebee's", "the Mercator", "Robin Hood", "Stegosaurus", "Ravel", "Daniel Boone", "William Tecumseh Sherman", "Prison Break", "a hippopotamus", "Black Beauty", "The Judgment of Paris", "Sinclair Lewis", "Leo", "85 %", "Jamestown", "Uruguay", "cat", "Mr. Humphries", "Boulder Dam", "Personal History", "Hellenism", "Barnoldswick", "bodies and heads", "the U.S. Holocaust Memorial Museum", "BET", "Havana Harbor"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6458333333333333}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-12810", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-1885", "mrqa_searchqa-validation-8232", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-13725", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-308", "mrqa_searchqa-validation-6225", "mrqa_searchqa-validation-7376", "mrqa_searchqa-validation-5449", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-14717", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-1657", "mrqa_newsqa-validation-23"], "SR": 0.609375, "CSR": 0.5724431818181819, "retrieved_ids": ["mrqa_squad-train-76024", "mrqa_squad-train-14417", "mrqa_squad-train-76280", "mrqa_squad-train-69514", "mrqa_squad-train-59716", "mrqa_squad-train-28651", "mrqa_squad-train-56245", "mrqa_squad-train-48877", "mrqa_squad-train-42852", "mrqa_squad-train-56116", "mrqa_squad-train-834", "mrqa_squad-train-81408", "mrqa_squad-train-45706", "mrqa_squad-train-3466", "mrqa_squad-train-42986", "mrqa_squad-train-44478", "mrqa_newsqa-validation-1361", "mrqa_hotpotqa-validation-5113", "mrqa_triviaqa-validation-356", "mrqa_searchqa-validation-16046", "mrqa_naturalquestions-validation-9687", "mrqa_hotpotqa-validation-294", "mrqa_hotpotqa-validation-1099", "mrqa_searchqa-validation-9672", "mrqa_squad-validation-6500", "mrqa_hotpotqa-validation-3953", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-15305", "mrqa_hotpotqa-validation-1656", "mrqa_hotpotqa-validation-1544", "mrqa_searchqa-validation-6090"], "EFR": 1.0, "Overall": 0.7383948863636364}, {"timecode": 55, "before_eval_results": {"predictions": ["the zebra", "Sarah McLachlan", "25-30", "Japan", "C Daryl Chessman", "grade point average", "grapefruit", "Detroit", "Tiger Woods", "John Paul II", "Blackbeard", "a goose", "Jane Goodall", "Big Ben", "Ethiopian", "1", "Stephen Crane", "Luxor", "gung ho", "a nickel", "Bill Clinton", "Wyoming", "the nasal septum", "Nantucket", "Abnormal Psychology", "Elvis Presley", "Gianlorenzo Bernini", "mosquitoes", "Frank Sinatra", "Mark Knopfler", "photons", "the National Archives Building", "low blood pressure", "Mousehunt", "Israel", "honey", "rugby", "Shrew", "a palace", "coffee", "Knott's Berry Farm", "Phaedra", "Carl von Linn", "Australia", "Cecil B. DeMille", "V-fib", "Barbary pirates", "bagels", "The Monkees", "Matilda Silicon carbide", "Don Juan", "the 1923 Pulitzer Prize - winning volume of poems written by Robert Frost", "Master Christopher Jones", "T.J. Miller", "7", "Jim Broadbent, John Cleese and Ricky Gervais", "Jerry Mouse", "AVN Adult Entertainment Expo", "England", "The Beatles", "identity documents belonging to Miguel Mejia Munera", "refused to refer the case of Mohammed al-Qahtani to prosecutors", "the 11th anniversary of the September 11, 2001, terror attacks.", "Council Grove"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6763157415501165}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.125, 1.0, 0.0, 0.8181818181818181, 0.3076923076923077, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-15701", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-15622", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-12914", "mrqa_searchqa-validation-11601", "mrqa_searchqa-validation-14248", "mrqa_searchqa-validation-7849", "mrqa_searchqa-validation-5988", "mrqa_searchqa-validation-12284", "mrqa_naturalquestions-validation-10546", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-7361", "mrqa_hotpotqa-validation-3169", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2215", "mrqa_hotpotqa-validation-2840"], "SR": 0.578125, "CSR": 0.5725446428571428, "retrieved_ids": ["mrqa_squad-train-30589", "mrqa_squad-train-15313", "mrqa_squad-train-77930", "mrqa_squad-train-80177", "mrqa_squad-train-51760", "mrqa_squad-train-66442", "mrqa_squad-train-414", "mrqa_squad-train-14628", "mrqa_squad-train-5940", "mrqa_squad-train-81093", "mrqa_squad-train-85944", "mrqa_squad-train-29900", "mrqa_squad-train-43206", "mrqa_squad-train-6536", "mrqa_squad-train-29995", "mrqa_squad-train-26350", "mrqa_hotpotqa-validation-550", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-7408", "mrqa_hotpotqa-validation-151", "mrqa_squad-validation-10328", "mrqa_searchqa-validation-8660", "mrqa_naturalquestions-validation-9088", "mrqa_squad-validation-7615", "mrqa_newsqa-validation-1468", "mrqa_hotpotqa-validation-5336", "mrqa_newsqa-validation-51", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-5843", "mrqa_searchqa-validation-13638", "mrqa_squad-validation-7778", "mrqa_newsqa-validation-3391"], "EFR": 1.0, "Overall": 0.7384151785714286}, {"timecode": 56, "before_eval_results": {"predictions": ["Jean Lafitte", "Nymphodorus", "argyle", "the Pacific Ocean", "Easter Sunday", "forgive", "Dalai Lama", "Jackie Joyner-Kersee", "a sousaphone", "Thomas L. Friedman", "tea", "arteries", "Nicholas II", "Amerigo Vespucci", "Patrick Henry", "Scampton, Lincolnshire", "Rolling Stone", "Ho Chi Minh", "the pituitary gland", "Ben Johnson", "Kentucky", "Roman Empire", "9 to 5", "Cambodia", "lunch", "Velvet Revolver", "Sears", "flavonoids", "a cherries", "Florence", "Ma Barker", "Joe DiMaggio", "the trinity knot", "Naples", "Nick and Norah's Infinite Playlist", "the Baruch Plan", "the Big Dipper", "wine", "silk", "\"The Safety Dance\"", "Manx cat", "the Moors", "a car clock", "North Carolina", "M&M'S Pe peanut Chocolate Candies", "a cake knife", "Tchaikovsky", "Versailles", "Panama Canal", "Cessna 172", "Abraham Lincoln", "Andy Serkis", "parthenogenesis", "Pangaea", "london", "mule", "60", "Silvia Navarro", "26 November", "John Morgan", "it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "Mary Phagan,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "off east  Africa"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6804452341137124}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913043, 1.0, 0.4615384615384615, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-5840", "mrqa_searchqa-validation-7212", "mrqa_searchqa-validation-6160", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10639", "mrqa_searchqa-validation-9458", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-11186", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-795", "mrqa_searchqa-validation-11589", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-2151", "mrqa_searchqa-validation-15973", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-3262", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-1022"], "SR": 0.578125, "CSR": 0.5726425438596492, "retrieved_ids": ["mrqa_squad-train-22604", "mrqa_squad-train-37973", "mrqa_squad-train-44374", "mrqa_squad-train-82497", "mrqa_squad-train-72680", "mrqa_squad-train-21382", "mrqa_squad-train-79231", "mrqa_squad-train-43684", "mrqa_squad-train-73681", "mrqa_squad-train-78270", "mrqa_squad-train-68619", "mrqa_squad-train-52561", "mrqa_squad-train-15384", "mrqa_squad-train-54898", "mrqa_squad-train-71177", "mrqa_squad-train-5305", "mrqa_naturalquestions-validation-2781", "mrqa_triviaqa-validation-6276", "mrqa_newsqa-validation-3305", "mrqa_hotpotqa-validation-2021", "mrqa_naturalquestions-validation-1533", "mrqa_squad-validation-9752", "mrqa_newsqa-validation-3111", "mrqa_naturalquestions-validation-4302", "mrqa_hotpotqa-validation-1312", "mrqa_searchqa-validation-6554", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-5470", "mrqa_squad-validation-1748", "mrqa_newsqa-validation-1546", "mrqa_searchqa-validation-10935"], "EFR": 1.0, "Overall": 0.7384347587719298}, {"timecode": 57, "before_eval_results": {"predictions": ["a spectator", "French toast", "Mexico", "plug in", "William Faulkner", "Patty Duke", "Hawthorne", "Hindu", "Juno", "Intel", "Hank Williams Jr.", "George C. Wallace", "Hernando", "an offensive", "West Virginia", "Edward Hopper", "an asteroid", "Mark Twain", "the Hubble Space Telescope", "Pop-Tarts", "Robert Johnson", "Steven Spielberg", "Adam Smith", "Tootsie", "water", "albino", "Bonn", "a wasteland", "chinchillas", "Tennessee", "the No Child Left Behind Act", "William S. Hart", "the Wiggles", "Francisco Franco", "Tennessee Williams", "unda", "Douglas Fairbanks, Jr.", "West Point", "The Beatles", "Steely Dan", "word", "Norway", "kotleta po-kyivsky", "George Clooney", "a diamond", "the Baltimore Orioles", "postcards", "Kentucky", "skateboarding", "Gaul", "blasters", "provides funding for operations, personnel, equipment, and activities", "Havana Harbor", "Melbourne", "Atticus Finch", "an elephant", "Planck", "Ella", "Edward Leonard \"Ed\" O'Neill", "\"First Blood\"", "Dancing With the Stars", "approximately 600 square miles of south-central Washington,", "February's Winter Games in Vancouver", "Upstairs Downstairs"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7272858796296297}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-7607", "mrqa_searchqa-validation-6688", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-5196", "mrqa_searchqa-validation-12764", "mrqa_searchqa-validation-8729", "mrqa_naturalquestions-validation-10533", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-381", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-1727"], "SR": 0.6875, "CSR": 0.5746228448275862, "retrieved_ids": ["mrqa_squad-train-12057", "mrqa_squad-train-83562", "mrqa_squad-train-44343", "mrqa_squad-train-75990", "mrqa_squad-train-86375", "mrqa_squad-train-55265", "mrqa_squad-train-36824", "mrqa_squad-train-61317", "mrqa_squad-train-22385", "mrqa_squad-train-12126", "mrqa_squad-train-46274", "mrqa_squad-train-81981", "mrqa_squad-train-35179", "mrqa_squad-train-38997", "mrqa_squad-train-74898", "mrqa_squad-train-18308", "mrqa_searchqa-validation-7387", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-595", "mrqa_triviaqa-validation-3828", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5568", "mrqa_squad-validation-5337", "mrqa_naturalquestions-validation-6897", "mrqa_newsqa-validation-384", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-4507", "mrqa_triviaqa-validation-7351", "mrqa_naturalquestions-validation-10321", "mrqa_searchqa-validation-4684", "mrqa_hotpotqa-validation-4433", "mrqa_squad-validation-2315"], "EFR": 1.0, "Overall": 0.7388308189655173}, {"timecode": 58, "before_eval_results": {"predictions": ["General Andrew Jackson", "Sri Lanka", "John Glenn", "Hinduism", "Lady Sings the Blues", "wedlock", "trans fat", "Gov. Robin McKinley", "Edward", "Hello, Dolly!", "Mesozoic", "Gettysburg", "Martin Lawrence", "plantain", "Heracles", "Bob Fosse", "stem cells", "a sawlass", "the Bodleian Library", "pupils", "a front", "James Franco", "salmon", "The Crow", "sheep's milk cheese", "James Watt", "1945", "a birthstone", "Ichabod Crane", "Morrie: An Old Man, a Young Man", "Heather Locklear", "noun", "Holden Caulfield", "toasted hazelnuts", "Saudi Arabia", "lamb", "LaDainian Tomlinson", "wheat", "Duke", "photoelectric", "Cape Town", "haploid", "Austin Powers", "sourdough", "mo Nissanite", "vice presidential running mate", "Joseph Stalin", "La Guardia", "Chastity", "Turandot", "Texas Rangers", "Henri Biva", "ice giants", "Amenhotep IV", "Zimbabwe", "colony", "phobias", "Haiti", "Argand lamp", "1891", "Dick Cheney", "1-1", "upper respiratory infection", "Russia"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6633556547619048}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-16405", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-4326", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5137", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-14926", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-11795", "mrqa_searchqa-validation-13956", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-2685", "mrqa_hotpotqa-validation-1482", "mrqa_newsqa-validation-2402", "mrqa_newsqa-validation-2472", "mrqa_naturalquestions-validation-3214"], "SR": 0.59375, "CSR": 0.5749470338983051, "retrieved_ids": ["mrqa_squad-train-13465", "mrqa_squad-train-64117", "mrqa_squad-train-1126", "mrqa_squad-train-60675", "mrqa_squad-train-20455", "mrqa_squad-train-46142", "mrqa_squad-train-29260", "mrqa_squad-train-39218", "mrqa_squad-train-12123", "mrqa_squad-train-3858", "mrqa_squad-train-5994", "mrqa_squad-train-38189", "mrqa_squad-train-47286", "mrqa_squad-train-31658", "mrqa_squad-train-57401", "mrqa_squad-train-399", "mrqa_squad-validation-10258", "mrqa_hotpotqa-validation-3084", "mrqa_searchqa-validation-227", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-10018", "mrqa_squad-validation-1912", "mrqa_naturalquestions-validation-9214", "mrqa_searchqa-validation-4582", "mrqa_hotpotqa-validation-1070", "mrqa_newsqa-validation-2222", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-1308", "mrqa_searchqa-validation-8374", "mrqa_hotpotqa-validation-2208", "mrqa_newsqa-validation-1225", "mrqa_hotpotqa-validation-97"], "EFR": 1.0, "Overall": 0.7388956567796611}, {"timecode": 59, "before_eval_results": {"predictions": ["Trade Mark Registration Act 1875", "Mary Magdalene", "The Pillow Book", "General Paulus", "the Grail", "butcher", "The Double", "Dr. Samuel Johnson", "Jessica Simpson", "Humble Pie", "the gallbladder", "peterloo", "Moses", "Leo Tolstoy", "Birmingham", "a two-dimensional figure", "The Magnificent Seven", "Australian shearers", "Theodore Roosevelt", "a raven", "John of Gaunt", "typhoid fever", "tin", "Microsoft", "John Galliano", "the Big Bang", "Willie Nelson", "European horseracing", "Stars on 45 Medley", "Lundy", "Guinea", "Nadia Comaneci", "Belgium", "Del", "John Keyse Sherwin", "non-Orthodox synagogues", "Stitch", "Herbert Asquith, 1st Earl of Oxford", "Nirvana and Kiss", "Mr. Humphries", "Paul Gauguin", "Connochaetes", "French", "50", "Charlie Harper", "nirvana", "Pet Sounds", "purple", "Brigit Forsyth", "aardvark", "Charles Darwin", "5.7 million", "Oklahoma", "Luke Luke 18 : 1 - 8", "8,515", "villanelle", "Awake", "2008", "China, Taiwan, Hong Kong and Mongolia,", "Patrick McGoohan", "a coyote", "heat", "A Tale of Two Cities", "Charlie Wilson"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7150049603174604}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true], "QA-F1": [0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-4273", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-3489", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-6759", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-945", "mrqa_triviaqa-validation-6132", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-577", "mrqa_triviaqa-validation-3208", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4592", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-4197", "mrqa_newsqa-validation-2061", "mrqa_searchqa-validation-7629"], "SR": 0.59375, "CSR": 0.5752604166666666, "retrieved_ids": ["mrqa_squad-train-24517", "mrqa_squad-train-44990", "mrqa_squad-train-21913", "mrqa_squad-train-62289", "mrqa_squad-train-49689", "mrqa_squad-train-58777", "mrqa_squad-train-73879", "mrqa_squad-train-47152", "mrqa_squad-train-82098", "mrqa_squad-train-76045", "mrqa_squad-train-6553", "mrqa_squad-train-70260", "mrqa_squad-train-15324", "mrqa_squad-train-62077", "mrqa_squad-train-11926", "mrqa_squad-train-83568", "mrqa_hotpotqa-validation-5482", "mrqa_searchqa-validation-6368", "mrqa_triviaqa-validation-5882", "mrqa_hotpotqa-validation-1456", "mrqa_squad-validation-5878", "mrqa_naturalquestions-validation-8062", "mrqa_squad-validation-3754", "mrqa_hotpotqa-validation-4362", "mrqa_squad-validation-8447", "mrqa_naturalquestions-validation-9239", "mrqa_squad-validation-1045", "mrqa_naturalquestions-validation-6285", "mrqa_newsqa-validation-2858", "mrqa_searchqa-validation-11252", "mrqa_triviaqa-validation-4946", "mrqa_hotpotqa-validation-1526"], "EFR": 0.9230769230769231, "Overall": 0.723573717948718}, {"timecode": 60, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1121", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-122", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4518", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-5828", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-159", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-1071", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10905", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11198", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-12032", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-12468", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15373", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16915", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3160", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3329", "mrqa_searchqa-validation-3454", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4232", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6187", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8373", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-9809", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4887", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9613", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1078", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3904", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4074", "mrqa_triviaqa-validation-4497", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-5183", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-690", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-998"], "OKR": 0.8671875, "KG": 0.48046875, "before_eval_results": {"predictions": ["15", "G\u00f6tene in Sweden", "William Shakespeare", "Rensselaer County", "more than 20", "Beauty and the Beast", "authoritarian tendencies", "Boletus edulis", "Overtime", "south-north motorway", "Eisstadion Davos", "2014 New Year Honours", "Domingo \"Sam\" Samudio", "Sunday", "Taylor Swift", "Asif Kapadia", "to steal the plans for the Death Star", "graffiti artists", "ESPN", "Bangor International Airport", "October 29, 1985", "Point of Entry", "Mickey's Christmas Carol", "the Harpe brothers", "1940s and 1950s", "Port Clinton", "deadpan sketch group", "Bharat Ratna", "Ronald Joseph Ryan", "subsequently broadcast internationally", "2011 Pulitzer Prize in General Nonfiction", "Eliot Cutler", "IT products and services", "American", "2010", "Critics' Choice Television Award for Best Supporting Actress in a Comedy Series", "Grover Krantz of Washington State University", "Picric acid", "23 March 1991", "1979", "Hannaford", "1968", "the post-Roman Republic", "Sparafucile", "Bill Clinton", "\"Twister\"", "94 episodes", "horror", "Baa, Baa and the flowers from Teletubbyland sing Mary, Mary, Quite Contrary", "jus sanguinis", "28,776", "the biblical name of a Canaanite god associated with child sacrifice", "the microscope's stage", "commemorating fealty and filial piety", "God", "Video", "charl Margery", "people Against Switching Sides (PASS)", "North Korea", "Jund Ansar Allah, or Soldiers of the Partisans of God,", "the Thames", "Pamela Anderson", "Henry Ford", "methane"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6513020833333334}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.4, 0.26666666666666666, 0.0, 0.4, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-5589", "mrqa_hotpotqa-validation-2263", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-1151", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-1570", "mrqa_naturalquestions-validation-182", "mrqa_triviaqa-validation-6429", "mrqa_triviaqa-validation-3314", "mrqa_newsqa-validation-2732"], "SR": 0.546875, "CSR": 0.5747950819672132, "retrieved_ids": ["mrqa_squad-train-65067", "mrqa_squad-train-61429", "mrqa_squad-train-82983", "mrqa_squad-train-70892", "mrqa_squad-train-21067", "mrqa_squad-train-70960", "mrqa_squad-train-43785", "mrqa_squad-train-68850", "mrqa_squad-train-14543", "mrqa_squad-train-68130", "mrqa_squad-train-59347", "mrqa_squad-train-39325", "mrqa_squad-train-25440", "mrqa_squad-train-47304", "mrqa_squad-train-52150", "mrqa_squad-train-30527", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-6559", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-3305", "mrqa_naturalquestions-validation-4211", "mrqa_hotpotqa-validation-4940", "mrqa_newsqa-validation-4089", "mrqa_squad-validation-2497", "mrqa_searchqa-validation-2778", "mrqa_triviaqa-validation-3909", "mrqa_newsqa-validation-2265", "mrqa_squad-validation-8581", "mrqa_searchqa-validation-7159", "mrqa_triviaqa-validation-5038", "mrqa_searchqa-validation-5196"], "EFR": 0.9655172413793104, "Overall": 0.7244687146693047}, {"timecode": 61, "before_eval_results": {"predictions": ["Awake", "Law Adam", "Daniel Wroughton Craig", "Sven Magnus \u00d8en Carlsen", "Volvo 850", "400 MW", "1991", "Dra\u017een Petrovi\u0107", "Lord's Resistance Army", "Adam Karpel", "Midtown Manhattan", "IFFHS World's Best Goalkeeper", "David May", "New Jersey", "Sir Derek George Jacobi", "Waimea Bay", "Willie Nelson and Kris Kristofferson", "the Mikoyan design bureau", "Nickelodeon on Sunset", "Terry the Tomboy", "an Anglo-Saxon saint", "Give Up", "Matt Winer", "the University of Kentucky", "WB Television Network", "Ice Princess", "Boxing Day, 2004", "liberty as its main idea, promoting free expression, freedom of choice, other social freedoms, and \"laissez-faire\" capitalism", "Australian", "Norse mythology", "Konstant\u012bns Raudive", "Melville", "5,922", "White Horse", "Black Abbots", "Moon Embracing the Sun", "the U.S. states of Kentucky, Virginia, and Tennessee", "2011", "five", "Veronica Hamel", "literary magazine", "French, English and Spanish", "Edward James Olmos", "John R. Leonetti", "\"Alceste\"", "Perth, Western Australia", "Cersei Jaimeister", "port of Baltimore west to Sandy Hook", "static test pressure", "Tayeb Salih", "\"The Simpsons\"' thirteenth season", "Ferm\u00edn Francisco de Lasu\u00e9n", "22 \u00b0 00 \u2032 N 80 \u00b000 \u2032 W \ufeff / \ufefb\ufffd 22.000 \u00b0 N 80.% \u00b0 W \uff7f /22.000 ; - 80.000", "Yuzuru Hanyu", "Jose Antonio Reyes", "Jordan", "Lulu", "sniff out cell phones.", "18", "the E.G. Buehrle Collection", "Prison Break", "Oscar Wilde", "Sicily", "Yukon"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7476866883116884}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.28571428571428575, 0.19047619047619047, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2135", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3542", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-4367", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-1670", "mrqa_hotpotqa-validation-4624", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-2187", "mrqa_naturalquestions-validation-5451", "mrqa_newsqa-validation-4032"], "SR": 0.65625, "CSR": 0.576108870967742, "retrieved_ids": ["mrqa_squad-train-53892", "mrqa_squad-train-44200", "mrqa_squad-train-55442", "mrqa_squad-train-2949", "mrqa_squad-train-2831", "mrqa_squad-train-24130", "mrqa_squad-train-66363", "mrqa_squad-train-1296", "mrqa_squad-train-31173", "mrqa_squad-train-84341", "mrqa_squad-train-1192", "mrqa_squad-train-31777", "mrqa_squad-train-35636", "mrqa_squad-train-58481", "mrqa_squad-train-62339", "mrqa_squad-train-11562", "mrqa_hotpotqa-validation-550", "mrqa_naturalquestions-validation-10501", "mrqa_triviaqa-validation-4608", "mrqa_searchqa-validation-6225", "mrqa_searchqa-validation-8155", "mrqa_naturalquestions-validation-3801", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1544", "mrqa_newsqa-validation-2395", "mrqa_searchqa-validation-5275", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-2004", "mrqa_newsqa-validation-743", "mrqa_squad-validation-7017"], "EFR": 0.9545454545454546, "Overall": 0.7225371151026393}, {"timecode": 62, "before_eval_results": {"predictions": ["at home, attending every soccer game and knowing what his kids like to eat for breakfast.", "refused Wednesday to soften the Vatican's ban on condom use", "propaganda show.", "company Polo", "punish participants", "Venezuela's Libertador military airfield", "Dore Gold, former Israeli ambassador to the United Nations", "The European Union", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "India", "The worst snowstorm to hit Britain in 18 years", "Bryant Purvis, 19, was arrested after the incident at Hebron High School in Carrollton, Texas.", "illegal immigrants", "the charter mandated the English king to cede certain basic rights to his citizens, ensuring that no man is above the law.", "Michael Jackson's", "Cash for Clunkers", "San Diego", "returning combat veterans", "Robert Park", "The Mexican military", "tusks", "modern and classic designs", "Thousands", "Thursday", "wife's name", "$17,000", "charity, Wheelchair for Iraqi Kids.", "Bob Johnson", "Matthew Fisher", "26", "angel", "$1,500", "the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "Former detainees", "commitment to regain the trust of those customers who are driving our vehicles.", "1994", "High Court Judge Justice Davis", "to provide security as needed.", "$83,03013", "$250,000", "Sally Anne Aldous, 29,", "Islamabad", "hundreds of people joined a campus rally to oppose racial intolerance.", "North Korean newspaper Rodong Sinmun", "Osama", "dismissed all charges", "was waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "Zimbabwe", "World Boxing Council welterweight champion in 2002-03,", "capturing it on videotape years ago,", "of Columbia National Guard,", "March 1930", "1961", "Rajendra Prasad", "lancaster", "Secretary of State William H. Seward", "conterminous United States", "Ice Princess", "Hispania Racing F1 Team", "small family car", "key", "Kansas", "Louis XVII", "Kim Basinger"], "metric_results": {"EM": 0.390625, "QA-F1": 0.47919551343264577}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 0.5333333333333333, 0.16666666666666669, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.2222222222222222, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.14285714285714285, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.5714285714285715, 0.0, 1.0, 0.8333333333333334, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-1481", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-829", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6501", "mrqa_hotpotqa-validation-1346", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5326"], "SR": 0.390625, "CSR": 0.5731646825396826, "retrieved_ids": ["mrqa_squad-train-45173", "mrqa_squad-train-11034", "mrqa_squad-train-50602", "mrqa_squad-train-33109", "mrqa_squad-train-40667", "mrqa_squad-train-22082", "mrqa_squad-train-479", "mrqa_squad-train-52840", "mrqa_squad-train-66672", "mrqa_squad-train-37316", "mrqa_squad-train-25986", "mrqa_squad-train-41232", "mrqa_squad-train-72494", "mrqa_squad-train-9403", "mrqa_squad-train-10231", "mrqa_squad-train-14915", "mrqa_hotpotqa-validation-2457", "mrqa_newsqa-validation-1230", "mrqa_hotpotqa-validation-5336", "mrqa_searchqa-validation-4326", "mrqa_newsqa-validation-2446", "mrqa_naturalquestions-validation-1327", "mrqa_searchqa-validation-11601", "mrqa_squad-validation-7525", "mrqa_squad-validation-1818", "mrqa_hotpotqa-validation-2639", "mrqa_triviaqa-validation-6429", "mrqa_searchqa-validation-1445", "mrqa_hotpotqa-validation-599", "mrqa_triviaqa-validation-959", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-650"], "EFR": 0.9743589743589743, "Overall": 0.7259109813797313}, {"timecode": 63, "before_eval_results": {"predictions": ["Israel", "billboards with an image of the burning World Trade Center", "Saturn", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law.", "Kgalema Motlanthe,", "Ken Choi,", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "holed up in my home,", "up to $50,000", "gun charges,", "January 24, 2006.", "usion teams", "Philippines", "used-luxury market", "in July", "in her home", "natural gas", "in the clubs of Hollywood.", "jazz", "almost 30 tunnels, including the 6.2-mile Moffat Tunnel,", "hid his money,", "40 lash for the incident which is said to have taken place in the capital Khartoum on August 21.", "KBR", "Ralph Lauren", "Dubai", "Al-Shabaab", "\"Wicked,\"", "269,000", "eight", "Dube, 43, was killed", "North Korea", "Tuesday,", "super-yacht designers", "Alina Cho", "Unseeded Frenchwoman Aravane Rezai", "80 percent of the woman's face", "1983", "he gave the victims \"assurances of the church's action\" after the April 18 meeting.", "collaborated with the Colombian government,", "a three-time road race world champion,", "\"We tortured (Mohammed al+) Qahtani,\"", "Yemen,", "11", "Matthew Fisher", "Afghanistan's restive provinces", "Dan Parris, 25, and Rob Lehr, 26,", "insect stings,", "Tennessee", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's war zone.", "cancer-causing toxic chemical.", "don't", "Sleeping with the Past", "on the two tablets", "in the pachytene stage of prophase I of meiosis during a process called synapsis", "fire", "9", "Edward III", "fourth", "Academy Award for Best Art Direction", "1958", "Ibrahim Hannibal", "Central Park Zoo", "draft- horse breed", "November"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5505179121610505}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.9411764705882353, 1.0, 0.10526315789473685, 0.4444444444444445, 0.8, 0.2608695652173913, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.10526315789473684, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.11764705882352942, 0.75, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.2666666666666667, 1.0, 0.16666666666666669, 1.0, 0.0, 0.9565217391304348, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-3049", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-7035", "mrqa_triviaqa-validation-7626", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-3383", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-10510"], "SR": 0.4375, "CSR": 0.571044921875, "retrieved_ids": ["mrqa_squad-train-15571", "mrqa_squad-train-38021", "mrqa_squad-train-73842", "mrqa_squad-train-16962", "mrqa_squad-train-38032", "mrqa_squad-train-26848", "mrqa_squad-train-44088", "mrqa_squad-train-77646", "mrqa_squad-train-25631", "mrqa_squad-train-37949", "mrqa_squad-train-7624", "mrqa_squad-train-54999", "mrqa_squad-train-21540", "mrqa_squad-train-70036", "mrqa_squad-train-50184", "mrqa_squad-train-38888", "mrqa_searchqa-validation-5111", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-4300", "mrqa_naturalquestions-validation-3419", "mrqa_searchqa-validation-15812", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3362", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-11741", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-4240", "mrqa_searchqa-validation-9334", "mrqa_triviaqa-validation-6163", "mrqa_newsqa-validation-2287", "mrqa_squad-validation-2272"], "EFR": 1.0, "Overall": 0.730615234375}, {"timecode": 64, "before_eval_results": {"predictions": ["\"We Found Love\"", "Dr. Albert Reiter,", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "40", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "new materials -- including ultra-high-strength steel and boron -- helped make the new truck safer,", "19 American tourists and two Egyptians -- the bus driver and a tour guide --", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr,", "great jazz", "The lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains poses a challenge for prosecutors,", "Sodra nongovernmental organization,", "in Port-au-Prince", "\"Toy Story\"", "0430, 1730, Sunday 31 May:", "WGC-CA Championship", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "has to do with the prime minister position that will be created in the new Zimbabwe government.", "collaborating with the Colombian government,", "Russian air force,", "Rod Blagojevich", "Fiona MacKeown", "50", "legitimacy of that race.", "President Obama", "John Lennon and George Harrison,", "Sharon Bialek", "1998.", "five days a week.", "Israel and the United States", "Monday.", "Frank Ricci,", "Sixteen", "The International Red Cross Committee, the U.N. High Commissioner for Refugees and UNICEF", "United Nations World Food Program", "Kenyan forces", "Daytime Emmy Lifetime Achievement Award", "since 1983.", "the foyer of the BBC building in Glasgow, Scotland", "Interior Ministry,", "EU naval force", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "attempted Burgranos", "two", "6-2", "U.S. Consulate in Rio de Janeiro,", "John Demjanjuk", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil,", "to secure more funds from the region.", "Redwood Original", "Drew Barrymore", "pulmonary heart disease ( cor pulmonale )", "a printed circuit board", "Richard Wagner", "H. H. Asquith", "Centers for Medicare and Medicaid Services", "FBI", "Macomb County", "Custer", "Louis XIV", "a waterbed", "Risk"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5799326492292823}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.4, 0.2857142857142857, 0.16666666666666669, 0.16666666666666669, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.10526315789473684, 0.3703703703703704, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08333333333333333, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.3, 0.13333333333333333, 0.8571428571428571, 0.23529411764705882, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-574", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-5934", "mrqa_hotpotqa-validation-2837"], "SR": 0.484375, "CSR": 0.5697115384615384, "retrieved_ids": ["mrqa_squad-train-29810", "mrqa_squad-train-26070", "mrqa_squad-train-41269", "mrqa_squad-train-70666", "mrqa_squad-train-63383", "mrqa_squad-train-19698", "mrqa_squad-train-8067", "mrqa_squad-train-2313", "mrqa_squad-train-59217", "mrqa_squad-train-7308", "mrqa_squad-train-60041", "mrqa_squad-train-5988", "mrqa_squad-train-31559", "mrqa_squad-train-40545", "mrqa_squad-train-4776", "mrqa_squad-train-21182", "mrqa_hotpotqa-validation-1788", "mrqa_triviaqa-validation-1346", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-9384", "mrqa_searchqa-validation-5846", "mrqa_squad-validation-1045", "mrqa_newsqa-validation-2415", "mrqa_naturalquestions-validation-8205", "mrqa_triviaqa-validation-1120", "mrqa_newsqa-validation-1481", "mrqa_squad-validation-1025", "mrqa_hotpotqa-validation-1618", "mrqa_searchqa-validation-5150", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-1890", "mrqa_newsqa-validation-444"], "EFR": 0.9696969696969697, "Overall": 0.7242879516317016}, {"timecode": 65, "before_eval_results": {"predictions": ["Cheshire", "Carol Ann Duffy", "liquidambar styraciflua", "Fredric Warburg", "Battleship", "Faith", "the Slavic women accompanying their husbands in the First Balkan War.", "Teutonic Knights", "9 venues", "James Harrison", "Germany", "Jonathan Katz", "Ford Island", "2011", "NCAA Division I", "Christian Slater", "Latium", "Paul Avery", "Berea College", "Christopher Lloyd Smalling", "January 4, 1976", "a zero-g-roll", "1971", "Clovis I", "Tie Domi", "2007", "writer", "Quasimodo", "Savin Yeatman-Eiffel", "Pieter van Musschenbroek", "23 July 1989", "actor", "Attorney General and as Lord Chancellor of England", "Socrates", "Fife", "Henry Mills", "Ribosomes", "Ronald Ryan", "A Hard Day's Night", "Humberside", "Dumfries and Galloway, south-west Scotland", "Rudolph the Red-Nosed Reindeer", "from 1989 until 1994", "Cecily Legler Strong", "Polish-Jewish", "Philip Aaberg", "2005", "Levon Helm", "Chengdu Aircraft Corporation", "White Knights of the Ku Klux Klan", "Reunited Worlds", "Lou Rawls", "the first year begins", "Michael English", "Parkinson's disease", "I Will survive", "Yeats", "Casa de Campo International Airport", "11", "why you broke up", "Carmen", "New York City", "Massachusetts", "in July"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7044671474358974}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.7499999999999999, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.7692307692307693, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-1244", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5637", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1433", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-2653", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-4711", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-4573", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-1180", "mrqa_searchqa-validation-2844", "mrqa_newsqa-validation-271"], "SR": 0.59375, "CSR": 0.5700757575757576, "retrieved_ids": ["mrqa_squad-train-75176", "mrqa_squad-train-21649", "mrqa_squad-train-26879", "mrqa_squad-train-31202", "mrqa_squad-train-32335", "mrqa_squad-train-47852", "mrqa_squad-train-20607", "mrqa_squad-train-41332", "mrqa_squad-train-82997", "mrqa_squad-train-30709", "mrqa_squad-train-17743", "mrqa_squad-train-65049", "mrqa_squad-train-12414", "mrqa_squad-train-52535", "mrqa_squad-train-24576", "mrqa_squad-train-9186", "mrqa_newsqa-validation-2204", "mrqa_hotpotqa-validation-296", "mrqa_newsqa-validation-1003", "mrqa_triviaqa-validation-2135", "mrqa_newsqa-validation-3764", "mrqa_naturalquestions-validation-7035", "mrqa_searchqa-validation-10640", "mrqa_triviaqa-validation-4273", "mrqa_searchqa-validation-11252", "mrqa_triviaqa-validation-7406", "mrqa_hotpotqa-validation-622", "mrqa_newsqa-validation-2653", "mrqa_squad-validation-7885", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-4815", "mrqa_hotpotqa-validation-4418"], "EFR": 0.9615384615384616, "Overall": 0.7227290938228438}, {"timecode": 66, "before_eval_results": {"predictions": ["Solomon", "an alligator", "cancer, diabetes and endocrinology", "quoit", "Ramona", "Tobacco Road", "M*A*S*H", "Opportunity", "Smokey Robinson", "scorpion", "Gladiator", "red and green", "trachea", "Cairo", "The Cotton Club", "a sandstorm", "Lord Byron", "neutrino", "Foster", "George Eliot", "clouds", "Sir Arthur Conan Doyle", "Mission San Juan Capistrano", "Auschwitz", "China", "Uganda", "caeser, vinaigrette", "Edward", "a garnet", "Bali", "Paris", "the decathlon", "Elizabeth II", "kings", "The Hollywood Ten", "a kayak", "a jumper", "Jean", "Yogi Berra", "China", "Hawaii", "birds", "peripheral vision", "Espresso", "Central Park", "head", "Steve Curtis", "blowtorches", "kinetic", "1453", "Reno", "16 seasons", "photodiode", "Michigan, south to northern Louisiana, west to Colorado, and east to Massachusetts", "Joan Crawford", "Bassenthwaite Lake", "moon", "All Nippon Airways", "Jack Ridley", "diplomat", "children's books", "Six people", "attempted burglary", "missile"], "metric_results": {"EM": 0.625, "QA-F1": 0.6950738916256158}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8275862068965517, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-13562", "mrqa_searchqa-validation-14972", "mrqa_searchqa-validation-5294", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-14263", "mrqa_searchqa-validation-10375", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-15880", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-1198", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-15274", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-14831", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-2870", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-5467", "mrqa_newsqa-validation-3829"], "SR": 0.625, "CSR": 0.5708955223880596, "retrieved_ids": ["mrqa_squad-train-84235", "mrqa_squad-train-19104", "mrqa_squad-train-86458", "mrqa_squad-train-1239", "mrqa_squad-train-46951", "mrqa_squad-train-3827", "mrqa_squad-train-36030", "mrqa_squad-train-52406", "mrqa_squad-train-69568", "mrqa_squad-train-18233", "mrqa_squad-train-118", "mrqa_squad-train-47799", "mrqa_squad-train-65041", "mrqa_squad-train-52725", "mrqa_squad-train-43216", "mrqa_squad-train-54437", "mrqa_newsqa-validation-1432", "mrqa_hotpotqa-validation-881", "mrqa_triviaqa-validation-2405", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2446", "mrqa_hotpotqa-validation-1661", "mrqa_searchqa-validation-8374", "mrqa_hotpotqa-validation-4927", "mrqa_naturalquestions-validation-7214", "mrqa_searchqa-validation-1108", "mrqa_hotpotqa-validation-526", "mrqa_searchqa-validation-13537", "mrqa_hotpotqa-validation-4852", "mrqa_searchqa-validation-4299", "mrqa_triviaqa-validation-5934", "mrqa_searchqa-validation-1723"], "EFR": 1.0, "Overall": 0.730585354477612}, {"timecode": 67, "before_eval_results": {"predictions": ["Steven Spielberg", "Baton Rouge", "Wilbur Wright", "Charles Lindbergh", "(John) Wood", "Stephen Sondheim", "507 feet", "a calculators", "Bill Wyman", "Surgeon", "T.S. Eliot", "lead", "Breslin", "French and Indian War", "a gravitational field", "Fisherman\\'s Wharf", "Santa Fe", "Ted Koppel", "Sex Pistols", "chess", "Michael Jordan", "Roustabout", "doughboy", "Brge Rosenbaum", "Muhammad Ali", "a rabbit", "Secretariat", "soupman", "a tooth", "tannins", "Homer", "a rudder", "a woman scorned", "Karol Jzef Wojtyla", "Will Rogers", "Hairspray", "Orlando", "Hopelessly Devoted", "Old Ironsides", "River Phoenix", "Sydney", "mutton", "easel", "Napoleon", "purple", "Peter the Great", "barn-raising", "corporal punishment", "Missouri", "Sweeney Todd", "Paris", "1956", "Tommy James", "`` Two Days Before the Day After Tomorrow ''", "aikido", "Salvador Dal\u00ed", "Jerry Zaks", "1993", "October 17, 2017", "from 1986 to 2013", "Afghanistan", "a mammoth", "co-writing credits and a share of the royalties", "Gary Grimes"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7161458333333334}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.16666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15468", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-7771", "mrqa_searchqa-validation-626", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7053", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-2305", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-11023", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-1882", "mrqa_triviaqa-validation-6594", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2152"], "SR": 0.671875, "CSR": 0.5723805147058824, "retrieved_ids": ["mrqa_squad-train-14034", "mrqa_squad-train-35412", "mrqa_squad-train-64368", "mrqa_squad-train-70114", "mrqa_squad-train-74800", "mrqa_squad-train-51579", "mrqa_squad-train-79405", "mrqa_squad-train-65492", "mrqa_squad-train-43842", "mrqa_squad-train-22558", "mrqa_squad-train-15854", "mrqa_squad-train-40192", "mrqa_squad-train-47095", "mrqa_squad-train-32688", "mrqa_squad-train-12483", "mrqa_squad-train-82385", "mrqa_searchqa-validation-6929", "mrqa_naturalquestions-validation-6137", "mrqa_hotpotqa-validation-2019", "mrqa_naturalquestions-validation-7408", "mrqa_hotpotqa-validation-3507", "mrqa_naturalquestions-validation-6896", "mrqa_hotpotqa-validation-4079", "mrqa_naturalquestions-validation-875", "mrqa_squad-validation-4015", "mrqa_naturalquestions-validation-9626", "mrqa_searchqa-validation-942", "mrqa_naturalquestions-validation-4185", "mrqa_searchqa-validation-16366", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-2086", "mrqa_naturalquestions-validation-3962"], "EFR": 0.9523809523809523, "Overall": 0.7213585434173669}, {"timecode": 68, "before_eval_results": {"predictions": ["robert's ring", "Omaha", "Antwerp", "the Matterhorn", "Loch Lomond", "the antimeridian", "Frasier", "a temporary need", "Denmark", "the \"ball in tube\" or electromechanical crash sensor", "a poor man", "a Cockney flower girl", "cholera", "E.E. Cummings", "Wilhelm Roentgen", "robert ehrlich,", "Yes", "the Green Hornet", "the Peabody Award", "yolk", "amniotic fluid", "Hell", "Diner", "Cleopatra", "pep", "New York", "Japan", "Jordan", "Derek Jeter", "Hans Christian Andersen", "an optional value", "defense", "\"The Tyger\"", "Shelley", "diamond", "carbon dioxide", "earthquakes", "Jr.", "Citizen Kane", "gravity", "Brady", "Clinton", "one's own court", "Tasmania", "Wyoming", "the Fellowship of the Ring", "the quick brown fox", "Denmark", "wheat", "\"Free Bird\"", "a frigate", "http://www.example.com/index.HTML", "presidential representative democratic republic", "the Pir Panjal Railway Tunnel", "Barcelona", "730,000", "Leander", "the Lewis and Clark Expedition", "Kingdom of Morocco", "1998", "club managers", "out of either heavy flannel or wool", "several weeks", "2002\u201303"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5743371212121211}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.36363636363636365, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16502", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-10973", "mrqa_searchqa-validation-16760", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-12041", "mrqa_searchqa-validation-13013", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-9738", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-13968", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-16947", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-2169", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8857", "mrqa_searchqa-validation-10953", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-15704", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-1848", "mrqa_triviaqa-validation-2502", "mrqa_hotpotqa-validation-2045", "mrqa_newsqa-validation-3500", "mrqa_hotpotqa-validation-541"], "SR": 0.515625, "CSR": 0.5715579710144927, "retrieved_ids": ["mrqa_squad-train-55017", "mrqa_squad-train-42765", "mrqa_squad-train-83820", "mrqa_squad-train-26020", "mrqa_squad-train-40689", "mrqa_squad-train-17804", "mrqa_squad-train-12534", "mrqa_squad-train-6315", "mrqa_squad-train-78806", "mrqa_squad-train-49393", "mrqa_squad-train-36247", "mrqa_squad-train-42181", "mrqa_squad-train-16547", "mrqa_squad-train-74132", "mrqa_squad-train-4395", "mrqa_squad-train-32720", "mrqa_naturalquestions-validation-953", "mrqa_hotpotqa-validation-5497", "mrqa_newsqa-validation-3048", "mrqa_naturalquestions-validation-9386", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-11993", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3325", "mrqa_hotpotqa-validation-5304", "mrqa_newsqa-validation-2233", "mrqa_searchqa-validation-12772", "mrqa_searchqa-validation-10392", "mrqa_newsqa-validation-591", "mrqa_naturalquestions-validation-8159", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-2086"], "EFR": 1.0, "Overall": 0.7307178442028985}, {"timecode": 69, "before_eval_results": {"predictions": ["Kentucky Fried Chicken", "a dickey", "Follies", "Aloysius", "Andrew Jackson", "Agamemnon", "spurs", "Edward Winslow", "\"Bah-dum.", "a canton", "Louisiana", "a shopping center", "percussus", "Pardon Richard Nixon", "Diana", "lollipop", "Cassiopeia", "Indiana Jones and the Crystal Skull", "HSN", "20", "Mendel", "Maria Callas", "Hulk Hogan", "Margaret Tobin", "horses", "A Hard Day's Night", "Making the Band", "Garland", "Autumn in New York", "a telephone operator", "Franklin D. Roosevelt", "William Shakespeare", "a post-apocalypse sci-fi short story", "La Salle", "lattice", "a penny", "Succotash", "the retina", "a wedding ceremony", "Lake Coeur d'Alene", "The Sopranos", "\"Hark\"", "Huguenots", "Brooklyn Dodgers", "king", "yellow", "Mascara", "Rooster", "ponderosa pine", "Homestead Act", "Donald Trump", "Kyrie Irving", "Mexico", "Aaron Harrison", "Crete", "Miles Morales", "Peter Nichols", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "1858", "McComb, Mississippi", "Newcastle", "five years", "\" Unfortunately, this is not an anomaly in Naples and in that neighborhood.\"", "mermaid"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6269072906940554}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2962962962962963, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-5203", "mrqa_searchqa-validation-843", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-10815", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-185", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-5237", "mrqa_searchqa-validation-12476", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-4606", "mrqa_searchqa-validation-705", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-5659", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-2762", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-2727"], "SR": 0.546875, "CSR": 0.5712053571428571, "retrieved_ids": ["mrqa_squad-train-5005", "mrqa_squad-train-58536", "mrqa_squad-train-19058", "mrqa_squad-train-72245", "mrqa_squad-train-75050", "mrqa_squad-train-67805", "mrqa_squad-train-19780", "mrqa_squad-train-80925", "mrqa_squad-train-82001", "mrqa_squad-train-70654", "mrqa_squad-train-75584", "mrqa_squad-train-23857", "mrqa_squad-train-39423", "mrqa_squad-train-24582", "mrqa_squad-train-4717", "mrqa_squad-train-31167", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-3402", "mrqa_triviaqa-validation-4784", "mrqa_hotpotqa-validation-2081", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-7509", "mrqa_hotpotqa-validation-4926", "mrqa_newsqa-validation-933", "mrqa_searchqa-validation-6684", "mrqa_hotpotqa-validation-5021", "mrqa_naturalquestions-validation-3419", "mrqa_newsqa-validation-733", "mrqa_squad-validation-7017", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-1711", "mrqa_triviaqa-validation-3393"], "EFR": 1.0, "Overall": 0.7306473214285714}, {"timecode": 70, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-947", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16953", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4642", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7212", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7593", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9643", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10254", "mrqa_squad-validation-10328", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1075", "mrqa_squad-validation-1183", "mrqa_squad-validation-1319", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-2975", "mrqa_squad-validation-3139", "mrqa_squad-validation-3168", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4147", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5532", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5826", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6624", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-821", "mrqa_squad-validation-8253", "mrqa_squad-validation-8410", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5488", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6565", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.875, "KG": 0.51328125, "before_eval_results": {"predictions": ["Polk", "a band", "delta", "barroco", "St. Petersburg", "Australia", "Prohibition", "Lettuce", "The Godfather", "Maria Sharapova", "McDonald's", "\"Sonny\" Corleone", "11", "\"The Stars and Stripes Forever\"", "Jackie Moon", "Pulp Fiction", "expungere", "the Rhine", "a rocket-propelled grenade", "dilithium", "Schwarzenegger", "Epstein-Barr virus", "hydrogen", "troops", "the U.S. Naval Academy", "Iowa", "indirect discourse", "a circle", "Pussycat Dolls Present", "Shakespeare", "a lump", "Vin Diesel", "Hitler", "Heath & Sons", "Odysseus", "USA Swimming", "Annapolis", "Maccabees", "Rolls Royce Phantom", "a doses", "the Caucasus Mountains", "Lafayette", "the gopher", "Mephistopheles", "Coca-Cola Company", "John Roberts, or Alito", "apogee", "a moon", "a mirror", "David Archuleta", "Union Carbide", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Ireland", "24 November 1949", "May", "Stockholm syndrome", "Ilkley", "Geographical Indication", "the northwest tip of Canisteo Peninsula in Amundsen Sea", "\"First Folio\"", "President Nicolas Sarkozy and Canada's Prime Minister Stephen Harper", "proud", "1000 square meters in forward deck space", "Larry Ellison"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6559275793650794}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true], "QA-F1": [0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.5555555555555556, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16561", "mrqa_searchqa-validation-8833", "mrqa_searchqa-validation-7803", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-7997", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-512", "mrqa_searchqa-validation-9498", "mrqa_searchqa-validation-13731", "mrqa_searchqa-validation-15266", "mrqa_searchqa-validation-958", "mrqa_searchqa-validation-8333", "mrqa_searchqa-validation-10678", "mrqa_searchqa-validation-11878", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-5212", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-2363", "mrqa_searchqa-validation-12155", "mrqa_hotpotqa-validation-2854", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-1701"], "SR": 0.53125, "CSR": 0.5706426056338028, "retrieved_ids": ["mrqa_squad-train-75886", "mrqa_squad-train-5783", "mrqa_squad-train-39397", "mrqa_squad-train-59332", "mrqa_squad-train-21677", "mrqa_squad-train-63184", "mrqa_squad-train-68862", "mrqa_squad-train-85450", "mrqa_squad-train-69452", "mrqa_squad-train-79531", "mrqa_squad-train-34107", "mrqa_squad-train-37646", "mrqa_squad-train-15817", "mrqa_squad-train-42498", "mrqa_squad-train-6813", "mrqa_squad-train-64207", "mrqa_searchqa-validation-4645", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-4353", "mrqa_newsqa-validation-534", "mrqa_naturalquestions-validation-10533", "mrqa_searchqa-validation-1794", "mrqa_naturalquestions-validation-3771", "mrqa_hotpotqa-validation-2156", "mrqa_searchqa-validation-16229", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-1825", "mrqa_squad-validation-7885", "mrqa_hotpotqa-validation-3307", "mrqa_naturalquestions-validation-10249", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-878"], "EFR": 1.0, "Overall": 0.7484253961267606}, {"timecode": 71, "before_eval_results": {"predictions": ["Funki Porcini", "119", "560", "Nine Inch Nails", "Klasky Csupo", "The song was written by band members Guy Berryman, Jonny Buckland, Will Champion, and Chris Martin, along with Brian Eno,", "Home of the Submarine Force", "McG", "River Shiel", "\"Hush\u2026 Hush, Sweet Charlotte\"", "Lommel differential equation", "Harry Booth", "Westland", "1.23 million", "Boston", "over 281", "Northern Ireland", "Easter Rising of 1916", "Park Hyung-Sik", "Conservative Party", "quantum mechanics", "gamecock", "Theo James Walcott", "April 8, 1943", "The Golden Egg", "their unusual behavior", "11", "Victoria Peak", "\"Back to December\"", "Volvo 850", "Hindi", "Statutory List of Buildings of Special Architectural or Historic Interest", "High Falls Brewery", "American painter and writer", "Fort Frederick", "Autopia", "Hindi", "Art Deco-style skyscraper", "Jon Walker", "\"Humble\"", "Mani", "Green Chair", "Sugar Ray Robinson", "Waimea Bay", "Juan Manuel Mata Garc\u00eda", "Umina Beach, New South Wales", "Mickey Mouse cup", "Kinnairdy Castle", "Madagascar, Mauritius, Mayotte, R\u00e9union, Seychelles, Comoro Islands, Morocco, Algeria, Tunisia, Australia, New Zealand, Canada, the United States, the Falkland Islands, and Peru", "Stephen James Ireland", "Lola Dee", "1924", "cell nucleus", "the inner core and growing bud", "Helen of Troy", "Vince Cable", "cubed", "A good vegan cupcake has the power to transform everything for the better,\"", "Sporting Lisbon", "Illness", "programming", "bowling", "Gin Rummy", "enamel"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6983722176610108}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.37037037037037035, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6206896551724138, 1.0, 1.0, 1.0, 0.8, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-1397", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-5371", "mrqa_hotpotqa-validation-1149", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-1799", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-3237", "mrqa_hotpotqa-validation-2678", "mrqa_hotpotqa-validation-1716", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5242", "mrqa_triviaqa-validation-2441", "mrqa_searchqa-validation-12240", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267"], "SR": 0.609375, "CSR": 0.5711805555555556, "retrieved_ids": ["mrqa_squad-train-43511", "mrqa_squad-train-49153", "mrqa_squad-train-11603", "mrqa_squad-train-85555", "mrqa_squad-train-86330", "mrqa_squad-train-44192", "mrqa_squad-train-68277", "mrqa_squad-train-51375", "mrqa_squad-train-4145", "mrqa_squad-train-30804", "mrqa_squad-train-59580", "mrqa_squad-train-36733", "mrqa_squad-train-22789", "mrqa_squad-train-22321", "mrqa_squad-train-6021", "mrqa_squad-train-34299", "mrqa_hotpotqa-validation-4883", "mrqa_newsqa-validation-3326", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-3127", "mrqa_searchqa-validation-11794", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-3499", "mrqa_squad-validation-7887", "mrqa_newsqa-validation-2100", "mrqa_naturalquestions-validation-2067", "mrqa_searchqa-validation-7018", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-2985", "mrqa_searchqa-validation-9334", "mrqa_newsqa-validation-566", "mrqa_triviaqa-validation-2829"], "EFR": 0.96, "Overall": 0.7405329861111111}, {"timecode": 72, "before_eval_results": {"predictions": ["U2 360\u00b0 Tour", "Brent Wilson", "Peter Kay's Car Share", "Michael Crawford", "Brazilian Jiu-Jitsu", "2012", "Pantone Inc.", "Colonel", "from 26\u201330 August 1914, during the first month of World War I.", "Germany", "River Clyde", "George Washington Bridge", "Argand lamp", "Lowestoft, Suffolk", "The Ethics of Ambiguity", "Bishop's Stortford Football Club", "Rural Electrification Act", "Vitor Vieira Belfort", "Carlos Coy", "Hawaii", "Cuban descent", "35,000", "24", "the Bahamian island of Great Exuma", "musical research", "Bonnie Franklin", "Premier Division, Divisions One and Two, and a Reserve Division", "Kelly Bundy", "train robbery", "Carson City", "Ben R. Guttery", "arts manager", "Montreal", "Samoa", "August 14, 1848", "Peel Holdings", "1692", "film", "Teddy Riley", "672 km2", "140 million", "Lamar Hunt", "Vienna", "Alemannic and the Bavarian-Austrian dialects of German", "ten", "SpongeBob SquarePants 4-D", "Seti I", "Raabta", "Joseph E. Grosberg", "January 15, 1975", "2015", "medieval realism", "energy loss", "The Satavahanas", "Louisa of Saxe-Coburg", "Audi A4", "Valletta", "national initiatives have their limitations,\"", "Heshmatollah Attarzadeh", "drug cartels", "Manhattan", "Turandot", "Champagne", "seabirds"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6976461038961039}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.3636363636363636, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-4558", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-1320", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-4088", "mrqa_hotpotqa-validation-1695", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-1006", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-6234", "mrqa_triviaqa-validation-6188", "mrqa_triviaqa-validation-4960", "mrqa_newsqa-validation-2194", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3199"], "SR": 0.5625, "CSR": 0.5710616438356164, "retrieved_ids": ["mrqa_squad-train-6823", "mrqa_squad-train-19962", "mrqa_squad-train-41137", "mrqa_squad-train-38951", "mrqa_squad-train-49004", "mrqa_squad-train-3854", "mrqa_squad-train-55752", "mrqa_squad-train-70522", "mrqa_squad-train-47979", "mrqa_squad-train-82881", "mrqa_squad-train-77155", "mrqa_squad-train-55508", "mrqa_squad-train-54233", "mrqa_squad-train-17053", "mrqa_squad-train-37284", "mrqa_squad-train-54304", "mrqa_searchqa-validation-4684", "mrqa_newsqa-validation-2843", "mrqa_naturalquestions-validation-7035", "mrqa_searchqa-validation-9672", "mrqa_hotpotqa-validation-347", "mrqa_newsqa-validation-1711", "mrqa_searchqa-validation-12155", "mrqa_naturalquestions-validation-1455", "mrqa_newsqa-validation-3801", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-3635", "mrqa_searchqa-validation-3567", "mrqa_newsqa-validation-1961", "mrqa_searchqa-validation-3613", "mrqa_naturalquestions-validation-3253", "mrqa_triviaqa-validation-3293"], "EFR": 1.0, "Overall": 0.7485092037671233}, {"timecode": 73, "before_eval_results": {"predictions": ["Bathsheba", "Poland", "Hillary Clinton", "Hannibal", "the Fields of Punishment", "Birmingham", "a coffee house", "a poor wood-cutter", "J.M.W. Turner", "heisenberg", "a person trained for travelling in space", "glockenspiel", "David Hockney", "Kyoto Protocol", "Joan Crawford", "Britain", "Kansas City", "South Carolina", "The first Survivor Series event", "taxis", "bell peppers", "piscina", "Edward III", "Alfred Pennyworth", "lighting", "Tesco", "Cologne", "northern prawn", "New York", "Nikola Tesla", "smartphones and similar devices", "Tennessee", "Grimbsy", "Sandstone Trail", "Robert Guerrero", "Virginia Plain", "Columbia", "Scotland", "adeola", "Spanish", "perigee", "constellations", "31 million men", "Kaminsky", "World Heavyweight", "arcelorMittal Orbit", "Madness", "Mao Zedong", "Sweet Home Alabama", "Greece", "St Helens", "a valuable way to feed the poor, and would relieve some pressure of the land redistribution process", "provided information about advance health care directives to adult patients upon their admission to the healthcare facility", "a cake", "High Knob", "Sacramento Kings", "About 200", "African National Congress Deputy President Kgalema Motlanthe", "Michelle Obama", "near Pakistan's border with Afghanistan", "a belfry", "Bret Maverick", "Ronald McDonald House", "#364"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6079727564102564}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5769230769230769, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3099", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-5315", "mrqa_triviaqa-validation-5341", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-5672", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-6598", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-4399", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-1673", "mrqa_triviaqa-validation-4776", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-614", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-4414", "mrqa_newsqa-validation-3358", "mrqa_searchqa-validation-16624", "mrqa_searchqa-validation-6760"], "SR": 0.546875, "CSR": 0.5707347972972974, "retrieved_ids": ["mrqa_squad-train-13582", "mrqa_squad-train-81187", "mrqa_squad-train-41634", "mrqa_squad-train-12259", "mrqa_squad-train-55473", "mrqa_squad-train-66804", "mrqa_squad-train-6271", "mrqa_squad-train-27274", "mrqa_squad-train-72912", "mrqa_squad-train-7625", "mrqa_squad-train-67599", "mrqa_squad-train-31876", "mrqa_squad-train-56079", "mrqa_squad-train-9756", "mrqa_squad-train-54005", "mrqa_squad-train-34353", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3679", "mrqa_hotpotqa-validation-2187", "mrqa_naturalquestions-validation-6234", "mrqa_squad-validation-1258", "mrqa_naturalquestions-validation-3394", "mrqa_searchqa-validation-15930", "mrqa_squad-validation-6559", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-3771", "mrqa_hotpotqa-validation-4883", "mrqa_triviaqa-validation-3744", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-5147", "mrqa_naturalquestions-validation-7509", "mrqa_searchqa-validation-7800"], "EFR": 0.8620689655172413, "Overall": 0.7208576275629077}, {"timecode": 74, "before_eval_results": {"predictions": ["iPod Classic or... Shuffle.", "1-0", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "Citizens are picking members of the lower house of parliament", "Susan Boyle", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith", "1983", "$40 and a loaf of bread.", "phyric Alm at the Sweden-based photojournalism agency Kontinent,", "are \"active athletes,\" far from couch potatoes,", "FBI", "10,000", "his business dealings", "Procol Harum", "California, Texas and Florida", "morphine sulfate oral solution 20 mg/ml.", "\"it should stay that way.\"", "Iran", "the Arab world and use the Internet for fun and not interfere with government and serious issues,", "Rawalpindi", "was seven months pregnant,", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "peanuts", "United States", "Samoa", "Vice's broadband television network", "Six", "Irish capital.", "customers", "13", "Whitney Houston", "Ferraris", "free fixes for the consumer.", "nine-wicket", "10 below", "Madhav Kumar Nepal", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "-- you know -- black is beautiful,\"", "fifth", "Iran", "German Foreign Ministry,", "JBS Swift Beef Company", "Hurricane Gustav", "President Obama", "murder", "Manny Pacquiao", "The American Civil Liberties Union", "1,073 immigration detainees", "flying", "Alfredo Astiz", "Los Angeles", "Super Bowl VIII", "3", "Jason Momoa", "whetstones", "Edinburgh", "evil or misunderstood", "Norbertine", "Anheuser-Busch", "Beauty and the Beast", "the Chiefs", "Cleopatra", "Philip", "Rigoletto"], "metric_results": {"EM": 0.5, "QA-F1": 0.6256953846592004}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false], "QA-F1": [0.0, 0.0, 0.9473684210526316, 0.19999999999999998, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.2, 0.7272727272727273, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2101", "mrqa_naturalquestions-validation-288", "mrqa_triviaqa-validation-3067", "mrqa_triviaqa-validation-369", "mrqa_searchqa-validation-14806", "mrqa_hotpotqa-validation-4101"], "SR": 0.5, "CSR": 0.5697916666666667, "retrieved_ids": ["mrqa_squad-train-74340", "mrqa_squad-train-56636", "mrqa_squad-train-58564", "mrqa_squad-train-71800", "mrqa_squad-train-59362", "mrqa_squad-train-64797", "mrqa_squad-train-41872", "mrqa_squad-train-21171", "mrqa_squad-train-52658", "mrqa_squad-train-35535", "mrqa_squad-train-38798", "mrqa_squad-train-37648", "mrqa_squad-train-64608", "mrqa_squad-train-28551", "mrqa_squad-train-81502", "mrqa_squad-train-54110", "mrqa_naturalquestions-validation-2778", "mrqa_hotpotqa-validation-3757", "mrqa_searchqa-validation-6554", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2152", "mrqa_searchqa-validation-8857", "mrqa_searchqa-validation-1369", "mrqa_naturalquestions-validation-1890", "mrqa_newsqa-validation-3973", "mrqa_hotpotqa-validation-5825", "mrqa_triviaqa-validation-317", "mrqa_hotpotqa-validation-3008", "mrqa_naturalquestions-validation-2884", "mrqa_triviaqa-validation-4273"], "EFR": 1.0, "Overall": 0.7482552083333334}, {"timecode": 75, "before_eval_results": {"predictions": ["1-1 draw.", "Ten South African ministers and the deputy president", "Sharon Bialek", "giving birth to baby daughter Jada,", "voluntary manslaughter", "The Kirchners", "Sharon Bialek", "Roy Foster", "March 24,", "African-Americans", "Bill Haas", "David Beckham", "rwanda", "Caster Semenya", "nuclear weapon", "$627,", "Turkish President Abdullah Gul.", "People Against Switching Sides (PASS)", "sedative.", "26", "Bob Dole,", "Kingman Regional Medical Center,", "assassination of President Mohamed Anwar al-Sadat", "Australian Environment Minister Peter Garrett", "over-fishing and nuclear waste issues", "Josef Fritzl", "Galveston, Texas,", "Sharon Bialek", "About 100,000 workers", "The two were separated", "San Simeon, California,", "a Hank Moody type,", "Fiona Mac Keown", "2001", "Steve Jobs", "a violent government crackdown seeped out.", "43 percent", "Jezebel.com", "Kenner, Louisiana", "Saturday", "Lashkar-e-Tayyiba (LeT), an Islamic militant group based in Pakistan.", "2005", "35,000 kilometers", "Ralph Lauren", "stressed out,\"", "71 percent of Americans consider China an economic threat to the United States,", "Oprah: A Biography", "the Bronx", "hopes the journalists and the flight crew will be freed,", "April 6, 1994", "murder in the beating death of", "February 7, 2018", "Johannes Gutenberg", "Blue laws", "buddha", "Colonel Bellowes", "nitrogen", "Araminta Ross", "ABC1 and ABC2", "G\u00f6tene in Sweden", "the kitchen sink", "a colloid", "(Benito Mussolini)", "roosevelts"], "metric_results": {"EM": 0.5, "QA-F1": 0.5764022435897436}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-4014", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-406", "mrqa_naturalquestions-validation-8696", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-177", "mrqa_searchqa-validation-11277", "mrqa_searchqa-validation-7856", "mrqa_triviaqa-validation-3733"], "SR": 0.5, "CSR": 0.568873355263158, "retrieved_ids": ["mrqa_squad-train-65128", "mrqa_squad-train-67449", "mrqa_squad-train-20867", "mrqa_squad-train-5603", "mrqa_squad-train-25611", "mrqa_squad-train-63543", "mrqa_squad-train-47252", "mrqa_squad-train-23327", "mrqa_squad-train-53481", "mrqa_squad-train-75980", "mrqa_squad-train-59553", "mrqa_squad-train-75176", "mrqa_squad-train-3871", "mrqa_squad-train-64264", "mrqa_squad-train-24965", "mrqa_squad-train-78348", "mrqa_naturalquestions-validation-1856", "mrqa_hotpotqa-validation-5300", "mrqa_searchqa-validation-5519", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-1670", "mrqa_hotpotqa-validation-1610", "mrqa_newsqa-validation-540", "mrqa_triviaqa-validation-2305", "mrqa_hotpotqa-validation-1378", "mrqa_newsqa-validation-3888", "mrqa_triviaqa-validation-6125", "mrqa_naturalquestions-validation-3658", "mrqa_searchqa-validation-4582", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-1320", "mrqa_naturalquestions-validation-9799"], "EFR": 0.96875, "Overall": 0.7418215460526316}, {"timecode": 76, "before_eval_results": {"predictions": ["cricket", "Alan Greenspan", "Bolivia", "Matalan", "Wayne Allwine", "Macbeth Soliloquy", "German Chancellor Angela Merkel", "Monopoly", "transsexual", "black", "lungs", "doubles", "Paul Gauguin", "Ben Jonson", "trapezoid", "Willy Lott", "13th", "Abu Dhabi", "london", "14", "lice", "palladium", "Hubble Space Telescope", "James Van Allen", "Rawalpindi", "Mexico", "Philip Glenister", "Miss Prism", "Charles Greville,", "Beethoven", "Haystacks", "late before it became NASA\u2019s premier center for robotic exploration of the solar system", "Margaret Thatcher", "Markus Liebherr", "Missouri", "Sensurround", "Venus", "Olympic Games", "Blue Ivy Carter", "Rihanna", "Tripoli", "euthanasia", "Eva Duarte de Per\u00f3n", "Doctor Who", "pink", "Glenn Close and Rade Serbedzija", "Illinois", "The Magnificent Seven", "Uranus", "typhoid fever", "Ross MacManus", "September 9, 2012", "Las Vegas", "seven", "Ricky Marco i Vives", "American pharmaceutical company", "Queens, New York", "United were held to a 1-1 draw", "Alwin Landry's supply vessel Damon Bankston", "elaborate ceilings", "The Big Sleep", "Dairy Queen", "Paul the Apostle", "Confederate victory"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6735863095238095}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.5, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-147", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-2144", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-4855", "mrqa_triviaqa-validation-2828", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-2047", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-6179", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-2333", "mrqa_naturalquestions-validation-4746", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2631", "mrqa_searchqa-validation-15341", "mrqa_naturalquestions-validation-767"], "SR": 0.578125, "CSR": 0.5689935064935066, "retrieved_ids": ["mrqa_squad-train-8249", "mrqa_squad-train-10482", "mrqa_squad-train-4796", "mrqa_squad-train-56668", "mrqa_squad-train-28524", "mrqa_squad-train-72520", "mrqa_squad-train-19449", "mrqa_squad-train-1349", "mrqa_squad-train-58832", "mrqa_squad-train-75701", "mrqa_squad-train-20376", "mrqa_squad-train-7454", "mrqa_squad-train-62270", "mrqa_squad-train-34794", "mrqa_squad-train-42911", "mrqa_squad-train-8710", "mrqa_hotpotqa-validation-5273", "mrqa_searchqa-validation-15880", "mrqa_newsqa-validation-3358", "mrqa_searchqa-validation-8471", "mrqa_triviaqa-validation-3314", "mrqa_searchqa-validation-8890", "mrqa_searchqa-validation-6160", "mrqa_newsqa-validation-1809", "mrqa_searchqa-validation-8857", "mrqa_searchqa-validation-10817", "mrqa_naturalquestions-validation-7346", "mrqa_searchqa-validation-11411", "mrqa_hotpotqa-validation-5497", "mrqa_triviaqa-validation-2593", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-1681"], "EFR": 0.9629629629629629, "Overall": 0.7406881688912939}, {"timecode": 77, "before_eval_results": {"predictions": ["nearly $162 billion in war funding", "poor.", "183", "\"I reject this course because it sets goals that are beyond what we can achieve at a reasonable cost, and what we need to achieve to secure our interests,\"", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Transport Workers Union leaders", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "President Bush", "November 26,", "Wednesday.", "change course", "machine guns and two silencers", "sitting in Renaissance-era clothes and holding a book.", "off the coast of", "his past and his future", "Spc. Megan Lynn Touma,", "Hussein's Revolutionary Command Council.", "amusement park", "$31,000", "Black History Month", "in a tenement in the Mumbai suburb of Chembur,", "Monday.", "buses, subways and trolleys", "The island's dining scene", "22", "in front of the BBC's Broadcasting House in central London", "Cyndi Mosteller,", "The president,", "Graham's wife", "hank Moody", "two", "ambassadors", "spend themselves a massive sports car collection or something similar,\"", "Wigan Athletic", "criminals", "The Ministry of Defense", "civilians,", "21-year-old", "Ignazio La Russa", "Passers-by", "high-tech companies", "NATO fighters", "New York City Mayor Michael Bloomberg", "Colorado prosecutor", "16 Indiana National Guard soldiers", "1940's", "Zeina,", "Symbionese Liberation Army", "South Africa", "\"I am sick of life -- what can I say to you?\"", "a tanker", "Rigor mortis is very important in meat technology", "Walter Pauk", "Louis Prima", "Alanis Morissette", "Uranus", "Baroness Thatcher", "February 16, 1944", "Fort Saint Anthony", "Wilmette, Illinois", "quotient", "Vermont", "Kiribati", "Jay Van Andel"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6345852064602064}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, true], "QA-F1": [0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.9333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1145", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-6088", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-1874", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-13106"], "SR": 0.5625, "CSR": 0.5689102564102564, "retrieved_ids": ["mrqa_squad-train-34900", "mrqa_squad-train-50396", "mrqa_squad-train-14582", "mrqa_squad-train-79684", "mrqa_squad-train-61375", "mrqa_squad-train-5910", "mrqa_squad-train-34445", "mrqa_squad-train-11319", "mrqa_squad-train-63367", "mrqa_squad-train-24225", "mrqa_squad-train-7510", "mrqa_squad-train-50329", "mrqa_squad-train-45164", "mrqa_squad-train-50285", "mrqa_squad-train-74957", "mrqa_squad-train-8661", "mrqa_searchqa-validation-172", "mrqa_hotpotqa-validation-5841", "mrqa_newsqa-validation-3818", "mrqa_hotpotqa-validation-3607", "mrqa_squad-validation-9426", "mrqa_hotpotqa-validation-1070", "mrqa_naturalquestions-validation-519", "mrqa_hotpotqa-validation-3821", "mrqa_squad-validation-1565", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-5171", "mrqa_triviaqa-validation-6501", "mrqa_newsqa-validation-3930", "mrqa_searchqa-validation-4408", "mrqa_naturalquestions-validation-3969", "mrqa_newsqa-validation-878"], "EFR": 0.9642857142857143, "Overall": 0.7409360691391942}, {"timecode": 78, "before_eval_results": {"predictions": ["test results", "the bombers", "\"He is a very special member of our family. We miss having his love and compassion in our home,\"", "Isabella", "5 1/2-year-old son, Ryder Russell,", "MBA in finance", "gun charges,", "forgery and flying without a valid license,", "Highway 18 near Grand Ronde, Oregon.", "There's no chance of it being open on time.", "peace with Israel", "two", "70,000 or so", "rural California,", "the prime minister's handling of the L'Aquila earthquake,", "650", "Expedia", "ireport form", "Virgin America", "The Italian government", "Sen. Piedad Cordoba", "helping on the sandbags lines", "four decades", "Damon Bankston", "The e-mails", "HPV (human papillomavirus)", "helicopters and unmanned aerial vehicles from the White House", "two tickets to Italy", "Dr. Maria Siemionow,", "Hillary Clinton", "Barack Obama:", "NATO", "assassination of", "to kill members of the Zetas", "June 6, 1944,", "Orbiting Carbon Observatory,", "a face-to-face interview with the president", "the Airbus A330-200", "divorced Goldman and married a Brazilian lawyer.", "poems telling of the pain and suffering of children just like her", "be silent.", "Sri Lanka", "Adidas", "2,700-acre", "identity documents belonging to Miguel Mejia Munera", "can indeed help people with irritable bowel syndrome,", "martial arts,", "a student who admitted to hanging a noose in a campus library,", "543", "Barack Obama", "The Louvre", "Bay of Plenty, Taupo and Wellington,", "Rihanna", "enemy", "johnson johnson", "Count Basie", "Billy Cox", "wooden", "orishas", "boxer", "a Chrysanthemums", "John Wesley", "Colonel Sanders", "multiple alternative realities rather than a novel"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6288599337132745}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.3636363636363636, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.8, 0.75, 1.0, 0.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6956521739130436, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.19999999999999998, 0.0, 0.8, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3240", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2609", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-5101", "mrqa_hotpotqa-validation-5784", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12910", "mrqa_naturalquestions-validation-2729"], "SR": 0.484375, "CSR": 0.5678401898734178, "retrieved_ids": ["mrqa_squad-train-6385", "mrqa_squad-train-26195", "mrqa_squad-train-58083", "mrqa_squad-train-27353", "mrqa_squad-train-84296", "mrqa_squad-train-50263", "mrqa_squad-train-82829", "mrqa_squad-train-7608", "mrqa_squad-train-595", "mrqa_squad-train-23347", "mrqa_squad-train-47927", "mrqa_squad-train-70237", "mrqa_squad-train-54381", "mrqa_squad-train-26584", "mrqa_squad-train-2952", "mrqa_squad-train-69254", "mrqa_searchqa-validation-16229", "mrqa_newsqa-validation-2632", "mrqa_naturalquestions-validation-2067", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-5504", "mrqa_newsqa-validation-3102", "mrqa_squad-validation-4849", "mrqa_hotpotqa-validation-2970", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-12472", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-2617", "mrqa_newsqa-validation-4085", "mrqa_hotpotqa-validation-2718", "mrqa_searchqa-validation-8857"], "EFR": 0.9696969696969697, "Overall": 0.7418043069140775}, {"timecode": 79, "before_eval_results": {"predictions": ["sorrow regarding the environment", "1989", "Bill Irwin", "Hon July Moyo", "creatine kinase", "2017", "Agra Cantonment - H. Nizamuddin Gatimaan Express", "the Rolling Stones", "My Summer Story", "2010", "2018", "1975", "the date on which the Constitution of India came into effect on 26 January 1950", "David Tennant", "the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "the Canadian Rockies continental divide", "Deuteronomy 5 : 4 -- 25", "the Senate", "John Musker", "Andy Cole", "each team", "an armed conflict without the consent of the U.S. Congress", "Sauron", "February 1775", "Supplemental oxygen", "the end of the spinal cord is about the level of the third lumbar vertebra, or L3, at birth", "`` Six flags over Texas '' is the slogan used to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "103", "twelve", "Reba McEntire and Linda Davis", "New Mexico", "Karen Gillan", "1832", "The sacroiliac joint or SI joint ( SIJ )", "quartermaster under the notorious Captain Flint", "Arkansas", "Mickey Rourke", "spin", "Ron Harper", "Number 4, Privet Drive, Little Whinging in Surrey, England", "differs in ingredients", "Sunday evenings", "Charles R Ranch, County Road 24, Las Vegas, New Mexico, USA", "a major fall in stock prices", "April 1979", "Reverend J. Long", "March 18, 2005", "`` save, rescue, savior ''", "It acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Mike Alstott", "Nicole Gale Anderson", "1.25, 250 years", "town of edgehill", "James Taylor", "as the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "goalkeeper", "Emancipation Proclamation", "4.6 million", "executive director of the Americas Division of Human Rights Watch,", "Pope Benedict XVI", "Afghanistan", "Erie Canal", "a lion", "the Black Sea"], "metric_results": {"EM": 0.484375, "QA-F1": 0.640036840271649}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8695652173913044, 1.0, 0.7692307692307692, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6086956521739131, 1.0, 0.6666666666666666, 0.0, 0.1111111111111111, 0.851063829787234, 1.0, 0.1111111111111111, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.12500000000000003, 1.0, 1.0, 0.5, 0.5, 0.72, 0.0, 1.0, 0.0, 0.5, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-1979", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-2813", "mrqa_triviaqa-validation-752", "mrqa_triviaqa-validation-5693", "mrqa_hotpotqa-validation-4599", "mrqa_searchqa-validation-10525"], "SR": 0.484375, "CSR": 0.566796875, "retrieved_ids": ["mrqa_squad-train-54321", "mrqa_squad-train-51994", "mrqa_squad-train-31365", "mrqa_squad-train-37298", "mrqa_squad-train-79448", "mrqa_squad-train-81099", "mrqa_squad-train-72535", "mrqa_squad-train-16323", "mrqa_squad-train-44442", "mrqa_squad-train-77194", "mrqa_squad-train-33355", "mrqa_squad-train-14646", "mrqa_squad-train-73431", "mrqa_squad-train-41191", "mrqa_squad-train-38432", "mrqa_squad-train-26001", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-5300", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-7936", "mrqa_naturalquestions-validation-5960", "mrqa_newsqa-validation-1225", "mrqa_searchqa-validation-9809", "mrqa_hotpotqa-validation-4101", "mrqa_newsqa-validation-1879", "mrqa_hotpotqa-validation-5359", "mrqa_searchqa-validation-9430", "mrqa_squad-validation-1818", "mrqa_searchqa-validation-2778", "mrqa_newsqa-validation-2667", "mrqa_squad-validation-6873", "mrqa_searchqa-validation-5418"], "EFR": 0.9090909090909091, "Overall": 0.7294744318181817}, {"timecode": 80, "UKR": 0.787109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12910", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3139", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1535", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.884765625, "KG": 0.50234375, "before_eval_results": {"predictions": ["prevent any contaminants in the sink from flowing into the potable water system by siphonage", "all - female population", "late - September through early January", "Moscazzano", "Prem Lata Agarwal", "Calpurnia", "2009", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "Joe Pizzulo and Leeza Miller", "19th - century", "the eighth series of the UK version of The X Factor", "Jenny Slate", "Gunpei Yokoi", "1990", "sixth season", "leaves of the plant species Stevia rebaudiana", "Julie Deborah Kavner", "Peggy Lipton", "infection", "Jewel Akens", "Wainier", "March 11, 2018", "Sauron", "12", "Frank Langella", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Carlos Tevez", "Southport, North Carolina", "John C. Reilly", "September 21, 2016", "Washington metropolitan area", "Hallertau in Germany", "1885", "Kevin Kline", "the U.S. State Department", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "Monk's Caf\u00e9", "Pittsburgh", "Spanish", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "Killer", "Norway", "directly elected", "1997", "Don Cook", "12 November 2010", "homicidal thoughts of a troubled youth", "W. Edwards Deming", "convergent plate boundary", "Sylvester Stallone", "1967", "Harrods,", "Respighi", "Steve Coogan", "Harvard University", "Bay of Fundy", "Lincoln Riley", "The Rosie Show", "presiding judge Shemsu Sirgaga", "2,000 euros ($2,963)", "dishwasher", "word first", "Albert Einstein", "wheeze"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6303367415514594}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.5, 0.4, 0.25, 1.0, 0.967741935483871, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2842", "mrqa_triviaqa-validation-5589", "mrqa_newsqa-validation-2045", "mrqa_searchqa-validation-11496"], "SR": 0.546875, "CSR": 0.5665509259259259, "retrieved_ids": ["mrqa_squad-train-79405", "mrqa_squad-train-45891", "mrqa_squad-train-37166", "mrqa_squad-train-5554", "mrqa_squad-train-53100", "mrqa_squad-train-52961", "mrqa_squad-train-16212", "mrqa_squad-train-70516", "mrqa_squad-train-34422", "mrqa_squad-train-68464", "mrqa_squad-train-82561", "mrqa_squad-train-53955", "mrqa_squad-train-21462", "mrqa_squad-train-24844", "mrqa_squad-train-41416", "mrqa_squad-train-56927", "mrqa_hotpotqa-validation-830", "mrqa_triviaqa-validation-2593", "mrqa_naturalquestions-validation-2621", "mrqa_searchqa-validation-6740", "mrqa_hotpotqa-validation-24", "mrqa_searchqa-validation-11993", "mrqa_squad-validation-1061", "mrqa_newsqa-validation-4085", "mrqa_squad-validation-3754", "mrqa_newsqa-validation-1376", "mrqa_triviaqa-validation-994", "mrqa_squad-validation-1759", "mrqa_searchqa-validation-14104", "mrqa_hotpotqa-validation-2486", "mrqa_naturalquestions-validation-485", "mrqa_searchqa-validation-4046"], "EFR": 0.9655172413793104, "Overall": 0.7412573834610472}, {"timecode": 81, "before_eval_results": {"predictions": ["Jason Momoa", "Woody Harrelson", "1623", "March 14, 1942", "Lafayette", "1973", "5 liters", "Kate Walsh", "from 13 to 22 June 2012", "2.5 %", "232", "in mid November", "Guwahati", "Nala", "Since 1979 / 80", "2017", "The Roman Empire", "Leslie and Ben", "electron shells", "hail", "compasses", "A production -- possibility frontier ( PPF ) or production possibility curve ( PPC )", "1987", "retina", "Eagle Ridge Outdoor pool in Coquitlam, BC", "gave the German Cabinet -- in effect, Chancellor Adolf Hitler -- the power to enact laws without the involvement of the Reichstag", "Charlton Heston", "the fourth ventricle", "October 27, 2017", "John Cooper Clarke", "Sunday night", "Cheryl Campbell", "Spanish", "55 - 75", "January 4, 2016", "Jeff Bezos", "Erica Rivera", "Most days are sunny throughout the year", "January 2, 1971", "Tracy McConnell", "gastrocnemius muscle", "due to Parker's pregnancy at the time of filming", "1 -- 3", "the First Battle of Panipat ( 1526 )", "1830", "Frank Oz", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Celtic", "Guant\u00e1namo or GTMO", "Morgan Freeman", "two installments", "muscle tissue", "toe-line", "congregational rabbi", "Debi Thomas", "Bhaktivedanta Manor", "Lowe's Companies, Inc.", "Honduran", "Al-Shabaab", "Columbian mammoth", "fled", "E.V. Club", "churrasco", "a refrigerator"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6194705060054324}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 0.9090909090909091, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.823529411764706, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-5214", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5185", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-2449", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-1512", "mrqa_searchqa-validation-7442", "mrqa_searchqa-validation-12256"], "SR": 0.53125, "CSR": 0.5661204268292683, "retrieved_ids": ["mrqa_squad-train-590", "mrqa_squad-train-66449", "mrqa_squad-train-82049", "mrqa_squad-train-66907", "mrqa_squad-train-46114", "mrqa_squad-train-70318", "mrqa_squad-train-74236", "mrqa_squad-train-19881", "mrqa_squad-train-56658", "mrqa_squad-train-52141", "mrqa_squad-train-77716", "mrqa_squad-train-60757", "mrqa_squad-train-9709", "mrqa_squad-train-67967", "mrqa_squad-train-81804", "mrqa_squad-train-5085", "mrqa_hotpotqa-validation-1175", "mrqa_naturalquestions-validation-9767", "mrqa_newsqa-validation-2155", "mrqa_naturalquestions-validation-5465", "mrqa_hotpotqa-validation-1070", "mrqa_newsqa-validation-2194", "mrqa_searchqa-validation-13731", "mrqa_naturalquestions-validation-5602", "mrqa_searchqa-validation-512", "mrqa_newsqa-validation-3102", "mrqa_naturalquestions-validation-2690", "mrqa_newsqa-validation-2667", "mrqa_triviaqa-validation-2047", "mrqa_hotpotqa-validation-2470", "mrqa_naturalquestions-validation-10495", "mrqa_searchqa-validation-8513"], "EFR": 0.9333333333333333, "Overall": 0.7347345020325203}, {"timecode": 82, "before_eval_results": {"predictions": ["Whittling", "Hans Christian Andersen", "purple", "Charles Lindbergh", "sucrose", "T.S. Eliot", "Superman Returns", "Nokomis", "Yale", "tidal streams", "The Nutcracker", "Over the hifls", "circumnavigate", "Kennebunkport", "Tarzan", "Theodore Roosevelt", "manx", "rum", "Baroque", "pterodactyl", "licorice", "Count Dracula", "John Hede r", "Sweden", "War & Peace", "Hanna Montana", "Van Allen Probes", "Mitch McConnell", "POTPOURRI", "the gallbladder", "The Invisible Man", "the Himalayas", "Chile", "Sri Lanka", "the St. Valentine's Day Massacre", "The Paley Center for Media", "\"Sayonara\"", "Oakland", "The Taming of the Shrew", "a proxy", "Andrew Johnson", "the ACL", "NASA", "Gavin MacLeod", "phyas tziripa", "The Count of Monte Cristo", "Eddie Albert and Eva Gabor", "Equus", "Wyandotte County", "Cy Young", "Stephen Sondheim", "December 27, 2015", "MFSK", "Andy Cole", "yellow", "Galileo", "alan", "Afro-American religions", "881 Seventh Avenue", "Awake", "exotic sports cars", "This is not a project for commercial gain.", "Group 2", "Washington"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6369791666666667}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5014", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-13546", "mrqa_searchqa-validation-11022", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-15074", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-3227", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-15465", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-7984", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-14521", "mrqa_searchqa-validation-14081", "mrqa_searchqa-validation-6416", "mrqa_naturalquestions-validation-8934", "mrqa_triviaqa-validation-1023", "mrqa_hotpotqa-validation-5173", "mrqa_hotpotqa-validation-5005", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3226"], "SR": 0.5625, "CSR": 0.5660768072289157, "retrieved_ids": ["mrqa_squad-train-28156", "mrqa_squad-train-55819", "mrqa_squad-train-32616", "mrqa_squad-train-83566", "mrqa_squad-train-65758", "mrqa_squad-train-72532", "mrqa_squad-train-38153", "mrqa_squad-train-50399", "mrqa_squad-train-50335", "mrqa_squad-train-15394", "mrqa_squad-train-45131", "mrqa_squad-train-10611", "mrqa_squad-train-22622", "mrqa_squad-train-67952", "mrqa_squad-train-53747", "mrqa_squad-train-12760", "mrqa_naturalquestions-validation-10442", "mrqa_squad-validation-7373", "mrqa_searchqa-validation-1198", "mrqa_newsqa-validation-270", "mrqa_searchqa-validation-14049", "mrqa_newsqa-validation-1339", "mrqa_searchqa-validation-14963", "mrqa_hotpotqa-validation-3321", "mrqa_naturalquestions-validation-74", "mrqa_searchqa-validation-5392", "mrqa_hotpotqa-validation-4954", "mrqa_searchqa-validation-1722", "mrqa_triviaqa-validation-6276", "mrqa_squad-validation-4813", "mrqa_searchqa-validation-11312", "mrqa_newsqa-validation-4033"], "EFR": 1.0, "Overall": 0.7480591114457831}, {"timecode": 83, "before_eval_results": {"predictions": ["Coca-Cola", "Oklahoma State", "a beetle", "Pippin", "Georgia", "Chesapeake Bay", "a dugout", "cement", "baths", "Gallo", "(James) Fenimore Cooper", "Out of Africa", "(Dean) Koontz", "potato chip", "the Bay of Bengal", "the Clark bar", "ottian", "Dresden", "John Ashcroft", "Phil of the Future", "Newman", "Death Valley", "rings", "\"Take It\"", "George Eliot", "the Eagles", "The Wizard of Oz", "Waylon Jennings", "jaded", "'S Sgt. Pepper's Lonely Hearts Club Band'", "palindrome", "parallelogram", "Scrubs", "Henrik Ibsen", "Elizabeth I", "Canticle", "Friedrich Nietzsche", "(Rodney) King", "Halloween", "the cold-blooded murder of the", "Tanzania", "Coca-Cola", "(Deepak) Chopra", "Rudolf Hess", "Baku in Azerbaijan", "Rings Twice", "Sylvester Stallone", "Stephen Sondheim", "Etch A Sketch", "safari", "Arkansas", "During his epic battle with Frieza", "August 2012", "The stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "Joseph Priestley", "Granada", "club nights", "1940s and 1950s", "Pulitzer Prize for Drama", "York County", "six", "Michelle Rounds", "Derek Mears", "Adidas"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6545304785793916}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.34782608695652173, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12304", "mrqa_searchqa-validation-11933", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-16183", "mrqa_searchqa-validation-2282", "mrqa_searchqa-validation-8962", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-15025", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-39", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-287", "mrqa_searchqa-validation-16469", "mrqa_searchqa-validation-15780", "mrqa_searchqa-validation-15171", "mrqa_searchqa-validation-2557", "mrqa_searchqa-validation-16629", "mrqa_naturalquestions-validation-7702", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-2381", "mrqa_hotpotqa-validation-5309"], "SR": 0.59375, "CSR": 0.56640625, "retrieved_ids": ["mrqa_squad-train-15101", "mrqa_squad-train-15318", "mrqa_squad-train-50966", "mrqa_squad-train-65461", "mrqa_squad-train-16317", "mrqa_squad-train-35026", "mrqa_squad-train-10344", "mrqa_squad-train-72373", "mrqa_squad-train-2312", "mrqa_squad-train-68856", "mrqa_squad-train-77300", "mrqa_squad-train-33781", "mrqa_squad-train-17672", "mrqa_squad-train-2087", "mrqa_squad-train-55219", "mrqa_squad-train-36530", "mrqa_hotpotqa-validation-2342", "mrqa_searchqa-validation-5051", "mrqa_newsqa-validation-4199", "mrqa_searchqa-validation-10678", "mrqa_searchqa-validation-1198", "mrqa_searchqa-validation-5840", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-4911", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-16046", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-10265", "mrqa_searchqa-validation-2151", "mrqa_naturalquestions-validation-5185", "mrqa_squad-validation-7103", "mrqa_squad-validation-7288"], "EFR": 1.0, "Overall": 0.748125}, {"timecode": 84, "before_eval_results": {"predictions": ["Portugal", "nor\u00f0rvegr", "a daisy", "John Galsworthy", "Belfast", "William Wymark Jacobs", "Kurguelen Island Group", "East of Eden", "John Buchan's grandson, Toby", "Doncaster Rovers", "bront\u00eb", "yokohama", "9", "Supertramp", "Sky", "Joanne Harris", "an abacus", "Eriksson", "bison", "Displacement", "Dave Lamb", "Grittar", "a star", "Jupiter", "white spirit", "aglet", "lemurs", "Ontario", "germanan eriksson", "Mickey Mouse", "cricket", "1973", "William Neil Connor", "Azerbaijan", "Mathematics", "Spain", "Adolf Hitler", "Chief Inspector of Prisons", "F. Scott Fitzgerald's 1925 novel", "golf", "mice", "Robert Devereux, 2nd Earl of Leicester", "Hamelin", "Prague", "George Osborne", "oxygen", "Toyota", "snakes", "HMS Amethyst", "hairdresser", "Antony", "Daya Jethalal Gada", "Spektor", "San Francisco, California ( the primary setting of the film )", "F\u00fchrer", "Malm\u00f6, Sweden", "Lake Buena Vista, Florida", "1,300 meters in the Mediterranean Sea.", "Afghanistan,", "in the mountains around Deutschneudorf", "a chimp", "tin", "Close Encounters of the Third Kind", "at sub-sonic speeds"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6205357142857143}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6, 0.5, 0.0, 0.0, 0.7142857142857143, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-7371", "mrqa_triviaqa-validation-4027", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-6937", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-6666", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4166", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2685", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-1068", "mrqa_naturalquestions-validation-2309"], "SR": 0.546875, "CSR": 0.5661764705882353, "retrieved_ids": ["mrqa_squad-train-1325", "mrqa_squad-train-54898", "mrqa_squad-train-81257", "mrqa_squad-train-21564", "mrqa_squad-train-21056", "mrqa_squad-train-44667", "mrqa_squad-train-65173", "mrqa_squad-train-42815", "mrqa_squad-train-57151", "mrqa_squad-train-49486", "mrqa_squad-train-68120", "mrqa_squad-train-27611", "mrqa_squad-train-48098", "mrqa_squad-train-56926", "mrqa_squad-train-76633", "mrqa_squad-train-69704", "mrqa_searchqa-validation-12594", "mrqa_newsqa-validation-1166", "mrqa_squad-validation-7288", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5010", "mrqa_searchqa-validation-12381", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-2021", "mrqa_searchqa-validation-8720", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5116", "mrqa_triviaqa-validation-293", "mrqa_triviaqa-validation-7299", "mrqa_naturalquestions-validation-2937", "mrqa_searchqa-validation-11920", "mrqa_hotpotqa-validation-3418"], "EFR": 0.7931034482758621, "Overall": 0.7066997337728196}, {"timecode": 85, "before_eval_results": {"predictions": ["Grand Harbour", "Eurasia", "Yewell Tompkins", "California", "9,000", "My Beautiful Dark Twisted Fantasy", "High school", "Stormzy", "30.9%", "William Shakespeare", "Nic Cester", "Kingdom of Morocco", "North America", "Prince George's County", "Ariel Ram\u00edrez", "IPod+HP", "four months in jail", "Objectivism", "Vixen", "stunt performances", "Stephen Crawford Young", "co-founder and lead guitarist of the alternative rock band R.E.M.", "League of the Three Emperors", "Frederick Alexander Lindemann,", "Sunflower County", "Alexandre Dimitri Song Billong", "Dutch", "Rabies", "Malayalam fantasy comedy film", "Sim Theme Park", "Outside", "novelty songs", "Nationalism", "Jennifer Joanna Aniston", "North Atlantic Conference", "Aubrey Posen", "Downtown", "Rockstar San Diego", "Strato of Lampsacus", "Gregg Berhalter", "Beno\u00eet Jacquot", "Manor of the More", "Anita Dobson", "500-room", "Rajmund Roman Thierry Pola\u0144ski", "STS-51-L", "chard County", "Saint Michael, Barbados", "Championnat National 3", "Boston, Massachusetts", "The King of Chutzpah", "one person", "anion", "Gibraltar", "the Seine", "Madonna", "colorblindness", "Virgin America", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Zelaya", "an opinion", "Japan", "Stalin", "Rupert\\'s Land"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8023237179487179}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4597", "mrqa_hotpotqa-validation-5200", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-875", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-2372", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-3950", "mrqa_naturalquestions-validation-1202", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-5450"], "SR": 0.71875, "CSR": 0.5679505813953488, "retrieved_ids": ["mrqa_squad-train-66947", "mrqa_squad-train-66991", "mrqa_squad-train-16251", "mrqa_squad-train-61898", "mrqa_squad-train-33738", "mrqa_squad-train-79949", "mrqa_squad-train-71513", "mrqa_squad-train-715", "mrqa_squad-train-7858", "mrqa_squad-train-15226", "mrqa_squad-train-32154", "mrqa_squad-train-36216", "mrqa_squad-train-18766", "mrqa_squad-train-10389", "mrqa_squad-train-71675", "mrqa_squad-train-54203", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-4462", "mrqa_searchqa-validation-12240", "mrqa_hotpotqa-validation-1695", "mrqa_hotpotqa-validation-283", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-1180", "mrqa_searchqa-validation-2617", "mrqa_newsqa-validation-3075", "mrqa_searchqa-validation-5247", "mrqa_naturalquestions-validation-3348", "mrqa_hotpotqa-validation-4711", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-5", "mrqa_triviaqa-validation-6442", "mrqa_searchqa-validation-9970"], "EFR": 1.0, "Overall": 0.7484338662790698}, {"timecode": 86, "before_eval_results": {"predictions": ["California", "inverted", "any unt toward medical occurrence in a patient or clinical investigation subject administered a pharmaceutical product", "18 - season", "December 24, 1836", "the bank, rather than the purchaser, is responsible for paying the amount", "Carlos Alan Autry Jr.", "20 years from the filing date subject to the payment of maintenance fees", "Sharecropping", "in the 1940s", "The Chainsmoker", "Steve Russell", "the President pro tempore", "Donna", "the Dutch", "A turlough", "Vancouver, British Columbia", "`` Elected Emperor of the Romans ''", "9 February 2018", "1960", "during the 2013 -- 14 television season", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "supervillains who pose catastrophic challenges to the world", "two", "Charbagh structure", "Elizabeth Dean Lail", "password recovery tool for Microsoft Windows", "semi-autonomous organisational units within the National Health Service in England", "May 1979", "13 to 22 June 2012", "60", "electron donors", "the pouring rain", "lizards", "Abbot Suger", "Mahalangur Himal sub-range of the Himalayas", "The Royalettes", "the winter solstice", "State Bar of Arizona", "Marie Fredriksson", "Geothermal gradient", "2001", "Hellenismos", "Neil Diamond", "September 2000", "blue", "Mishnah", "1066", "April 1979", "around 1872", "Atlanta, Georgia", "\u0430\u043b\u0444\u0430\u0432\u0438\u0442", "US", "\"Wooden Heart\"", "France", "American rapper", "November 10, 2017", "jobs", "London", "France's famous Louvre museum", "ten", "Dragnet", "Harry Potter", "Fairfax"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5518104838089082}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.14285714285714285, 0.0, 1.0, 0.19999999999999998, 1.0, 0.5882352941176471, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.9387755102040816, 1.0, 0.6666666666666666, 0.0, 1.0, 0.058823529411764705, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615385, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-8297", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-916", "mrqa_triviaqa-validation-3891", "mrqa_triviaqa-validation-2813", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3536", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2614", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-1911", "mrqa_hotpotqa-validation-82"], "SR": 0.46875, "CSR": 0.5668103448275862, "retrieved_ids": ["mrqa_squad-train-50679", "mrqa_squad-train-319", "mrqa_squad-train-58839", "mrqa_squad-train-30731", "mrqa_squad-train-78140", "mrqa_squad-train-76980", "mrqa_squad-train-66784", "mrqa_squad-train-73807", "mrqa_squad-train-65037", "mrqa_squad-train-80967", "mrqa_squad-train-69157", "mrqa_squad-train-36774", "mrqa_squad-train-51620", "mrqa_squad-train-68080", "mrqa_squad-train-19091", "mrqa_squad-train-77369", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3302", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-2498", "mrqa_triviaqa-validation-4855", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-452", "mrqa_searchqa-validation-16500", "mrqa_naturalquestions-validation-7509", "mrqa_naturalquestions-validation-6326", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3874", "mrqa_searchqa-validation-12173", "mrqa_hotpotqa-validation-1695", "mrqa_searchqa-validation-14517"], "EFR": 0.7941176470588235, "Overall": 0.7070293483772819}, {"timecode": 87, "before_eval_results": {"predictions": ["$10 billion", "Don Draper", "Nafees Syed", "Two pages -- usually high school juniors who serve Congress as messengers -- have been dismissed", "Marcell J Hansen", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "nuclear warheads", "Current TV", "riders a 19th-century experience on cruises complete with the carnival-like sounds of the steam-whistle calliope.", "going somewhere very special, far away,", "peanuts, nuts, shellfish and fish", "Piers Morgan Tonight", "Dubai", "in the bedrooms of their two-floor home in the St. Louis suburb of Columbia, Illinois,", "jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "free services.", "U.S. President-elect Barack Obama", "Mark Obama Ndesandjo", "Michael Hayden,", "30,000", "Kris Allen", "Ashura.", "$17,000", "250,000", "Itsy Bitsy Teeny Yellow Polka Dot Bikini.", "Juarez drug cartel.", "Muslim festival of Eid al-Adha.", "French army helicopter taking off from French frigate Nivose,", "U.S. troops involved in the operation.", "Marcus Schrenker", "150", "cancer", "U.S. Chamber of Commerce", "a", "the underprivileged.", "an open window", "Jiverly Wong,", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "bronze", "98 people,", "President Obama", "in Austin, Texas,", "Cologne, Germany,", "former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq and that a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "Tennessee.", "five minutes before commandos descended from ropes that dangled from helicopters,", "The president would legally be able to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "Mary Procidano", "The son of Gabon's former president", "Samoa", "Derek Mears", "January to May 2014", "Frederick County", "RMS Titanic", "Calcium carbonate", "The Great Leap", "Bangladesh", "Elisha Nelson Manning", "CBS", "\"Dr. Gr\u00e4sler, Badearzt\"", "a seal", "glaucoma", "Carl Sandburg", "Meriwether Lewis"], "metric_results": {"EM": 0.5625, "QA-F1": 0.652913882992008}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.25, 0.4, 0.6153846153846153, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.9333333333333333, 0.5, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.625, 0.07142857142857144, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-931", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-3855", "mrqa_hotpotqa-validation-3563"], "SR": 0.5625, "CSR": 0.5667613636363636, "retrieved_ids": ["mrqa_squad-train-57540", "mrqa_squad-train-76642", "mrqa_squad-train-9476", "mrqa_squad-train-27886", "mrqa_squad-train-54508", "mrqa_squad-train-9160", "mrqa_squad-train-58199", "mrqa_squad-train-34512", "mrqa_squad-train-1997", "mrqa_squad-train-81735", "mrqa_squad-train-76826", "mrqa_squad-train-40665", "mrqa_squad-train-32191", "mrqa_squad-train-71456", "mrqa_squad-train-85056", "mrqa_squad-train-62450", "mrqa_searchqa-validation-806", "mrqa_newsqa-validation-652", "mrqa_hotpotqa-validation-5010", "mrqa_triviaqa-validation-5810", "mrqa_naturalquestions-validation-6285", "mrqa_searchqa-validation-6929", "mrqa_naturalquestions-validation-1611", "mrqa_triviaqa-validation-3208", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-2081", "mrqa_naturalquestions-validation-124", "mrqa_hotpotqa-validation-1747", "mrqa_triviaqa-validation-7361", "mrqa_naturalquestions-validation-1856", "mrqa_hotpotqa-validation-881", "mrqa_triviaqa-validation-5397"], "EFR": 0.9285714285714286, "Overall": 0.7339103084415585}, {"timecode": 88, "before_eval_results": {"predictions": ["Caylee,", "at least 18 federal agents and two soldiers", "$22 million", "federal officers' bodies", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "the 11th century Preah Vihear temple", "police to question people if there's reason to suspect they're in the United States illegally.", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "France,", "poems", "Cairo, in Jeddah and Dubai.", "Cleve Landsberg,", "Saturday", "Columbian mammoth", "the Taliban", "genocide, crimes against humanity, and war crimes.", "in July 1999,", "the county jail in Spanishfork,", "304,000", "purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "28", "an independent homeland", "Miguel Cotto", "South Africa", "seven", "at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.\"", "returning combat veterans", "scientific reasons.", "the release of the four men", "United States, NATO member states, Russia and India", "75", "Ameneh Bahrami", "will not support the Stop Online Piracy Act,", "Ma Khin Khin Leh,", "12", "The Rosie Show", "The station", "Larry Zeiger", "The son of Gabon's former president", "Mogadishu", "Seoul.", "better conditions for inmates, like Amnesty International.", "the longest domestic relay in Olympic history,", "IV cafe.", "Joan Rivers", "Ciudad Juarez,", "New York Post's Page 6 gossip column.", "Indonesian", "Robert Park", "14", "late 2018 or early 2019", "Graub\u00fcnden, in the eastern Alps region of Switzerland", "2,050 metres ( 6,730 ft ) at the Urubamba River below the citadel of Macchu Piccu", "cutis anserina", "battle of agincourt", "blind beggar", "Belgian", "Geelong Football Club", "Richard L. Thompson", "a fisheye lens", "a clavichord", "orchids", "John Knox"], "metric_results": {"EM": 0.5, "QA-F1": 0.6161768090504504}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true], "QA-F1": [0.6666666666666666, 0.888888888888889, 1.0, 1.0, 0.1111111111111111, 0.0, 0.08, 1.0, 0.4615384615384615, 0.0, 1.0, 0.20000000000000004, 1.0, 1.0, 1.0, 1.0, 0.25, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.07407407407407408, 1.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4347826086956522, 0.7777777777777778, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-999", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-929", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2388", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-8794", "mrqa_triviaqa-validation-5342", "mrqa_searchqa-validation-14968", "mrqa_searchqa-validation-2827"], "SR": 0.5, "CSR": 0.5660112359550562, "retrieved_ids": ["mrqa_squad-train-81472", "mrqa_squad-train-36629", "mrqa_squad-train-15179", "mrqa_squad-train-8469", "mrqa_squad-train-45497", "mrqa_squad-train-27177", "mrqa_squad-train-13273", "mrqa_squad-train-20330", "mrqa_squad-train-21437", "mrqa_squad-train-82473", "mrqa_squad-train-32292", "mrqa_squad-train-14701", "mrqa_squad-train-53851", "mrqa_squad-train-80382", "mrqa_squad-train-16640", "mrqa_squad-train-52749", "mrqa_searchqa-validation-1108", "mrqa_searchqa-validation-306", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-893", "mrqa_searchqa-validation-9320", "mrqa_searchqa-validation-10400", "mrqa_triviaqa-validation-7626", "mrqa_hotpotqa-validation-5373", "mrqa_naturalquestions-validation-10656", "mrqa_hotpotqa-validation-622", "mrqa_searchqa-validation-5294", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-10902", "mrqa_triviaqa-validation-2027", "mrqa_newsqa-validation-2945", "mrqa_naturalquestions-validation-7143"], "EFR": 0.90625, "Overall": 0.7292959971910112}, {"timecode": 89, "before_eval_results": {"predictions": ["Ariel Binns", "Ignazio La Russa", "because the Indians were gathering information about the rebels to give to the Colombian military.", "collaborating with the Colombian government,", "potential revenues from oil and gas", "2001", "$24.1 million,", "Two United Arab Emirates based companies", "Chadian President Idriss Deby", "U.S. Navy", "two hunters", "U.S. State Department and British Foreign Office", "legendary Amber Room", "U.S. President-elect Barack Obama", "Web", "News of the World tabloid.", "Too many glass shards left by beer drinkers in the city center,", "not for sale,", "\"an eye for an eye,\"", "fear of losing their licenses to fly.", "Jenny Sanford", "Kurdish militant group in Turkey", "a vast settlement of people left without loved ones, without homes, without life's belongings.", "\"falling space debris,\"", "J. Crew outfits on Inauguration Day,", "France", "581 points", "Robert Barnett,", "The Mexican military", "At least 14", "Venezuela", "41,", "Wednesday at the age of 95.", "Idriss Deby hopes the journalists and the flight crew will be freed,", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "\"illegitimate.\"", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "two", "The company Haeftling,", "Saturday.", "five minutes before commandos descended", "nose, cheeks, upper jaw and facial tissue from a female cadaver", "sniff out cell phones.", "The cause of the child's death will be listed as homicide by undetermined means,", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "for the rest of the year", "in July", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Ghana", "The Glasgow, Scotland concert", "Casey Anthony's", "2006 -- 07", "member", "John Vincent Calipari", "New Zealand", "Jessica Simpson", "fox hunting", "Los Angeles", "The S7 series", "Hilux pickup truck", "eggshells", "Manchester", "Gertrude Stein", "Dick Van Dyke"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6828493976451937}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.125, 0.125, 0.2857142857142857, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.05714285714285715, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.33333333333333337, 0.8235294117647058, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 0.5217391304347826, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1499", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-1000", "mrqa_hotpotqa-validation-5380"], "SR": 0.5625, "CSR": 0.5659722222222222, "retrieved_ids": ["mrqa_squad-train-80310", "mrqa_squad-train-46658", "mrqa_squad-train-59703", "mrqa_squad-train-69580", "mrqa_squad-train-40914", "mrqa_squad-train-7822", "mrqa_squad-train-35540", "mrqa_squad-train-66632", "mrqa_squad-train-65996", "mrqa_squad-train-67826", "mrqa_squad-train-57633", "mrqa_squad-train-46771", "mrqa_squad-train-8011", "mrqa_squad-train-46527", "mrqa_squad-train-43397", "mrqa_squad-train-64915", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-13968", "mrqa_naturalquestions-validation-6289", "mrqa_hotpotqa-validation-2544", "mrqa_squad-validation-9093", "mrqa_naturalquestions-validation-4837", "mrqa_searchqa-validation-5846", "mrqa_hotpotqa-validation-2421", "mrqa_searchqa-validation-2086", "mrqa_triviaqa-validation-6314", "mrqa_hotpotqa-validation-2678", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-1734", "mrqa_searchqa-validation-287", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-16947"], "EFR": 0.9285714285714286, "Overall": 0.7337524801587302}, {"timecode": 90, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-342", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8553", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.90234375, "KG": 0.5140625, "before_eval_results": {"predictions": ["eels", "chas chandler", "Stevie Wonder", "Sir Tom Finney", "Jackson Pollock", "the Black Sea", "Jumping Jack Flash", "Joseph Priestley", "Dumbo", "New Zealand", "Call for the Dead", "Bill Bryson", "Hans Christian Andersen", "Christian Louboutin", "Vespa", "naked", "Laputa", "a Hungarian Horntail,", "Jumanji", "Flo Rida", "at", "The Princess bride", "word Options", "pig", "Dancing With The Stars", "Australia", "Leicester", "eulogy", "Andr\u00e9s Iniesta", "Bath", "1924", "a barred spiral galaxy", "Duty Free", "Mark Twain", "fruit", "carbon", "ffestiniog and Welsh Highland Railway", "khalifa Abdullah", "Johnny Mathis", "Sergio Garc\u00eda Fern\u00e1ndez", "Chad", "arthur", "Yulia Tymochenko", "E. Nesbit", "czar Alexander I", "John F. Kennedy", "Sheree Murphy", "jekyll", "R34", "Yukon is one of Canada's three territories, in the country's far northwest", "\"our MUTUAL FRIend\"", "Chlorofluorocarbons", "three levels for visitors, with restaurants on the first and second levels", "the coffee shop Monk's", "Ling", "DreamWorks Animation", "Port Melbourne", "338", "Pakistan", "could be secretly working on a nuclear weapon", "U.S. Marine Band", "John Adams", "Les Veilles", "Austria"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6015625}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-2698", "mrqa_triviaqa-validation-5650", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3335", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-2111", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-4011", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1997", "mrqa_triviaqa-validation-7125", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-339", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1218", "mrqa_searchqa-validation-6163", "mrqa_searchqa-validation-8979", "mrqa_hotpotqa-validation-5203"], "SR": 0.53125, "CSR": 0.5655906593406593, "retrieved_ids": ["mrqa_squad-train-51022", "mrqa_squad-train-48919", "mrqa_squad-train-11763", "mrqa_squad-train-46622", "mrqa_squad-train-43976", "mrqa_squad-train-45390", "mrqa_squad-train-48681", "mrqa_squad-train-25824", "mrqa_squad-train-37368", "mrqa_squad-train-30626", "mrqa_squad-train-42446", "mrqa_squad-train-25016", "mrqa_squad-train-14795", "mrqa_squad-train-31999", "mrqa_squad-train-78532", "mrqa_squad-train-3781", "mrqa_newsqa-validation-637", "mrqa_hotpotqa-validation-294", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-4719", "mrqa_newsqa-validation-1339", "mrqa_hotpotqa-validation-4203", "mrqa_searchqa-validation-6757", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7009", "mrqa_hotpotqa-validation-2468", "mrqa_newsqa-validation-733", "mrqa_hotpotqa-validation-2156", "mrqa_naturalquestions-validation-5583", "mrqa_searchqa-validation-14281"], "EFR": 0.8333333333333334, "Overall": 0.7150191735347986}, {"timecode": 91, "before_eval_results": {"predictions": ["dumbo", "Ernest Hemingway", "Switzerland", "Mexican Orange Blossom", "Perry Mason", "Hyderabad", "James I", "trapezium", "Canada", "a 1934 Austin seven box saloon", "seven", "Vancouver, British Columbia, Canada", "Nigeria", "Switzerland", "Union Gap", "Cologne", "\u201cPunky Brewster\u201d", "cactus", "gin", "piano", "Dick Cheney", "at the north-west corner of the central business district", "Virginia", "dysmenorrhea", "pasta harvest", "the witch trials", "sailor", "\u201cMy Favorite Martian,", "plutocracy", "Bahrain", "Austria-Hungary", "Ace of Spades", "the Soviet Union", "China", "the eye", "Venice", "New Zealand", "1973", "St Pauls", "Brighton", "Margaret Thatcher", "the popes", "Jimmy Carter", "Bradley", "Argentina", "Genesis", "special sauce", "khrushchev", "arsenic", "john Peel", "hsborough", "December 14, 2017", "mitosis", "Morgan Freeman", "Mike Fiers", "coca wine", "politician", "fast cars, drink and celebrity parties.", "urged NATO to take a more active role in countering the spread of the", "Trevor Rees", "(Edvard) GRIEG", "Buddhism", "(Thomas Francis) Eagleton", "water"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6781994047619048}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6589", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-4024", "mrqa_triviaqa-validation-3942", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-3775", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-2032", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-1046", "mrqa_naturalquestions-validation-9781", "mrqa_hotpotqa-validation-2210", "mrqa_newsqa-validation-767", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-11019"], "SR": 0.609375, "CSR": 0.5660665760869565, "retrieved_ids": ["mrqa_squad-train-22491", "mrqa_squad-train-56648", "mrqa_squad-train-9110", "mrqa_squad-train-36253", "mrqa_squad-train-72751", "mrqa_squad-train-31938", "mrqa_squad-train-52088", "mrqa_squad-train-60527", "mrqa_squad-train-48096", "mrqa_squad-train-43262", "mrqa_squad-train-7501", "mrqa_squad-train-30607", "mrqa_squad-train-29048", "mrqa_squad-train-19903", "mrqa_squad-train-66469", "mrqa_squad-train-55984", "mrqa_newsqa-validation-2099", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-1944", "mrqa_newsqa-validation-1893", "mrqa_squad-validation-1025", "mrqa_newsqa-validation-4059", "mrqa_searchqa-validation-14441", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-1695", "mrqa_searchqa-validation-16829", "mrqa_naturalquestions-validation-9999", "mrqa_hotpotqa-validation-4266", "mrqa_squad-validation-8832", "mrqa_searchqa-validation-8772", "mrqa_naturalquestions-validation-10367", "mrqa_searchqa-validation-3569"], "EFR": 0.88, "Overall": 0.7244476902173913}, {"timecode": 92, "before_eval_results": {"predictions": ["raping and killing a 14-year-old Iraqi girl.", "Karen Floyd", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "Krishna Rajaram,", "U.S. President-elect Barack Obama", "Courtney Love,", "Iran's President Mahmoud Ahmadinejad", "a gym", "Current TV", "gun", "\"E! News\"", "1983", "nude beaches.", "tennis", "the administration's progress,", "19", "Max Foster,", "military trials for some Guantanamo Bay detainees.", "Iran of trying to build nuclear bombs,", "A Lion Among Men.", "Robert Hawkins from going on a murderous rampage at an Omaha, Nebraska, shopping mall", "the bill a victory for the president.", "to clean up Washington State's decommissioned Hanford nuclear site,", "delivers a big speech", "The island's dining scene", "Saturday,", "warns business owners to close their shops during daily prayers, or they will be temporarily shut down,", "a residential dike", "gasoline", "they did not receive a fair trial.", "more than 2.5 million copies,", "abducting each other for ransoms or retribution.", "Egypt", "1:55 a.m. PT", "Barack Obama", "iTunes Music Store,", "president Robert Mugabe", "her boyfriend,", "12.3 million", "ties", "three", "to encourage readers to get involved in service and volunteerism in their communities.", "75 percent", "Hu Jintao", "1,500", "volatile and dangerous.", "two counts of murder.", "young self-styled anarchists", "first grand Slam,", "in the heart of Los Angeles.", "Alberto Espinoza Barron,", "2013", "Vincenzo Peruggia", "April 1979", "Taking of Pelham", "Hercule Poirot", "1997", "Debbie Reynolds", "43rd", "USS Essex", "Daylight Saving Time", "Earhart", "uranium", "Pakistan"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7526551726459816}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.13333333333333333, 1.0, 1.0, 0.2857142857142857, 0.4, 0.9411764705882353, 1.0, 1.0, 0.6666666666666666, 0.5833333333333334, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-45", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-2917", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1516", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-4531", "mrqa_hotpotqa-validation-1316", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-1681"], "SR": 0.578125, "CSR": 0.5661962365591398, "retrieved_ids": ["mrqa_squad-train-76191", "mrqa_squad-train-62862", "mrqa_squad-train-1835", "mrqa_squad-train-61899", "mrqa_squad-train-17847", "mrqa_squad-train-73060", "mrqa_squad-train-60156", "mrqa_squad-train-13675", "mrqa_squad-train-6352", "mrqa_squad-train-13046", "mrqa_squad-train-81756", "mrqa_squad-train-47090", "mrqa_squad-train-30157", "mrqa_squad-train-43388", "mrqa_squad-train-26736", "mrqa_squad-train-72249", "mrqa_searchqa-validation-5051", "mrqa_newsqa-validation-3835", "mrqa_squad-validation-2315", "mrqa_hotpotqa-validation-1192", "mrqa_searchqa-validation-4865", "mrqa_hotpotqa-validation-5200", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-1720", "mrqa_triviaqa-validation-577", "mrqa_triviaqa-validation-3421", "mrqa_naturalquestions-validation-10205", "mrqa_newsqa-validation-1358", "mrqa_searchqa-validation-13129", "mrqa_newsqa-validation-25", "mrqa_naturalquestions-validation-4951", "mrqa_searchqa-validation-14963"], "EFR": 0.8888888888888888, "Overall": 0.7262514000896056}, {"timecode": 93, "before_eval_results": {"predictions": ["David Beckham", "They are co-chair the Genocide Prevention Task Force.", "March 8", "to overthrow the socialist government of Salvador Allende in Chile,", "Kearny, New Jersey.", "in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "five minutes before commandos descended from ropes that dangled from helicopters,", "Democratic", "Kim Il Sung", "Roy Foster's", "2-0", "managing his time.", "U.S. Consulate in Rio de Janeiro,", "Ameneh Bahrami", "Molotov cocktails,", "Facebook and Google,", "The U.S. Coast Guard Sunday continues its search for a missing sailor whose five Texas A&M University crew mates were hoisted out of the Gulf of Mexico", "customers are lining up for vitamin injections that promise", "Oxbow,", "Sarah Brown's", "Two people were found dead and a third person is still believed missing", "summer", "Charles Jubert,", "Friday,", "\"a whole new treasure repository of fossils\"", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "producing rock music with a country influence.", "parents", "\"I think everything he has said about the tech world being a meritocracy.", "Jason Chaffetz", "the underprivileged.", "March 24,", "North Korea,", "The Mexican military", "Bill Haas", "to protect ocean ecology, address climate change and promote sustainable ocean economies.", "Saturday's killing of a 15-year-old boy", "Somalia's piracy problem was fueled by environmental and political events.", "Swat Valley.", "Japanese Foreign Ministry spokesman Hidenobu Sobashima", "Grayback forest-firefighters", "Steven Gerrard", "three", "a bronze medal in the women's figure skating final,", "having an affair with a woman in Argentina.", "the Beatles", "At least 88", "Illinois Reform Commission", "use of torture and indefinite detention", "\"Watchmen\" (No. 4)", "severe flooding", "Pastoral farming", "Kaley Christine Cuoco", "Hermann Ebbinghaus", "Route 66", "Morten Skovsby", "Black Wednesday", "Cambridge University", "early 20th-century Europe", "American tour", "Mickey Mouse", "Daiquiri", "Israel", "UNESCO / ILO"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6754423153332305}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.9473684210526316, 0.625, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.11764705882352941, 1.0, 1.0, 1.0, 0.8333333333333334, 0.21428571428571427, 0.0, 0.2222222222222222, 0.5263157894736842, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.18181818181818182, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1910", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-4092", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3157", "mrqa_triviaqa-validation-3946", "mrqa_hotpotqa-validation-3326", "mrqa_searchqa-validation-6"], "SR": 0.546875, "CSR": 0.5659906914893618, "retrieved_ids": ["mrqa_squad-train-59904", "mrqa_squad-train-64560", "mrqa_squad-train-56732", "mrqa_squad-train-7463", "mrqa_squad-train-31177", "mrqa_squad-train-14908", "mrqa_squad-train-66024", "mrqa_squad-train-28216", "mrqa_squad-train-70903", "mrqa_squad-train-40238", "mrqa_squad-train-78857", "mrqa_squad-train-9698", "mrqa_squad-train-59109", "mrqa_squad-train-29097", "mrqa_squad-train-28786", "mrqa_squad-train-48474", "mrqa_newsqa-validation-877", "mrqa_squad-validation-9426", "mrqa_searchqa-validation-10624", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-4362", "mrqa_naturalquestions-validation-346", "mrqa_newsqa-validation-3518", "mrqa_triviaqa-validation-2144", "mrqa_newsqa-validation-3543", "mrqa_hotpotqa-validation-5784", "mrqa_naturalquestions-validation-2498", "mrqa_searchqa-validation-15045", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-1944", "mrqa_hotpotqa-validation-1492"], "EFR": 0.9655172413793104, "Overall": 0.7415359615737345}, {"timecode": 94, "before_eval_results": {"predictions": ["The gunmen also took hostage Lunsmann's 14-year-old son, Kevin,", "23-year-old", "Starr", "Thirty to 40 ships", "Larry Ellison,", "California, Texas and Florida,", "Alberto Espinoza Barron,", "review their emergency plans", "4,000", "Whitney Houston", "Marines", "Lillo Brancato Jr.", "United States, NATO member states, Russia", "a trust fund", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "many riders say it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "$10 billion", "opium trade", "Casa de Campo International Airport in the Dominican Republic", "Pakistan's", "her dancing against a stripper's pole.", "women.", "Blagojevich", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Iraqi Prime Minister Nouri al-Maliki", "Alfredo Astiz,", "President Obama,", "Majid Movahedi,", "step down as majority leader.", "three", "1960", "education and energy, innovation and infrastructure, fair trade and reform.\"", "engage in learning differently, enjoy a customized approach", "the first", "Tom Baer.", "Charlotte Gainsbourg and Willem Dafoe", "The public endorsement", "tie salesman", "fight against terror will respect America's values.", "2005", "Two UH-60 Blackhawk helicopters", "Cirque du Soleil", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Ghana", "Christopher Savoie", "246", "London's Waterloo", "Chievo", "the last few months,", "nearly 28 years of rule.", "a nuclear weapon", "2020 National Football League ( NFL ) season", "John Cooper Clarke", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Chicago", "Kolkata", "avant garde", "MGM Grand Garden Special Events Center", "The Royal Navy", "Manchester United", "Bangkok", "a roof", "Cana", "KXII"], "metric_results": {"EM": 0.546875, "QA-F1": 0.7076601193270242}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true], "QA-F1": [0.6956521739130436, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.14285714285714288, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.15384615384615385, 1.0, 0.5, 1.0, 1.0, 0.888888888888889, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8333333333333333, 1.0, 0.0, 0.4615384615384615, 0.6363636363636364, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7368421052631579, 1.0, 1.0, 0.0, 0.1212121212121212, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-985", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-1131", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-4069"], "SR": 0.546875, "CSR": 0.5657894736842105, "retrieved_ids": ["mrqa_squad-train-16082", "mrqa_squad-train-29476", "mrqa_squad-train-34551", "mrqa_squad-train-27147", "mrqa_squad-train-20480", "mrqa_squad-train-23729", "mrqa_squad-train-45", "mrqa_squad-train-42291", "mrqa_squad-train-66457", "mrqa_squad-train-54691", "mrqa_squad-train-5635", "mrqa_squad-train-29145", "mrqa_squad-train-36514", "mrqa_squad-train-37047", "mrqa_squad-train-69041", "mrqa_squad-train-18323", "mrqa_naturalquestions-validation-3199", "mrqa_hotpotqa-validation-3151", "mrqa_newsqa-validation-2246", "mrqa_hotpotqa-validation-4839", "mrqa_naturalquestions-validation-10618", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-1734", "mrqa_searchqa-validation-8890", "mrqa_hotpotqa-validation-599", "mrqa_naturalquestions-validation-8096", "mrqa_newsqa-validation-2222", "mrqa_hotpotqa-validation-3203", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-2414", "mrqa_searchqa-validation-12914", "mrqa_searchqa-validation-8460"], "EFR": 0.9655172413793104, "Overall": 0.7414957180127042}, {"timecode": 95, "before_eval_results": {"predictions": ["Aksel Sandemose", "pubs, bars and restaurants", "Retina display", "Chick tract", "Julia Verdin", "second cousin", "to prevent the opposing team from scoring goals", "Graffiti", "ARY Digital Network", "Drifting", "Larry Wayne Gatlin", "1980", "Key West", "its riverside location,", "Scottish", "Skipton", "Atlas ICBM", "PPG Paints Arena", "Kelly Bundy", "John Joseph Travolta", "Love Letter", "SBS", "AVN Adult Entertainment Expo", "119 minutes", "50 million", "Intelligent Design", "Sinngedichte", "Hidden America with Jonah Ray", "Irish", "Metro-Goldwyn-Mayer", "1 Squadron and 3 Squadron", "The Summer Olympic Games", "1692", "Art Deco-style skyscraper", "Christian Stephen Yelich", "25 November 2015", "Hyuna", "Mathieu Kassovitz", "Kentucky Music Hall of Fame", "Louis King", "Danish", "Hindi", "two", "McComb, Mississippi", "D\u00e2mbovi\u021ba River", "24800 mi", "saint", "143,007", "Michael Edward \" Mike\" Mills", "American newspaper based in New York City", "his fourth term", "2001", "a single, implicitly structured data item", "Marshall Sahlins", "Roberta Flack", "Sir Walter Scott", "Braille", "Caster Semenya", "one of its diplomats in northwest Pakistan", "20,000-capacity O2 Arena.", "jazz", "books to read for kids who like Harry Potter", "the Erie Canal", "Andy Warhol"], "metric_results": {"EM": 0.65625, "QA-F1": 0.764423076923077}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.923076923076923, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-2013", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-8669", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3651", "mrqa_searchqa-validation-9300", "mrqa_searchqa-validation-3482"], "SR": 0.65625, "CSR": 0.5667317708333333, "retrieved_ids": ["mrqa_squad-train-24706", "mrqa_squad-train-4034", "mrqa_squad-train-17075", "mrqa_squad-train-66094", "mrqa_squad-train-55583", "mrqa_squad-train-75180", "mrqa_squad-train-21938", "mrqa_squad-train-63199", "mrqa_squad-train-7835", "mrqa_squad-train-33681", "mrqa_squad-train-57968", "mrqa_squad-train-40579", "mrqa_squad-train-30281", "mrqa_squad-train-61288", "mrqa_squad-train-65415", "mrqa_squad-train-33893", "mrqa_newsqa-validation-3012", "mrqa_triviaqa-validation-6337", "mrqa_searchqa-validation-9320", "mrqa_searchqa-validation-554", "mrqa_squad-validation-8581", "mrqa_hotpotqa-validation-3381", "mrqa_triviaqa-validation-6300", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-3430", "mrqa_newsqa-validation-1809", "mrqa_searchqa-validation-7224", "mrqa_newsqa-validation-3124", "mrqa_searchqa-validation-5765", "mrqa_hotpotqa-validation-618", "mrqa_newsqa-validation-452", "mrqa_searchqa-validation-4211"], "EFR": 1.0, "Overall": 0.7485807291666666}, {"timecode": 96, "before_eval_results": {"predictions": ["Black Abbots", "Audrey Mossom", "Nicolas Winding Refn", "PBS", "Ed Lee", "James Victor Chesnutt", "Wayne Rooney", "South Asia and the Middle East", "Martin Ingerman", "Rice University", "NCAA Division I", "Richard Price", "Reich Chancellery", "Sun Woong", "pubs, bars and restaurants", "Julian WikiLeaks,", "Milan", "1885", "Pac-12", "Lamar Wyatt", "Homer Hickam, Jr.", "Straits of Gibraltar", "Port Clinton", "Patrick Dempsey", "Issaquah", "Taylor Swift", "Appleby-in-Westmorland", "2000", "Mickey's Christmas Carol", "Michael Phelps, Ryan Lochte, Peter Vanderkaay, and Keller", "Orson Welles", "in 1902", "Brittany Snow", "Lapland", "Elvis' Christmas Album", "CD Castell\u00f3n", "Coronation Street", "Zimbabwe", "commercial", "Jack Elam", "1998", "Johnny Cash, Waylon Jennings", "Brad Silberling", "ITV", "1982", "the Marx Brothers film of the same name", "Kentucky", "Agent Vinod", "James Worthy", "Harrods", "2007 Formula One season", "January 2004", "Eastern Redbud", "left - sided heart failure", "Karl Marx", "Laurence Olivier", "Indian Ocean", "Rodong Sinmun", "two months ago,", "Arizona", "Vatican City", "a Porch", "Eric Knight", "unknown origin"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7934027777777778}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2033", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1985", "mrqa_hotpotqa-validation-4441", "mrqa_naturalquestions-validation-7624", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-776", "mrqa_searchqa-validation-69"], "SR": 0.734375, "CSR": 0.5684600515463918, "retrieved_ids": ["mrqa_squad-train-27722", "mrqa_squad-train-31491", "mrqa_squad-train-40984", "mrqa_squad-train-58287", "mrqa_squad-train-8958", "mrqa_squad-train-48445", "mrqa_squad-train-22056", "mrqa_squad-train-36218", "mrqa_squad-train-52126", "mrqa_squad-train-78948", "mrqa_squad-train-27575", "mrqa_squad-train-49768", "mrqa_squad-train-35522", "mrqa_squad-train-44592", "mrqa_squad-train-3272", "mrqa_squad-train-83038", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-2813", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3590", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-707", "mrqa_newsqa-validation-2169", "mrqa_squad-validation-1818", "mrqa_searchqa-validation-5450", "mrqa_hotpotqa-validation-5535", "mrqa_newsqa-validation-490", "mrqa_searchqa-validation-7243", "mrqa_triviaqa-validation-3946", "mrqa_searchqa-validation-6173", "mrqa_newsqa-validation-3473", "mrqa_triviaqa-validation-2080"], "EFR": 0.9411764705882353, "Overall": 0.7371616794269255}, {"timecode": 97, "before_eval_results": {"predictions": ["New Orleans", "poker", "Budapest", "Hoppin' John", "capuchin", "bass", "El Cid", "Vestal Virgins", "contract", "Akihito", "lead", "Israel", "(St.) Matthew", "Nancy Astor", "imperative", "the bald eagle", "high altitude", "Bergen", "an leap year", "Little Miss Muffet", "Gila", "The Hague", "Zyrtec", "Buddhism", "Carson City", "Syria", "Cherry, Cherry", "the Council of Better Business Bureaus", "Linda Tripp", "a stationwagon", "Aqua Teen Hunger Force", "(James) Webb", "economics", "United Nations", "diseases", "Rocky Mountain Fever", "euros", "Lebanon", "typewriters", "Isadora Duncan", "Jaws 2", "Custer", "nag", "Iliad", "Motor Trend", "the U.S. Federal Aviation Administration", "Manhattan", "Naxos", "titanium", "Jamaica", "St. Elsewhere", "Lord Banquo", "average speed 112 km / h", "Paul Lynde", "John Part", "March 10, 1997", "Hubble Space Telescope", "Household Words", "Rockland County", "Trilochanapala", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Turkey", "the Haitian National Police (HNP)", "Cuban"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7725725446428572}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6875000000000001, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-2127", "mrqa_searchqa-validation-4397", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-12404", "mrqa_searchqa-validation-2730", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-10588", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-16039", "mrqa_naturalquestions-validation-6519", "mrqa_triviaqa-validation-3516", "mrqa_hotpotqa-validation-2278", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-2757", "mrqa_triviaqa-validation-5852"], "SR": 0.6875, "CSR": 0.5696747448979591, "retrieved_ids": ["mrqa_squad-train-57763", "mrqa_squad-train-48473", "mrqa_squad-train-86458", "mrqa_squad-train-79288", "mrqa_squad-train-37463", "mrqa_squad-train-8122", "mrqa_squad-train-55136", "mrqa_squad-train-63374", "mrqa_squad-train-62353", "mrqa_squad-train-33019", "mrqa_squad-train-37232", "mrqa_squad-train-78852", "mrqa_squad-train-31245", "mrqa_squad-train-39313", "mrqa_squad-train-61612", "mrqa_squad-train-31198", "mrqa_newsqa-validation-1333", "mrqa_naturalquestions-validation-4401", "mrqa_newsqa-validation-1225", "mrqa_searchqa-validation-8650", "mrqa_searchqa-validation-9970", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-11252", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-1841", "mrqa_triviaqa-validation-674", "mrqa_naturalquestions-validation-5651", "mrqa_newsqa-validation-1820", "mrqa_naturalquestions-validation-8669", "mrqa_newsqa-validation-915", "mrqa_triviaqa-validation-7636"], "EFR": 0.9, "Overall": 0.7291693239795919}, {"timecode": 98, "before_eval_results": {"predictions": ["Scary Movie 2", "a Mall", "godliness", "Time", "the Annunciation", "the Thames", "Alyssa Milano", "drowsiness", "lily", "Alaska", "Yellowstone", "(Marcel) Duchamp", "Little Red Riding Hood", "Warsaw", "the math teacher was fat", "the English Channel", "Michelin", "a celebration", "Simple Simon", "hot chocolate", "vibrations", "a metronome", "gigabytes", "the Phillie Phanatic", "GILBERT & SULLIVAN", "Pringles", "Gentlemen Prefer Blondes", "a Stratocaster", "anchors", "Romeo and Juliet", "a mirror and prism system", "Pamela Anderson", "trampolining", "King of the Hill", "Bermuda", "Tiger Woods", "dark places", "Elton John", "the Sphinx", "Toy Story", "endure", "density", "hockey", "Heather Locklear", "the Explorer 3in1 Game Set", "wheels", "the Messiah", "a crone", "Peter", "whole hog", "Target", "the pretribulation, premillennial, Christian eschatological interpretation of the Biblical apocalypse", "71 -- 74 \u00b0 C ( 160 -- 165 \u00b0 F )", "Darth Vader's flagship, the Devastator, chases the Tantive IV above Tatooine", "cotton", "microscope", "(Donatello) Maggiore", "1939", "Leafcutter John", "University of Vienna", "Ferraris, a Lamborghini and an Acura NSX", "80 percent", "Michael Jackson", "north"], "metric_results": {"EM": 0.625, "QA-F1": 0.6938244047619048}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, false, true, true], "QA-F1": [0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9200", "mrqa_searchqa-validation-8408", "mrqa_searchqa-validation-15322", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11345", "mrqa_searchqa-validation-13276", "mrqa_searchqa-validation-8717", "mrqa_searchqa-validation-8288", "mrqa_searchqa-validation-11182", "mrqa_searchqa-validation-15535", "mrqa_searchqa-validation-10022", "mrqa_searchqa-validation-15136", "mrqa_searchqa-validation-416", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-13668", "mrqa_searchqa-validation-85", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-5275", "mrqa_triviaqa-validation-7303", "mrqa_triviaqa-validation-6743", "mrqa_hotpotqa-validation-537", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-1683"], "SR": 0.625, "CSR": 0.5702335858585859, "retrieved_ids": ["mrqa_squad-train-21591", "mrqa_squad-train-9936", "mrqa_squad-train-68889", "mrqa_squad-train-24857", "mrqa_squad-train-15823", "mrqa_squad-train-8337", "mrqa_squad-train-27880", "mrqa_squad-train-5611", "mrqa_squad-train-84061", "mrqa_squad-train-86100", "mrqa_squad-train-10849", "mrqa_squad-train-42050", "mrqa_squad-train-51432", "mrqa_squad-train-26362", "mrqa_squad-train-66413", "mrqa_squad-train-75584", "mrqa_searchqa-validation-13731", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-5076", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-625", "mrqa_triviaqa-validation-5589", "mrqa_searchqa-validation-10400", "mrqa_squad-validation-7615", "mrqa_newsqa-validation-998", "mrqa_hotpotqa-validation-1099", "mrqa_searchqa-validation-11687", "mrqa_newsqa-validation-3290", "mrqa_triviaqa-validation-4960", "mrqa_hotpotqa-validation-5601", "mrqa_searchqa-validation-5540"], "EFR": 0.875, "Overall": 0.7242810921717171}, {"timecode": 99, "UKR": 0.80078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1673", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12845", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14027", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6260", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8234", "mrqa_searchqa-validation-8339", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_searchqa-validation-999", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6448", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7028", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-959"], "OKR": 0.890625, "KG": 0.534375, "before_eval_results": {"predictions": ["file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Teen Patti", "Miami Beach, Florida,", "Kevin Evans", "WBO welterweight title from Miguel Cotto", "The group also has also been linked to the March attack on the Sri Lankan cricket team in the Pakistani city of Lahore.", "said.", "Addis Ababa,", "Saturn", "piano", "the Bronx.", "two years", "Tug boat owner Roger Rouzier", "$17,000", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Haleigh Cummings,", "The sailboat, named Cynthia Woods,", "saying Chaudhary's death was warning to management.", "violent separatist campaign", "\" Michoacan Family,\"", "two", "exotic sports", "Bryant Purvis,", "rock music", "Kurt Cobain's", "Department of Homeland Security Secretary Janet Napolitano", "Gary Brooker", "E. coli bacteria", "July", "engineering and construction", "India in Mumbai on Wednesday.", "a one-shot victory in the Bob Hope Classic on the final hole", "Dubai", "U.S. Vice President Dick Cheney", "Rwanda", "Tehran, Iran.", "The Louvre", "Daytime Emmy Lifetime Achievement Award.", "money or other discreet aid for the effort if it could be made available,", "checkposts and military camps in the Mohmand agency", "Somalia's piracy problem was fueled by environmental and political events.", "five", "1994", "start a dialogue of peace", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "President Obama", "The Ministry of Defense", "two days.", "sons", "Michael Arrington,", "BMW 3-Series", "1973", "1975", "1546", "Kaiser Chiefs", "rivers", "Ecuador", "Afghanistan", "University College of North Staffordshire", "Nelson County", "the run of the Bulls", "Jean-Michel Basquiat", "the Coast Guard", "iron"], "metric_results": {"EM": 0.625, "QA-F1": 0.7442032742584214}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.33333333333333337, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.5555555555555556, 0.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-1822", "mrqa_newsqa-validation-2895", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-1771", "mrqa_newsqa-validation-2609", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-653", "mrqa_newsqa-validation-2968", "mrqa_searchqa-validation-14651"], "SR": 0.625, "CSR": 0.57078125, "retrieved_ids": ["mrqa_squad-train-68918", "mrqa_squad-train-45648", "mrqa_squad-train-31951", "mrqa_squad-train-61462", "mrqa_squad-train-67522", "mrqa_squad-train-78602", "mrqa_squad-train-40980", "mrqa_squad-train-85398", "mrqa_squad-train-28020", "mrqa_squad-train-37779", "mrqa_squad-train-79104", "mrqa_squad-train-37823", "mrqa_squad-train-55908", "mrqa_squad-train-72185", "mrqa_squad-train-83640", "mrqa_squad-train-54872", "mrqa_newsqa-validation-2446", "mrqa_searchqa-validation-9486", "mrqa_hotpotqa-validation-2837", "mrqa_newsqa-validation-3330", "mrqa_naturalquestions-validation-10495", "mrqa_hotpotqa-validation-893", "mrqa_naturalquestions-validation-10442", "mrqa_searchqa-validation-14960", "mrqa_newsqa-validation-256", "mrqa_hotpotqa-validation-3321", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-5374", "mrqa_triviaqa-validation-6323", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-9006", "mrqa_triviaqa-validation-2685"], "EFR": 0.9583333333333334, "Overall": 0.7509791666666666}]}