{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]', diff_loss_weight=1.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]_result.json', stream_id=3, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4010, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["The Jones et al. and Briffa reconstructions", "help transfer and dissipate excess energy", "Historical and Critical Dictionary", "human settlement and development of the land", "over three days", "eight", "polynomial algebra", "60 days", "American Association of University Women", "mid-Cambrian period", "1926", "The outcome of most votes can be predicted beforehand", "Newton", "a disaster", "between the 1960s and 1990s", "James Hutton", "British", "growth and investment", "Each step had to be successfully accomplished before the next ones could be performed", "Downtown Riverside", "plague of Athens in 430 BC", "expelled Jews", "interleukin 1", "DuMont Television Network", "The city of Fresno", "1762", "colloblasts", "self", "Alta California", "300 km long and up to 40 km wide", "two-phased system", "On the Councils and the Church", "Super Bowl LI", "1968", "\"right\", \"just\", or \"true\"", "pastors", "meritocracy", "Vistula River", "arrows, swords, and leather shields", "23 November 1963", "since the 1960s", "zero net force", "2012", "CD4", "income inequality", "Tuition Fee Supplement", "April 1523", "the California State Automobile Association and the Automobile Club of Southern California", "Von Miller", "lymphocytes or an antibody-based humoral response", "unsuccessful", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Percy Shelley", "education, sanitation, and traffic control", "six divisions", "six series of theses", "Pi\u0142sudski", "Disney\u2013ABC Television Group", "SAP Center", "Genghis Khan", "materials melted near an impact crater", "William the Silent", "182 million tons", "John G. Trump"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8592347756410257}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4191", "mrqa_squad-validation-9426", "mrqa_squad-validation-368", "mrqa_squad-validation-3776", "mrqa_squad-validation-4769", "mrqa_squad-validation-9194", "mrqa_squad-validation-10305", "mrqa_squad-validation-2812", "mrqa_squad-validation-6559", "mrqa_squad-validation-2153", "mrqa_squad-validation-7246"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["occupational stress", "Sunni Arabs from Iraq and Syria", "The Lower Rhine", "Mike Tolbert", "silt up the lake", "cigarette advertising from all television and radio networks", "Dorothy and Michael Hintze", "climate change", "northwest", "Turkey", "Economist", "The Tyneside flat", "73", "Derek Jacobi", "ten minutes", "Sydney", "Basel", "ideal strings", "unstable molecules", "lasting damage", "December 2014", "other locations throughout Scotland", "eastern", "Newton", "Westwood One", "2008", "the colonies of British America and New France", "early 1546", "lower bounds", "2011", "became the University of Northumbria at Newcastle", "between 1859 and 1865", "8\u20134\u20134 system", "a green algal derived chloroplast", "over 200 awards", "William the Conqueror", "The Service Module was discarded", "18 million volumes", "north", "detention", "collenchyma tissue", "Louis Adamic", "cytokine T IGF-\u03b2", "England", "Yuri Gagarin", "antigenic variation", "Knaurs Lexikon", "Chester, South Carolina", "1992", "three", "9.1 million", "rich and well socially standing", "pharynx", "1969", "Wardenclyffe Tower", "Tower Theatre", "700,000", "Pittsburgh", "jeopardy/2516_Qs.txt at master  jedoublen/jeopardy", "Heroes struggle... Animated, Action, Adventure, Fantasy, Sci-Fi.... Lego Star Wars: The Yoda Ch", "American baseball, until the late 1940s, excluded, with some big exceptions in... The color line was broken for good when Jackie Robinson signed with the Brooklyn", "the hand of cards which he supposedly held at the time of his death... killed by the assassin Jack McCall in Deadwood, Black Hills, August 2, 1876", "His... (initial capital letter) a German-built enciphering machine developed for commercial", "Europe"], "metric_results": {"EM": 0.765625, "QA-F1": 0.809766865079365}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714285, 0.16, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9752", "mrqa_squad-validation-5549", "mrqa_squad-validation-8718", "mrqa_squad-validation-8891", "mrqa_squad-validation-9744", "mrqa_squad-validation-5337", "mrqa_squad-validation-8000", "mrqa_squad-validation-3885", "mrqa_squad-validation-1568", "mrqa_squad-validation-6523", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-4596"], "SR": 0.765625, "CSR": 0.796875, "EFR": 1.0, "Overall": 0.8984375}, {"timecode": 2, "before_eval_results": {"predictions": ["John Sutcliffe", "can also concentrate wealth, pass environmental costs on to society", "6800", "The British provided medical treatment for the sick and wounded French soldiers", "Ticonderoga Point", "October 16, 1973", "1980s", "seven", "books and articles for magazines and journals", "Roger NFL", "the oceans and seas", "2 million", "by over 100%", "1350", "North America", "Euclid's fundamental theorem of arithmetic", "Maria de la Queillerie", "to encourage investment", "Julia Butterfly Hill", "dry areas", "587,000", "March 1974", "quickly", "end of the 19th century", "DeMarcus Ware", "University of North Florida", "motivated students", "platyctenids", "elementary school education certificate", "1220", "two", "30.0%", "Raghuram Rajan", "VHF channel 7", "The Pink Triangle", "ideological", "Methodist institutions", "Ticonderoga", "six", "Los Angeles Dodgers", "19th", "a course of study", "a delay costs money", "Funchess", "Watt", "1,000 m3/s", "Thuringia", "rivers", "anti-Semitic policies", "visor helmet", "Catholic", "Hollywood", "Long Island Sound", "Sweden's", "an invaluable service as usurers in medieval society", "an African American", "first woman governor", "an athlete who plays cricket", "a type of falcon", "Alaska", "an Austrian and American film actress and inventor", "an English ship", "the Association of American Universities", "50JJB Sports Fitness Clubs"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6990767045454545}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 0.6, 0.5, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665]}}, "before_error_ids": ["mrqa_squad-validation-7525", "mrqa_squad-validation-10258", "mrqa_squad-validation-1565", "mrqa_squad-validation-85", "mrqa_squad-validation-802", "mrqa_squad-validation-10114", "mrqa_squad-validation-9061", "mrqa_squad-validation-8322", "mrqa_squad-validation-1960", "mrqa_squad-validation-5678", "mrqa_squad-validation-2786", "mrqa_squad-validation-1877", "mrqa_squad-validation-2497", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-13452", "mrqa_searchqa-validation-12341", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-5603"], "SR": 0.640625, "CSR": 0.7447916666666667, "EFR": 0.9565217391304348, "Overall": 0.8506567028985508}, {"timecode": 3, "before_eval_results": {"predictions": ["public schools", "connection id in a table", "males", "electrical, water, sewage, phone, and cable facilities", "the level of the top tax rate", "\"Wise up or die.\"", "VideoGuard UK", "highly-paid", "pump this into the mesoglea", "Fred Silverman", "atmospheric engine", "force", "ctenophores", "the trial and rehabilitation of Joan of Arc", "John Hurt", "an Australian public X.25 network operated by Telstra", "Arizona Cardinals", "Von Miller", "Indianapolis Colts", "42%", "1957", "if government is \u201cthe voice of the people,\u201d as it is often called, shouldn\u2019t that voice be heeded", "orogenic wedges", "one", "Catholic", "Edict of Fontainebleau", "Fort Caroline", "Pittard Sullivan", "wealth", "Niagara Falls", "Hugh Downs", "the Saracens", "3D printing technology", "Daniel Burke", "internal strife", "Matthew Murray", "400 AD", "the United States", "Satya Nadella", "the difference between a problem and an instance", "Richard E. Grant, Jim Broadbent, Hugh Grant and Joanna Lumley", "Inner Mongolia", "cortisol and catecholamines", "small protein complexes about 40 nanometers across", "isopentenyl pyrophosphate synthesis", "1963", "his hotel room", "Italy", "Joan Bath", "he Bumble", "Khartoum", "William Henry Harrison", "Playboy rabbit", "Type O-positive personalities", "Puerto Rico", "Court TV", "not in Egypt", "The Prairie Wolf", "\"Inhospitable Sea\"", "he became Israel's Minister of Defense", "Joan Van Dinh", "active athletes", "helicopters and boats", "$17,000"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6416304181929182}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4615384615384615, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.4444444444444445, 0.4615384615384615, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7103", "mrqa_squad-validation-7357", "mrqa_squad-validation-4452", "mrqa_squad-validation-3247", "mrqa_squad-validation-230", "mrqa_squad-validation-363", "mrqa_squad-validation-6967", "mrqa_squad-validation-10083", "mrqa_squad-validation-3139", "mrqa_squad-validation-5542", "mrqa_squad-validation-1670", "mrqa_squad-validation-7885", "mrqa_squad-validation-6263", "mrqa_squad-validation-8839", "mrqa_squad-validation-7711", "mrqa_searchqa-validation-8170", "mrqa_searchqa-validation-12199", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-14454", "mrqa_searchqa-validation-6624", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-3588"], "SR": 0.578125, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 4, "before_eval_results": {"predictions": ["whether he stood by their contents", "1850s", "Troika", "complexity classes", "Parliament of the United Kingdom at Westminster", "teaching", "Dancing with the Stars", "8 November 2010", "it may have been a combination of anthrax and other pandemics", "coastal beaches and the game reserves", "1524", "2p \u2212 1", "horizontal", "Crash the Super Bowl", "collenchyma tissue", "somewhere around a billion years ago", "Croatia", "Port of Long Beach", "Edinburgh", "McManus", "Papin", "Cricket", "William Morris", "T. J. Ward", "the Daleks", "San Diego", "Saracen", "heat and pressure", "1072", "Chevron", "Africa", "New York City", "Marshall Cohen", "Hypersensitivity", "Business Connect", "Henry Young Darracott Scott", "European Court of Justice held that a Commissioner giving her dentist a job, for which he was clearly unqualified, did in fact not break any law", "income inequality", "Bainbridge's", "1294", "chao", "Apollo 17", "8 Mile", "Helen Thorpe", "Stephen Hawking", "Hoooterville", "Europe", "Helium", "San Martn", "Dracula Anderson's", "Ghinegraves", "Moscow", "the crystal anniversary", "prairie crocus", "Detroit River", "Cleveland", "the New Testament", "Dublin", "\"suits\"", "the space-time continuum", "Albert Einstein", "Torchwood", "1990", "Zimbabwe"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7149557571684588}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5878", "mrqa_squad-validation-5029", "mrqa_squad-validation-8685", "mrqa_squad-validation-821", "mrqa_squad-validation-5344", "mrqa_squad-validation-7615", "mrqa_squad-validation-2679", "mrqa_squad-validation-1061", "mrqa_squad-validation-4147", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-9601", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-6929", "mrqa_triviaqa-validation-3650"], "SR": 0.671875, "CSR": 0.696875, "EFR": 1.0, "Overall": 0.8484375}, {"timecode": 5, "before_eval_results": {"predictions": ["November 2006", "achievement-oriented", "a malfunction in the chameleon circuit", "SAP Center", "March 2011", "above the top of the range", "12th", "1226", "a special episode of The Late Late Show", "a bird", "P", "theNP-complete knapsack problem", "1928", "when the immune system is less active than normal", "disturbances", "Fraud", "from 1562 to 1598", "Emmerich Rhine Bridge", "ten", "1993", "chloroplasts", "oxygen", "US$100,000", "Bakersfield", "Bruno Mars", "patients' prescriptions and patient safety issues", "2009", "the Common Core", "Recognized Student Organizations", "a cubic interpolation formula", "Thomas Edison", "the University of Paris", "phagocytes", "15", "Satyagraha", "two", "microorganisms", "1968", "Aloha \u02bbOe", "over 1.6 million", "Eric Whitacre", "gourd-bows", "Malcolm Young", "New York City", "the waltz Gunstwerber", "Tennessee River", "Odisha", "January 28, 2016", "138,535", "Adam Rex", "1933", "Sivakumar, S. V. Subbaiah, Jayachitra, Srividya, Shubha, Kamal Haasan and Jayasudha", "1968", "a Chaplain to the Forces", "astronomer", "Warrington", "1866", "Chattahoochee", "the Provisional Irish Republican Army", "pneumonoultramicroscopicsilicovolcanoconiosis", "Boutros Ghali", "Vertikal-T", "The Wizard of Oz", "was angry over the treatment of Muslims"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6909855769230768}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8531", "mrqa_squad-validation-434", "mrqa_squad-validation-9185", "mrqa_squad-validation-1759", "mrqa_squad-validation-1860", "mrqa_squad-validation-8696", "mrqa_squad-validation-3496", "mrqa_squad-validation-2634", "mrqa_squad-validation-664", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-2158", "mrqa_newsqa-validation-1468", "mrqa_searchqa-validation-13492", "mrqa_newsqa-validation-3290"], "SR": 0.609375, "CSR": 0.6822916666666667, "EFR": 1.0, "Overall": 0.8411458333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["UHF", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "Africa", "1976", "inverse", "computational problems", "60", "Warsaw", "5,000 years", "Edgar Atheling", "Writers Guild of America", "UNEP", "Conservative", "11 million members", "a water-cooled undergarment", "the Queen", "3 in 1,000,000", "2009 onwards", "Super Bowl Opening Night", "Fresno Street and Thorne Ave", "southern and central parts of France", "2014", "40%", "Dwight D. Eisenhower", "innate immune system", "15\u20131", "the history of arms", "Industry and manufacturing", "this contact with nature made him stronger, both physically and mentally.", "1543", "\u015ar\u00f3 Donkeyie\u015bcie", "Hmong or Laotian", "Stromules", "ignition sources are minimized", "Johnny Herbert", "jus sanguinis", "Pharrell Williams", "James Dearden", "J\u00f3zsef Pulitzer", "June 17, 2007", "The Frost Report", "the National Basketball Development League", "Danish", "169 CE", "Kealakekua Bay", "Dave Lee Travis", "Northrop P-61 Black widow", "People v. Turner", "\"Veyyil\"", "Helena Sternlicht", "Highlands Course", "Illinois", "Cartoon Cartoon Fridays", "Timo Hildebrand", "2016", "Ginger Rogers", "the Corps of Discovery", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "1,281,900 servicemembers", "giraffe", "navy", "Ecuador", "Abraham Lincoln", "Surface Runoff"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6971153846153846}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-10424", "mrqa_squad-validation-1730", "mrqa_squad-validation-1114", "mrqa_squad-validation-8523", "mrqa_squad-validation-10107", "mrqa_squad-validation-4070", "mrqa_squad-validation-7770", "mrqa_squad-validation-1232", "mrqa_squad-validation-1009", "mrqa_squad-validation-3483", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2183", "mrqa_naturalquestions-validation-4953", "mrqa_newsqa-validation-3782", "mrqa_searchqa-validation-5374"], "SR": 0.640625, "CSR": 0.6763392857142857, "EFR": 1.0, "Overall": 0.8381696428571428}, {"timecode": 7, "before_eval_results": {"predictions": ["convection of the mantle", "Continental Edison Company in France", "South", "T. J. Ward", "25 minutes of transmission length", "1903", "1993", "King George III", "occupancy permit", "Hereford", "many individuals in the LDS Church, often a trusted friend, who may hold any office, from Elder to Bishop, or no office at all", "Arts & Entertainment Television", "NASA", "Albert Einstein", "Karl von Miltitz", "a matter of custom or expectation, such as isolating businesses to a business district and residences to a residential district", "Silk Road", "a war erupted in the Philippines", "39", "Bolshevik leaders", "University of Aberdeen", "transportation, sewer, hazardous waste and water", "plague of Athens in 430 BC", "Pedro Men\u00e9ndez de Avil\u00e9s", "expansion", "a deterministic Turing machine", "linear", "when the oxygen concentration is too high", "1290", "17,786,419,", "smallest state on the Australian mainland", "Montreal", "7000301604928199000", "was an incident on March 5, 1770, in which British Army soldiers shot and killed people while under attack by a mob", "Pakistan, India, and Bangladesh are among the largest individual contributors with around 8,000 units each", "2000", "Anna Faris", "Samantha Jo", "the human hands and face", "Aldis Hodge", "Steve Hale", "a multilayer", "January 2017 patch", "Idaho", "September 6, 2019", "multinational retail corporation", "Glenn Close", "Jack Gleeson", "claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "when the forward reaction proceeds at the same rate as the reverse reaction", "heart", "writ of certiorari", "is a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Shawn", "September 30", "Kelly Osbourne, Ian `` Dicko '' Dickson,", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "a Californio nobleman and master living in Los Angeles during the era of Spanish rule", "\"Household Words\"", "56", "Hindu scriptures", "St. Barnabus", "a vowel", "gold"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6753361222111223}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.8571428571428571, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.07692307692307691, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1239", "mrqa_squad-validation-2315", "mrqa_squad-validation-6000", "mrqa_squad-validation-6878", "mrqa_squad-validation-10007", "mrqa_squad-validation-360", "mrqa_squad-validation-1819", "mrqa_squad-validation-2881", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-3491", "mrqa_triviaqa-validation-7579", "mrqa_searchqa-validation-12472", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3744"], "SR": 0.5625, "CSR": 0.662109375, "EFR": 0.9285714285714286, "Overall": 0.7953404017857143}, {"timecode": 8, "before_eval_results": {"predictions": ["ivory", "The Dornbirner Ach", "a certain number of teacher's salaries are paid by the State", "non-deterministic time", "five", "December 2014", "an inauspicious typhoon", "four", "prophet", "10 July 1856 \u2013 7 January 1943", "1999", "CBS Sports apps on tablets, Windows 10, Xbox One and other digital media players", "3\u20132.7 billion years ago", "the New Testament from Greek", "Von Miller", "economists with the Standard & Poor's rating agency", "Germany", "a vestigial red algal derived chloroplast", "two or more teachers working harmoniously to fulfill the needs of every student in the classroom", "a computer network funded by the U.S. National Science Foundation (NSF)", "type III secretion system", "worker, capitalist/business owner, landlord", "tungsten", "the state (including the judges)", "the oxidant", "he may have intercepted Marconi's European experiments in July 1899", "the American Revolution", "Book of Discipline", "Fat Albert", "1943", "Big Fucking German", "Chelmsford City", "William Novak", "22,500 acres", "1951", "Abu Dhabi, United Arab Emirates", "86,112", "American", "the Firth of Forth Site of Special Scientific Interest", "one live albums, one compilation album, one video album, twelve extended plays, nineteen singles and fourteen music videos", "Blue Valley Northwest High School", "The Birds", "Battle of the Rosebud", "Homebrewing", "Pablo Escobar", "Brian A. Miller", "26 June 2013", "25 million records", "320 years (1206\u20131526)", "Geraldine Sue Page", "Rochdale, North West England", "Charles Reed Bishop", "marine applications", "Marco Fu", "2015", "Ian Fleming", "her translation of and commentary on Isaac Newton's book \"Principia\" containing basic laws of physics", "BeBe Winans", "Henry VIII", "July", "purple", "Dumont d'Urville Station", "under normal conditions", "a spiritual conversion"], "metric_results": {"EM": 0.625, "QA-F1": 0.7488988824696545}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6, 1.0, 0.125, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.8, 0.8, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_squad-validation-2359", "mrqa_squad-validation-1156", "mrqa_squad-validation-531", "mrqa_squad-validation-2272", "mrqa_squad-validation-1912", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-391", "mrqa_triviaqa-validation-4490", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-5851"], "SR": 0.625, "CSR": 0.6579861111111112, "EFR": 1.0, "Overall": 0.8289930555555556}, {"timecode": 9, "before_eval_results": {"predictions": ["1999", "zero", "Mao Zedong", "Chebyshev", "1960", "introduction of Beroe", "1000 CE", "Orkney Vikings", "Sunspot, New Mexico", "Sonderungsverbot", "amending treaty", "environment in which they lived", "a genetic disease", "C. J. Anderson", "Cadeby", "Warraghiggey", "starts accidentally adding oxygen to sugar precursors", "World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP", "Sunni pan-Islamism", "11 points", "yes or no, or alternately either 1 or 0", "black jerseys with matching white pants", "the Mongols", "English", "Newton", "1940s and 1950s", "arthur of Ord and Tore on the Black Isle, in Ross and Cromarty, Scotland", "music arranger and pianist", "Taoiseach", "Duval County", "Bill Ponsford", "$10\u201320 million", "Manasseh Cutler Hall", "Denmark", "Hindi", "the \"Pour le M\u00e9rite\" 1", "Giuseppe Verdi", "Edward Trowbridge Collins Sr.", "1930s and 1940s", "Christopher McCulloch", "2016\u201317", "Carson City", "Wembley Stadium", "19th", "Cesar Millan", "Bob Dylan", "Michael Lewis Greenwell", "21 days of major combat operations", "The Life of Charlotte Bront\u00eb", "2015", "Bill Curry", "Jack White", "Kim Yoon-seok and Ha Jung-woo", "superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe", "Rick Barry", "18th congressional district", "the BBC", "2008", "Ringo Starr", "arthur", "Musa Qala", "arvaquin, Avelox, Noroxin and Floxin", "arthur", "Jamaica"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7054308356117567}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.0, 0.5, 0.7272727272727272, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5263157894736842, 0.4, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9897", "mrqa_squad-validation-1025", "mrqa_squad-validation-8832", "mrqa_squad-validation-260", "mrqa_squad-validation-1652", "mrqa_squad-validation-502", "mrqa_hotpotqa-validation-2670", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-2645", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_searchqa-validation-13221"], "SR": 0.578125, "CSR": 0.65, "EFR": 1.0, "Overall": 0.825}, {"timecode": 10, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3894", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4633", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5145", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5527", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-690", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-599", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9687", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-13863", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14454", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16753", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-4300", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6624", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-9383", "mrqa_squad-validation-10007", "mrqa_squad-validation-10010", "mrqa_squad-validation-10031", "mrqa_squad-validation-10083", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10130", "mrqa_squad-validation-10153", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10234", "mrqa_squad-validation-10249", "mrqa_squad-validation-10258", "mrqa_squad-validation-10305", "mrqa_squad-validation-10345", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10478", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1073", "mrqa_squad-validation-1075", "mrqa_squad-validation-1075", "mrqa_squad-validation-1096", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1232", "mrqa_squad-validation-1239", "mrqa_squad-validation-1254", "mrqa_squad-validation-1296", "mrqa_squad-validation-1372", "mrqa_squad-validation-1529", "mrqa_squad-validation-1543", "mrqa_squad-validation-1586", "mrqa_squad-validation-1632", "mrqa_squad-validation-1652", "mrqa_squad-validation-1681", "mrqa_squad-validation-1723", "mrqa_squad-validation-1730", "mrqa_squad-validation-1731", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-1819", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1877", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-1976", "mrqa_squad-validation-1985", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2111", "mrqa_squad-validation-2153", "mrqa_squad-validation-2181", "mrqa_squad-validation-2189", "mrqa_squad-validation-2246", "mrqa_squad-validation-2247", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2320", "mrqa_squad-validation-2359", "mrqa_squad-validation-2413", "mrqa_squad-validation-2442", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2568", "mrqa_squad-validation-2591", "mrqa_squad-validation-260", "mrqa_squad-validation-2617", "mrqa_squad-validation-2628", "mrqa_squad-validation-2634", "mrqa_squad-validation-2644", "mrqa_squad-validation-2679", "mrqa_squad-validation-2721", "mrqa_squad-validation-2723", "mrqa_squad-validation-2765", "mrqa_squad-validation-2808", "mrqa_squad-validation-2812", "mrqa_squad-validation-2881", "mrqa_squad-validation-2941", "mrqa_squad-validation-2949", "mrqa_squad-validation-2975", "mrqa_squad-validation-2977", "mrqa_squad-validation-30", "mrqa_squad-validation-3111", "mrqa_squad-validation-3139", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3248", "mrqa_squad-validation-3269", "mrqa_squad-validation-3296", "mrqa_squad-validation-3377", "mrqa_squad-validation-3429", "mrqa_squad-validation-3483", "mrqa_squad-validation-3496", "mrqa_squad-validation-3534", "mrqa_squad-validation-3552", "mrqa_squad-validation-360", "mrqa_squad-validation-363", "mrqa_squad-validation-368", "mrqa_squad-validation-3705", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3776", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3885", "mrqa_squad-validation-3952", "mrqa_squad-validation-3981", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4070", "mrqa_squad-validation-4095", "mrqa_squad-validation-4107", "mrqa_squad-validation-4121", "mrqa_squad-validation-4121", "mrqa_squad-validation-4147", "mrqa_squad-validation-4191", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4516", "mrqa_squad-validation-457", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4689", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4758", "mrqa_squad-validation-4797", "mrqa_squad-validation-480", "mrqa_squad-validation-4840", "mrqa_squad-validation-4898", "mrqa_squad-validation-4988", "mrqa_squad-validation-4997", "mrqa_squad-validation-502", "mrqa_squad-validation-5029", "mrqa_squad-validation-5061", "mrqa_squad-validation-5096", "mrqa_squad-validation-5108", "mrqa_squad-validation-5200", "mrqa_squad-validation-5222", "mrqa_squad-validation-5270", "mrqa_squad-validation-5272", "mrqa_squad-validation-5287", "mrqa_squad-validation-531", "mrqa_squad-validation-5337", "mrqa_squad-validation-5344", "mrqa_squad-validation-5347", "mrqa_squad-validation-5382", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5549", "mrqa_squad-validation-5551", "mrqa_squad-validation-556", "mrqa_squad-validation-5621", "mrqa_squad-validation-5670", "mrqa_squad-validation-5741", "mrqa_squad-validation-5794", "mrqa_squad-validation-5839", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6000", "mrqa_squad-validation-6027", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6259", "mrqa_squad-validation-6260", "mrqa_squad-validation-6263", "mrqa_squad-validation-6277", "mrqa_squad-validation-6295", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-65", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6538", "mrqa_squad-validation-6548", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6600", "mrqa_squad-validation-6612", "mrqa_squad-validation-6624", "mrqa_squad-validation-664", "mrqa_squad-validation-6656", "mrqa_squad-validation-6657", "mrqa_squad-validation-6666", "mrqa_squad-validation-6695", "mrqa_squad-validation-6749", "mrqa_squad-validation-6858", "mrqa_squad-validation-6861", "mrqa_squad-validation-6878", "mrqa_squad-validation-6880", "mrqa_squad-validation-6888", "mrqa_squad-validation-689", "mrqa_squad-validation-6898", "mrqa_squad-validation-6900", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-6967", "mrqa_squad-validation-7018", "mrqa_squad-validation-7021", "mrqa_squad-validation-7033", "mrqa_squad-validation-7036", "mrqa_squad-validation-7062", "mrqa_squad-validation-7123", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7246", "mrqa_squad-validation-7268", "mrqa_squad-validation-7302", "mrqa_squad-validation-7312", "mrqa_squad-validation-7323", "mrqa_squad-validation-7357", "mrqa_squad-validation-7362", "mrqa_squad-validation-7373", "mrqa_squad-validation-738", "mrqa_squad-validation-7391", "mrqa_squad-validation-741", "mrqa_squad-validation-7450", "mrqa_squad-validation-7458", "mrqa_squad-validation-7466", "mrqa_squad-validation-7470", "mrqa_squad-validation-755", "mrqa_squad-validation-7562", "mrqa_squad-validation-7603", "mrqa_squad-validation-764", "mrqa_squad-validation-767", "mrqa_squad-validation-7686", "mrqa_squad-validation-7711", "mrqa_squad-validation-7736", "mrqa_squad-validation-7744", "mrqa_squad-validation-7765", "mrqa_squad-validation-7770", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7892", "mrqa_squad-validation-7902", "mrqa_squad-validation-7957", "mrqa_squad-validation-7970", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8232", "mrqa_squad-validation-8236", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8572", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8685", "mrqa_squad-validation-8696", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8743", "mrqa_squad-validation-8760", "mrqa_squad-validation-8763", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8794", "mrqa_squad-validation-8797", "mrqa_squad-validation-8832", "mrqa_squad-validation-8837", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-8965", "mrqa_squad-validation-904", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9236", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-9317", "mrqa_squad-validation-9330", "mrqa_squad-validation-938", "mrqa_squad-validation-9401", "mrqa_squad-validation-9426", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-9579", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9628", "mrqa_squad-validation-9744", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9817", "mrqa_squad-validation-9890", "mrqa_squad-validation-9892", "mrqa_squad-validation-9897", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7579"], "OKR": 0.900390625, "KG": 0.41875, "before_eval_results": {"predictions": ["Thermochemical techniques", "head writer and executive producer", "1987", "James O. McKinsey", "North", "one-eighth the number of French Catholics", "coastal beaches and the game reserves", "Vicodin", "\u00a34.2bn", "Katharina von Bora", "9 March 1508", "Shoushi Li", "his means of seizing power", "ideal strings", "after the Franco-German War", "German", "San Andreas Fault", "NYPD Blue", "Northern Chinese", "Oireachtas funds", "the Marconi Company", "countries with bigger income inequalities", "John Robert Cocker", "$10,000 Kelly", "U.S. Army", "Richard Masur", "1988", "Bergen County", "The Ones who Walk Away from Omelas", "hiphop", "Lithuanian national team", "historic buildings, arts, and published works", "Disney Parks Christmas Day Parade", "Esp\u00edrito Santo Financial Group", "Guillermo del Toro", "Wolf Creek", "YouTube", "onset and progression of Alzheimer's disease", "San Francisco 49ers", "Lake Placid, New York", "Iron Man 3", "Bury St Edmunds", "singer, songwriter, actress, and radio and television presenter", "Chicago", "Martin \"Marty\" McCann", "247,597", "Mandalay Entertainment", "actor", "EA-18G Growler", "Barnoldswick", "Asia-Pacific War", "A41 road", "Heather Elizabeth Langenkamp", "Leona Lewis", "The Ministry of Utmost Happiness", "BAFTA TV Award Best Actor winner in 1956", "Rodney Crowell", "Andy Serkis", "having or seeing nosebleeds or bleeding to death", "a\u00efda", "Musharraf", "cancer", "a small and welcoming environment for Jewish", "by heating sodium azide, NaN3"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7344398656898657}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2976", "mrqa_squad-validation-1480", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-99", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1133", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-7417", "mrqa_newsqa-validation-850", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-5418"], "SR": 0.640625, "CSR": 0.6491477272727273, "EFR": 0.9565217391304348, "Overall": 0.7318370182806324}, {"timecode": 11, "before_eval_results": {"predictions": ["1887", "semi-arid savanna", "2014", "Levi's Stadium", "misguided", "San Jose Marriott", "1972", "second-largest global producer", "Decision Time", "Victorian Government", "American Revolutionary War", "pep rally", "human", "the Treaties establishing the European Union", "heat shield to survive a trans-lunar reentry", "Amazonia", "Daniel Andrews", "UNESCO's World Heritage list", "Richard E. Grant", ", 49\u201315,", "NCAA's Division I", "Mark Helfrich", "Wal-Mart Canada Corp.", "\"Louie\" Zamperini", "Che Guevara", "Carol Ann Duffy", "Karl-Anthony Towns Jr.", "1978", "Danish", "Ukrainian", "1977\u20132012", "\"John\" Alexander Florence", "\"brainwash\"", "9Lives", "\"valley of the hazels'", "Art Bell", "Lady Frederick Windsor", "Eminem", "Point Place", "Knowlton School", "Delilah Rene", "Don Bluth", "Columbus, Ohio", "Czech Kingdom", "Jon M. Chu", "Sacramento Kings", "1984 South Asian Games", "Tufts College", "Harrods", "Toni Braxton", "\"Girl Meets World\"", "Dakota Johnson", "The City of Newcastle", "Nippon Professional Baseball", "Canada", "in the pancreas", "privatized", "denarius", "insects", "was attacked by small-arms, machine-propelled grenades and \"multiple others from a nearby building where soldiers were taking RPG and machine gun fire,\"", "\"the most dangerous precedent in this country, violating all of our due process rights,\"", "\"Annie Get Your Gun\"", "New York", "Hebrew"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6929349296536795}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1219", "mrqa_squad-validation-8447", "mrqa_squad-validation-7288", "mrqa_squad-validation-4015", "mrqa_squad-validation-234", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-3881", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-5556", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-953", "mrqa_triviaqa-validation-818", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-3580", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7583"], "SR": 0.578125, "CSR": 0.6432291666666667, "EFR": 1.0, "Overall": 0.7393489583333335}, {"timecode": 12, "before_eval_results": {"predictions": ["late 1545", "several hundred thousand, some 30%", "five", "every two years", "one-man", "The Hoppings", "Mycobacterium tuberculosis", "C. J. Anderson", "Harvey Martin", "stratigraphic", "environmental determinism", "Wellington", "problem instance", "at rest", "a sin", "Small subunit ribosomal RNAs in several Chlorophyta and euglenid chloroplasts lack motifs", "Alemannic", "Edmonton, Canada", "Tony Burke", "Eugene O'Neill", "Max Kellerman", "created the American Land-Grant universities and colleges", "VfL Wolfsburg", "1946", "Julia Verdin", "Pendlebury", "McLaren-Honda", "Bismarck", "Comedy Film Nerds", "2016 World Indoor Championships", "MG Cars", "January 18, 1977", "The O2 Arena", "The Soloist", "Nikita Khrushchev", "Hal Linden", "Prime Minister of Pakistan", "February 18, 1965", "automobiles", "Republican", "Allison J71", "Chad", "NBA All-Star Game and All-NBA Team", "Emilia Fox", "Freeform", "Mark Masons' Hall", "Law Adam", "American", "Chief Strategy Officer", "November 15, 1903", "De La Soul", "American", "Archbishop of Canterbury", "Via Vai", "1985", "after releasing Xander from the obligation to be Sweet's `` bride ''", "when the cell is undergoing the metaphase of cell division", "California", "a multi-user real-time virtual world described entirely in text", "Dubai", "Iran", "Halle Berry", "Sindbad", "Abid Ali Neemuchwala"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7454842032967033}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-914", "mrqa_squad-validation-3765", "mrqa_squad-validation-8852", "mrqa_squad-validation-9190", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-3071", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-4362", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-3242", "mrqa_searchqa-validation-13537"], "SR": 0.671875, "CSR": 0.6454326923076923, "EFR": 1.0, "Overall": 0.7397896634615385}, {"timecode": 13, "before_eval_results": {"predictions": ["Tower District", "computational problem", "\"social and political action,\"", "Duran Duran", "30", "NYPD Blue", "chemical bonds", "John and Benjamin Green", "Lippe", "between AD 0\u20131250", "2 million", "a statement to the chamber setting out the Government's legislative programme for the forthcoming year", "40,000", "Citadel Broadcasting", "more than $45,000", "stream capture", "400 feet", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "Robert Remak", "Eddie Murphy", "Audrey II", "Human fertilization", "balance sheet", "terrestrial", "Peter Andrew Beardsley MBE", "Leo Arnaud", "lumbar cistern", "to universalize the topic of the song into something everyone could relate to and ascribe personal meaning to in their own way", "1 US dollar worth close to 5,770 guaranies", "digitization of social systems", "Yugoslav model of state organization, as well as a `` middle way '' between planned and liberal economy", "Middlesex County", "Sweden had been an active supporter of the League of Nations", "2009", "lightning", "they were weaker when it came to training and tertiary education", "AIM", "George Strait", "silk, hair / fur", "Anakin Luke", "Prince James, Duke of York and of Albany", "Paspahegh Indians", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Florida", "Toronto City Airport", "Manchuria", "Ben Savage", "at a given temperature", "Max", "Magyarorsz\u00e1g z\u00e1szlaja", "the church at Philippi", "2003", "The Sun", "northern China", "Mackinac Bridge", "Barbarella", "Bergen", "Balvenie Castle", "scraped together his last salary, some money he made from trading sugar bought at a discount from the supermarket where he worked, and funds borrowed from friends to secure a visitor's visa and bus ticket", "South Africa where locals are jobless, hungry and in need of basic services such as clean water, sanitation and housing", "Frida Kahlo", "\"to pay lip service to someone/something\"", "Elizabeth Gaskell", "Walter Reed Army Medical Center"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6275353461945031}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.25, 0.13953488372093023, 1.0, 1.0, 0.0, 0.1904761904761905, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.18181818181818182, 0.0, 0.5454545454545454, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3599", "mrqa_squad-validation-4304", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-6020", "mrqa_hotpotqa-validation-877", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-16103", "mrqa_hotpotqa-validation-3149"], "SR": 0.5625, "CSR": 0.6395089285714286, "EFR": 1.0, "Overall": 0.7386049107142858}, {"timecode": 14, "before_eval_results": {"predictions": ["reaffirmed Catholicism as the state religion of France, but granted the Protestants equality with Catholics under the throne and a degree of religious and political freedom within their domains", "reached an all-time high between 2005 and 2010", "the Marches", "public policy", "cartels", "Anglo-Saxon populations", "Ancient Egypt", "Battle of Olustee", "patrimonial feudalism", "Daniel arap Moi", "near Millingen aan de Rijn", "an electric lighting company in Tesla's name, Tesla Electric Light & Manufacturing", "1303", "oxidant", "three", "Lance Cpl. Maria Lauterbach", "1994", "Empire of the Sun", "54 bodies", "Roger Federer", "a cancer-causing toxic chemical", "the two remaining crew members from the helicopter", "July for A Country Christmas", "citizenship", "40", "Six", "to alleviate the flooding", "Expedia", "Kabul", "\"I'm just getting started.\"", "Communist Party of Nepal", "Bob Dole", "Eden Park", "in her home for 12 of the past 18 years", "12.3 million people worldwide", "2050", "in all of Lifeway's 100-plus stores nationwide", "Osan Air Base", "18", "National Park Service", "AS Roma beat Lecce 3-2", "Bob Bogle", "40 militants and six Pakistan soldiers dead", "1959", "Pakistan's High Commission in India", "his father", "President Obama's race", "U.S. President-elect Barack Obama", "the Obama administration", "Ed McMahon", "peace sign", "Muslim festival of Eid al-Adha", "Larry Ellison", "Revolutionary Armed Forces of Colombia", "an unknown recipient", "Jules Shear", "Soviet Union", "Vienna", "2008\u201309 UEFA Champions League", "310", "New England", "Achaemenid Empire", "2017", "Algernod Lanier Washington"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6352192078754579}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false], "QA-F1": [0.8095238095238095, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.5, 0.6666666666666666, 0.4, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3163", "mrqa_squad-validation-1886", "mrqa_squad-validation-1064", "mrqa_squad-validation-7017", "mrqa_squad-validation-1287", "mrqa_squad-validation-3532", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-324", "mrqa_hotpotqa-validation-2080", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-1723", "mrqa_hotpotqa-validation-5370"], "SR": 0.515625, "CSR": 0.63125, "EFR": 0.967741935483871, "Overall": 0.7305015120967743}, {"timecode": 15, "before_eval_results": {"predictions": ["Robert R. Gilruth", "Arthur Woolf", "Ten", "multiple revisions, demonstrating Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "seven", "Funchess", "St. George's United Methodist Church", "1914", "Wales", "more than 70,000", "Nikita Khrushchev", "increasing unemployment", "X is no more difficult than Y, and we say that X reduces to Y", "March 22", "he acted in self defense in punching businessman Marcus McGhee", "anyone", "Superman had been fighting crime in print since 1938,", "Peruvians", "power-sharing talks to take place in the next few weeks", "Senate", "15,000", "Kim Jong Un", "Tim Baker", "Philip Markoff", "Democratic", "in Japan", "North Korea intends to launch a long-range missile in the near future,", "District of Columbia National Guard", "forgery and flying without a valid license", "work for Grayback Forestry in Medford, Oregon", "14", "All three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "school", "Monday and Tuesday", "27-year-old", "almost 100 vessels off Somalia's coast", "Venus Williams", "allergies to peanuts, nuts, shellfish, peanuts, tree nuts, wheat and soy", "more than two years,", "Manchester United", "Nafees A. Syed", "Robert Barnett", "military commissions", "Alfredo Astiz,", "American girl", "Illness", "procedures", "Jaime Andrade", "56", "Michael Jackson", "High Court Judge Justice Davis", "racially motivated", "Adam Lambert", "Arlington National Cemetery's Section 60", "as early as 1571, with exports to other states occurring around 1858", "Del Norte and Humboldt Counties", "citric", "Denali", "Cond\u00e9 Nast", "death", "Mason", "high and dry", "capitol building", "New Orleans Saints"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6535283521303258}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 0.888888888888889, 0.2857142857142857, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-1748", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-4202", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2052", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-6596", "mrqa_triviaqa-validation-210", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-4897", "mrqa_searchqa-validation-3948", "mrqa_hotpotqa-validation-3685"], "SR": 0.546875, "CSR": 0.6259765625, "EFR": 0.9655172413793104, "Overall": 0.7290018857758621}, {"timecode": 16, "before_eval_results": {"predictions": ["\"exterminate\" all non-Dalek beings", "San Diego", "30\u201375%", "\"to implement Islamic values in all spheres of life.\"", "James Watt", "comb jellies", "NewcastleGateshead", "1989", "priest", "Denmark, Iceland and Norway", "the need for alliances", "Anderson", "1969", "Debbie Gibson", "to collect menstrual flow", "at least 18 or 21 years old", "the negative of the base 10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "during the eighth series of the UK version of The X Factor", "April 6, 1917", "three", "Montgomery", "Upon closure at birth", "it was to last four years unless renewed by the Reichstag", "December 1, 2009", "Miami Heat", "Timothy B. Schmit", "Alan Shearer", "The 1700 Cascadia earthquake", "throughout Mexico, in particular the Central and South regions, and by people of Mexican ancestry living in other places, especially the United States", "Portugal. The Man", "in muscles", "1960", "a two - year terms", "Harry", "September 19, 2017", "Andy Serkis", "The Abbott and Costello Show", "John Smith", "Idaho", "Ali", "James Hutton", "Jason Paige", "Jackie Robinson", "Rufus and Chaka Khan", "the body - centered cubic ( BCC ) lattice", "Kimberlin Brown", "merengue", "3", "Olivia Olson", "erosion", "Office of Inspector General", "an integral membrane protein", "Conrad Lewis", "Justice Harlan", "a person employed to write or type what another dictates or to copy what has been written by another,", "Purple Rain", "Oakland, California", "World Famous Gold & Silver Pawn Shop", "Crandon, Wisconsin,", "a racially-tinged remark made by his former caddy,", "Psycho", "sapphire", "Jimmy Carter", "yellow"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7213915993010821}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false], "QA-F1": [0.888888888888889, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5517241379310345, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.42857142857142855, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7778", "mrqa_squad-validation-9610", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9811", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-3385", "mrqa_triviaqa-validation-1394", "mrqa_hotpotqa-validation-5030", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-3751", "mrqa_triviaqa-validation-262"], "SR": 0.640625, "CSR": 0.6268382352941176, "EFR": 1.0, "Overall": 0.7360707720588235}, {"timecode": 17, "before_eval_results": {"predictions": ["diversity", "\"Provisional Registration\"", "the Tyne and wear Metro", "non-combustible substances that corrode, such as iron", "Creon", "QuickBooks", "1892", "The coordinating lead authors", "Six", "ideal pulleys", "\u00d6gedei Khan", "Princes Park", "47", "Polk", "Mrs. Eastwood & Company", "first train robbery", "Las Vegas", "General Manager", "Owsley Stanley", "The visit", "First Division", "Unbreakable", "plays for Turkish club Be\u015fikta\u015f", "\"The Maze Runner\"", "Agra", "1.6 million", "actress", "Gaius Julius Caesar Augustus Germanicus", "Jeff Van Gundy", "Thomas Robsahm", "Tamil", "1972", "2013", "Golden Globe Award for Best Actor", "Ron Goldman", "1", "Cleopatra VII Philopator", "political", "Tomorrowland", "16,116", "footballer", "Bishop's Stortford", "JoJo", "late eighteenth century", "The School Boys", "Operation Iceberg", "Texas Longhorns", "Hordaland", "1968", "30", "\"Pierement Waltz\"", "October 22, 2012", "Soldier in Truck", "Noah Schnapp", "Paradise, Nevada", "Neptune", "France", "70,000 or so", "Miami Beach, Florida", "go where no man has gone before", "Estonian", "Copenhagen", "Aristophanes", "Civil War"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6961929563492063}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.33333333333333337, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2788", "mrqa_squad-validation-3490", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-1308", "mrqa_naturalquestions-validation-7408", "mrqa_triviaqa-validation-4029", "mrqa_newsqa-validation-1720", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-3630"], "SR": 0.578125, "CSR": 0.6241319444444444, "EFR": 1.0, "Overall": 0.735529513888889}, {"timecode": 18, "before_eval_results": {"predictions": ["Lagos and Quiberon Bay", "seven months old", "Sava Kosanovi\u0107", "a noble death", "\"Monte Carlo\"", "large compensation pools", "Graz", "5,792", "Lucas\u2013Lehmer", "Wahhabism", "February 26, 1948", "Hordaland", "the town of El Nacimiento in M\u00fazquiz Municipality", "Stephen James Ireland", "Koch Industries", "Washington", "\"Southern Isles\"", "Children's Mercy Park", "High Falls Brewery", "technical director", "Mike Holmgren", "Nathan Bedford Forrest", "Kim Hyun-ah", "green and yellow", "Isobel", "Urijah Faber", "Barack Obama's", "Guthred", "Peel Holdings", "No. 5 on the \"Billboard\" 200", "College Football Friday Primetime", "Marco Hietala", "The Hawks played.500 basketball in February, which included a 99\u201398 victory over the Detroit Pistons on February 7.", "Sarah Hurst", "An invoice, bill or tab", "lenny Bruce", "Clarence Nash", "Golden Valley, Minnesota", "Marco Fu", "Syracuse", "Durban International Convention Centre", "Ryan Babel", "Bob Dylan", "\"out and back\"", "Luca Guadagnino", "Jennifer Lynne \"Gbaja-Biamila\" Brown", "11 Grands Prix wins", "National Collegiate Athletic Association", "Bill Cosby", "age thirteen", "Argentinian", "Dana Fox", "Sunday, November 2, 2003", "northwest Washington", "Jenny", "Nadia Comaneci", "The Mayor of Casterbridge", "Friday", "Luca di Montezemolo", "the Social Democratic", "\"Gone\"", "sexual harassment", "President Obama and Britain's Prince Charles", "Gloria Allred,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6239583333333334}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.5714285714285715, 0.4, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9057", "mrqa_squad-validation-8020", "mrqa_squad-validation-9592", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-1461", "mrqa_newsqa-validation-2163", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-3464", "mrqa_newsqa-validation-2844"], "SR": 0.53125, "CSR": 0.6192434210526316, "EFR": 1.0, "Overall": 0.7345518092105264}, {"timecode": 19, "before_eval_results": {"predictions": ["CEPR", "the Commission and Council", "7,200", "scoil phr\u00edobh\u00e1ideach", "90 to 95 percent", "the Middle Rhine area", "56.2%", "time", "Christ's message and teachings", "Bayern Munich", "fifth", "five", "A123 Systems, LLC", "\"the backside.\"", "Bothtec", "the Manhattan Project", "1975", "3,000", "youngest TV director ever", "Parlophone", "About 200", "Marco Fu", "Orfeo ed Euridice", "seventh", "Kristin Scott Thomas, Anne Bancroft, James Fox, Derek Jacobi, and Sean Penn.", "Golden Calf", "Houston Rockets", "Summerlin, Clark County, Nevada", "Argentinian", "Europe", "Noel Gallagher.", "Savannah River Site", "a family member", "Switzerland", "second largest", "Frank Lowy", "Fat Man", "Manley MacDonald", "Nye County", "Herman's Hermits", "Mani", "Pendlebury, Lancashire", "300 km north west", "1932", "Arrowhead Stadium", "1885", "House of Borromeo", "power directly or elect representatives from among themselves to form a governing body, such as a parliament", "Sydney", "Michael Redgrave", "KlingStubbins", "Big 12", "the Argand lamp", "2017", "RAF", "John McEnroe", "Matricide", "helicopters and robotic surveillance craft to the \"border states\"", "a one-shot victory in the Bob Hope Classic on the final hole", "\"a system of control\"", "Dune", "the zodiac", "the Anne", "Juno"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7247177301864802}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9093", "mrqa_squad-validation-1665", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-32", "mrqa_naturalquestions-validation-954", "mrqa_newsqa-validation-1445", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-2686", "mrqa_searchqa-validation-7051"], "SR": 0.6875, "CSR": 0.62265625, "EFR": 1.0, "Overall": 0.735234375}, {"timecode": 20, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1582", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2022", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2295", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3571", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3693", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-3881", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-447", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5718", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4155", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-1038", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-402", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-642", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12472", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-1723", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10115", "mrqa_squad-validation-10249", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10328", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1064", "mrqa_squad-validation-1064", "mrqa_squad-validation-1114", "mrqa_squad-validation-1162", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1255", "mrqa_squad-validation-1296", "mrqa_squad-validation-1366", "mrqa_squad-validation-1480", "mrqa_squad-validation-1529", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1597", "mrqa_squad-validation-1723", "mrqa_squad-validation-1748", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-2040", "mrqa_squad-validation-2059", "mrqa_squad-validation-2111", "mrqa_squad-validation-2144", "mrqa_squad-validation-2153", "mrqa_squad-validation-2315", "mrqa_squad-validation-234", "mrqa_squad-validation-2475", "mrqa_squad-validation-2536", "mrqa_squad-validation-2568", "mrqa_squad-validation-2628", "mrqa_squad-validation-2701", "mrqa_squad-validation-2786", "mrqa_squad-validation-287", "mrqa_squad-validation-2898", "mrqa_squad-validation-2976", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3198", "mrqa_squad-validation-3240", "mrqa_squad-validation-3483", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4015", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4259", "mrqa_squad-validation-4263", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-4545", "mrqa_squad-validation-457", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4646", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5200", "mrqa_squad-validation-5276", "mrqa_squad-validation-531", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5542", "mrqa_squad-validation-556", "mrqa_squad-validation-5611", "mrqa_squad-validation-5655", "mrqa_squad-validation-5678", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6263", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-6500", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6523", "mrqa_squad-validation-6538", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6657", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-689", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-7001", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7062", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7152", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7302", "mrqa_squad-validation-7302", "mrqa_squad-validation-7323", "mrqa_squad-validation-7391", "mrqa_squad-validation-7403", "mrqa_squad-validation-7450", "mrqa_squad-validation-7603", "mrqa_squad-validation-7692", "mrqa_squad-validation-7732", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8077", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8219", "mrqa_squad-validation-8225", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8447", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-8612", "mrqa_squad-validation-864", "mrqa_squad-validation-8668", "mrqa_squad-validation-8685", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-876", "mrqa_squad-validation-8836", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9061", "mrqa_squad-validation-9076", "mrqa_squad-validation-9101", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9240", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9652", "mrqa_squad-validation-9716", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9956", "mrqa_squad-validation-9991", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4403", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7642"], "OKR": 0.833984375, "KG": 0.41640625, "before_eval_results": {"predictions": ["Bento de Moura Portugal", "Mongolia", "polynomial-time reduction", "All India Muslim League", "CBS Sports apps on tablets, Windows 10, Xbox One and other digital media players", "1967", "antigen from a pathogen", "Encoded Archival Details (EAD)", "Trey Parker and Matt Stone", "First Family of Competitive eating", "Democratic Unionist Party", "local South Australian and Australian produced content", "\"Naked Killer\" (1992)", "7 June 1926 to 17 December 1926", "The Summer Olympic Games", "1937", "John Lee Hancock", "Hordaland", "Agent 99", "Bonnie Franklin", "Edmonton, Alberta", "Skyscraper", "Love Actually", "a creek", "Virginia", "URO VAMTAC", "The bald eagle", "32 people", "\"Lithuania, Our Fatherland\"", "Philadelphia Naval Shipyard", "The Books", "Rochdale", "2013\u201314 Premier League", "Clara Petacci", "Jamie Fraser (Sam Heughan)", "Lionel Brockman Richie Jr.", "\"The Two Noble Kinsmen\"", "Eucritta melanolimnetes", "Jenji Kohan", "Johnson & Johnson", "YouTube celebrity PewDiePie", "Germanic", "November 5, 2002", "Tamara Ecclestone Rutland", "Thomas Joseph \"T. J.\" Lavin", "Stalybridge Celtic", "Adelaide Lightning", "Kohlberg K Travis Roberts & Co.", "\"My Love from the Star\"", "Tottenham ( ) or Spurs", "Sam Bettley", "Ernest Hemingway", "Nia Kay", "The Fixx", "season seven", "1876", "dynamite", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "head injury", "The Tomb of Sheikh Salim Chishti", "teeth", "Profit maximization happens when marginal cost is equal to marginal revenue", "electron shells", "1901"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6538032280219781}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.0, 0.25, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.8, 0.7499999999999999, 0.6666666666666666, 1.0, 0.28571428571428575, 0.0, 0.5, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1764", "mrqa_squad-validation-525", "mrqa_squad-validation-5504", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-4819", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-7737", "mrqa_triviaqa-validation-7266", "mrqa_searchqa-validation-9565", "mrqa_naturalquestions-validation-3295"], "SR": 0.546875, "CSR": 0.6190476190476191, "EFR": 1.0, "Overall": 0.7149032738095238}, {"timecode": 21, "before_eval_results": {"predictions": ["priest", "December 12", "Hostmen", "Apollo 8", "Antigone", "pyrenoid and thylakoids", "Sugarfoot", "\"Preacher\"", "Forbes", "Mitsubishi", "Tabasco", "1 January 1788", "Lowestoft", "Christopher Nolan", "Taylor Swift", "Al Horford", "Johan Leysen", "1854", "Switzerland", "New York Giants", "professional footballer", "Puli Alam", "Lauren Alaina", "Ian Fleming's", "Mary Bonauto, Susan Murray, and Beth Robinson", "27 November 1956", "a split 7", "Hindi", "United States Auto Club", "\"Neptune's Party\"", "Albany High School", "BAFTA TV Award Best Actor", "\"The Bob Edwards Show\"", "the heaviest album of all", "Field Marshal Lord Gort", "15,024", "Prime Minister of Denmark 1852\u20131853 as head of the Cabinet of Bluhme I", "most awarded female act of all-time", "Cylon Number Six", "2007", "3,000", "Chinese Coffee", "A.P. M\u00f8ller", "Mineola", "Power 108", "John Richard Schlesinger", "fourth-ranking", "Blue", "Esperanza Spalding", "the Ruul", "Lonestar", "1916", "American", "18", "one - mile - wide", "David Jason", "olea europaea", "Turkey", "Michael Jackson may soon return to the stage, at least for a \"special announcement.\"", "\"Entourage\"", "the Tet Offensive", "Erica Rivera", "Native Americans", "Madison's"], "metric_results": {"EM": 0.625, "QA-F1": 0.7310448232323232}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.18181818181818182, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8651", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-5216", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-6307"], "SR": 0.625, "CSR": 0.6193181818181819, "EFR": 0.9583333333333334, "Overall": 0.7066240530303031}, {"timecode": 22, "before_eval_results": {"predictions": ["during the later decades of the 17th century", "WLQP-LP", "Sunni extremist groups", "mid-Eocene", "Soviet Union", "enthusiasm", "the 34th President of the United States", "receive the benefits of the Morrill Acts of 1862 and 1890", "Prussian", "Violet", "German", "Shameless", "Indianola", "the Vampire Intelligences", "Nassau County", "What Are Little Boys Made Of", "Andries Jonker", "President John F. Kennedy and Jacqueline Kennedy", "Mollie Elizabeth King", "1959", "129,007", "the San Francisco 49ers", "Big Machine Records", "the Parthian Empire", "almost 3 million people", "Matt Groening", "June 10, 1982", "Philip K. Dick", "John Anthony \"Jack\" White", "Samuel Burl \"Sam\" Kinison", "Boston, Massachusetts", "Lisa", "four months in jail", "Galleria Vittorio Emanuele II", "Paul Avery", "31 October 1783", "Puli Alam", "the Peninsular War in Spain during the Napoleonic Wars", "1838", "Sir John Major", "Ashland is home to Scribner-Fellows State Forest", "Manchester Victoria station in air rights space", "the east of Ireland", "Estadio de L\u00f3pez Cort\u00e1zar", "Sunday, November 2, 2003", "Jesus", "Agent Carter", "Plies", "Tim \"Ripper\" Owens", "Walt Disney Productions", "chalk quarry", "Rhode Island", "electron donors", "Johnson", "Nala", "Peter Townsend.", "the river of trade", "the Sunday Post", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "series of experts weigh in on questions such as potential for abuse, pharmacological effects, history and current patterns abuse.", "88-year-old", "a canoe made from a hollowed tree trunk", "we/wee", "series of famous people who died unexpectedly"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7376971546468829}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.29629629629629634, 0.08695652173913043, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-968", "mrqa_squad-validation-3754", "mrqa_squad-validation-9647", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-1008", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-4826", "mrqa_hotpotqa-validation-1531", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-1061", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-2945", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-6928"], "SR": 0.640625, "CSR": 0.6202445652173914, "EFR": 1.0, "Overall": 0.7151426630434783}, {"timecode": 23, "before_eval_results": {"predictions": ["Jones et al. 1998, Pollack, Huang & Shen 1998, Crowley & Lowery 2000 and Briffa 2000", "the Neckar", "static discs", "Pittsburgh Steelers", "14,000", "Scandinavia and northern Europe", "1911", "ESPN's \"SportsCenter\"", "Prince Antoni Radziwi\u0142\u0142", "northern Italy's Lombardy region", "Caesars Palace Grand Prix", "Detroit, Michigan", "Wilton Mall or Viaport Rotterdam Square", "the Sun", "Point of Entry", "Wilmette, Illinois", "Malayalam cinema", "Pendlebury, Lancashire", "Leona Lewis", "Laura Dern", "Melbourne", "democracy and personal freedom", "Cool Runnings", "Indian", "Labour", "Thor", "Copa Airlines", "Chiltern Hills", "Rudebox", "Washington", "Massachusetts", "Edward James Olmos", "mixed Gaelic and Norse ancestry and culture", "Slaughterhouse-Five", "Nashville", "simple language", "Telugu and Tamil", "Peshwa", "Joseph I", "Oracle Corporation", "1999", "William Shakespeare", "January 23, 1898", "1953", "Bergen", "Scribner", "Apprendi v. New Jersey", "Oregon State Beavers", "Jack Elam", "1907", "The Design Inference", "R&B", "pilgrimages to Jerusalem", "art pottery", "about restoring someone's faith in love and family relationships", "a filly", "Moby Dick", "Christine Keeler", "137", "park bench facing Lake Washington", "Sunday", "Jackie Robinson", "the DEW Line", "a snake"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7941840277777779}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.4, 0.5, 0.0, 0.7777777777777778, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-994", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-354", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-2866", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-3186", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-1059"], "SR": 0.703125, "CSR": 0.6236979166666667, "EFR": 0.9473684210526315, "Overall": 0.7053070175438597}, {"timecode": 24, "before_eval_results": {"predictions": ["Los Angeles", "variously combustion chamber", "13th-century", "spinat", "ACL", "Bury Football Club", "Las Vegas", "Suzuki YZF-R6", "Koninklijke Ahold N.V.", "east", "Gettysburg Address", "field of science", "Robert \"Bobby\" Germaine, Sr.", "3D computer-animated comedy", "Pacific War", "Amy Poehler", "footballer", "British Labour Party", "USC Marshall School of Business", "thematic", "1937", "Martin Scorsese", "Maxwell Smart", "\"The Walking Dead\"", "2008", "Yasir Hussain", "Let's Make Sure We Kiss Goodbye", "Ronald Ryan", "Elena Verdugo", "soccer", "Peel Holdings", "Chechen Republic", "alcoholic drinks", "Zaire", "Debbie Harry", "Michael Burger", "novelist and poet", "1986", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Hilary Duff", "100 million", "a Albanian political party", "2015 Masters Tournament", "John Schlesinger", "Venice", "Rockstar San Diego", "A.S. Roma", "gender queer", "Gothic Revival", "Melbourne's City Centre", "South West Peninsula League", "Bury St Edmunds, Suffolk, England", "1608", "metamorphic rock", "Rugrats in Paris", "auk", "zeny", "Broadway", "Sen. Barack Obama", "District Attorney Larry Abrahamson", "Spanish Davis Cup hero Fernando Verdasco,", "pH", "calcium", "Si-Tchun."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6301215277777779}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 0.8, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.8571428571428571, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.7499999999999999, 1.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3202", "mrqa_squad-validation-10449", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-1741", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-234", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-9626", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-1684", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-1361", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-14782"], "SR": 0.53125, "CSR": 0.62, "EFR": 1.0, "Overall": 0.7150937500000001}, {"timecode": 25, "before_eval_results": {"predictions": ["August 15, 1971", "Geordie", "Peter Howell", "many elements of the old language", "formalize a unified front in trade and negotiations with various Indians", "French", "Upper Manhattan, New York City", "2017", "Logan International Airport", "Pain Language", "Serhiy Paradzhanov", "no. 3 on the British box office chart", "John John Florence", "Heart Heartbreak", "July 16, 1971", "Microsoft Office file formats", "Baldwin, Nassau County, New York", "Sir Elton Hercules John", "Firestorm", "the Ruul", "March 14, 2000", "David Wells", "Northern Lights", "the Chengdu Aircraft Corporation (CAC) of China", "the Bhaktivedanta Institute of ISKCON", "Minnesota", "Oklahoma", "Jim Aaron Diamond", "Smithfield, Rhode Island,", "Julie Taymor", "29 September\u20132 October 2011", "Columbia Records", "1943", "Maria Brink", "\"The Braes o' Bowhether\"", "Cody Miller", "Darkroom", "Unfaithful", "Christopher Nolan", "The Blue Album", "1992", "2016 United States elections", "bushwhackers", "Princes Park in Melbourne", "The Late Late Show", "2012", "1978", "Patsy Swayze", "John Morgan", "June", "an organ", "Macau", "49 cents", "issued upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Jocelyn Flores", "the purpose of emphasis or heightened effect", "Egyptian", "tiger", "1620", "November 1", "1991-1993", "Thomas Nast", "ice hockey", "Betty la fea"], "metric_results": {"EM": 0.59375, "QA-F1": 0.707328869047619}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8888888888888888, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-1348", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-3386", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2853", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-1718", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-2092", "mrqa_triviaqa-validation-4040", "mrqa_triviaqa-validation-699", "mrqa_triviaqa-validation-3925", "mrqa_newsqa-validation-1230", "mrqa_searchqa-validation-172"], "SR": 0.59375, "CSR": 0.6189903846153846, "EFR": 0.9615384615384616, "Overall": 0.7071995192307693}, {"timecode": 26, "before_eval_results": {"predictions": ["Italian physicist", "illiberal Islamic regimes", "The Judicial Council", "CBS", "berceuse", "Blue Jean", "ibex", "Hebridean isle", "prostate", "Vitus Bering", "fuel", "Burundi", "larva", "hoof", "Der Zauberberg", "double reed", "Canada", "Komodo dragon", "fibroids", "Four Times", "egypt", "won't get you a guppy", "Ivanhoe", "radio waves", "e.g. Onomatopoeia Metaphor", "Isis", "Eliza Doolittle", "fibromyalgia", "a deposition arising in or related to a bankruptcy case that begins by filing a... The release, prior to trial, of a person accused of a crime, under specified.... Information presented in testimony", "Franklin D. Roosevelt", "Pregnant women", "Violeta Barrios de Chamorro", "Take My Breath Away", "Rafael Nadal", "Bizkaia", "Ich bin ein Berliner", "Caesar's wife", "Good fiction", "Neverbeen Kissed", "Antichrist", "William Augustus, duke of Cumberland", "Day-O", "Nanjing", "asparagus beetles", "euf Wiedersehen", "blubber", "Catalysts", "Stanford-Binet Intelligence Scales", "Ferdinand von Zeppelin", "Deep Purple", "Jesus", "etymology", "to solve its problem of lack of food self - sufficiency", "Vice President, Speaker of the House of Representatives, President pro tempore of the Senate, and then the heads of federal executive departments who form the Cabinet of the United States", "the Bulgarian 2nd Army", "20 numbered,", "Time Bandits", "egypt", "Kerry Marie Butler", "No. 60", "three centuries", "Consumer Product Safety Commission", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the General Assembly"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4395833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.16666666666666666, 0.33333333333333337, 0.4, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15646", "mrqa_searchqa-validation-10928", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-2775", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-2526", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-4299", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-6143", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-13623", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-4190", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-7196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-2819", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1748"], "SR": 0.34375, "CSR": 0.6087962962962963, "EFR": 0.9761904761904762, "Overall": 0.7080911044973546}, {"timecode": 27, "before_eval_results": {"predictions": ["May 18, 1756", "canceled", "modern", "more than half of the global wealth", "Lismore", "STS-51-L", "Paradise, Nevada", "English", "Newcastle upon Tyne, England", "Colonel", "Cody Miller", "Virginia", "Hertz Corporation", "Wiz Khalifa", "Disney California Adventure", "Maria Brink", "The Sound of Music", "G\u00f6tene", "Argentine", "novelty songs, comedy, and strange or unusual recordings", "Perfect 10", "6teen", "South America", "Princes Park", "Phil Collins", "the Knight Company", "My Gorgeous Life", "Ashanti Region", "the Dutch Empire", "Culiac\u00e1n, Sinaloa", "Northampton, England", "Black Panthers", "John Murray", "beer", "Dara Grace Torres", "nine vertical feet", "1909", "about 5320 km", "3,384,569", "anvil", "House of Hohenstaufen", "James G. Kiernan", "Johnnie Ray", "Forrest Gump", "2009", "1919", "a skerry", "1620 to 1691", "the exiled House of Stuart", "Mickey Gilley", "Blue Origin", "December 31, 2015", "2017", "Eleanor Roosevelt", "Utah, Arizona, Wyoming, and Oroville, California", "Google", "the earth-moon system", "English rock group the Kinks", "throwing three punches but said only one connected.", "sought Cain's help finding a job", "1975", "the sperm whale", "libraries", "NASA"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7390286796536796}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7559", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-878", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-1233", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-2555", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4650", "mrqa_hotpotqa-validation-1965", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-2829", "mrqa_triviaqa-validation-1527", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2586"], "SR": 0.609375, "CSR": 0.6088169642857143, "EFR": 1.0, "Overall": 0.7128571428571429}, {"timecode": 28, "before_eval_results": {"predictions": ["British patrons", "Laverne & Shirley", "South", "Roger Bacon", "apogee", "ap credit policies", "Pitney Bowes", "apoorns to leprechauns", "apverbs 13:24", "apogee", "apubec", "Edith Piaf", "the Krntnertor Theater", "Sappho", "apogee", "Colorado River", "Hershey's", "Timothy Leary", "apogee", "Thought Police", "The Street Lawyer", "apogee", "apogee", "apogee", "a 1.5 km swim", "calcium", "David Beckham", "Wisconsin", "apogee", "To Build a Fire", "apoids", "Docu Drama", "apse", "centigrade", "silver", "British Broadcasting Corporation", "penguin", "Clive Staples Lewis", "Blackwater USA", "apogee", "apogee", "July", "Arsinoe II", "New Jersey", "apogee", "apogee", "apogee", "asthma", "duck", "a trumpet", "apogee", "Marion", "liquids", "its population", "Sarah Silverman", "Anwar Sadat", "apo\u00f1ola", "Barings Bank", "Esp\u00edrito Santo Financial Group", "Earvin \"Magic\" Johnson Jr.", "Elliot Fletcher", "laundry service", "drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.", "shoreline of the city of Quebradillas"], "metric_results": {"EM": 0.375, "QA-F1": 0.42211174242424243}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-11819", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-13291", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-8603", "mrqa_searchqa-validation-4398", "mrqa_searchqa-validation-574", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-6355", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-3781", "mrqa_searchqa-validation-11293", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-13129", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-2602", "mrqa_searchqa-validation-841", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-7159", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-4972", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3961"], "SR": 0.375, "CSR": 0.6007543103448276, "EFR": 1.0, "Overall": 0.7112446120689655}, {"timecode": 29, "before_eval_results": {"predictions": ["tennis", "11 million", "GTE", "High school", "John Lee Hancock", "2007", "Westfield Tea Tree Plaza", "Philadelphia", "237 square miles", "Gal Gadot", "1860", "Eddie Izzard", "US", "Miracle", "Richard Wayne Snell", "poet, and writer", "Humberside Airport", "8 Simple Rules", "2015", "Kind Hearts and Coronets", "Centennial Olympic Stadium", "Leatherheads", "Polka", "Sinngedichte", "James G. Kiernan", "Australia", "heavy metal band", "Anah\u00ed", "Toxics Release Inventory", "Tampa Bay Lightning", "tabasco", "Patricia Arquette", "\"Secrets and Lies\"", "Bingham", "coca wine", "Crystal Dynamics", "Geraldine Page", "pornographicstar", "Europe", "179", "three", "Sam the Sham", "pinball", "Genesee Brewing Company", "Las Vegas Strip in Paradise, Nevada", "PPG Paints Arena", "new king in 1714", "J35", "politician", "Romance", "Bohemia", "Macomb", "birth", "biochemistry", "provides the public with financial information about a nonprofit organization", "Elgar", "Stockholm syndrome", "magnetism", "Morgan Tsvangirai.", "Empire of the Sun", "Amanda Knox's", "Robert Bruce", "Donna Reed", "gulls"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6633184523809523}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5826", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-3066", "mrqa_naturalquestions-validation-9088", "mrqa_triviaqa-validation-5397", "mrqa_triviaqa-validation-3393", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3211", "mrqa_searchqa-validation-5455", "mrqa_searchqa-validation-9860"], "SR": 0.5625, "CSR": 0.5994791666666667, "EFR": 1.0, "Overall": 0.7109895833333334}, {"timecode": 30, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1180", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1342", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1488", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-172", "mrqa_hotpotqa-validation-1726", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2199", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2825", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-3071", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3191", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3402", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4285", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4467", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5247", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-5471", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5561", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5713", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-704", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-98", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1199", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1503", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11434", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-13831", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-2489", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-2686", "mrqa_searchqa-validation-3516", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6045", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-8245", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9601", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10223", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10424", "mrqa_squad-validation-10449", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1064", "mrqa_squad-validation-1096", "mrqa_squad-validation-1114", "mrqa_squad-validation-1177", "mrqa_squad-validation-1232", "mrqa_squad-validation-1255", "mrqa_squad-validation-1258", "mrqa_squad-validation-1296", "mrqa_squad-validation-1480", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1723", "mrqa_squad-validation-1940", "mrqa_squad-validation-1976", "mrqa_squad-validation-2153", "mrqa_squad-validation-2272", "mrqa_squad-validation-2301", "mrqa_squad-validation-234", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2591", "mrqa_squad-validation-287", "mrqa_squad-validation-2976", "mrqa_squad-validation-306", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3296", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3716", "mrqa_squad-validation-3767", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4026", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4259", "mrqa_squad-validation-4286", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4545", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4810", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5276", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5532", "mrqa_squad-validation-556", "mrqa_squad-validation-5578", "mrqa_squad-validation-5611", "mrqa_squad-validation-5779", "mrqa_squad-validation-5839", "mrqa_squad-validation-5881", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6443", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6888", "mrqa_squad-validation-6967", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7125", "mrqa_squad-validation-7152", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7323", "mrqa_squad-validation-7403", "mrqa_squad-validation-7458", "mrqa_squad-validation-7603", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-8219", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-864", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8839", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9061", "mrqa_squad-validation-9191", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9521", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9647", "mrqa_squad-validation-9652", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2654", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-830"], "OKR": 0.86328125, "KG": 0.46875, "before_eval_results": {"predictions": ["any member of the Scottish Government", "quadratic", "1879", "Xiu Li Dai", "St. Theodosius Russian Orthodox Cathedral", "northwest Washington", "1924", "England", "Millerlite", "the status line", "Benjamin Franklin", "December 2, 2013", "into the intermembrane space", "Chinese", "Philadelphia", "The United States Secretary of State", "electrical activity produced by skeletal muscles", "thick skin", "an Islamic shrine located on the Temple Mount in the Old City of Jerusalem", "Sylvester Stallone", "Anakin Skywalker", "September 27, 2017", "generating a long - term infection that can be very difficult to eradicate", "the economy", "Victory gardens", "Paul Hogan", "961", "northern China", "gathering money from the public", "a beach in Malibu, California", "Kitty Softpaws", "homicidal thoughts of a troubled youth", "a part of the continent of North America, Greenland has been politically and culturally associated with Europe ( specifically Norway and Denmark, the colonial powers, as well as the nearby island of Iceland ) for more than a millennium", "Sun Tzu", "18th century", "Setsuko Thurlow", "the temporal lobes", "the Douze at Pont l'Abb\u00e9", "DNA replication", "mining", "Keith Thodeaux", "six - hoop game", "Atlantic", "butane", "Julia Roberts", "12 November 2010", "Brazil, China, France, Germany, India, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey, the United Kingdom, the United States, and the European Union", "Aaron Harrison", "Panning", "CBS Television City, studios 41 and 43 in Hollywood", "Johnny Depp", "James Chadwick", "the Swirral Edge ridge", "eutrophication", "Worcester Cathedral", "Germany", "Rachel, Nevada", "Atlanta", "more than 30 Latin American and Caribbean nations", "two women", "police dogs", "a astronomical viewing facility", "Antonio Doto", "Hannibal"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5607063448999401}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666665, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.5641025641025641, 0.0, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.6666666666666666, 0.0, 1.0, 0.30769230769230765, 1.0, 0.1904761904761905, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9473684210526316, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.25, 1.0, 0.6666666666666666, 0.5454545454545454, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1818", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-6224", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-2498", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-6465", "mrqa_triviaqa-validation-782", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2873", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-414", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-5042"], "SR": 0.40625, "CSR": 0.5932459677419355, "EFR": 1.0, "Overall": 0.732711693548387}, {"timecode": 31, "before_eval_results": {"predictions": ["lighter and seem to lose something in the process", "fertilized eggs", "September 19 - 22, 2017", "The Deserted Village", "John Roberts", "in contemporary Earth, where the sudden appearance of a worldwide storm causes 98 % of the world's population to disappear, and zombie - like creatures rise to attack the remainder", "between the Eastern Ghats and the Bay of Bengal", "a bow bridge with 16 arches shielded by ice guards", "12 to 36 months old", "Tom Brady", "Chelsea", "Darlene Cates", "fascia surrounding skeletal muscle", "Jerry Leiber and Mike Stoller", "a Norwegian town", "Gloria", "BBC sitcom, Only Fools and Horses", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Robin", "Jack Barry", "Missouri River", "Donna", "August 18, 1945", "19 June 2018", "Daniel A. Dailey", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "the King James Bible", "many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "international educational foundation headquartered in Geneva, Switzerland", "October 28, 2007", "Jaydev Shah", "domestication of the wild mouflon in ancient Mesopotamia", "Action Jackson", "in a thousand years", "state or other organizational body that controls the factors of production", "north pole", "in Ephesus in AD 95 -- 110", "1984", "sport utility vehicles", "Americans", "1916", "30 years after Return of the Wars", "John Joseph Patrick Ryan", "American musical group founded by Marcus Bowens and Jermaine Fuller", "Rashidun Caliphs", "off the rez", "Woody Paige", "Ren\u00e9 Descartes", "2007 via Valve's Steam content distribution platform", "Asuka", "Diary of a Wimpy Kid : The Long Haul", "three", "Steve Biko", "Rudolph", "chess", "Anne Fletcher", "October 21, 2016", "Centers for Medicare and Medicaid Services", "14", "Robert Barnett,", "Oaxaca, Mexico", "Algeria", "a bottle", "pipa"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6614070982845999}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false], "QA-F1": [0.8421052631578948, 0.0, 1.0, 0.0, 1.0, 0.0625, 1.0, 0.3636363636363636, 0.5714285714285715, 0.0, 0.5, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.2222222222222222, 0.8405797101449275, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.375, 0.9859154929577464, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 0.8333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3494", "mrqa_squad-validation-4566", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-6272", "mrqa_triviaqa-validation-2748", "mrqa_newsqa-validation-3785", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-4046"], "SR": 0.53125, "CSR": 0.59130859375, "EFR": 0.9, "Overall": 0.71232421875}, {"timecode": 32, "before_eval_results": {"predictions": ["September 1565", "Anglo-Saxon populations who migrated to and conquered much of England after the end of Roman Imperial rule", "Pin the Tail on the Donkey", "the martini", "the Wiener Sangerknaben", "cinnamon", "the universe started", "Halloween", "R.E.M.", "Gale Sayers", "French Presidential Power and the Stability of the French Fifth Republic", "Georgia", "Abraham Lincoln", "the Yangtze River", "school", "Ngan Le", "Sharon Epatha Merkerson", "air pressure", "Alec Douglas-Home", "skull", "\"Dr. Feelbad\"", "the Deaf President Now protest", "\"Cushnoc\"", "\"bushed\"", "the orangutan", "bee sting emergency kit", "school of Islam", "gangrene", "\"ex\"", "\"Bonnie and Clyde\"", "John Harvard", "the Roman branch of the Indo-European language family", "\"David Cassidy: Man Undercover\"", "(Judy Garland)", "Guatemala", "\"JK\" Rowling", "Hillary Clinton", "school president of Ecuador", "Albert Einstein", "school", "Barnsdall Art Park", "Little Women", "Tulipa", "do or die", "Providence", "Tasmanian devil John Quincy", "Mother Vineyard", "South Africa", "Howard Athenaeum", "carbon monoxide", "school", "tooth", "Gibraltar, a British Overseas Territory, located at the southern tip of the Iberian Peninsula", "1999", "9 February 2018", "Argentina", "Sarah Sawyer", "Aviva USA", "Russian Empire", "1967", "44,300", "228", "Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "know what is important in life, and it's not your car.\""], "metric_results": {"EM": 0.359375, "QA-F1": 0.43732638888888886}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, false], "QA-F1": [0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3088", "mrqa_squad-validation-5276", "mrqa_searchqa-validation-10043", "mrqa_searchqa-validation-608", "mrqa_searchqa-validation-5405", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-16138", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-11993", "mrqa_searchqa-validation-5348", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-12676", "mrqa_searchqa-validation-15919", "mrqa_searchqa-validation-1108", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-15681", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-9970", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-7460", "mrqa_searchqa-validation-4645", "mrqa_searchqa-validation-8650", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-2559", "mrqa_searchqa-validation-3246", "mrqa_searchqa-validation-391", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-11899", "mrqa_searchqa-validation-554", "mrqa_naturalquestions-validation-3959", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-5763", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-2395"], "SR": 0.359375, "CSR": 0.584280303030303, "EFR": 0.975609756097561, "Overall": 0.7260405118255727}, {"timecode": 33, "before_eval_results": {"predictions": ["11", "R\u00fcdesheim", "Dirty Diana", "Tennessee Williams", "Ring Magazine", "the dough", "John Henry", "Zombies", "Colombia", "belle K. Davis", "Friday Night Lights", "Halloween", "the Emperor", "a port-wine stain", "the Empire State Building", "pinta", "Czechoslovakia", "Ferris B Mueller", "Mike Judge", "Unforgiven", "Court TV", "galaxies", "Germany", "Gunsmoke", "astronomer", "Candy girl", "AT&T", "asthma", "Microsoft", "the blue agave", "Puerto Rico", "24 hours", "a flying saucer", "Shakespeare", "a liter", "Edward II", "The Silence of the Lambs", "(Dan) Aykroyd", "stuffing", "a fraction", "carbonite", "Spain", "the phi phenomenon", "an obelisk", "Sam Kinison", "Katharine Hepburn", "Harry S. Truman", "Kublai Khan", "the Abkhazia", "New York City", "a bow", "Newfoundland", "538", "a narcissistic ex-lover", "Lee Baldwin", "a googol", "Laos", "Wigan", "1995", "Champion Jockey", "Boeing 757", "the Bush administration", "200", "8 p.m."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6041666666666666}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9098", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-7868", "mrqa_searchqa-validation-15910", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-6554", "mrqa_searchqa-validation-3111", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-9672", "mrqa_searchqa-validation-2404", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-227", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11808", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-5234", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-3445", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-591"], "SR": 0.53125, "CSR": 0.5827205882352942, "EFR": 0.9666666666666667, "Overall": 0.7239399509803921}, {"timecode": 34, "before_eval_results": {"predictions": ["Hisao Yamada", "Percy Shelley", "3D computer-animated comedy", "aluminum foil", "Montreal, Quebec, Canada", "Lego", "Daniil Shafran", "Doc Hollywood", "Richard L. Thompson", "Virgin", "John Christopher Lujack Jr.", "February 2017", "Freddie Jackson", "Michael Swango", "Roman Polanski", "322,520", "1754", "Cate Blanchett", "Westfield Marion", "Montana State University", "1961", "Eisenhower Executive Office Building", "137th", "Mohsin Fani", "High Falls Brewery", "2016", "1998", "Sean", "2015", "Mel Blanc", "Corendon Dutch Airlines", "Tamil", "number five", "Champion Jockey", "University of the District of Columbia", "Jennifer Aniston", "Larry Eustachy", "Anne Perry", "March 17, 2015", "Julie Taymor", "Mika H\u00e4kkinen", "nine", "a bass", "Buck Owens and the Buckaroos", "January 1788", "Lord Chancellor of England", "MGM Resorts International", "Cleveland, Ohio", "The song also features rap parts from Darryl, RB Djan and Ryan Babel", "Mark Anthony \"Baz\" Luhrmann", "Syracuse University", "\"personal earnings\" (such as salary and wages), \"business income\"", "1969", "georgia", "the referee", "Botticelli", "the circus", "John Keats", "Dr. Maria Siemionow, the head of plastic surgery research at the Cleveland, Ohio, hospital", "it has not intercepted any Haitianians attempting illegal crossings", "Arsene Wenger", "atoms", "Bellerophon", "Monica Lewinsky"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7597470238095239}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.4, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-5145", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5530", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-421", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-517", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-4302", "mrqa_triviaqa-validation-2977", "mrqa_newsqa-validation-1678", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-7800"], "SR": 0.65625, "CSR": 0.5848214285714286, "EFR": 1.0, "Overall": 0.7310267857142858}, {"timecode": 35, "before_eval_results": {"predictions": ["June 4, 2014", "highly diversified", "Walter Pauk", "2018", "a noble gas", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "Peter Hansen", "Ishaan Anirudh Sinha", "the Charlotte Hornets of the National Basketball Association", "one person", "Orographic lift", "August 8, 1945", "6 March 1983", "an alternative is to cool all the atmosphere by spraying the whole atmosphere as if drawing letters in the air", "Around 1200", "April 21, 2015", "1854", "at the Mount Mannen in Norway and at the Isle of Sheppey in England", "August 5, 1937", "from the Kennedy Space Center ( KSC ) in Florida", "Rocky Dzidzornu -- congas", "35 to 40 hours per week", "Monastic orders, especially the Cistercians and the Carthusians", "since the early 20th century", "Authority", "`` speed limit ''", "coercivity", "near the mouth of the Pinarus River and the town of Issus", "1992", "ulcerative colitis", "September 1995", "Turducken", "membranes that envelop the brain and spinal cord", "abdicated in November 1918", "Massachusetts", "Hans Zimmer, Steve Mazzaro & Missi Hale", "c. 1000 AD", "from Times Square in New York City west to Lincoln Park in San Francisco", "Jerry Leiber and Mike Stoller", "111", "49 cents", "December 1, 1969", "Central Germany", "1978", "Javier Fern\u00e1ndez", "by observing the magnetic stripe `` anomalies '' on the ocean floor", "`` something that is to be expressed through some medium, as speech, writing or any of various arts ''", "the five permanent members", "peninsular mainland", "Santo Domingo", "during season two", "born November 28, 1973", "the Battle of the Somme", "Frederick William III", "Majorca", "National Football League", "Arlo Looking Cloud", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "\"She returned to Pakistan in October after President Pervez Musharraf signed an amnesty lifting corruption charges.\"", "spend $60 billion on America's infrastructure", "Wigan Athletic", "Ireland", "asteroids", "Colorado"], "metric_results": {"EM": 0.421875, "QA-F1": 0.6116530603469458}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.5882352941176471, 1.0, 0.923076923076923, 0.8, 1.0, 0.0, 0.8571428571428571, 1.0, 0.15384615384615385, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4210526315789474, 1.0, 0.5, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-9165", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-1985", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-6276", "mrqa_hotpotqa-validation-3613", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1970"], "SR": 0.421875, "CSR": 0.5802951388888888, "EFR": 1.0, "Overall": 0.7301215277777777}, {"timecode": 36, "before_eval_results": {"predictions": ["lithosphere", "S\u00fcleyman", "Skatoony", "number 1", "Satchmo, Satch or Pops", "San Antonio", "Polish", "Danish", "Milwaukee Bucks", "1994", "glee", "1965", "100 million", "Oneida Limited", "Wilmington, North Carolina, United States", "Pieter van Musschenbroek", "Southbank in Victoria", "London", "Australian", "Rochdale, North West England", "Bardot", "Mario Lemieux, OC, CQ", "\"Neptune's Party\"", "The Sun", "2000 Summer Olympics", "Ferdinand Magellan", "King of France", "1901", "Michael Fassbender", "Nanna Popham Britton", "Minette Walters", "leopard", "the Moselle", "Anne and Georges", "the Bank of China Tower", "American playwright and Nobel laureate in Literature", "Cheshire County", "Bob Gibson", "1770", "1974", "the Great Northern Railway", "Woody Woodpecker", "2", "Edward James Olmos", "IFFHS World's Best Goalkeeper", "three", "1989 until 1994", "Pittsburgh Steelers", "9 venues", "1993 to 2001", "Double Crossed", "the United States economy first went into an economic recession", "needle - like", "Bacon", "human rights lawyer", "constant letters", "1812", "energy propels the boat that travels between 5 and 10 knots an hour.", "greece judges Giovanni Falcone and Paolo Borsellino", "Kingdom City", "Crown Princess Juliana of the Netherlands", "constant display round or oval hives", "Santa Fe", "1992"], "metric_results": {"EM": 0.546875, "QA-F1": 0.63984375}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5714285714285715, 1.0, 0.28571428571428575, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4952", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3342", "mrqa_hotpotqa-validation-2883", "mrqa_hotpotqa-validation-3150", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-4490", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-2319", "mrqa_naturalquestions-validation-9421", "mrqa_triviaqa-validation-994", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-743", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-6519"], "SR": 0.546875, "CSR": 0.5793918918918919, "EFR": 0.9655172413793104, "Overall": 0.7230443266542405}, {"timecode": 37, "before_eval_results": {"predictions": ["lasting damage", "Brett Favre", "Robert Turner", "Wool Sack dress", "Billy the Kid", "Oliver Twist", "Hans Christian Andersen", "topaz", "volcanic craters", "Cameroon Pidgin English", "43,560 square feet", "Destiny's Child", "Bishop of Rome", "Kentucky", "Danny \" Danny\" Ocean", "valkyries", "Little Women & Good Wives", "\"Ich bin ein Berliner\"", "a stone", "the Japanese navy", "difference", "Gogol", "Malcolm X", "Rocky Mountain columbine", "vu", "Michigan", "Sigmund Freud", "a shark", "T. S. Eliot", "Dumpling", "an unmarked grave", "New Zealand", "sugar", "Theology of God", "FORDuctORS", "Stephen Decatur", "Castor & Pollux", "Paraguay", "R2-D2", "6 to 8 glasses", "tense", "Vassar College", "forensic medicine", "National Air and Space Museum", "Vespa", "Warren G. Harding", "Emma Peel", "Tennessee", "Richard I", "Will Rogers", "Bee Gees", "Honor\u00e9 Mirabeau", "in 2018", "Hanna Alstr\u00f6m", "COHSE", "hypertext", "Scotland", "1898", "1887", "Bonkyll Castle", "more and more suspicious of the way their business books were being handled.", "Top Gun", "Dame Elizabeth,", "1875"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5951388888888889}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.5, 0.4, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-16031", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-7058", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-3599", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-3714", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-8720", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9251", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-13049", "mrqa_searchqa-validation-8295", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-10821", "mrqa_searchqa-validation-2859", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-8890", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-7359", "mrqa_triviaqa-validation-1283", "mrqa_hotpotqa-validation-1884", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-2552", "mrqa_hotpotqa-validation-4203"], "SR": 0.46875, "CSR": 0.5764802631578947, "EFR": 1.0, "Overall": 0.7293585526315789}, {"timecode": 38, "before_eval_results": {"predictions": ["the butcher Market", "Shrove Tuesday", "Eyelet", "Soundgarden", "a pew", "Russia", "the Penguin", "Digoxin", "Canada", "sopra", "pole vault", "California", "Jordan", "the plate cylinder", "the Battle of Waterloo", "Ukraine", "Goombah", "Paris", "#49 David Geffen", "Hallmark Cards", "Joan of Arc", "John Tyler", "a figure of speech", "La-Z-Boy", "water vapor", "a subgenus", "Narnia", "East Germany", "Linda Keene", "Judges 5", "Vlad Tepes", "Marlee Matlin", "Tadpoles", "TIL Qatar", "debts", "Lady Jane Grey", "yellow fever", "Days Inn", "Guatemala", "Harold Edward \"Red\" Grange", "Simon", "printing", "couscous", "1917", "Colonel (Tom) Parker", "the lilac", "American Pie", "the Emerald cut", "a bowhead whale", "Wayne Woodrow", "Sweet Home Alabama", "American country music singer George Strait", "Toledo", "1960", "\"The Nutcracker\"", "Dodoma", "the aggregation framework", "martial arts action films", "Black Swan", "the Sun", "girls around 11 or 12", "welterweight", "President Barack Obama,", "Lewiston"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6036458333333332}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-6090", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-14171", "mrqa_searchqa-validation-12204", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-10160", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3640", "mrqa_searchqa-validation-5033", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7368", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-6757", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3197", "mrqa_naturalquestions-validation-3087", "mrqa_triviaqa-validation-6420", "mrqa_triviaqa-validation-270", "mrqa_hotpotqa-validation-1192", "mrqa_newsqa-validation-30", "mrqa_triviaqa-validation-700"], "SR": 0.515625, "CSR": 0.5749198717948718, "EFR": 1.0, "Overall": 0.7290464743589744}, {"timecode": 39, "before_eval_results": {"predictions": ["failed to renew the existing carriage agreements negotiated with NTL and Telewest", "preserved corpses having sex", "\"To My Mother\"", "Lucky Dube,", "Festival Foods", "fallen comrades lost in the heat of battle.", "Samuel Herr, 26, and Juri Kibuishi,", "1918-1919", "participate in Iraq's government.", "body", "Honduras", "University of San Simeon, California", "the FBI", "201-262-2800", "stop selling unapproved pain-relief drugs.", "were directly involved in an Internet broadband deal with a Chinese firm.", "Iraq", "40 militants and six Pakistan soldiers", "1973", "Argentina", "Washington", "burns", "Laura Ling and Euna Lee", "January 24, 2006", "\"bystander effect\"", "Haleigh Cummings,", "The Casalesi Camorra clan", "nine", "stops, speed racers, stop", "two", "two", "anxious.", "Courtney Love,", "war crimes", "Hartsfield-Jackson International Airport", "bartering", "\"Dance Your Ass Off.\"", "December", "Uncle Jack", "the United States", "Illness", "reached under the counter, grabbed his gun and told the robber to drop the bat and get down on his knees.", "a building falls down", "African National Congress", "abuse", "blew himself up", "two", "poorest children.", "14", "11", "Royal Navy servicemen who have been helping the Iraqis to protect oil fields around the port town of Umm Qasr,", "Clarence Darrow", "dry fructose", "to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything '", "1768", "chariot", "George Orwell", "Takura", "1955", "\"I'm Shipping Up to Boston\"", "a lock", "Anna Mathilda McNeill", "a Liberty Bond", "email"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4929623538011696}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.9473684210526316, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.08, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.08333333333333333, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2864", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2946", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-1259", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-4211", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-1585", "mrqa_searchqa-validation-16932", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-11411", "mrqa_triviaqa-validation-90"], "SR": 0.421875, "CSR": 0.57109375, "EFR": 1.0, "Overall": 0.72828125}, {"timecode": 40, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1348", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2749", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2860", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3171", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-543", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4110", "mrqa_naturalquestions-validation-4211", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6680", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10534", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-2423", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-2883", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7583", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-8890", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-9163", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9967", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-138", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3341", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3787", "mrqa_squad-validation-3813", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4107", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5222", "mrqa_squad-validation-5237", "mrqa_squad-validation-525", "mrqa_squad-validation-5269", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-531", "mrqa_squad-validation-5319", "mrqa_squad-validation-5483", "mrqa_squad-validation-5501", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5741", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-689", "mrqa_squad-validation-7001", "mrqa_squad-validation-7002", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-741", "mrqa_squad-validation-7458", "mrqa_squad-validation-7470", "mrqa_squad-validation-7525", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7736", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9744", "mrqa_squad-validation-9845", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9969", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830"], "OKR": 0.85546875, "KG": 0.49609375, "before_eval_results": {"predictions": ["deflate", "Gov. Jan Brewer.", "Casey Anthony,", "Tehran,", "Britain's", "said such joint exercises between nations are not unusual. \"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "they did not receive a fair trial.", "Gordon Brown", "regulators in the agency's Colorado office", "Oprah: A Biography", "Vivek Wadhwa,", "You've been overwhelmed with the outpouring of support and concern towards Bowe and our family. As you know, the situation is extremely difficult for everyone involved.", "Herman Thomas", "eight-day", "a long-range missile", "He is the wildest wild man of rock and his four-decade career has been marked by drug addiction, self mutilation and onstage nudity", "he is committed to equality,", "Harry Nicolaides,", "in central Cairo,", "if we don't start paying attention to security, we're worried that we might find ourselves in five or 10 years saying we've made a big mistake.", "opium poppies", "animal products.", "New Haven firefighter", "at the University of Alabama in Huntsville,", "at the age of 23", "U.S. ambassador to Afghanistan", "The Real Housewives of Atlanta", "sing a, may I say, legendary closing number.", "September,", "Dr. Jennifer Arnold and husband Bill Klein,", "Waterloo Bridge", "two", "ties", "a missile", "Nicole", "was charged with murder in connection with the death of a woman who may have been contacted through a Craigslist ad,", "Drew Kesse,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the execution.", "At least 14", "farmer Alan Graham", "vitamin injections that promise to improve health and beauty.", "delivers a big speech", "Ennis, County Clare", "$17,000", "Swedish Prime Minister Fredrik Reinfeldt", "Anjuna beach in Goa", "the Cleveland Clinic.", "South African", "that Peterson had his personal.40-caliber Glock when police found him.", "a rally at the State House", "J. Presper Eckert", "Latitude", "excessive growth", "Musicians", "One Thousand and One", "George IV", "in May 2011", "Violet", "Kinnairdy Castle", "the hippopotamus", "the boll weevil", "Alexander Solzhenitsyn", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5896077847056667}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6341463414634146, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.06666666666666667, 0.20000000000000004, 1.0, 0.8, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.15789473684210525, 0.0, 1.0, 1.0, 0.5, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.20000000000000004, 0.4, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2320", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-4653", "mrqa_hotpotqa-validation-3836", "mrqa_searchqa-validation-16464"], "SR": 0.453125, "CSR": 0.5682164634146342, "EFR": 0.9714285714285714, "Overall": 0.7231633819686412}, {"timecode": 41, "before_eval_results": {"predictions": ["Jim Nantz and Phil Simms", "Texas", "Detroit", "birds", "James Bond", "Taps", "Seal", "Dr. Strangelove", "a cat", "Atlanta", "Ridi, Pagliaccio, sul tuo amore in franto", "a baseball movie starring Kevin", "Coors Field", "Boise State", "Doc Holliday", "Chicken Run", "Zeus", "\"Rama, #1\"", "hydrogen", "Svengali", "Magda", "Mammoth Cave", "a sousaphone", "the 2/27/10 M8.8 Chile earthquake", "Poseidon", "Queen Elizabeth II", "The 39 Steps", "kynikos", "Judges", "oreo", "St. Lawrence", "the seashore", "Indiana Jones", "Staten Island", "Bill Clinton", "Cloverfield", "Paraguay", "Rassendyll", "the East Sea", "You see death not as something to be feared, but as a final rest and reward for a life well spent", "resent", "in # Quiz", "a calculator", "Tuesday", "Olivia Newton-John", "Robert Cohn", "oil", "South Africa", "De Hooch", "Arnold J. Toynbee", "Lisanne Falk", "2004", "Rachel Sarah Bilson", "Pradyumna", "the Wirral", "George H. W. Bush", "blood", "If the citizen's heart was heavier than a feather", "Indooroopilly Shopping Centre", "the first freshman to finish as the runner-up", "opium", "Laura Mansfield", "found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "Tarzan"], "metric_results": {"EM": 0.5, "QA-F1": 0.5743686868686869}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true], "QA-F1": [0.20000000000000004, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-597", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-2509", "mrqa_searchqa-validation-6563", "mrqa_searchqa-validation-4782", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-14733", "mrqa_searchqa-validation-2758", "mrqa_searchqa-validation-3701", "mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4268", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-12223", "mrqa_searchqa-validation-2756", "mrqa_searchqa-validation-15316", "mrqa_naturalquestions-validation-3124", "mrqa_triviaqa-validation-6163", "mrqa_triviaqa-validation-5356", "mrqa_hotpotqa-validation-3713", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-3404"], "SR": 0.5, "CSR": 0.5665922619047619, "EFR": 1.0, "Overall": 0.7285528273809524}, {"timecode": 42, "before_eval_results": {"predictions": ["the Divine Right of Kings", "Mussolini", "Cher", "deuce", "Tarsus", "Charles Chaplin", "Dancing On Ice", "Hermes", "Vietnam", "a sword", "The Detection Club", "Cold Blood", "Youth Can Change the World", "Jackie Joyner", "a whale", "Nelson Mandela", "the Perseid", "Cuba Libre", "The American Experience", "Tanzania", "Oscar Wilde", "Mexico", "Maryland", "Borneo", "mckinley", "Walla Walla", "Netflix", "Roger Bannister", "the Bauhaus Movement", "(Scott) Peterson", "an enigma", "Franco", "Bolivia", "rugby", "Ireland", "Vanna White", "Catherine II", "blue", "the distributor", "ROE", "Elizabeth Cady Stanton", "the Bavarian Alps", "Francis Ford Coppola", "wives and concubines", "to meander", "The Wind in the Willows", "\"Honey, I just forgot to duck\"", "hexadecimal", "The Two Gentlemen of Verona", "the chimpanzee", "the Red Cross", "pigs", "August 2012", "Jack Nicklaus", "the Central African Republic", "six", "Yalta", "Emad Hashim", "The Suite Life on Deck", "The Daily Stormer", "Bayern Munich", "the United States", "cancer", "the Boston Fern"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6567708333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-16519", "mrqa_searchqa-validation-11329", "mrqa_searchqa-validation-13712", "mrqa_searchqa-validation-519", "mrqa_searchqa-validation-2589", "mrqa_searchqa-validation-16809", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-7260", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-7779", "mrqa_searchqa-validation-10935", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-16153", "mrqa_searchqa-validation-12772", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7323", "mrqa_searchqa-validation-14283", "mrqa_naturalquestions-validation-7702", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-2135", "mrqa_hotpotqa-validation-5307", "mrqa_newsqa-validation-3131", "mrqa_triviaqa-validation-6337"], "SR": 0.59375, "CSR": 0.5672238372093024, "EFR": 1.0, "Overall": 0.7286791424418604}, {"timecode": 43, "before_eval_results": {"predictions": ["Orthogonal", "Henry I", "Tomorrow Never Dies", "Liechtenstein", "a bit of running's equivalent of breaking the sound barrier", "nathan", "Columbus", "Brett Favre", "South African", "Brian Deane", "Pakenham", "Argentina", "William Conrad", "1875", "Andrew Lloyd Webber", "Iran", "Fairey Swordfish", "Arran", "London County", "Playboy", "a man's/boy's hairdresser", "Matalan", "Chesney Wold", "boise", "a griffin", "red", "The Pink Panther", "M\u0142yniec", "Judy Cassab", "a gold rings", "Karl Marx and Friedrich Engels", "Utrecht", "Union of Post Office Workers", "Strangeways", "Carousel", "14", "Richard Wagner", "the brain", "\"Garp\"", "(Frederick) William Herschel", "Belgium", "October 31st", "a beetle", "Deacon Blues", "Pompey", "(Denali)", "auction houses", "haddock", "L. P. Hartley", "Italy", "a snake", "Andy Serkis", "2001", "the human hands", "the Distinguished Service Cross", "Shenae Grimes-Beech", "five-time", "prostate cancer,", "the U.S. Holocaust Memorial Museum,", "the 1800s", "Dale", "The Chase", "London", "Yerushalayim"], "metric_results": {"EM": 0.546875, "QA-F1": 0.596875}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10394", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-4052", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2244", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-9414"], "SR": 0.546875, "CSR": 0.5667613636363636, "EFR": 0.9655172413793104, "Overall": 0.7216900960031347}, {"timecode": 44, "before_eval_results": {"predictions": ["co-chair", "a cappella group", "iceland", "Evita", "Victoria", "Sikhism", "william turpin", "Sinclair Lewis", "Argentina", "glaze", "Guatemala", "olive", "M\u00fcnchen", "violin", "a double-hung window", "Paul Nash", "anton", "first among equals", "robin", "Indira Priyadarshini Gandhi", "Colombia", "jean Baptiste Say,", "Uranus", "Prince Igor", "monaco", "an indoor football league", "watt", "The Wicker Man", "nathaniel hkeye", "Gorky", "South Africa", "hovercraft", "john McEnroe", "white", "john Mellencamp", "Tina Turner", "glouce", "brash", "bees", "harold wilson", "william Gallagher", "anton", "adrian ladd", "soft and full", "Wolfgang Amadeus Mozart", "\"Bubba\" Watson, Jr.", "gloucester", "Richard Lester", "December", "peregrines", "steel", "1 October 2006", "Cee - Lo", "Hitler", "Marcus Tullius Reynolds", "35,000", "Tel Aviv University", "response to a civil disturbance call", "central London offices", "\"Hillbilly Handfishin'\"", "diogenes", "Roosevelt, Churchill", "flanker", "an integral membrane protein that builds up a proton gradient across a biological membrane"], "metric_results": {"EM": 0.578125, "QA-F1": 0.644828869047619}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.2857142857142857, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8581", "mrqa_triviaqa-validation-5869", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-7543", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-57", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-954", "mrqa_hotpotqa-validation-773", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-220", "mrqa_searchqa-validation-13219", "mrqa_searchqa-validation-9558"], "SR": 0.578125, "CSR": 0.5670138888888889, "EFR": 0.9629629629629629, "Overall": 0.7212297453703703}, {"timecode": 45, "before_eval_results": {"predictions": ["the Apollo spacecraft", "1853", "Daniel A. Dailey", "hyperarousal, or the acute stress response", "anakin Skywalker", "Plank", "Ann Gillespie", "near Chesapeake Bay", "a loanword of the Visigothic word guma `` man", "March 26, 1973", "drizzle, rain", "Tommy Shaw", "1858", "Charles Perrault", "John Daly", "1998", "Elizabeth Dean Lail", "March 31, 2017", "Victor Salva", "Aristotle", "in 2007", "movement of the Earth's continents relative to each other", "eight", "more than a million", "the last book accepted into the Christian biblical canon", "1995", "Rock Island, Illinois", "1926", "2006 -- 06", "Antarctica", "on the vaginal floor", "Donny Osmond", "Ace", "from the Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s,", "Teddy Randazzo", "muscle cells", "Christopher Jones", "one of the uses of money", "the last Ice Age", "Neal Dahlen", "balance sheet", "London, United Kingdom", "Hellenism", "232", "starting on January 2, 1971", "Andy Cole", "\u20b9 39.97 lakh", "the main type of cell found in lymph", "a crust of mashed potato", "the team", "$72", "the first woman to fly solo from England to Australia", "12", "bukwus", "Westgate Las Vegas Resort & Casino", "Gloria Trevi", "postal delivery", "he dropped his children off at a relative's house,", "the leader of a drug cartel that set off two grenades during a public celebration in September, killing eight people and wounding more than 100.", "slumdog Millionaire", "Clifford Odets", "Margaret Mitchell", "Microsoft", "Jeremy Brett"], "metric_results": {"EM": 0.5, "QA-F1": 0.625807159792198}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.06451612903225806, 0.4, 0.0, 1.0, 0.4210526315789474, 0.0, 1.0, 0.5, 0.13333333333333333, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.25, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-47", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-8596", "mrqa_triviaqa-validation-1421", "mrqa_triviaqa-validation-2110", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-3832", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-947"], "SR": 0.5, "CSR": 0.5655570652173914, "EFR": 0.96875, "Overall": 0.7220957880434783}, {"timecode": 46, "before_eval_results": {"predictions": ["Gamal Abdul Nasser", "2016", "a line of committed and effective Sultans", "Germany", "contributed military and civilian police personnel to peace operations", "Steve Hale", "King Saud University", "Parashara", "John Dalton", "Vienna", "pepsin", "Carol Worthington", "pagan custom", "1928", "April 29, 2009", "19 June 2018", "Todd Bridges", "Ben Savage", "the senior-most judge of the supreme court", "multinational retail corporation", "chili con carne", "Andrea Brooks", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "Britney Spears", "by October 1986", "232", "many forested parts of the world", "The fossilized remains were originally called Plesippus shoshonensis", "1998", "It was a Confederate victory, followed by a disorganized retreat of the Union forces", "mitosis", "an Aldabra giant tortoise", "Texhoma", "marriage officiant", "The expression was inspired by Andy Warhol's words `` In the future, everyone will be world - famous for 15 minutes ''", "The Royalettes", "centigrade", "the division of Italy into independent states, the restoration of the Bourbon kings of Spain, and the enlargement of the Netherlands to include what in 1830 became modern Belgium", "Andrew Johnson", "muscle contraction", "four distinct levels", "if the concentration of a compound exceeds its solubility", "Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "Amanda Leighton", "August Darnell", "Stephen Graham -- Detective Superintendent Dave Kelly", "will first encounters on the planet that his family crash lands on", "the plane crash", "Tatsumi", "Ernest Hemingway", "September 2000", "Venus", "Laos", "\u00ef\u00bf\u00bdNastase", "\"Famous Ghost Stories\"", "Scotty Grainger Jr.", "uncle", "Steve Jobs", "a motion for a preliminary injunction against a Mississippi school district and high school in federal court Tuesday over the April 2 prom.", "The woman then called the university public safety office, which alerted local police.", "the poverty line", "the uterus", "They cool down mainly by panting, which releases evaporated water... asphalt, cement or even sand, their paw pads may get burned", "(John) Y. Brown Jr."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5828025894062079}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.4, 1.0, 0.8333333333333333, 0.0, 0.0, 0.3076923076923077, 1.0, 0.5, 1.0, 0.5, 0.10526315789473684, 1.0, 0.0, 0.07999999999999999, 1.0, 0.0, 0.5, 0.923076923076923, 0.8, 1.0, 0.0, 0.5, 0.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7333333333333334, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-7027", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-3771", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-959", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-1475", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-14439", "mrqa_searchqa-validation-15864"], "SR": 0.46875, "CSR": 0.5634973404255319, "EFR": 1.0, "Overall": 0.7279338430851064}, {"timecode": 47, "before_eval_results": {"predictions": ["2014", "Ren\u00e9 Descartes", "Sauron", "Dante Pastula", "Havana Harbor during the Cuban revolt against Spain", "sedimentary rock", "April 10, 2018", "Tracy McConnell", "North Atlantic Ocean", "self - closing flood barrier", "111", "2 %", "an Irish feminine name", "the head of the Imperial Family and the traditional head of state of Japan", "November 17, 2017", "an instant messaging client that was first developed and popularized by the Israeli company Mirabilis in 1996", "appellate court", "Julianne Hough as Brittany Tierney / Katie Feldman", "84", "William Chatterton Dix", "`` Killer Within ''", "Broken Hill and Sydney", "appendicular skeleton", "a database maintained by the United States federal government, listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "a couple broken apart by the Iraq War", "Johannes Gutenberg", "National Industrial Recovery Act ( NIRA )", "Pittsburgh", "the ulnar nerve", "The Republicans, who were loyal to the democratic, left - leaning and relatively urban Second Spanish Republic", "31 October 1972", "twelve", "Matt Flinders", "The person who has existence in two paradise", "IMS is not intended to standardize applications, but rather to aid the access of multimedia and voice applications from wireless and wireline terminals", "Ra\u00fal Eduardo Esparza", "PC2", "4 September 1936", "the field is limited to drivers who meet more exclusive criteria", "H.L. Hunley", "Orangeville, Ontario, Canada", "dromedary", "the Jews", "Abraham Gottlob Werner", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "1984", "2005", "March 18, 2005", "Fats Waller", "the fovea centralis", "the Russian army", "one of the Vikings nine realms", "Rudyard Kipling", "THE PENGUIN", "Otto Eduard Leopold,", "Ukrainian", "237", "Apple employees", "she offered her \"sincere apologies for any offense.\"", "in July", "Margaret Mitchell", "a cross with the Risen Christ licit", "Lisa Lisa Lisa", "right-hand"], "metric_results": {"EM": 0.515625, "QA-F1": 0.650059269796112}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8108108108108109, 0.0, 1.0, 0.888888888888889, 0.25, 1.0, 0.13333333333333333, 1.0, 0.1111111111111111, 1.0, 0.7999999999999999, 0.0, 1.0, 0.0, 1.0, 0.25, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-8179", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-10321", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-7358", "mrqa_triviaqa-validation-3828", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-272", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-16263"], "SR": 0.515625, "CSR": 0.5625, "EFR": 0.967741935483871, "Overall": 0.7212827620967742}, {"timecode": 48, "before_eval_results": {"predictions": ["Magdalen College", "Rana Daggubati", "13 people and injuring 145", "V. Prakash Kumar", "12 wins, 3 defeats and 1 draw", "Kristy Lee Cook", "LA Galaxy", "The Volvo 850", "February 14, 1859", "\"'Tis the Fifteenth Season\"", "Biola University in La Mirada, California", "Academy Award for Best Art Direction", "2012 NBA draft", "October 13, 1980", "tomato", "Gracie Mansion", "Arsenal Football Club", "Operation Neptune", "Steve Prohm", "a super-regional shopping mall owned by Simon Property Group,", "Charlotte Carnegie", "2000 World Rally Championship", "seven players have had 50\u201340\u201390 seasons.", "28 June 1945", "University of California", "Miami Gardens, Florida", "Indian", "Paige O'Hara", "Graham Hill", "The Emperor of Japan", "Hillary Clinton presidential campaign, 2016", "1896", "formerly held the American record for the most time in space (381.6 days)", "the D\u00e2mbovi\u021ba River", "Philip Mark Quast", "Pierce County", "PPG Paints Arena", "May 10, 1976", "Rodrick Heffley", "Operation Julin", "BAFTA TV Award Best Actor", "the Slavic women accompanying their husbands in the First Balkan War", "1641", "Marty Ingels", "Carl David Tolm\u00e9 Runge", "1941", "June 2, 2008", "Charice", "Sleepy Brown", "Waimea Bay", "Ustad Vilayat Khan", "the part of the brain", "his last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "restoring someone's faith in love and family relationships", "Moose the dog, better known as Eddie in US sitcom Frasier,", "gold wedding anniversary", "the auk family, Alcidae", "nearly $162 billion in war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "Bailey, Colorado, home,", "The Sopranos", "Winslow Homer", "Elizabeth II", "the Crimean War", "Alexey Pajitnov,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6075948033031169}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.5, 0.5714285714285715, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5454545454545454, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.3076923076923077, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.823529411764706, 0.18181818181818182, 0.0, 0.0, 0.3870967741935484, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-1757", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-4394", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-438", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-5535", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1997", "mrqa_searchqa-validation-14284"], "SR": 0.46875, "CSR": 0.5605867346938775, "EFR": 0.9705882352941176, "Overall": 0.7214693689975991}, {"timecode": 49, "before_eval_results": {"predictions": ["Graham Brand Organization", "Dubai", "Silverstone", "Ted", "Triumph and Disaster", "1720", "the Battle of Agincourt", "\"The Ram\" Robinson", "beetle", "cuticle", "100 years", "Rudyard Kipling", "dragonflies", "Cole Porter", "liriope", "heston", "Big Brother", "edo", "Beaujolais", "Christchurch", "Paul Dukas", "Tom Watson", "9", "ear", "Tokyo", "low-E", "keeper of the Longstone (Fame Islands) lighthouse", "God bless America, My home sweet home", "Dangerous Minds", "death", "Apollo", "\"daft as a brush\"", "South Korea", "Boxing Day", "St Pancras International", "fish", "wain", "strep", "Scarborough", "Alan Turing", "Newton", "Calcium carbonate", "Bombay", "Anna", "eddie", "naxos", "sporty hats", "Hitachi", "plutarch", "New Belgrade", "the gizzard", "Detroit Tigers", "counter clockwise", "3.5 million years old", "Tampa Bay Lightning", "Theatre Ventures, Inc.", "Battle of Dresden", "Haiti", "$150 billion", "root out terrorists within its borders", "Latvia", "alligator", "Jerry Lee Rice", "Tarzan"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6338541666666666}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-7216", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-6646", "mrqa_triviaqa-validation-293", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-6612", "mrqa_naturalquestions-validation-2621", "mrqa_hotpotqa-validation-1029", "mrqa_newsqa-validation-93", "mrqa_searchqa-validation-2086", "mrqa_searchqa-validation-11198", "mrqa_searchqa-validation-5649"], "SR": 0.5625, "CSR": 0.5606249999999999, "EFR": 0.9285714285714286, "Overall": 0.7130736607142858}, {"timecode": 50, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2584", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-438", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-4499", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5405", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3559", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14232", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16791", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7018", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7260", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7002", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-21", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96"], "OKR": 0.81640625, "KG": 0.4625, "before_eval_results": {"predictions": ["Medusa", "Hawaii", "Easy Rider", "scrabble", "Percy Bysshe Shelley", "Billy Joel", "a pardon", "(Con) Edison", "Joseph Conrad", "Roman Polanski", "Dogberry", "Battle Creek", "the Red Sea", "Mary Todd Lincoln's", "Mary Poppins", "isaac america", "phonetics", "The Naked Brothers Band", "Julianne Moore", "saddle bags", "Holly Golightly", "a quilt", "anemoi", "butter", "The Tagus", "the CIO", "acting out the Bible", "USS nautilus", "bantu", "Denmark", "a student loan", "steak & kidney pie", "the fife", "Seattle", "Michael Jordan", "John Quincy Adams", "the French Legion of Honour", "Louis XIII", "Korea", "December 23, 1777,", "chancellor of West Germany", "Washington Irving", "Crimean Peninsula", "almond Joy", "the White House", "a gastropod shell", "Julius Caesar", "One dollar and eighty-seven cents", "Dean Acheson", "Pittsburgh Steelers", "jury trials in certain civil cases", "Malina Weissman", "Kyla Pratt", "Jonathan Goldstein", "about a quarter (fourth) of a full barrel", "the Jews", "Spain", "Sean Yseult", "1754", "Robert Harper", "outside the Iranian consulate in Peshawar,", "in his native Philippines", "she's in love", "September 21, 2014"], "metric_results": {"EM": 0.53125, "QA-F1": 0.615438988095238}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6167", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-12468", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-664", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-11463", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-4385", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-15599", "mrqa_searchqa-validation-9986", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-674", "mrqa_hotpotqa-validation-3612", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-551", "mrqa_hotpotqa-validation-4911"], "SR": 0.53125, "CSR": 0.5600490196078431, "EFR": 1.0, "Overall": 0.7088066789215686}, {"timecode": 51, "before_eval_results": {"predictions": ["Reddi-wip", "dachshund", "Saturn", "bagny Taggart", "Risk", "a Bar Mitzvah", "cauliflower ear", "Clark Gable", "Katharine Hepburn", "Metacomet", "surrender", "Tarsus", "the Niagara Falls", "Hannibal Lecter", "The Man Without A Country", "the Arc de Triomphe de", "George Frideric Handel", "cologne", "Indonesia", "Florence Henderson", "Linus Pauling", "gold", "the English Channel", "a whelp", "water", "Ohio", "Million Dollar Baby", "rum", "organ", "Papua New Guinea", "Macy's Department Store", "Jeb Bush", "the Arctic Ocean", "water", "Port-au-Prince", "the \"Coastal\" name", "humility", "Michael Phelps", "rice", "gas masks", "\"to look like\"", "\"Juno\"", "the breast", "water jets", "Louis XIV of France", "a suspension bridge", "faerie", "wearily", "JetBlue", "Ryan Seacrest", "a key", "Lake Michigan", "Spanish colonies", "home state of Texas", "piscinae", "the wren", "Islamabad", "Indianapolis Motor Speedway", "$26 billion", "Deftones", "Adam Lambert and Kris Allen", "27-year-old", "more than two years,", "Michael Madhusudan Dutta"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6149553571428571}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-14737", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-13086", "mrqa_searchqa-validation-16385", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10836", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-10726", "mrqa_searchqa-validation-3994", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-10574", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-10180", "mrqa_searchqa-validation-8660", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-2254", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-2705", "mrqa_newsqa-validation-3066"], "SR": 0.546875, "CSR": 0.5597956730769231, "EFR": 1.0, "Overall": 0.7087560096153847}, {"timecode": 52, "before_eval_results": {"predictions": ["Alfred Preis", "\"Nip/Tuck\"", "Adelaide Lightning", "Homeland", "1983", "3.9 mi", "Hilux", "Roc Me Out", "\"Mona Leaves-a\"", "Anna Clyne", "Flushed Away", "the Elbow River", "Mickey's Christmas Carol", "Ellie Kemper", "Aamir Khan", "Eugene Levy", "25 million", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu", "Julianne Moore", "drummer Seb Rochford", "Samantha Spiro", "David Anthony O'Leary", "Los Angeles Galaxy", "Faysal Qureshi", "Total Nonstop Action Wrestling", "Don Hahn", "Nobel Prize in Physics", "American folk-song", "Leinster", "Blue Grass Airport", "Tim Whelan", "the Cleveland Celtics", "Ashley Jensen", "Franc Roddam", "Ben R. Guttery", "Pieter van Musschenbroek", "ABC", "Roseann O'Donnell", "\"media for the 65.8 million,\"", "1902", "the USS \"Enterprise\"", "Las Vegas", "Todd Emmanuel Fisher", "finished goods", "John M. Dowd", "August 9, 2017", "MGM Grand Las Vegas", "(n\u00e9e Dickins)", "Clara Petacci", "1986 to 2013", "Bill Ponsford", "one of Jesus'disciples", "Eddie Oparei", "the Gaget, Gauthier & Co. workshop", "Martin Van Buren", "Robert Boyle", "Vienna", "Harrison Ford", "the campus of a school", "27-year-old's", "the Squirrel", "(Persian)", "condensation", "Kitty Kelley"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6373647186147186}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-1584", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-3321", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-3286", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-10", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-3391", "mrqa_triviaqa-validation-2082", "mrqa_newsqa-validation-2415", "mrqa_searchqa-validation-11381", "mrqa_searchqa-validation-12184"], "SR": 0.53125, "CSR": 0.5592570754716981, "EFR": 1.0, "Overall": 0.7086482900943396}, {"timecode": 53, "before_eval_results": {"predictions": ["London", "April 2, 2018", "Welch, West Virginia", "800", "2010", "Mark Jackson", "Indonesia", "December 24, 1836", "2 September 1990", "BC Jean", "Billy Bishop Toronto City Airport on the Toronto Islands", "off the southernmost tip of the South American mainland", "Roger Dean Stadium", "Jesse Wesley Williams", "19 June 2018", "Prince William", "Sanchez Navarro", "Jason Mantzoukas", "pigs", "Pittsburgh", "2018", "to the left of the dinner plate", "headdresses", "in a Norwegian town", "1960", "1840s", "AMX - 13", "semi-automatic", "Humpty Dumpty", "displacement", "halogenated paraffin hydrocarbons", "200 to 500 mg up to 7 litres", "Mike Gabriel", "November 5, 2017", "blood flow to those organs involved in intense physical activity", "Qutab Ud - Din - Aibak", "M\u00e1ximo Gomez", "muscles", "Robin", "March 26, 1973", "New England Patriots", "New York City", "S", "31 - member Senate", "Ajay Tyagi", "from the heraldic crest carved in the lintel on St. Ignatius'family home in Azpeitia, Spain", "Brooklyn Heights", "book and architecture", "19 June 2018", "Efren Manalang Reyes", "California", "Barcelona", "molybdenum", "Henri Paul", "January 4, 1976", "1921", "General Allenby", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "some great travel spots to be altered or ruined by global climate change.", "22", "altitude", "Boots", "Foo Fighters", "Jennifer Grey"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6618748132994456}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.2, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.11764705882352941, 0.33333333333333337, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8873", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-3801", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-3837", "mrqa_hotpotqa-validation-5720", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1638", "mrqa_searchqa-validation-6725"], "SR": 0.578125, "CSR": 0.5596064814814814, "EFR": 0.9629629629629629, "Overall": 0.7013107638888888}, {"timecode": 54, "before_eval_results": {"predictions": ["the martini", "a final contest", "New York City", "Barack Obama", "Cottage cheese", "Tony Gwynn", "Atonement", "Kentucky", "collagen", "Just say no", "typewriters", "Diane Arbus", "Cincinnati", "Cleopatra, Queen of Denial", "the Suez Canal", "Planet of the Apes", "garret", "Adam Sandler", "the compass", "Erasmus", "a member", "William Shakespeare", "phobias", "San Jose", "piano", "the Byzantine Empire", "Dunkirk", "Black", "Psalms", "a pearl", "Gelato", "Jesus", "viruses", "George Balanchine", "Alfred Stieglitz", "Bryan Adams", "Africa", "Gaius Cassius Longinus", "Applebee's", "the Mercator", "Robin Hood", "sauropods", "Boris Godunov", "Daniel Boone", "William Tecumseh Sherman", "Prison Break", "a hippopotamus", "a horse", "Charles II", "Sinclair Lewis", "Leo III", "85 %", "Jamestown", "Uruguay", "stasis", "Mr. Humphries", "Boulder Dam", "Personal History", "Hellenism", "Barnoldswick", "bodies and heads", "the U.S. Holocaust Memorial Museum", "BET", "Havana Harbor"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6989583333333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-12810", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-1876", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-308", "mrqa_searchqa-validation-7376", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-1618", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-1657", "mrqa_newsqa-validation-23"], "SR": 0.65625, "CSR": 0.5613636363636363, "EFR": 1.0, "Overall": 0.7090696022727273}, {"timecode": 55, "before_eval_results": {"predictions": ["a zebra", "Sarah McLachlan", "modest", "Japan", "C Daryl Chessman", "grade point average", "grapefruit", "Detroit", "Tiger Woods", "John Paul II", "Blackbeard", "a goose", "Jane Goodall", "Big Ben", "Ethiopian", "nd", "Stephen Crane", "Luxor", "gung-ho", "a nickel", "Bill Clinton", "Wyoming", "the nasal cavity", "Nantucket", "Abnormal Psychology", "Never let me go", "Gianlorenzo Bernini", "mosquitoes", "Frank Sinatra", "a British rock band", "photons", "the National Archives Building", "low blood pressure", "Mousehunt", "Israel", "honey", "Rugby Football Union", "a courtship between", "a palace", "coffee", "Knott\\'s Berry Farm", "Phaedra", "Carl Linnaeus", "Australia", "Jodie Foster", "ventricular fibrillation", "Barbary pirates", "cinnamon", "an American sitcom", "Matilda Silicon carbide", "modest", "a 1923 Pulitzer Prize - winning volume of poems written by Robert Frost", "Master Christopher Jones", "T.J. Miller", "7", "Mark Renton", "Jerry Mouse", "AVN Adult Entertainment Expo", "England and Ireland", "The Beatles", "identity documents", "refused to refer the case of Mohammed al-Qahtani to prosecutors", "the 11th anniversary of the September 11, 2001, terror attacks.", "Philip Billard Municipal Airport"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6493130133755134}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.8181818181818181, 0.3076923076923077, 0.888888888888889]}}, "before_error_ids": ["mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-15701", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-15622", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-6240", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-12914", "mrqa_searchqa-validation-4022", "mrqa_searchqa-validation-11601", "mrqa_searchqa-validation-14248", "mrqa_searchqa-validation-7849", "mrqa_searchqa-validation-5988", "mrqa_searchqa-validation-12284", "mrqa_naturalquestions-validation-10546", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-7361", "mrqa_hotpotqa-validation-3169", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2215", "mrqa_hotpotqa-validation-2840"], "SR": 0.53125, "CSR": 0.5608258928571428, "EFR": 0.9666666666666667, "Overall": 0.7022953869047619}, {"timecode": 56, "before_eval_results": {"predictions": ["Jean Lafitte", "Isomachus of Croton", "argyle", "the Pacific Ocean", "Easter", "\"Of course\" God will forgive me", "Dalai Lama", "a heptathlon", "a tuba", "The World is Flat: A Brief History of the Twenty-first Century", "tea", "Arteries", "Nicholas", "Amerigo Vespucci", "Patrick Henry", "Essen", "punk", "Ho Chi Minh", "the pituitary gland", "Ben Johnson", "Kentucky", "Theodosius I (379-395)", "9 to 5", "Russia", "lupper", "Velvet Revolver", "Sears", "chia seeds", "a cherries", "Florence", "Ma Barker", "Joe DiMaggio", "Tie", "Naples", "Nick and Norah\\'s Infinite Playlist", "the Baruch Plan", "a Big Dipper", "wine", "silk", "\"The Safety Dance\"", "the Cymric cat", "the Balconies of Lima", "a GPS", "North Carolina", "M&M'S Peanuts Chocolate Candies", "a cake knife", "Peter", "Versailles", "the Panama Canal", "Cessna 172", "General McClellan", "Ty Olsson", "Parthenogenesis", "Pangaea", "london", "mule", "60", "Silvia Navarro", "26 November", "John Morgan", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "Mary Phagan,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "off the coast of Somalia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.598812819125319}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.12121212121212123, 1.0, 0.4615384615384615, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-665", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-12387", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-10639", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-9458", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-11186", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-795", "mrqa_searchqa-validation-3454", "mrqa_searchqa-validation-11589", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-2151", "mrqa_searchqa-validation-15973", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-3262", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-1022"], "SR": 0.515625, "CSR": 0.560032894736842, "EFR": 1.0, "Overall": 0.7088034539473684}, {"timecode": 57, "before_eval_results": {"predictions": ["a spectator", "French toast", "Mexico", "plug in", "Faulkner", "Patty Duke", "Hawthorne", "Hindu", "Juno", "Intel", "Hank Williams Jr.", "George C. Wallace", "the state\\'s", "an offensive", "West Virginia", "Edward Hopper", "asteroids", "Huckleberry Finn", "the Hubble Space Telescope", "Pop-Tarts", "Robert Johnson", "John H. Miller", "Adam Smith", "Tootsie", "roots", "albino", "Bonn", "a wasteland", "chinchillas", "Tennessee", "the No Child Left Behind Act", "William S. Hart", "the A\\'s", "Francisco Franco", "Tennessee Williams", "four", "Robert Downey Jr.", "West Point", "Revolver", "Steely Dan", "I", "Norway", "kotleta po-kyivsky", "George Clooney", "a diamond", "the Baltimore Orioles", "postcards", "Kentucky", "Skateboarding", "Gaul", "blasters", "funding for operations, personnel, equipment, and activities", "Havana Harbor", "Melbourne", "Atticus Finch", "a donkey", "Max Planck", "Ella Fitzgerald", "Ed \"Ed\" O'Neill", "First Blood", "Dancing With the Stars", "approximately 600 square miles of south-central Washington,", "February's Winter Games in Vancouver", "Upstairs Downstairs"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6737589667277166}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.25, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-3396", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-9485", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-7607", "mrqa_searchqa-validation-6688", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-5196", "mrqa_searchqa-validation-13808", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-12764", "mrqa_searchqa-validation-8729", "mrqa_naturalquestions-validation-10533", "mrqa_triviaqa-validation-3812", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-381", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-1727"], "SR": 0.609375, "CSR": 0.5608836206896552, "EFR": 1.0, "Overall": 0.708973599137931}, {"timecode": 58, "before_eval_results": {"predictions": ["Gov. Andrew Jackson", "Sri Lanka", "7.7", "Hinduism", "Billie Holiday", "wedlock", "trans fat", "Fairies", "Edward of Bordeaux", "Hello, Dolly!", "the Mesozoic Era", "Gettysburg", "Martin Lawrence", "plantain", "Heracles", "Fosse", "embryonic stem cells", "a cutlass", "the Bodleian Library", "the pupil", "a front", "James Franco", "salmon", "The Crow", "a sheep's milk cheese", "James Watt", "1945", "a birthstone", "Ichabod Crane", "Morrie: An Old Man, a Young Man", "Heather Locklear", "word", "Holden Caulfield", "Chocolate Hazelnut Truffles", "Saudi Arabia", "lamb", "LaDainian Tomlinson", "wheat", "Duke", "a photoelectric cell system", "Cape Town", "sperm", "Austin Powers", "sourdough", "Moissanite", "vice presidential running mate", "Stalin", "La Guardia", "Chastity", "Turandot", "Texas Rangers", "Camille Pissarro", "ice giants", "Amenhotep IV", "Zimbabwe", "colony", "the fear of \"going mad\" or losing control,", "Haiti", "a parabolic reflector", "1891", "U.S. Vice President Dick Cheney", "1-1.", "an upper respiratory infection,", "Russia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5940104166666667}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false], "QA-F1": [0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-7364", "mrqa_searchqa-validation-11844", "mrqa_searchqa-validation-16405", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-16918", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-4326", "mrqa_searchqa-validation-16956", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5137", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-14926", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-13956", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-2685", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-119", "mrqa_newsqa-validation-2472", "mrqa_naturalquestions-validation-3214"], "SR": 0.515625, "CSR": 0.5601165254237288, "EFR": 1.0, "Overall": 0.7088201800847458}, {"timecode": 59, "before_eval_results": {"predictions": ["the Monet Folie-Bergeres", "Mary Magdalene", "The Pillow Book", "General Paulus", "the Grail", "butcher", "The Double", "Dr. Samuel Johnson", "Jessica Simpson", "Zeppelin", "the gallbladder", "peterloo massacre", "Aaron", "Leo Tolstoy", "Birmingham", "the Penrose triangle", "The Magnificent Seven", "the Australian shearers' strike", "Theodore Roosevelt", "raven", "John of Gaunt", "typhoid fever", "germanium", "Microsoft", "John Galliano", "the Big Bang", "Willie Nelson", "horseracing", "\"Stars on 45 Medley\"", "Lundy", "Guinea", "Nadia Comaneci", "Belgium", "Charlton", "Turnbull & Asser", "non-Orthodox synagogues", "Stitch", "Herbert Asquith,", "Nirvana and Kiss", "Mr. Humphries", "Paul Gauguin", "wildebeest", "the Low Countries", "50", "Charlie Harper", "nirvana", "Tarzan", "purple", "Bob Ferris", "aardvark", "Charles Darwin", "5.7 million", "Oklahoma", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "8,515", "villanelle", "Awake", "2008", "China, Hong Kong and Mongolia", "Patrick McGoohan", "a coyote", "heating", "A Tale of Two Cities", "Charlie Wilson"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7011837121212121}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-4273", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-3489", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-6759", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-577", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1182", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4592", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-4197", "mrqa_newsqa-validation-883", "mrqa_newsqa-validation-2061"], "SR": 0.609375, "CSR": 0.5609375, "EFR": 0.88, "Overall": 0.684984375}, {"timecode": 60, "UKR": 0.689453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1121", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-122", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4518", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-5828", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-159", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-1071", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10905", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11198", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-12032", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-12468", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15373", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16915", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3160", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3329", "mrqa_searchqa-validation-3454", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4232", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6187", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8373", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-9809", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4887", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9613", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1078", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3904", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4074", "mrqa_triviaqa-validation-4497", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-5183", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-690", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-998"], "OKR": 0.791015625, "KG": 0.46875, "before_eval_results": {"predictions": ["15", "G\u00f6tene in Sweden", "William Walton", "Rensselaer County", "more than 20 principal operations and manufacturing facilities worldwide", "Beauty and the Beast", "authoritarian tendencies", "penny bun", "Overtime", "north", "Hockey Club Davos", "26 December 1982", "Domingo \"Sam\" Samudio", "Sunday", "Taylor Swift", "Asif Kapadia", "Rogue One", "Graffiti", "ESPN", "Bangor International Airport", "October 29, 1985", "Point of Entry", "Mickey's Christmas Carol", "the Harpe brothers", "the 1940s and 1950s", "Sandusky", "deadpan sketch group", "Bharat Ratna", "Ronald Joseph Ryan", "as a TV series", "the 2011 Pulitzer Prize in General Nonfiction", "Eliot Cutler", "IT products and services", "American", "1865", "Critics' Choice Television Award", "Jeff Meldrum", "Picric acid", "23 March 1991", "1979", "Hannaford", "1968", "post-Roman Republic", "Rigoletto", "Bill Clinton", "\"The Tonight Show\"", "94", "South Korean horror film", "vous dirai-je, maman", "law", "28,776", "a Canaanite god associated with child sacrifice", "over the specimen", "commemorating fealty and filial piety", "(Cain) and Eve's eldest son,", "video", "colonel", "people against Switching Sides (PASS)", "North Korea", "jund Ansar Allah", "thames", "Pamela Anderson", "Henry Ford", "methane"], "metric_results": {"EM": 0.515625, "QA-F1": 0.629985119047619}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.28571428571428575, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.8, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-804", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-5589", "mrqa_hotpotqa-validation-2263", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-3991", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-1151", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-1570", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-182", "mrqa_triviaqa-validation-6429", "mrqa_triviaqa-validation-3314", "mrqa_newsqa-validation-2732"], "SR": 0.515625, "CSR": 0.5601946721311475, "EFR": 0.967741935483871, "Overall": 0.6954310715230037}, {"timecode": 61, "before_eval_results": {"predictions": ["Awake", "Law Adam", "Daniel Craig", "Magnus Carlsen", "Volvo 850", "400 MW", "1991", "Dra\u017een Petrovi\u0107", "Lord's Resistance Army", "Andrew Joseph \" Andy\" Cohen", "Manhattan", "IFFHS World's Best Goalkeeper", "David May", "New Jersey", "Sir Derek George Jacobi", "Waimea Bay", "Willie Nelson and Kris Kristofferson", "the Mikoyan design bureau", "Nickelodeon on Sunset", "Terry the Tomboy", "a saint", "Give Up", "Matthew Ward Winer", "University of Kentucky", "WB Television Network", "Ice Princess", "on Boxing Day, 2004", "liberty as its main idea, promoting free expression, freedom of choice, other social freedoms, and \"laissez-faire\" capitalism", "Australian", "Norse mythology", "Konstant\u012bns Raudive", "Melville", "5,922", "White Horse", "Black Abbots", "Moon Embracing the Sun", "Kentucky, Virginia, and Tennessee", "2011", "five", "Veronica Hamel", "literary magazine", "French, English and Spanish", "Edward James Olmos", "John R. Leonetti", "\"Alceste\"", "Perth", "Cersei Lannister", "Baltimore", "a stopwatch feature", "Joseph Conrad", "\"The Simpsons\"' thirteenth season", "Ferm\u00edn Francisco de Lasu\u00e9n", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W \ufeff / \ufffdrous 22.000 \u00b0 N 80.000", "Yuzuru Hanyu", "Jesse Lingaard", "Jordan", "lulu", "sniff out cell phones.", "18", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "Prison Break", "Oscar Wilde", "Sicily", "Yukon"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7604084457209457}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.19047619047619047, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.64, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3542", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-1670", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-4477", "mrqa_naturalquestions-validation-5451", "mrqa_triviaqa-validation-4367", "mrqa_newsqa-validation-4032"], "SR": 0.6875, "CSR": 0.5622479838709677, "EFR": 0.95, "Overall": 0.6922933467741935}, {"timecode": 62, "before_eval_results": {"predictions": ["at home, attending every soccer game and knowing what his kids like to eat for breakfast", "flew from Rome to Yaounde, the capital of Cameroon,", "Den of Spies", "company Polo", "punish participants in this week's bloody mutiny, which killed nearly 100 army officers and civilians,", "Venezuela", "Israeli ambassador to the United Nations", "European Union", "a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "India", "snowstorm", "a member of the group dubbed the \"Jena 6\"", "illegal immigrants", "the Declaration of Independence,", "reyes", "Cash for Clunkers", "in San Diego,", "returning combat veterans", "Jesus Christ", "Mexican military", "a fossil", "jose antonio reyes", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "Thursday", "a passenger's name", "$17,000", "to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "\"The Roland S. Martin Show\"", "Matthew Fisher,", "26", "an angel", "$1,500", "the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "American Civil Liberties Union", "that the company's products are roadworthy.", "1994", "Raymond Thomas,", "Port-au-Prince", "$83,03013", "$250,000", "reyes", "Islamabad", "hundreds of people joined a campus rally to oppose racial intolerance.", "KCNA", "Osama", "the release of the four men -- Jesus Ortiz, 19; Stalin Felipe, 19, Kevin Taveras, 20; and Rondell Bedward, 21; all of the New York metropolitan area,", "said that \"exceptional circumstances surround these memos and require their release.\"", "the capital city of Harare.", "Vernon Forrest,", "sexual assault with a minor", "the District of Columbia National Guard,", "in March 1930", "1961", "Rajendra Prasad", "Manchester", "Secretary of State William H. Seward", "the Cascade Range", "Ice Princess", "Hispania Racing F1 Team", "small family car", "delete", "Kansas", "Louis XVII", "Kim Basinger"], "metric_results": {"EM": 0.328125, "QA-F1": 0.41428463309763713}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.6363636363636364, 0.0, 0.0, 0.0, 0.125, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 1.0, 0.4, 1.0, 0.21428571428571427, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.07692307692307691, 0.12903225806451613, 0.0, 1.0, 0.0, 0.888888888888889, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2831", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-1289", "mrqa_naturalquestions-validation-7628", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6501", "mrqa_hotpotqa-validation-1346", "mrqa_searchqa-validation-5326"], "SR": 0.328125, "CSR": 0.558531746031746, "EFR": 0.9767441860465116, "Overall": 0.6968989364156515}, {"timecode": 63, "before_eval_results": {"predictions": ["Israel", "billboards", "Saturn", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law.", "Kgalema Motlanthe,", "Ken Choi,", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "democracy,", "up to $50,000 for her,", "gun charges", "January 24, 2006.", "usion teams", "Philippines", "used", "July", "her home", "natural gas", "in the mouth.", "jazz", "the Rockies", "beat and binding Andrade, one of the kidnapper put a gun to Valencia's head.", "40 lash for the incident which is said to have taken place in the capital Khartoum on August 21.", "KBR.", "Ralph Lauren", "Dubai", "Al-Shabaab", "the classic \"The Wonderful Wizard of Oz\" tale,", "269,000", "eight", "Dube, 43, was killed", "North Korea", "Tuesday in Los Angeles.", "Wally", "Alina Cho", "WTA Tour titles at Strasbourg and Bali prior to Madrid", "the nose, cheeks, upper jaw and facial tissue", "1983", "made one of his strongest statements to date on the sex abuse scandal sweeping the Roman Catholic Church,", "collaborating with the Colombian government,", "three-time road race world champion, as well as a double winner of the women's Tour de France, and the clear favorite for gold in Seoul.", "\"We tortured (Mohammed al-) Qahtani,\"", "Yemen.", "11", "Matthew Fisher", "Afghanistan's restive provinces", "Dan Parris, 25, and Rob Lehr, 26,", "insect stings,", "Tennessee", "help evacuate them,", "the chemical at the Qarmat Ali water pumping plant in southern Iraq", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "Sleeping with the Past", "in the ark of the covenant", "the pachytene stage of prophase I of meiosis", "the Great Chicago Fire", "4", "Edward III", "fourth", "BAFTA Award for Best Production Design", "1974", "abraham Hannibal", "Bronx Park", "Percheron", "November"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5448327630665744}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.19999999999999998, 1.0, 0.10526315789473685, 0.4444444444444445, 0.8, 0.2608695652173913, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473684, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.4615384615384615, 0.8750000000000001, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.0, 0.15384615384615383, 0.9166666666666666, 1.0, 0.0, 0.7777777777777778, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-3049", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-7035", "mrqa_triviaqa-validation-1159", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-2480", "mrqa_hotpotqa-validation-3383", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1794"], "SR": 0.390625, "CSR": 0.555908203125, "EFR": 1.0, "Overall": 0.701025390625}, {"timecode": 64, "before_eval_results": {"predictions": ["We Found Love", "The 19-year-old woman", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "40", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "-- including ultra-high-strength steel and boron -- helped make the new truck safer, but also could make it more expensive to repair after a collision.", "19", "Paul McCartney and Ringo Starr", "great jazz music", "homicide.", "Sodra nongovernmental organization,", "on the family's blog", "\"Toy Story\"", "5", "the 12th on the Blue Monster course at Doral", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "signed a power-sharing deal with the opposition party's breakaway faction,", "\"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "Russian air force,", "Rod Blagojevich", "Fiona MacKeown", "50", "the legitimacy of that race.", "President Obama", "John Lennon and George Harrison,", "aron Bialek", "in the United States since 1998.", "45 minutes, five days a week", "Israel", "Monday.", "New Haven, Connecticut, firefighter Frank Ricci,", "Sixteen", "neither Sudanese nor orphans,", "EU naval force", "Kenyan forces", "Daytime Emmy Lifetime Achievement Award", "since 1983", "the foyer of the BBC building in Glasgow, Scotland", "The UNHCR", "EU naval force", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "attempted burgl stemming from a fatal encounter with police officer Daniel Enchautegui.", "two", "6-2 6-1", "U.S. Consulate in Rio de Janeiro,", "John Demjanjuk", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "citizenship to a man", "secure more funds from the region", "Redwood Original", "Emily Blunt", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "The board base for physically supporting and wiring the", "Richard Wagner", "H. H. Asquith", "The Arizona Health Care Cost Containment System", "FBI", "Macomb County", "Custer", "Louis XIV", "a waterbed", "Risk"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5405484210882607}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.3636363636363636, 0.0, 1.0, 0.4, 0.17391304347826084, 1.0, 0.33333333333333337, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.10526315789473684, 0.0, 0.07407407407407408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.5714285714285715, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08333333333333333, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3, 0.5, 0.7692307692307693, 0.23529411764705882, 0.0, 0.14285714285714288, 0.22222222222222224, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-574", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-5934", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-2837"], "SR": 0.4375, "CSR": 0.5540865384615384, "EFR": 1.0, "Overall": 0.7006610576923077}, {"timecode": 65, "before_eval_results": {"predictions": ["North West England", "Carol Ann Duffy", "Liquidambar styraciflua", "William Shirer", "Battleship", "Hurricane Faith", "the First Balkan War", "Teutonic Knights", "9", "James Harrison", "Germany", "Brian Doyle- Murray", "Ford Island", "2011", "I", "Tim Allen", "Latium in central Italy,", "Paul Avery", "Berea College", "Christopher Lloyd Smalling", "January 4, 1976", "a zero-g-roll", "1971", "Clovis I", "Tie Domi", "2007", "writer", "Quasimodo, the deformed bell-ringer of Notre Dame", "Savin Yeatman-Eiffel of Sav!", "Pieter van Musschenbroek", "20 May 1973", "actress", "Attorney General and as Lord Chancellor of England", "Plato", "Fife, Scotland", "Henry Mills", "ribosomal RNA", "Ronald Ryan", "A Hard Day's Night", "Humberside", "Dumfries and Galloway,", "\"A Charlie Brown Christmas\"", "from 1989 until 1994", "Cecily Strong", "Polish", "Philip Aaberg", "in 2005", "Levon Helm", "Chengdu Aircraft Corporation", "White Knights of the Ku Klux Klan", "Reunited Worlds", "Lou Rawls", "the first year begins", "Mark Lowry", "Parkinson's disease", "I Will survive", "William Butler Yeats", "Casa de Campo International Airport", "11", "why you broke up,", "Bizet", "Pisa", "Massachusetts", "in July"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6472126831501831}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.2857142857142857, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.7692307692307693, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-1244", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5637", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2872", "mrqa_hotpotqa-validation-5662", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1433", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-4711", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-10550", "mrqa_triviaqa-validation-4573", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-1180", "mrqa_searchqa-validation-14683", "mrqa_newsqa-validation-271"], "SR": 0.515625, "CSR": 0.5535037878787878, "EFR": 0.9354838709677419, "Overall": 0.687641281769306}, {"timecode": 66, "before_eval_results": {"predictions": ["Solomon", "alligator", "#2 in metro Atlanta and #9 in Georgia", "a throwing game", "Ramona", "Tobacco Road", "Survivor", "Opportunity seldom knocks twice", "Smokey Robinson", "a frog", "Gladiator", "primaries", "Tracheal Esophageal Fistula", "Cairo", "The Cotton Club", "a sandstorm", "George Byron", "neutrino", "Jodie Foster", "George Eliot", "clouds", "Sherlock Holmes", "California Missions", "Auschwitz", "China", "Uganda", "6 g carb, 2 g fiber, 1.7 g fat, 0.2 g sat fat, 8 mg sodium", "Edward", "pomegranate", "Bali", "Paris", "decathlon", "Elizabeth II", "kings", "blacklist", "a small boat", "a jumper", "Jean", "Yogi Berra", "China", "Hawaii", "birds", "peripheral vision", "Espresso", "Delacorte", "head", "Vanessa Williams", "Chocolate Cake", "potential energy", "the Byzantine Empire", "Reno", "16 seasons", "photoelectric, or optical smoke detector", "the upper peninsula of Michigan, south to northern Louisiana, west to Colorado, and east to Massachusetts", "Joan Crawford", "Bassenthwaite Lake", "the moon", "Vanilla Air Inc.", "Jack", "diplomat", "children's books", "Six people", "attempted burglary", "missile"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6107421875000001}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.7499999999999999, 0.9375, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-2806", "mrqa_searchqa-validation-13562", "mrqa_searchqa-validation-14186", "mrqa_searchqa-validation-8242", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-14972", "mrqa_searchqa-validation-5294", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-10375", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-15880", "mrqa_searchqa-validation-5807", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-1198", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-15274", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-14831", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-2870", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-5467", "mrqa_newsqa-validation-3829"], "SR": 0.546875, "CSR": 0.5534048507462687, "EFR": 1.0, "Overall": 0.7005247201492537}, {"timecode": 67, "before_eval_results": {"predictions": ["Steven Spielberg", "Baton Rouge", "Wilbur Wright", "Charles Lindbergh", "Keanu Reeves", "Stephen Sondheim", "15", "a calculating machine", "Bill Wyman", "Surgeon", "T.S. Eliot", "lead", "Keanu Reeves", "French", "gravitational", "Fisherman\\'s Wharf", "Santa Fe", "Rush Limbaugh", "Sex Pistols", "chess", "Michael Jordan", "fairground", "doughboy", "Brge Rosenbaum", "Zora Folley", "a rabbit", "Secretariat", "The Soup Nazi", "a lance head", "citric acid", "Homer", "a rudder", "a woman scorned", "Pope John Paul II", "Will Rogers", "Hairspray", "Oklahoma City", "Hopelessly Devoted", "pirates", "River Phoenix", "the Sydney, Australia Harbor", "mutton", "palette", "Napoleon", "the flag of Mongolia", "Peter the Great", "a barn", "ibrik coffee", "Missouri", "Sweeney Todd", "Paris", "1956", "Tommy James and the Shondells", "Two Days Before the Day After Tomorrow", "jujitsu", "Salvador Dali", "Robert De Niro", "1993", "October 17, 2017", "from 1986 to 2013", "Afghanistan,", "a mammoth", "co-writing credits", "Gary Grimes"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6357886904761905}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15468", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-11812", "mrqa_searchqa-validation-7771", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-626", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-288", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-8769", "mrqa_searchqa-validation-2305", "mrqa_searchqa-validation-15211", "mrqa_searchqa-validation-4698", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-15818", "mrqa_naturalquestions-validation-1882", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-512", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2152"], "SR": 0.578125, "CSR": 0.5537683823529411, "EFR": 0.9629629629629629, "Overall": 0.6931900190631808}, {"timecode": 68, "before_eval_results": {"predictions": ["papal rings", "Omaha", "Charles I of England", "the Matterhorn", "Loch Lomond", "Alaska", "Frasier Crane", "a temporary need", "Denmark", "\"ball in tube\" or electromechanical crash sensor", "someone", "George Bernard Shaw", "cholera", "Edward Estlin", "Wilhelm Conrad Roentgen", "Glendening", "Yes", "the Green Hornet", "Rita STREISAND", "geolu", "before labor at term", "300", "Diner", "\" Cleopatra\"", "pizza crust", "St. Petersburg", "Japan", "the Jordan River", "Derek Jeter", "Hans Christian Andersen", "an optional value", "defense", "\"The Tyger\"", "Percy Shelley", "diamonds", "baking soda", "earthquakes", "Jr.", "Citizen Kane", "gravity", "Mathew Brady", "Clinton", "a spike", "Tasmania", "Wyoming", "\"eye\" in your sky", "brown fox", "Denmark", "wheat", "\"Sweet Home\"", "\"CV-64\"", "a protocol ( http ), a hostname ( www.example.com ), and a file name ( index. html )", "presidential representative democratic republic", "the Udhampur - Srinagar - Baramulla railway tunnel", "Barcelona", "China", "Leander", "the Corps of Discovery", "Morocco", "1998", "club managers", "flannel or wool", "several weeks,", "2011"], "metric_results": {"EM": 0.5, "QA-F1": 0.6096996753246753}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16502", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-16760", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-12041", "mrqa_searchqa-validation-13013", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-9738", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-7831", "mrqa_searchqa-validation-4154", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-4617", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-1342", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-2169", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-1266", "mrqa_searchqa-validation-3197", "mrqa_searchqa-validation-15704", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-1848", "mrqa_hotpotqa-validation-3156", "mrqa_newsqa-validation-3500"], "SR": 0.5, "CSR": 0.5529891304347826, "EFR": 0.96875, "Overall": 0.6941915760869566}, {"timecode": 69, "before_eval_results": {"predictions": ["Kentucky Fried Chicken", "\"turt sweater\"", "Follies", "\"Berenice, Queen of Egypt\"", "Andrew Jackson", "Agamemnon", "spurs", "Robert Bartlett", "\"Bah-dum\"", "cantons", "Louisiana", "tree-lined", "percussus", "the pardon power", "Artemis", "strawberry", "Constellations", "Indiana Jones", "Fox Network", "20 feet away", "Gregor Mendel", "Maria Callas", "Hulk Hogan", "Margaret Tobin", "a horse", "A Hard Day\\'s Night", "Making the Band", "Judy Garland", "Autumn in New York", "telephone operator", "Franklin D. Roosevelt", "William Shakespear", "\"I Have No Mouth\"", "La Salle", "lattice", "a penny", "succotash", "the retina", "graduation ceremonies", "Lake Coeur d'Alene", "The Sopranos", "\"Hark\"", "Huguenots", "the Brooklyn Dodgers", "king", "yellow", "Mascara", "Rooster Cogburn", "ponderosa pine", "Homestead Act", "Donald Trump", "Tim Duncan", "Mexico", "Aaron Harrison", "Crete", "Spider-Man", "Peter Nichols", "the Newell Highway between Melbourne and Brisbane", "1858", "Kentwood, Louisiana", "Newcastle", "five years", "Camorra -- the name for organized crime in Naples -- is strong.", "a mermaid"], "metric_results": {"EM": 0.53125, "QA-F1": 0.605}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.32, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-855", "mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-7946", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-5203", "mrqa_searchqa-validation-843", "mrqa_searchqa-validation-1777", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-10815", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-5237", "mrqa_searchqa-validation-12476", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-4606", "mrqa_searchqa-validation-705", "mrqa_searchqa-validation-5659", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8460", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-5206", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-2727"], "SR": 0.53125, "CSR": 0.5526785714285714, "EFR": 1.0, "Overall": 0.7003794642857143}, {"timecode": 70, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-947", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16953", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4642", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7212", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7593", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9643", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10254", "mrqa_squad-validation-10328", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1075", "mrqa_squad-validation-1183", "mrqa_squad-validation-1319", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-2975", "mrqa_squad-validation-3139", "mrqa_squad-validation-3168", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4147", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5532", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5826", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6624", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-821", "mrqa_squad-validation-8253", "mrqa_squad-validation-8410", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5488", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6565", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.830078125, "KG": 0.51015625, "before_eval_results": {"predictions": ["Polk", "Stanch", "delta", "Baroque", "St. Petersburg", "Australia", "Prohibition", "Onomastic Sobriquets", "The Godfather", "Maria Sharapova", "McDonald\\'s", "John Cazale", "11", "\"The Stars and Stripes Forever\"", "Jackie Moon", "Pulp Fiction", "expunge", "the Rhine", "a missile", "dilithium", "Schwarzenegger", "the Epstein-Barr virus", "hydrogen", "a double-time", "the U.S. Naval Academy", "Iowa", "indirect discourse", "a circle", "Pussycat Dolls", "Shakespeare", "a lump", "Vin Diesel", "Hitler", "Heath", "the Odyssey", "Michael Phelps", "Annapolis", "the Maccabees", "Rolls Royce", "a doses", "the Caucasus Mountains", "Lafayette", "the gopher", "Mephistopheles", "Coca-Cola", "Warren Burger", "apogee", "the moon", "a mirror", "david archuleta", "Union Carbide", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Ireland", "24 November 1949", "the month of May", "Stockholm syndrome", "Ilkley", "Geographical Indication tag", "just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "All's Well That ends Well", "Obama and Britain\\'s Prince Charles", "the Olympic medal she and her mom always wanted,", "1000 square meters in forward deck space,", "Larry Ellison"], "metric_results": {"EM": 0.625, "QA-F1": 0.6987847222222223}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true], "QA-F1": [0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.14285714285714285, 0.16666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16561", "mrqa_searchqa-validation-8833", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-7997", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-9498", "mrqa_searchqa-validation-15266", "mrqa_searchqa-validation-958", "mrqa_searchqa-validation-10678", "mrqa_searchqa-validation-11878", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-5212", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-12155", "mrqa_triviaqa-validation-1074", "mrqa_hotpotqa-validation-3395", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-1701"], "SR": 0.625, "CSR": 0.5536971830985915, "EFR": 0.9583333333333334, "Overall": 0.718890478286385}, {"timecode": 71, "before_eval_results": {"predictions": ["Funki Porcini", "119", "560", "VH1's \"100 Greatest Artists of Hard Rock\"", "Klasky Csupo", "the music genres of electronic rock, electropop and R&B", "the US Naval Submarine Base New London submarine school", "Amber Laura Heard", "River Shiel", "\"Vera Cruz\"", "the Lommel differential equation", "Harry Booth", "Southland", "1.23 million", "Boston, Massachusetts", "281", "Northern Ireland", "1916 Easter Rising", "Park Hyung-Sik", "Conservative Party", "quantum mechanics", "the \"battlefield of Europe\"", "Theo James Walcott", "April 8, 1943", "\"L'homme qui voulait savoir (The Man Who Wanted to Know)", "their unusual behavior", "11", "Victoria Peak", "\"Back to December\"", "the Swedish manufacturer Volvo Cars", "Hindi", "a listed building", "High Falls Brewery", "an American painter and writer who wrote the autobiography \"The Bite in the Apple\"", "Frederick Louis, Prince of Wales", "Autopia", "Hindi", "Art Deco-style skyscraper", "Jon Walker", "Beautiful", "Mani", "Green Chair", "Walker Smith Jr.", "Waimea Bay", "Juan Manuel Mata Garc\u00eda", "Umina Beach", "Mickey Mouse cup", "Kinnairdy Castle", "Antigua & Barbuda, Argentina, South Africa, Fernando P\u00f3, S\u00e3o Tom\u00e9, Madagascar, Mauritius, Mayotte", "Stephen James Ireland", "Lola Dee", "1924", "the cell nucleus", "the inner core", "Helen of Troy", "Vince Cable", "the cube root of any real number", "\"A good vegan cupcake has the power to transform everything for the better,\"", "Sporting Lisbon", "upper respiratory infection", "Informatics", "bowling", "Gin Rummy", "enamel"], "metric_results": {"EM": 0.5625, "QA-F1": 0.645815686050061}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.923076923076923, 1.0, 0.125, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.22222222222222224, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-1397", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-1799", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-3471", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-2678", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-1716", "mrqa_naturalquestions-validation-2502", "mrqa_triviaqa-validation-2441", "mrqa_newsqa-validation-2006", "mrqa_searchqa-validation-12240", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267"], "SR": 0.5625, "CSR": 0.5538194444444444, "EFR": 1.0, "Overall": 0.727248263888889}, {"timecode": 72, "before_eval_results": {"predictions": ["U2 360\u00b0 Tour", "Brent Wilson was fired", "Peter Kay's Car Share", "Michael Crawford", "Brazilian Jiu-Jitsu", "2012 Olympic bronze medalist", "CMYKOG process", "Colonel", "26\u201330 August 1914, during the first month of World War I.", "Germany", "River Clyde", "George Washington Bridge", "Argand lamp", "Lowestoft, Suffolk", "The Ethics of Ambiguity", "Bishop's Stortford Football Club", "Rural Electrification Act of 1936", "Vitor Vieira Belfort", "Carlos Coy", "Hawaii", "Cuban descent", "35,000", "24 NCAA sports", "the Bahamian island of Great Exuma", "scholarly analysis and research-based study of music", "Bonnie Franklin", "the Cheshire League Premier Division", "Kelly Bundy", "Canada's first train robbery", "Carson City", "Ben R. Guttery", "deputy director of the Tate", "Montreal", "New Zealand", "February 14, 1859", "Peel Holdings", "1692", "film", "Teddy Riley", "672 km2", "140 million", "Lamar Hunt", "Vienna", "the Alemannic and the Bavarian-Austrian dialects of German", "10", "SpongeBob SquarePants 4-D", "Seti I", "2012 Bollywood film, \"Agent Vinod\"", "Joseph E. Grosberg", "January 15, 1975", "2015", "historical fiction", "energy loss", "the Indo - Greek kings to the northwest", "Victoria is the only child of the fourth son of King George III: Edward, duke of Kent.", "Audi A4", "Malta (officially named Republic of Malta) is the city of Valletta", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "Lynne Tracy.", "drug cartels", "New York City", "\"Turandot\"", "Champagne", "seabirds"], "metric_results": {"EM": 0.53125, "QA-F1": 0.7055746336996336}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.7499999999999999, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-1320", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-4088", "mrqa_hotpotqa-validation-1695", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-1260", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-4489", "mrqa_hotpotqa-validation-1006", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-6234", "mrqa_triviaqa-validation-6188", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-2795", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-1604", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3199"], "SR": 0.53125, "CSR": 0.5535102739726028, "EFR": 1.0, "Overall": 0.7271864297945206}, {"timecode": 73, "before_eval_results": {"predictions": ["Bathsheba", "Poland", "Hillary Clinton", "Hannibal", "Elysium", "Birmingham", "a coffee house", "a poor wood-cutter", "J.M.W. Turner", "Heisenberg", "astronaut", "glockenspiel", "David Hockney", "Kyoto Protocol", "alexandrina", "Croatian", "New York", "South Carolina", "Survivor Series", "taxis", "bell peppers", "piscina", "Edward III", "Bruce Wayne", "lighting", "Tesco", "Cologne", "northern prawn", "Midtown", "Nikola Tesla", "smartphones and similar devices", "North Carolina", "Grimbsy", "a Sandstone Trail", "Robert Guerrero", "Virginia Plain", "Columbia", "Scotland", "Freema Agyeman", "Spanish", "a toilet", "Medusa", "2011", "alexandrina", "The Greatest", "ArcelorMittal Orbit", "Madness", "Mao Zedong", "Sweet Home Alabama", "Greece", "St Helens", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest", "required many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the", "biscuit", "High Knob", "Sacramento Kings", "200", "African National Congress Deputy President Kgalema Motlanthe", "Michelle Obama", "Quetta, the capital of Balochistan province,", "a belfry", "Maverick", "Ronald McDonald House", "#364"], "metric_results": {"EM": 0.5, "QA-F1": 0.5652504105090312}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1379310344827586, 0.9714285714285714, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3099", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-5315", "mrqa_triviaqa-validation-5341", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-5672", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-6862", "mrqa_triviaqa-validation-6598", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-1673", "mrqa_triviaqa-validation-4776", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-5719", "mrqa_triviaqa-validation-614", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-4414", "mrqa_hotpotqa-validation-5878", "mrqa_newsqa-validation-3358", "mrqa_searchqa-validation-16624"], "SR": 0.5, "CSR": 0.5527871621621622, "EFR": 0.875, "Overall": 0.7020418074324325}, {"timecode": 74, "before_eval_results": {"predictions": ["iPod Classic", "3-2", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "lower house of parliament,", "Simon Cowell", "Adam Sandler, Bill Murray", "1983", "free milk.", "journalists and aid workers are prohibited from entering the Ogaden,", "are \"active athletes,\" far from couch potatoes,", "the FBI", "10,000", "his business dealings for possible securities violations", "former Procol Harum bandmate", "California, Texas and Florida,", "morphine sulfate oral solution", "\"it should stay that way.\"", "Iran", "censorship", "Rawalpindi", "killed Lauterbach", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "allergen-free", "the United States", "Samoa", "oregon fire Lines", "Six", "Dublin", "customers", "13", "Whitney Houston", "Ferraris", "free fixes for the consumer.", "nine-wicket", "10 below", "Madhav Kumar Nepal", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "black is beautiful", "fifth", "Iran", "Australian", "JBS Swift Beef Company,", "Hurricane Gustav", "Canadian Prime Minister Stephen Harper,", "murder", "Manny Pacquiao", "Itawamba County School District", "1,073 immigration detainees", "forgery and flying without a valid license,", "Alfredo Astiz,", "Los Angeles", "Super Bowl VIII", "3", "Steve Ford", "sharpening steels", "Edinburgh", "true", "Norbertine", "Anheuser-Busch", "Beauty and the Beast", "the Chiefs", "Cleopatra", "Philip", "Sparafucile"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5986221974297632}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 0.9473684210526316, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6, 0.0, 0.0, 0.8, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.7272727272727273, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-380", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1373", "mrqa_triviaqa-validation-3067", "mrqa_searchqa-validation-14806"], "SR": 0.484375, "CSR": 0.551875, "EFR": 1.0, "Overall": 0.726859375}, {"timecode": 75, "before_eval_results": {"predictions": ["1-1 draw", "Ten South African ministers and the deputy president", "Sharon Bialek", "last year after giving birth to baby daughter Jada,", "voluntary manslaughter", "The Kirchners", "Sharon Bialek", "Roy Foster", "March 24,", "African-Americans", "Bill Haas", "David Beckham", "rwanda", "Caster Semenya", "nuclear weapon", "$627,", "Turkish President Abdullah Gul.", "People Against Switching Sides (PASS)", "sedative", "26", "Bob Dole,", "Kingman Regional Medical Center,", "assassination of President Mohamed Anwar al-Sadat", "Australian Environment Minister Peter Garrett", "the piracy incident was discussed as one of the \"tests\" of President Obama that Joe Biden warned about during the campaign.", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Veracruz, Mexico,", "Sharon Bialek", "About 100,000 workers", "were separated", "San Simeon, California,", "hank Moody", "Fiona MacKeown", "2001", "Toy Story", "a violent government crackdown seeped out.", "43 percent", "Jezebel.com", "Kenner, Louisiana", "Saturday", "Pakistan was used by probably non-state actors,\"", "in 2006 and 2007", "Hong Kong's Victoria Harbor", "Marco Polo", "stressed out", "consider China an economic threat to the United States,", "Kitty Kelley", "from the Bronx", "\"Chadian President Idriss Deby hopes the journalists and the flight crew will be freed,", "in 1994", "murder in the beating death of", "February 7, 2018", "1439", "Blue laws", "Siddhartha", "Prince Bumpo", "Nitrogen", "Harriet Tubman", "ABC1 and ABC2", "G\u00f6tene in Sweden", "the kitchen sink", "whey", "to the Young Men of Italy", "roosevelt"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5644211440058479}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.8750000000000001, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7777777777777778, 0.0, 0.6666666666666666, 0.7368421052631579, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-4014", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-1313", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-406", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-177", "mrqa_hotpotqa-validation-1622", "mrqa_searchqa-validation-7856"], "SR": 0.453125, "CSR": 0.5505756578947368, "EFR": 0.9714285714285714, "Overall": 0.7208852208646617}, {"timecode": 76, "before_eval_results": {"predictions": ["cricket", "Alan Greenspan", "Bolivia", "Matalan", "Ub Iwerks", "Macbeth Soliloquy", "Angela Merkel", "The Landlord's Game", "transsexual", "red", "the skull, jaw, shoulder, rib cage, and pelvis", "doubles", "Paul Gauguin", "Ben Jonson", "a parallelogram", "Willy Lott", "19-9", "Abu Dhabi", "lamas", "14", "lice", "palladium", "Hubble Space Telescope", "James Van Allen", "Rawalpindi", "Mexico", "Philip Glenister", "Miss Prism", "Emma Hamilton", "Beethoven", "Haystacks", "in late 2016", "Margaret Thatcher", "Mauricio Pochettino", "USS Missouri", "Sensurround", "eAGLE", "Olympic Games", "Blue Ivy Carter", "Rihanna", "Tripoli", "rhianna", "Eva Per\u00f3n", "Doctor who", "pink", "Glenn Close and Rade Serbedzija", "Illinois", "The Magnificent Seven", "Uranus", "typhoid fever", "Ross MacManus", "September 9, 2012", "Santa Clara Pueblo, New Mexico, USA", "seven", "Ricky Rubio", "United States and Canada", "Queens, New York", "Manchester United's perfect start to the English Premier League season came to a halt", "Alwin Landry's supply vessel Damon Bankston", "intricate Flemish tapestries", "The Big Sleep", "Dairy Queen", "Paul the Apostle", "Confederate victory"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6672619047619047}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-2144", "mrqa_triviaqa-validation-2756", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-4855", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-4242", "mrqa_triviaqa-validation-2047", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-6179", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2631", "mrqa_searchqa-validation-15341", "mrqa_naturalquestions-validation-767"], "SR": 0.609375, "CSR": 0.5513392857142857, "EFR": 0.96, "Overall": 0.7187522321428571}, {"timecode": 77, "before_eval_results": {"predictions": ["nearly $162 billion in war funding", "poor.", "183", "\"I reject this course because it sets goals that are beyond what we can achieve at a reasonable cost, and what we need to achieve to secure our interests.\"", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Transport Workers Union leaders", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "President Bush", "November 26,", "Wednesday.", "change course", "machine guns and two silencers", "Jackson sitting in Renaissance-era clothes and holding a book.", "off the coast of Dubai", "his past and his future", "Spc. Megan Lynn Touma,", "Hussein's Revolutionary Command Council.", "McDonald's", "$31,000", "Black History Month", "tenement in the Mumbai suburb of Chembur,", "Monday.", "weren't taking it well.", "The island's dining scene", "22", "the foyer of the BBC building in Glasgow, Scotland", "the governor", "President Sheikh Sharif Sheikh Ahmed", "Graham's wife", "hank Moody", "two", "ambassadors", "move in a casino, buy a drink in a pub or see the horror film \" Hostel: Part II,\"", "Wigan Athletic", "criminals", "The Ministry of Defense", "civilians,", "21-year-old", "Ignazio La Russa", "Passers-by", "Pixar's", "NATO fighters", "New York City Mayor Michael Bloomberg", "A Colorado prosecutor", "16 Indiana National Guard soldiers", "1940's", "The Maraachlis' daughter, Zeina,", "bank robbery", "South Africa", "\"I am sick of life -- what can I say to you?\"", "a tanker", "Rigor mortis is very important in meat technology", "Walter Pauk", "Louis Prima", "Alanis Morissette", "venus", "Margaret Thatcher", "February 16, 1944", "Fort Saint Anthony", "Wilmette, Illinois", "fractions", "Vermont", "Kiribati", "Jay Van Andel"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6068808795371294}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true], "QA-F1": [0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1145", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-6088", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-1874", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-13106"], "SR": 0.5625, "CSR": 0.5514823717948718, "EFR": 1.0, "Overall": 0.7267808493589744}, {"timecode": 78, "before_eval_results": {"predictions": ["the test results", "the bombers", "\"He is a very special member of our family. We miss having his love and compassion in our home,\"", "Isabella", "5 1/2-year-old son, Ryder Russell,", "finance", "gun charges,", "flying", "Dayton, Oregon, in the Willamette Valley to the Pacific coast.", "There's no chance of it being open on time.", "the peace with Israel", "two", "70,000", "rural California,", "Silvio Berlusconi.", "650", "Expedia", "the ireport form", "Virgin America", "The Italian government", "Cordoba,", "helping on the sandbags lines", "four decades", "Damon Bankston", "The e-mails", "The HPV vaccine", "helicopters and unmanned aerial vehicles", "two tickets to Italy", "Dr. Maria Siemionow,", "Hillary Clinton", "Barack Obama:", "NATO", "assassination of", "to kill members of the Zetas cartel", "June 6, 1944,", "Orbiting Carbon Observatory,", "The nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "the Airbus A330-200", "divorced Goldman and married a Brazilian lawyer.", "poems", "be silent.", "Tamil insurgents", "Adidas", "2,700-acre sanctuary in rural Tennessee.", "documents belonging to Miguel Mejia Munera", "can indeed help people with irritable bowel syndrome,", "martial arts,", "a student who admitted to hanging a noose in a campus library,", "543", "Barack Obama", "The Louvre", "New Zealand", "Elvis Presley", "enemy", "johnson", "Count Basie Orchestra", "Billy Cox", "wooden", "Yoruba", "boxer", "chrysanthemums", "John Wesley", "Colonel Sanders", "depicting multiple alternative realities rather than a novel"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6575439755771466}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.1904761904761905, 1.0, 0.0, 0.5, 1.0, 1.0, 0.18181818181818182, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.75, 1.0, 0.0, 1.0, 0.4, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.25, 0.6956521739130436, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3240", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2609", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-5101", "mrqa_hotpotqa-validation-5784", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12910", "mrqa_naturalquestions-validation-2729"], "SR": 0.515625, "CSR": 0.5510284810126582, "EFR": 1.0, "Overall": 0.7266900712025317}, {"timecode": 79, "before_eval_results": {"predictions": ["`` Mercy Mercy Me ( The Ecology ) '' was the second single from Marvin Gaye's 1971 album, What's Going On", "in the United Kingdom in 1989", "Bill Irwin", "Hon July Moyo", "glycine and arginine", "2017 / 18 Divisional Round", "12951 / 52 Mumbai Rajdhani Express", "the Rolling Stones", "My Summer Story", "In 2010", "2018", "in 1975", "the Constitution of India came into effect on 26 January 1950", "David Tennant", "the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "the Canadian Rockies continental divide east to central Saskatchewan,", "Deuteronomy 5 : 4 -- 25", "the President", "Hans Christian Andersen", "Andy Cole", "each team", "in case of `` a national emergency created by attack upon the United States, its territories or possessions, or its armed forces", "Sauron", "1775", "Supplemental oxygen", "the level of the third lumbar vertebra, or L3, at birth", "`` Six flags over Texas ''", "103", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Reba McEntire and Linda Davis", "New Mexico", "Karen Gillan", "in the fifth century", "The sacroiliac joint or SI joint ( SIJ )", "`` the immortal Hawke ''", "Arkansas", "Mickey Rourke", "a revolution or orbital revolution", "Rick Marshall", "Number 4, Privet Drive, Little Whinging in Surrey, England", "a blend of ground beef and other ingredients and is usually served with gravy or brown sauce", "Sunday evenings", "15 Bonanza Creek Lane, Santa Fe, New Mexico, USA", "major fall in stock prices", "in April 1979", "Reverend J. Long", "2005", "`` save, rescue, savior ''", "as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Brad Johnson", "Nicole Gale Anderson", "10", "23rd October 1642", "James Taylor", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "in the Premier League, the top flight of English football", "the Emancipation Proclamation", "4.6 million", "executive director of the Americas Division of Human Rights Watch,", "Pope Benedict XVI", "Afghanistan", "Erie", "the lion", "the Black Sea"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6765773026392832}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7692307692307692, 0.4, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.22857142857142856, 1.0, 1.0, 0.0, 0.16666666666666666, 0.24999999999999997, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.0, 0.8571428571428571, 0.12500000000000003, 0.8, 1.0, 1.0, 0.5, 0.6956521739130435, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-9409", "mrqa_triviaqa-validation-752", "mrqa_triviaqa-validation-5693", "mrqa_hotpotqa-validation-3900", "mrqa_searchqa-validation-2932", "mrqa_searchqa-validation-10525"], "SR": 0.515625, "CSR": 0.5505859375, "EFR": 0.9354838709677419, "Overall": 0.7136983366935483}, {"timecode": 80, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12910", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3139", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1535", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.767578125, "KG": 0.465625, "before_eval_results": {"predictions": ["prevent any contaminants in the sink from flowing into the potable water system by siphonage", "all - female population", "late - September through early January", "Moscazzano", "Prem Lata Agarwal", "calpurnia", "2009", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "Joe Pizzulo and Leeza Miller", "19th - century India", "they signed with Simon Cowell's record label Syco Music", "Jenny Slate", "Gunpei Yokoi", "January 15, 2010", "The Sixth Extinction II : Amor Fati ''", "the leaves of the plant species Stevia rebaudiana", "Julie Deborah Kavner", "Peggy Lipton", "infection, irritation, or allergies", "Jewel Akens", "`` Wat '', or `` Wa'ter ''", "March 11, 2018", "their bearers", "12", "Frank Langella", "Number 4, Privet Drive, Little Whinging in Surrey, England", "shared", "Southport, North Carolina", "John C. Reilly", "Wednesday, September 21, 2016", "the Washington metropolitan area", "the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho ( including the communities of Parma, Wilder, Greenleaf, and Notus )", "1885", "Kevin Kline", "the Vital Records Office of the states, capital district, territories and former territories", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "Monk's Caf\u00e9", "20 locations all within the Pittsburgh metropolitan area", "Spanish missionaries, ranchers and troops", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "Butch", "Norway", "435", "1997", "American country music duo Brooks & Dunn", "12 November 2010", "the homicidal thoughts of a troubled youth", "W. Edwards Deming", "convergent plate boundary", "Sylvester Stallone", "1967", "the Coney Island Old Island Pier in New York, NY", "Fontane di Roma", "Steve Coogan", "American theoretical physicist", "the Bay of Fundy", "Lincoln Riley", "The Rosie Show", "presiding judge Shemsu Sirgaga", "2,000 euros", "dishwasher", "n", "(Albert) Einstein", "wheezing"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5721150153609831}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.1, 0.4, 1.0, 0.625, 0.5, 0.4, 1.0, 0.33333333333333337, 0.967741935483871, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.8, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2102", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-5589", "mrqa_hotpotqa-validation-4160", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-2730", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-9418"], "SR": 0.46875, "CSR": 0.5495756172839505, "EFR": 0.9705882352941176, "Overall": 0.6897358955156137}, {"timecode": 81, "before_eval_results": {"predictions": ["Jason Momoa", "Tommy Lee Jones", "1623", "March 14, 1942", "Lafayette", "April 3, 1973", "5 liters", "Diane", "13 to 22 June 2012", "2.5 %", "232", "mid November", "Guwahati", "Sarafina", "1979 / 80", "2017", "Augustus", "Leslie", "electron shells", "rain", "compasses", "production -- possibility frontier ( PPF )", "1987", "in the eye", "Eagle Ridge Outdoor pool in Coquitlam, BC", "gave the German Cabinet -- in effect, Chancellor Adolf Hitler -- the power to enact laws without the involvement of the Reichstag", "E.E. Southon", "the fourth ventricle", "October 27, 2017", "John Cooper Clarke", "Sunday night", "Cheryl Campbell", "Spanish", "55 - 75", "January 4, 2016", "Jeff Bezos", "Erica Rivera", "sunny", "January 2, 1971", "Ted Mosby", "gastrocnemius", "Parker's pregnancy at the time of filming", "4", "the First Battle of Panipat ( 1526 )", "1830", "Frank Oz", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Celtic", "Guant\u00e1namo Bay in Cuba", "Morgan Freeman", "two installments", "in the muscle tissue of vertebrates", "throw", "synagogues", "Katarina Witt", "Bhaktivedanta Manor", "Lowe's Companies, Inc.", "Honduran", "Al-Shabaab", "the teeth and lower jaw of Columbian mammoth fossil \"Zed.\"", "J'ai vcu", "hateful", "churrasco", "refrigerator"], "metric_results": {"EM": 0.453125, "QA-F1": 0.564124503968254}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666667, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-2144", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-5214", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5185", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-2449", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-1512", "mrqa_searchqa-validation-7442", "mrqa_searchqa-validation-12256"], "SR": 0.453125, "CSR": 0.5483993902439024, "EFR": 0.9714285714285714, "Overall": 0.6896687173344949}, {"timecode": 82, "before_eval_results": {"predictions": ["Whittling", "Hans", "a swing state", "Charles Lindbergh", "sucrose", "T.S. Eliot", "Superman Returns", "Nokomis", "Yale University", "tidal streams", "The Nutcracker", "Over the hifls", "circumnavigate", "Maine", "EBzine", "Theodore Roosevelt", "manx", "rum", "Baroque", "P", "licorice", "Dracula", "skating", "Sweden", "War and Peace", "Hannah Montana", "Van Allen", "Mitch McConnell", "bravery or valor", "the gallbladder", "The Invisible Man", "Himalaya", "Chile", "Sri Lanka", "the St. Valentine's Day Massacre", "Saturday Night Live", "Sayonara", "Oakland", "The Taming of the Shrew", "a proxy", "Andrew Johnson", "the knee", "NASA", "Gavin MacLeod", "envy hognose", "The Count of Monte Cristo", "New Jersey", "Peter Shaffer", "Wyandotte County", "Denton True \"Cy\" Young", "Stephen Sondheim", "October 29, 2015", "MFSK", "Andy Cole", "yellow", "Galileo Galilei", "Stanley", "Afro-American religion", "New York City,", "Awake", "Ferraris, a Lamborghini and an Acura NSX", "has been criticized for creating a new television show which looks at how children as young as eight would cope without their parents for two weeks.", "Group 2", "U.S. state of Washington"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6421223958333333}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0625, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-5014", "mrqa_searchqa-validation-10648", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-13546", "mrqa_searchqa-validation-11022", "mrqa_searchqa-validation-8553", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-3227", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-9510", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-14521", "mrqa_searchqa-validation-6416", "mrqa_searchqa-validation-8439", "mrqa_naturalquestions-validation-8934", "mrqa_hotpotqa-validation-5173", "mrqa_hotpotqa-validation-5005", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3226", "mrqa_naturalquestions-validation-3281"], "SR": 0.53125, "CSR": 0.5481927710843373, "EFR": 1.0, "Overall": 0.6953416792168674}, {"timecode": 83, "before_eval_results": {"predictions": ["Coca-Cola", "to have had OU and OSU undefeated", "spoiled", "Pippin", "Georgia", "the Chesapeake Bay", "a dugout", "cement", "heating", "(Tom) Oakley", "(Mark) Cooper", "Platoon", "Leon Uris", "potato chip", "the Bay of Bengal", "the Clark bar", "do", "Dresden", "(John) Ashcroft", "Phil of the Future", "Newman", "the Badwater Basin in Death Valley,", "rings", "pick up and hold", "(Mary Ann) Evans", "the Eagles", "(Edith) Baum", "(Kylon Jennings) Jennings", "jaded", "Sgt. Pepper's Lonely Hearts Club Band", "palindrome", "a trapezoid", "Scrubs", "(Henrik) Ibsen", "Elizabeth I", "Canticle", "Friedrich Nietzsche", "the rioting", "Halloween", "the cold-blooded murder of the", "Tanzania", "Coca-Cola", "(Deepak) Chopra", "Rudolf Hess", "the village of Shushenskoye", "Rings Twice", "(Sylvester Stallone) Stallone", "(Edna)plum", "the Etch A Sketch", "safari", "the Arkansas Diamond Mine", "During his epic battle with Frieza", "1997", "The stability, security, and predictability of British law and government", "(Carl Wilhelm) Scheele", "Granada", "the North West of England", "1940s and 1950s", "Best Musical", "York County", "six", "Michelle Rounds", "Derek Mears", "Adidas"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6911458333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12304", "mrqa_searchqa-validation-11933", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-16183", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-15025", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-39", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-16469", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-15780", "mrqa_searchqa-validation-15171", "mrqa_searchqa-validation-2557", "mrqa_searchqa-validation-1066", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-11096", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-2381", "mrqa_hotpotqa-validation-5309"], "SR": 0.609375, "CSR": 0.5489211309523809, "EFR": 1.0, "Overall": 0.6954873511904762}, {"timecode": 84, "before_eval_results": {"predictions": ["Portugal", "Noreg", "daisy", "joseph galsworthy", "Belfast", "William Wymark Jacobs", "the Kurguelen Island Group", "the Trasks", "(John) Buchan", "Doncaster Rovers", "bront\u00eb", "Honshu", "9", "Supertramp", "joseph hodge", "Joanne Harris", "abacus", "Eriksson", "bison", "velocity", "(James Valentine)", "Grittar", "the earth", "joseph", "White spirit", "aglet", "lemurs", "Ontario", "Fabio Capello", "Goofy", "cricket", "1973", "William Neil Connor", "Baku", "Mathematics", "Spain", "Ferdinandorsche", "Chief Inspector with HM Inspector of Prisons", "Moulin Rouge", "golf", "a dog", "Robert Devereux", "Hamelin", "Prague", "George Osborne", "oxygen", "Toyota", "a snakes", "HMS Amethyst", "a hairdresser", "Antony", "Disha Vakani", "Spektor", "Vancouver, British Columbia", "Deputy F\u00fchrer", "nearly 8 km", "Anaheim, California", "about 1,300 meters (9 miles) south of Beirut.", "Afghanistan,", "Deutschneudorf,", "a chimp", "tin", "Wings of Desire", "repel bullets and fly at sub-sonic speeds"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6273103275401068}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7272727272727273, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.8, 0.6666666666666666, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.25, 0.1]}}, "before_error_ids": ["mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-4027", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-6937", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-6666", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2685", "mrqa_newsqa-validation-796", "mrqa_searchqa-validation-1820", "mrqa_naturalquestions-validation-2309"], "SR": 0.53125, "CSR": 0.5487132352941176, "EFR": 0.9666666666666667, "Overall": 0.6887791053921569}, {"timecode": 85, "before_eval_results": {"predictions": ["the Grand Harbour", "Eurasia", "Rita Moreno", "California", "9,000", "My Beautiful Dark Twisted Fantasy", "secondary school study", "Shut Up", "Nazareth", "William Shakespeare", "Nicholas John \"Nic\" Cester", "the Kingdom of Morocco", "Rocky Mountain goat", "Prince George's County", "Ariel Ram\u00edrez", "Apple iPod 4G with an HP logo on the back", "four months in jail", "Objectivism", "Megalyn Echikunwoke", "stunt performances", "Stephen Crawford Young", "co-founder and lead guitarist of the alternative rock band R.E.M.", "dreikaiserbund", "Frederick Alexander Lindemann,", "Sunflower County", "Alexandre Dimitri Song Billong", "Dutch", "Rabies", "Malayalam", "Sim Theme Park", "Outside", "novelty songs", "Sri Lanka Nidahas Pakshaya", "Nancy Dow", "the North Atlantic Conference", "Aubrey Posen", "Downtown", "Rockstar San Diego", "Strato of Lampsacus", "Gregg Berhalter", "Beno\u00eet Jacquot", "the Manor of the More", "Anita Dobson", "500-room", "Rajmund Roman Thierry Pola\u0144ski", "STS-51-L", "\"Orchard County\"", "Saint Michael, Barbados", "Championnat National 3", "Boston, Massachusetts", "\"The King of Chutzpah\"", "one person", "polyatomic anion", "Gibraltar", "the Seine", "jimmy bassey", "dalton", "Singapore Airlines", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Jose Manuel Zelaya", "a cause", "Japan", "joseph Stalin", "Rupert\u2019s Land"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7434695512820513}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4597", "mrqa_hotpotqa-validation-5200", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-2506", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-2372", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-5448", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-1156", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-3930", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-2592", "mrqa_searchqa-validation-5450"], "SR": 0.6875, "CSR": 0.5503270348837209, "EFR": 1.0, "Overall": 0.6957685319767443}, {"timecode": 86, "before_eval_results": {"predictions": ["California", "inverted", "any unfavourable and unintended sign ( including an abnormal laboratory finding ), symptom, or disease temporally associated with the use of a medicinal ( investigational ) product", "18 - season", "December 24, 1836", "the bank, rather than the purchaser, is responsible for paying the amount", "Carroll O'Connor", "20 years from the filing date", "tenants", "in the 1940s", "Daya", "Steve Russell", "the President pro tempore", "Donna", "the Dutch", "A turlough", "Vancouver, British Columbia", "the Prince - Electors", "9 February 2018", "in 1933 and 1960", "the 2013 -- 14 television season", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "supervillains who pose catastrophic challenges to the world", "two", "the Charbagh structure", "Elizabeth Dean Lail", "Microsoft Windows", "semi-autonomous organisational units within the National Health Service in England", "May 1979", "13 to 22 June 2012", "60", "electron donors", "in the pouring rain", "monitor lizards", "Abbot Suger", "the Himalayas", "The Royalettes", "the winter solstice", "State Bar of Arizona", "Per Gessle", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "2001", "Hellenion", "Urge Overkill", "2000", "blue", "the Mishnah", "1078", "April 1979", "around 1872", "Atlanta, Georgia", "the Glagolitic alphabet", "the US", "Wooden Heart", "France", "rapper", "November 10, 2017", "jobs", "Buenos Aires", "France's famous Louvre", "a sector", "M*A*S*H", "Harry Potter", "Fairfax"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6130611204271919}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 0.19999999999999998, 0.0, 0.9090909090909091, 0.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.18181818181818182, 0.9387755102040816, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.9523809523809523, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-916", "mrqa_triviaqa-validation-3891", "mrqa_triviaqa-validation-2813", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2614", "mrqa_hotpotqa-validation-82"], "SR": 0.484375, "CSR": 0.5495689655172413, "EFR": 0.9090909090909091, "Overall": 0.6774350999216301}, {"timecode": 87, "before_eval_results": {"predictions": ["$10 billion", "Don Draper", "Nafees A. Syed,", "Two pages -- usually high school juniors who serve Congress as messengers -- have been dismissed", "Marcell Jansen", "capital murder and three counts of attempted murder", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "California-based Current TV", "a 19th-century experience on cruises complete with the carnival-like sounds of the steam-whistle calliope.", "a city of romance, of incredible architecture and history.", "peanuts, nuts, shellfish and fish", "Piers Morgan Tonight", "Dubai", "Columbia, Illinois,", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "Free skiing", "Steven Chu", "Mark Obama Ndesandjo", "Hayden", "30,000", "Kris Allen", "the major Shiite holy day of Ashura.", "more than $17,000", "250,000 unprotected civilians", "Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.", "the Juarez drug cartel.", "Muslim festival", "the Indian Ocean waters near the Gulf of Aden,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "his business dealings", "150", "cancer", "U.S. Chamber of Commerce", "a Muslim with Lebanese heritage, but her family is \"not defined by religion,\"", "Lee Myung-Bak", "an open window", "Jiverly Wong,", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "bronze medal in the women's figure skating final,", "98 people,", "President Obama", "in short order, he had canceled the card, but not before his credit union checking account had been debited,", "Cologne, Germany,", "a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "Tennessee", "five minutes before commandos descended from ropes that dangled from helicopters,", "The president would legally be able to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "Mary Procidano,", "The son of Gabon's former president", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Derek Mears", "January to May 2014", "Charles County", "RMS Titanic ( / ta\u026a\u02c8t\u00e6n\u026ak / )", "calcium carbonate", "How to Do a Grand Jete", "Bangladesh", "Elisha Nelson Manning", "CBS", "Dr. Gr\u00e4sler, Badearzt", "seal", "glaucoma", "Carl Sandburg", "Meriwether Lewis"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6422433035714286}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 0.2857142857142857, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 0.26666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.625, 0.07142857142857144, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-3052", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-931", "mrqa_newsqa-validation-1318", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-7115", "mrqa_triviaqa-validation-3855", "mrqa_hotpotqa-validation-3563"], "SR": 0.546875, "CSR": 0.5495383522727273, "EFR": 1.0, "Overall": 0.6956107954545455}, {"timecode": 88, "before_eval_results": {"predictions": ["Caylee Anthony,", "two soldiers", "$22 million", "The federal officers' bodies", "we tortured (Mohammed al-) Qahtani,\"", "near a disputed border temple that was the site of clashes last year,", "orders immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally.", "\"Oprah: A Biography,\"", "and Jquante Crews,", "Chad", "poems telling of the pain and suffering of children", "to use the Internet for fun and not interfere with government and serious issues,", "Emmy-winning Patrick McGoohan,", "Saturday", "Columbian mammoth", "the Taliban's", "genocide,", "in July 1999,", "in the county jail in Spanishfork,", "304,000", "suspected members of five organized crime rings with ties to Europe, Asia, Africa and the Middle East.", "28", "an independent homeland since 1983.", "Miguel Cotto", "South Africa", "seven", "to best your own fuel economy achievements,\" said Brinley.", "returning combat veterans", "scientific reasons.", "dismissed all charges", "United States, NATO member states, Russia and India", "75 percent", "Ameneh Bahrami", "will not support the Stop Online Piracy Act,", "Ma Khin Khin Leh,", "12", "\"The Rosie Show,\"", "The station", "Larry Zeiger", "The son of Gabon's former president", "in the Horn of Africa,", "Olympic", "part of the proceeds", "the longest domestic relay in Olympic history,", "vitamin injections that promise to improve health and beauty.", "Donald Trump.", "in Nuevo Leon,", "the New York Post's Page 6 gossip column.", "Indonesian", "Robert Park", "At least 14", "late 2018 or early 2019", "in Graub\u00fcnden, in the eastern Alps region of Switzerland", "2,050 metres ( 6,730 ft )", "cutis anserina", "Battle of Agincourt", "club", "Belgian", "the Geelong Football Club in the Australian Football League (AFL) home and away season", "Richard L. Thompson", "a fisheye lens", "a clavichord", "bird of paradise", "John Knox"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5924725196142531}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, true], "QA-F1": [0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.5789473684210525, 0.0, 1.0, 1.0, 0.2222222222222222, 0.1111111111111111, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.10526315789473685, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.16666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.4347826086956522, 0.8421052631578948, 0.4, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-999", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-4007", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3162", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-1586", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-798", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-8794", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-277", "mrqa_hotpotqa-validation-342", "mrqa_searchqa-validation-14968"], "SR": 0.484375, "CSR": 0.5488061797752809, "EFR": 1.0, "Overall": 0.6954643609550561}, {"timecode": 89, "before_eval_results": {"predictions": ["a powerful brand", "Ignazio La Russa", "collaborating with the Colombian government,", "collaborating with the Colombian government,", "potential revenues from oil and gas", "2000,", "$12.7 million,", "\"Gas Cities LLC,\"", "Chadian President Idriss Deby", "U.S. Navy", "two hunters", "U.S. State Department and British Foreign Office", "the hunt for Nazi Gold", "U.S. President-elect Barack Obama", "control and censorship remain rife across the Middle East and North Africa,", "News of the World tabloid.", "Too many glass shards left by beer drinkers in the city center,", "not for sale,", "blind Majid Movahedi,", "if they can demonstrate they have been satisfactorily treated", "Jenny Sanford", "Kurdish Workers' Party,", "left without loved ones, without homes, without life's belongings.", "\"falling space debris,\"", "At Wilhelmina Kids,", "France", "581 points", "Robert Barnett, a prominent Washington attorney,", "The Mexican military", "14", "Venezuela", "41,", "Wednesday at the age of 95.", "Idriss Deby hopes the journalists and the flight crew will be freed,", "the report should spur U.S. diplomacy to prevent Iran from developing nuclear weapons", "\"illegitimate.\"", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "2", "Haeftling,", "Saturday.", "five minutes before commandos descended", "A severely disfigured woman", "to sniff out cell phones.", "homicide by undetermined means,", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "for the rest of the year", "July", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Brazil", "Glasgow, Scotland", "Casey Anthony,", "2006 -- 05", "31 - member Senate and a 150 - member House of Representatives", "John Vincent Calipari", "New Zealand", "Jessica Simpson", "fox hunting", "Los Angeles", "S7", "Hilux pickup truck", "eggshells", "Manchester", "Gertrude Stein", "Dick Van Dyke"], "metric_results": {"EM": 0.484375, "QA-F1": 0.611489811051232}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.125, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.05714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.8235294117647058, 0.07142857142857144, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.4, 0.5217391304347826, 1.0, 0.6666666666666666, 0.42857142857142855, 0.0, 0.0, 1.0, 0.5, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1499", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-2011", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-1533", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5380"], "SR": 0.484375, "CSR": 0.5480902777777779, "EFR": 1.0, "Overall": 0.6953211805555556}, {"timecode": 90, "UKR": 0.681640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-342", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8553", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.830078125, "KG": 0.5203125, "before_eval_results": {"predictions": ["epipelagic", "alexandrovna", "Stevie Wonder", "Sir Tom Finney", "Jackson Pollock", "the Black Sea", "Jumping Jack Flash", "jimmy priestley", "Dumbo", "New Zealand", "Call for the Dead", "Bill Bryson", "Hans Christian Andersen", "Christian Louboutin", "Vespa", "phobias", "Laputa", "a Hungarian Horntail", "Jumanji", "Flo Rida", "at sign", "The Princess bride", "Advanced", "pig", "Dancing With the Stars", "Australia", "Leicester", "Wind", "Andr\u00e9s Iniesta", "BATH", "1924", "a barred spiral", "Duty Free", "Mark Twain", "fruit", "carbon", "Caernarfon", "khalifa Abdullah", "dpurves Member", "Sergio Garc\u00eda Fern\u00e1ndez", "Chad", "a Celtic Priestess", "Yulia Tymochenko", "E. Nesbit", "czar Alexander I", "John F. Kennedy", "Charles Kewell", "Charles", "r34", "Yukon", "ELiver Twist", "Chlorofluorocarbons", "The tower has three levels for visitors, with restaurants on the first and second levels", "the coffee shop Monk's", "Mani", "DreamWorks Animation", "Port Melbourne", "338", "Pakistan", "is the U.N. nuclear watchdog agency's strongest warning yet that Iran could be aiming to build a nuclear weapon.", "United States Marine Band", "John Adams", "Les Feuilles d'automnes", "Austria"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5355654761904762}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3683", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-5650", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3335", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-268", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-2111", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-4011", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1997", "mrqa_triviaqa-validation-6807", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-339", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-724", "mrqa_searchqa-validation-6163", "mrqa_searchqa-validation-8979", "mrqa_hotpotqa-validation-5203"], "SR": 0.484375, "CSR": 0.5473901098901099, "EFR": 0.9696969696969697, "Overall": 0.7098236659174159}, {"timecode": 91, "before_eval_results": {"predictions": ["dumbo", "Ernest Hemingway", "Switzerland", "Mexican orange blossom", "Perry Mason", "england", "James I", "trapezium", "Canada", "My Little Baby Austin Seven", "seven", "Vancouver, British Columbia, Canada,", "niger", "Switzerland", "the Union Gap", "england", "air Bud", "england", "gin", "guitar", "john McCain's", "the north-west corner of the central business district", "Virginia", "menstrual cramps", "pasta", "witch trials", "sailor", "my Favorite Martian", "plutocracy", "Bahrain", "Bosnia and Herzegovina", "Ace of Spades", "communism", "China", "doxycycline", "Venice", "henry", "1973", "england cathedrals", "Brighton", "d.offical", "popes", "jimmy Carter", "net Worth", "Argentina", "george", "sauce", "khrushchev", "argon", "john Peel", "hsborough", "May 2017", "mitosis", "Morgan Freeman", "Mike Fiers", "coca wine", "engirundho Vandhaal", "a massive sports car collection or something similar,\"", "calling on NATO to do more to stop the Afghan opium trade", "Trevor Rees-Jones,", "edvard gRIEG", "Buddhism", "New York", "water"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5616274350649351}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.2727272727272727, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6589", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-4024", "mrqa_triviaqa-validation-6471", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3775", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-7584", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-2032", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-1046", "mrqa_hotpotqa-validation-2210", "mrqa_hotpotqa-validation-5273", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-2960", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-11019"], "SR": 0.515625, "CSR": 0.5470448369565217, "EFR": 0.967741935483871, "Overall": 0.7093636044880786}, {"timecode": 92, "before_eval_results": {"predictions": ["murder, rape, conspiracy", "Carol Fowler", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "Krishna Rajaram,", "U.S. President-elect Barack Obama", "Kurt Cobain's", "President Mahmoud Ahmadinejad,", "a gym", "California-based Current TV", "gun", "\"E! News\"", "1983", "nude beaches.", "tennis", "Barack Obama,", "19", "Max Foster,", "military commissions", "Iran of trying to build nuclear bombs,", "a lion Among Men.", "any indicators or signs that he was going to go off so drastically", "\"surge\" strategy", "to clean up Washington State's decommissioned Hanford nuclear site,", "deliver a big speech", "the island's dining scene", "Saturday,", "warns business owners to close their shops during daily prayers,", "ice", "gasoline", "they did not receive a fair trial.", "2.5 million", "abducting each other for ransoms or retribution.", "Egypt", "about three minutes after launch", "Barack Obama's", "iTunes Music Store,", "president Robert Mugabe", "The sole survivor of the crash that killed Princess Diana", "12.3 million", "ties", "three", "to encourage readers to get involved in service and volunteerism in their communities.", "75 percent", "Chinese President Hu Jintao", "1,500", "always hot and humid", "two counts of murder.", "Alexandros Grigoropoulos,", "16th", "in the heart of Los Angeles.", "Alberto Espinoza Barron,", "2013", "Vincenzo Peruggia", "April 1979", "morocco", "hercule poirot", "1997", "Debbie Reynolds", "43rd", "USS Essex (CV-9)", "Daylight Saving Time", "michelle earhart", "uranium", "Pakistan"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6266806722689076}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9411764705882353, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8235294117647058, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2160", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2917", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-2958", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1516", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-4531", "mrqa_hotpotqa-validation-1316", "mrqa_hotpotqa-validation-224", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-1681"], "SR": 0.46875, "CSR": 0.5462029569892473, "EFR": 1.0, "Overall": 0.7156468413978494}, {"timecode": 93, "before_eval_results": {"predictions": ["Bologna", "We agreed to co-chair the Genocide Prevention Task Force,", "March 8", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "Kearny, New Jersey.", "her husband had knocked her down, held a loaded gun to her head", "five minutes before commandos descended", "Democratic", "Kim Il Sung", "Roy Foster's", "2-0", "managing his time.", "U.S. Consulate in Rio de Janeiro,", "Majid Movahedi,", "bricks and concrete", "Facebook and Google,", "Veracruz, Mexico,", "customers are lining up for vitamin injections that promise to improve health and beauty.", "Oxbow, a town of about 238 people,", "Sarah Brown's", "those missing in a North Carolina food plant heavily damaged in a morning explosion,", "summer", "a vast settlement of people left without loved ones, without homes, without life's belongings.", "Friday.", "\"the most important discovery\"", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "producing rock music with a country influence.", "more than 78,000 parents of children ages 3 to 17", "\"I think Arrington truly believes everything he has said about the tech world being a meritocracy.", "Republican", "the underprivileged.", "March 24,", "North Korea,", "Mexican military", "American Bill Haas", "to protect ocean ecology, address climate change and promote sustainable ocean economies.", "killing of a 15-year-old boy", "Somalia's piracy problem was fueled by environmental and political events.", "between government soldiers and Taliban militants in the Swat Valley.", "Japanese Foreign Ministry spokesman Hidenobu Sobashima", "Grayback forest-firefighters", "Steven Gerrard", "three", "Olympic medal", "her husband's infidelity", "the Beatles", "At least 88 people had been hurt, 28 of them seriously enough to go to a hospital,", "the Illinois Reform Commission", "use of torture and indefinite detention", "No. 4", "severe flooding", "Pastoral farming", "Kaley Christine Cuoco", "Hermann Ebbinghaus", "Route sixty-six", "wagner", "Black Wednesday", "Cambridge University", "early 20th-century Europe", "American tour", "Mickey Mouse", "Daiquiri", "Israel", "the UNESCO / ILO Recommendation concerning the Status of Teachers"], "metric_results": {"EM": 0.5, "QA-F1": 0.633100122871942}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false], "QA-F1": [0.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 0.782608695652174, 0.2857142857142857, 0.6666666666666666, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 0.21428571428571427, 0.0, 0.7777777777777777, 0.7272727272727273, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.13333333333333333, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.11764705882352941, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_newsqa-validation-1910", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-4092", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-3631", "mrqa_triviaqa-validation-3951", "mrqa_hotpotqa-validation-3326", "mrqa_searchqa-validation-6", "mrqa_naturalquestions-validation-7261"], "SR": 0.5, "CSR": 0.5457114361702128, "EFR": 1.0, "Overall": 0.7155485372340425}, {"timecode": 94, "before_eval_results": {"predictions": ["Lunsmann's son, Kevin, and her 19-year-old Filipino nephew, Romnick Jakaria,", "23-year-old", "Starr", "Thirty to 40", "Larry Ellison,", "California, Texas and Florida,", "Alberto Espinoza Barron,", "review their emergency plans and consider additional security measures", "4,000", "Whitney Houston", "Marines", "Lillo Brancato Jr.", "United States, NATO member states, Russia", "money won't cast a spell on him.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "many riders say it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "$10 billion", "opium trade", "Casa de Campo International Airport in the Dominican Republic", "Pakistan's", "dancing against a stripper's pole.", "male veterans struggling with homelessness and addiction.", "CNN's Campbell Brown", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Iraqi Prime Minister Nouri al-Maliki", "Alfredo Astiz,", "Senate Sotomayor,", "Majid Movahedi,", "Reid's dismissal,", "three", "1960.", "education, infrastructure, energy", "engage in learning differently, enjoy a customized approach and hone critical thinking skills.", "the first", "Larry", "Charlotte Gainsbourg and Willem Dafoe", "public endorsement of Bob Dole,", "Ralph Lauren", "Barack Obama sent a message that fight against terror will once against honor some of the most cherished ideals of our republic: respect for the rule of law, individual rights, and America's moral leadership.", "2005", "Two UH-60 Blackhawk helicopters", "Starr", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Ghana", "Christopher Savoie", "246", "London's Waterloo Bridge", "Chievo", "the last few months,", "Since 1980, the 84-year-old Mugabe has been the country's only ruler.", "a nuclear weapon", "2020 National Football League ( NFL ) season", "John Cooper Clarke", "late 1980s", "Chicago", "kolkata", "photographer", "MGM Grand Garden Special Events Center", "The Royal Navy (RN)", "Manchester United", "Bangkok", "roof", "Cana", "KXII"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6666216696739953}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true], "QA-F1": [0.8, 1.0, 0.33333333333333337, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.8571428571428571, 0.0, 1.0, 0.15384615384615385, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 0.5116279069767442, 1.0, 1.0, 0.0, 0.1212121212121212, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 0.9090909090909091, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-1131", "mrqa_naturalquestions-validation-8685", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-565", "mrqa_hotpotqa-validation-4069"], "SR": 0.515625, "CSR": 0.5453947368421053, "EFR": 1.0, "Overall": 0.7154851973684211}, {"timecode": 95, "before_eval_results": {"predictions": ["Aksel Sandemose", "pubs, bars and restaurants", "Retina display", "Chick tracts", "Julia Verdin", "a British naval officer and statesman, an uncle of Prince Philip, Duke of Edinburgh, and second cousin once removed of Elizabeth II", "to prevent the opposing team from scoring goals", "Graffiti", "ARY Digital Network", "Drifting", "Larry Wayne Gatlin", "9 October 19408 December 1980", "Key West", "its riverside location,", "Scottish", "Skipton", "Atlas ICBM", "PPG Paints Arena", "Kelly Bundy", "John Joseph Travolta", "Love Letter", "SBS", "AVN Adult Entertainment Expo", "119 minutes", "50 million", "Intelligent Design", "Sinngedichte", "Hidden America with Jonah Ray", "Irish", "Metro-Goldwyn-Mayer", "RAF Tangmere, West Sussex", "Summer Olympic Games", "1692", "Art Deco-style skyscraper", "was drafted out of high school by the Marlins", "25 November 2015", "Hyuna", "Mathieu Kassovitz", "Kentucky Music Hall of Fame", "Louis King", "Danish", "Hindi", "two", "McComb, Mississippi", "D\u00e2mbovi\u021ba River", "24800 mi", "saint", "143,007", "Michael Edward \" Mike\" Mills", "Edward Vincent Sullivan (September 28, 1901 \u2013 October 13, 1974) was an American television personality, sports and entertainment reporter", "his fourth term", "2001", "a set of related data", "Marshall Sahlins", "Roberta Flack", "Sir Walter Scott", "Braille", "Caster Semenya", "one of its diplomats in northwest Pakistan on Thursday,", "London's", "\"heroes\"", "a professional Wizard", "the Erie Canal", "Andy Warhol"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7739604145854146}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.923076923076923, 1.0, 0.4, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.09523809523809525, 0.8, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-2013", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-8669", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3651", "mrqa_searchqa-validation-9300", "mrqa_searchqa-validation-3482"], "SR": 0.671875, "CSR": 0.5467122395833333, "EFR": 1.0, "Overall": 0.7157486979166666}, {"timecode": 96, "before_eval_results": {"predictions": ["Black Abbots", "Elsie Audrey Mossom", "Nicolas Winding Refn", "PBS", "Ed Lee", "James Victor Chesnutt", "Wayne Rooney", "South Asia and the Middle East", "Martin Ingerman", "Rice University", "NCAA Division I", "Tom Rob Smith", "Reich Chancellery", "Sun Woong", "pubs, bars and restaurants", "Brad Pitt", "Milan", "1885", "Pac-12 Conference", "Lamar Wyatt", "Homer Hickam, Jr.", "Straits of Gibraltar", "Port Clinton", "Patrick Dempsey", "Issaquah", "Taylor Swift", "Appleby-in-Westmorland", "2000", "Mickey's Christmas Carol", "The American relay of Michael Phelps, Ryan Lochte, Peter Vanderkaay", "Orson Welles", "in 1902", "Brittany Snow", "northernmost province, Lapland", "Elvis' Christmas Album", "CD Castell\u00f3n", "Coronation Street", "Zimbabwe", "Lifestyle cities", "Jack Elam", "1998", "Johnny Cash, Waylon Jennings", "Brad Silberling", "ITV", "1982", "the Marx Brothers film", "Tennessee", "Agent Vinod", "James Worthy", "Harrods", "2007", "January 2004", "redbud", "left - sided heart failure", "(Max) Weber", "Laurence Olivier", "Indian Ocean", "Rodong Sinmun", "two months ago", "a Texas ranch that's home to members of a polygamist sect,", "Vatican City", "a porch", "Eric Knight", "an expression of unknown origin"], "metric_results": {"EM": 0.671875, "QA-F1": 0.8051857864357864}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 0.8, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2033", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-3381", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7760", "mrqa_triviaqa-validation-3064", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-776", "mrqa_searchqa-validation-69", "mrqa_naturalquestions-validation-9071"], "SR": 0.671875, "CSR": 0.5480025773195876, "EFR": 1.0, "Overall": 0.7160067654639175}, {"timecode": 97, "before_eval_results": {"predictions": ["Louisiana", "poker", "Budapest", "Hoppin' John", "capuchin", "bass", "El Cid", "Vestal virgins", "contract", "Akihito, emperor of Japan", "lead", "Israel", "Matthew", "Nancy Astor", "imperative", "the bald eagle", "high altitude", "Oslo", "leap year", "Little Miss Muffet", "Gila", "Amsterdam", "Zyrtec", "Buddhism", "Carson City", "Syria", "Cherry", "Council of Better Business Bureaus", "Linda Tripp", "a shooting brake", "Aqua Teen Hunger Force", "former Senator for Virginia", "economics", "United Nations", "diseases", "Rocky Mountain Fever", "euros", "Lebanese", "typewriters", "Isadora Duncan", "Jaws 2", "George Armstrong Custer", "nag", "Iliad", "Motor Trend", "the U.S. Department of Transportation", "Staten Island", "Aethra", "marx", "Xaymaca", "St. Elsewhere", "Lord Banquo", "Agra Cantonment - H. Nizamuddin Gatimaan Express", "Paul Lynde", "John Part", "March 10, 1997", "Hubble Space Telescope", "\"Household Words\"", "Rockland County", "Trilochanapala", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Turkey", "The first line of law and order", "Hoyo de Monterrey"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7289104775432901}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.1818181818181818, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6875000000000001, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1155", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-4397", "mrqa_searchqa-validation-8960", "mrqa_searchqa-validation-16243", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-12404", "mrqa_searchqa-validation-2730", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-10588", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-2784", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-2927", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-3416", "mrqa_triviaqa-validation-3516", "mrqa_hotpotqa-validation-2278", "mrqa_newsqa-validation-310", "mrqa_triviaqa-validation-5852"], "SR": 0.609375, "CSR": 0.5486288265306123, "EFR": 0.96, "Overall": 0.7081320153061224}, {"timecode": 98, "before_eval_results": {"predictions": ["Scary Movie", "a mall", "piety", "Time", "the Annunciation of the Lord", "the Thames", "Alyssa Milano", "drowsiness", "lilies", "Alaska", "Yellowstone", "Marcel Duchamp", "Little Red Riding Hood", "Vaduz", "the tongue", "the English Channel", "Palmer", "a media event", "Simple Simon", "hot chocolate", "vibrations", "a metronome", "gigabytes", "the Phillie Phanatic", "GILBERT & SULLIVAN", "a Pringles can", "the Gentlemen Prefer Blondes", "a Stratocaster", "the ship's five anchors", "Romeo", "a mirror", "Pamela Anderson", "trampoline", "the Salute Bishop Va.", "Jamaica", "(Tiger) Woods", "dark places", "Elton John", "the Sphinx", "Toy Story", "lump", "density", "ice hockey", "Heather Locklear", "the Pong", "pasta salads", "\"He's not the Messiah, he's a very naughty boy\"", "a crone", "Peter", "whole hog", "Target", "Left Behind", "71 -- 74 \u00b0 C ( 160 -- 165 \u00b0 F )", "the Devastator", "cotton", "Mary Decker", "marx", "1868", "Leafcutter John", "University of Vienna", "Ferraris, a Lamborghini and an Acura NSX", "80 percent", "Michael Jackson", "north"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7328869047619047}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3541", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-15322", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-14461", "mrqa_searchqa-validation-13276", "mrqa_searchqa-validation-8717", "mrqa_searchqa-validation-3853", "mrqa_searchqa-validation-8288", "mrqa_searchqa-validation-92", "mrqa_searchqa-validation-15535", "mrqa_searchqa-validation-16364", "mrqa_searchqa-validation-15136", "mrqa_searchqa-validation-416", "mrqa_searchqa-validation-9016", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-8451", "mrqa_searchqa-validation-13668", "mrqa_searchqa-validation-85", "mrqa_triviaqa-validation-6743", "mrqa_hotpotqa-validation-537", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-1683"], "SR": 0.640625, "CSR": 0.5495580808080809, "EFR": 0.9565217391304348, "Overall": 0.7076222139877032}, {"timecode": 99, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1673", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12845", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14027", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6260", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8234", "mrqa_searchqa-validation-8339", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_searchqa-validation-999", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6448", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7028", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-959"], "OKR": 0.81640625, "KG": 0.51796875, "before_eval_results": {"predictions": ["ordered the immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guantanamo Bay, Cuba.", "role in \"Gandhi\"", "Brooklyn, New York,", "Evans", "WBO welterweight title", "Lashkar-e-Jhangvi,", "the administration said it had not lost contact with any planes during the computer glitches.", "Addis Ababa,", "Saturn", "piano", "the Bronx.", "two years,", "The few workers who went into the water swam to safety,", "$17,000", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Haleigh Caldwell,", "The sailboat, named Cynthia Woods,", "said Chaudhary's death was warning to management.", "violent separatist campaign", "Michoacan Family,\"", "two women killed in a stampede at one of his events in Angola on Saturday,", "exotic sports", "Bryant Purvis,", "role as a bride in the 2007 movie \"License to Wed\"", "Kurt Cobain", "Department of Homeland Security Secretary Janet Napolitano", "Gary Brooker", "E. coli bacteria", "in July", "engineering and construction company", "India", "a one-shot victory in the Bob Hope Classic", "Dubai", "U.S. Vice President Dick Cheney", "Rwanda", "Tehran,", "France's", "Daytime Emmy Lifetime Achievement Award.", "money or other discreet aid for the effort if it could be made available,", "checkposts and military camps in the Mohmand agency,", "Somalia's piracy problem was fueled by environmental and political events.", "five", "1994", "start a dialogue of peace based on the conversations she had with Americans along the way.", "would compromise the public broadcaster's appearance of impartiality.", "President Obama", "The Ministry of Defense", "Wednesday.", "sons", "Michael Arrington,", "Lexus, Lincoln, Infiniti ororsche", "1973", "1975", "1546", "Kaiser Chiefs", "rivers", "Ecuador", "Afghanistan", "University College of North Staffordshire", "Nelson County", "the bulls in Pamplona", "Jean-Michel Basquiat", "the Coast Guard", "Iron"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7560113497613498}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [0.48484848484848486, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 0.5, 1.0, 0.0, 0.8, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5555555555555556, 0.0, 1.0, 1.0, 1.0, 0.1, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1822", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2609", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-653", "mrqa_newsqa-validation-2968", "mrqa_searchqa-validation-14651"], "SR": 0.65625, "CSR": 0.5506249999999999, "EFR": 0.9545454545454546, "Overall": 0.7128309659090909}]}