{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]_result.json', stream_id=4, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4210, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["fall of 1937", "The Skirmish of the Brick Church", "beliefs of Sunni Islamic thinkers", "\"The Lodger\"", "Londonistan", "a high-level marketing manager", "Houston, Texas", "cone-shaped", "San Francisco Bay Area's Levi's Stadium", "Ren\u00e9 Lalique", "absolution", "$105 billion", "ABC Cable News", "trial division", "their belief in the validity of the social contract", "Hyde Park", "four years", "Grey Street", "most of the items in the collection, unless those were newly accessioned into the collection", "literacy and numeracy", "Luther", "prime elements", "the Aveo", "one week", "Steymann v Staatssecretaris van Justitie", "1937", "The governments of the United States, Britain, Germany and France", "mother-of-pearl", "cholera", "Tower District", "ring theory", "Euclid's fundamental theorem of arithmetic", "Tony Hawk", "Beyonc\u00e9", "The Book of Discipline", "USSR", "Schmalkaldic League", "2006", "70%", "Einstein", "Genghis Khan", "four half-courses per term", "2011", "Brownlee", "Tracy Wolfson", "the wisdom and prudence of certain decisions of procurement", "1971", "the Uighurs surrendered to the Mongols first", "cnidarians", "CBS", "842 pounds", "two of Tesla's uncles", "up to \u00a332,583", "the City council", "three", "shopping", "4 weeks", "propulsion, electrical power and life support", "William Smilie", "George Westinghouse", "1279", "complexity classes", "\"everything that smacks of sacrifice\"", "a system to function"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7404040404040404}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9630", "mrqa_squad-validation-7687", "mrqa_squad-validation-4836", "mrqa_squad-validation-131", "mrqa_squad-validation-2297", "mrqa_squad-validation-5505", "mrqa_squad-validation-1802", "mrqa_squad-validation-9136", "mrqa_squad-validation-9061", "mrqa_squad-validation-116", "mrqa_squad-validation-5877", "mrqa_squad-validation-6294", "mrqa_squad-validation-7214", "mrqa_squad-validation-3699", "mrqa_squad-validation-8247", "mrqa_squad-validation-4419", "mrqa_squad-validation-553", "mrqa_squad-validation-3811", "mrqa_squad-validation-2092"], "SR": 0.703125, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 1, "before_eval_results": {"predictions": ["the Pulfrich effect", "semi-legal", "Westinghouse Electric", "Pax Mongolica", "The date of 2035", "internal strife", "the Marburg Colloquy", "Northumbria University", "non-cryogenic", "the defense and justification of empire-building", "the Carm Michael numbers", "1999", "Scorpion", "October 16, 2012", "a commune", "the metal locking screw on the camera lens", "Eldon Square Shopping Centre", "type III secretion system", "$680 billion", "296", "New Collegiate Division", "four", "18 million volumes", "15,100", "Warner Bros. Presents", "V\u03b39/V\u03b42 T cells", "1985", "the Augustinian friars", "third", "2012", "gold", "force model that is independent of any macroscale position vector", "378", "tourism", "the Jews", "all", "many celebrated seasons", "Charles-Fer Ferdinand University", "The Nationals", "a computational problem where a single output (of a total function) is expected for every input", "Katharina von Bora", "1888", "the middle of the continent", "Schmalkaldic League", "4:51", "Knaurs Lexikon", "constant pressure", "detective shows", "the southern and central parts of France", "Maria Fold and thrust Belt", "making it seem like climate change is more serious by overstating the impact", "1945", "1876", "Elway", "spring of 1349", "Extreme Makeover: Home Edition", "the Wesleyan Holiness Consortium", "it has settled as one of the pillars of history", "The four Railroads are fairly lucrative properties", "The Sphinx would devour anyone who could not answer her riddle", "The Smashing Pumpkins are an American alternative rock band from Chicago, Illinois, formed", "the 107th justice to serve on the United States Supreme Court", "32-year-long investigation into the enigmatic hijacker", "If the citizen's heart was heavier than a feather they would face torment in a lake of fire"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8365891167494787}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.2666666666666667, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 0.18181818181818182, 0.3076923076923077, 0.0, 0.0, 0.2105263157894737]}}, "before_error_ids": ["mrqa_squad-validation-8546", "mrqa_squad-validation-9024", "mrqa_squad-validation-10466", "mrqa_squad-validation-1030", "mrqa_squad-validation-1189", "mrqa_squad-validation-1600", "mrqa_squad-validation-4287", "mrqa_squad-validation-2166", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-1274", "mrqa_hotpotqa-validation-3713"], "SR": 0.78125, "CSR": 0.7421875, "EFR": 0.9285714285714286, "Overall": 0.8353794642857143}, {"timecode": 2, "before_eval_results": {"predictions": ["Solim\u00f5es Basin", "the seal of the Federal Communications Commission", "Islamization", "the E. W. Scripps Company", "Grand Canal d'Alsace", "food security", "exothermic", "Newcastle Diamonds", "the wisdom and prudence of certain decisions of procurement", "D\u00fcrer", "Grover Cleveland", "Erg\u00e4nzungsschulen", "concrete", "the National Science Foundation (NSF) beginning in 1985 to promote advanced research and education networking in the United States", "The Newlywed Game", "the German-Swiss border", "the \"blurring of theological and confessional differences in the interests of unity.\"", "microbes", "300", "the electrostatic force", "Jim Nantz and Phil Simms", "1530", "from 12:00 to 6:00 p.m. Eastern Time", "employ consultant pharmacists and/or provide consulting services", "the murder of Christ", "1708", "the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)", "the Religious Coalition for Reproductive Choice", "The Arrow", "intractable problems", "yellow fever", "silver and inlaid with gold", "Richard Wilkinson and Kate Pickett", "elsewhere in the Northern United Kingdom", "president and CEO", "Von Miller", "the weak force", "a diverse phylum of bacteria capable of carrying out photosynthesis", "vaccination", "the plague theory", "the loss of soil fertility and weed invasion", "the fact (Fermat's little theorem)", "11th", "the most popular show at the time", "Robert of Jumi\u00e8ges", "student populations", "German", "Persia", "superheaters", "two", "Johann von Staupitz", "lectures", "the land gets more", "What a wonderful World", "the White House", "Dugout canoe", "Ganges River", "the Heritage 1981 brand", "Britney Spears", "the title My Fair Lady", "Don Bradman", "Ford Motor Co.", "Fidenza", "Charles Scribner's"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7270272435897436}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6399999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9768", "mrqa_squad-validation-5521", "mrqa_squad-validation-4847", "mrqa_squad-validation-597", "mrqa_squad-validation-5828", "mrqa_squad-validation-10460", "mrqa_squad-validation-8777", "mrqa_squad-validation-4974", "mrqa_squad-validation-9023", "mrqa_squad-validation-1188", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-9859", "mrqa_searchqa-validation-6857", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-3869"], "SR": 0.671875, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 3, "before_eval_results": {"predictions": ["Winter Film Capital of the World", "sarcasm and attempts to humiliate pupils", "collenchyma tissue", "24 March 1879", "Scottish Constitutional Convention", "constant factors and smaller terms", "1996", "CBS", "genetic branches", "a religious basis", "14,000", "Killer T cells", "about 11 million", "third", "The Earth's mantle", "expansions", "Necessity-based", "glaucophyte chloroplasts", "artisans and farmers", "chloroplast", "pharmacists", "September 2007", "a declining state of mind", "a all-Gemini veteran crew", "civil disobedience", "Sociologist", "World News Tonight", "the carriage of their respective basic channels", "cytotoxic", "Liao, Jin, and Song", "Tyneside Classical", "nine", "18 million", "four", "Christian Whiton", "his mother", "Johann Gerhard", "Korean", "The Time of the Doctor", "7 January 1900", "90\u00b0", "the Natives of these localities are very badly disposed towards the French, and are entirely devoted to the English", "stolen", "Centrum", "$200,000", "4,686", "demographics and economic", "the Boeing 707", "Karl Marx", "President Abraham Lincoln", "Phantom Manor", "a Norwegian crown prince", "South Africa", "fibre optics", "the Swiss Family", "a cone-shaped utensil", "Yasser Arafat", "a cricket", "a syncretic religion", "a dowry", "cricket", "Piper Halliwell", "Prince James, Duke of York and of Albany ( later King James II & VII )", "Charles Perrault"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6696180555555555}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9337", "mrqa_squad-validation-1714", "mrqa_squad-validation-9597", "mrqa_squad-validation-10107", "mrqa_squad-validation-8703", "mrqa_squad-validation-6409", "mrqa_squad-validation-6670", "mrqa_squad-validation-6884", "mrqa_squad-validation-7083", "mrqa_squad-validation-2406", "mrqa_squad-validation-10186", "mrqa_squad-validation-1509", "mrqa_searchqa-validation-6999", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-350", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-495"], "SR": 0.609375, "CSR": 0.69140625, "EFR": 0.96, "Overall": 0.825703125}, {"timecode": 4, "before_eval_results": {"predictions": ["Mars", "education", "Kingdom of Prussia", "May 18, 1756", "odd prime", "economic growth by collecting resources from colonies, in combination with assuming political control by military and political means", "quantum electrodynamics", "topographic", "Hassan al Banna", "regional burden sharing", "Indianapolis Colts", "Pole Mokotowskie", "a school or other place of formal education", "Francis Blackburne", "black earth", "photolysis of ozone by light of short wavelength", "smart ticketing", "State Route 99", "F and \u2212F are equal in magnitude and opposite in direction", "free", "Air", "1,548", "whether the bill is within the legislative competence of the Parliament", "two", "Galileo Galilei", "Catholic", "patient care rounds drug product selection", "a vicious and destructive civil war", "greater scarcity", "The Eleventh Doctor", "86", "Arizona Cardinals", "not designed to fly through the Earth's atmosphere", "2015", "one hunting excursion", "a bishop", "to punish Christians by God", "socialist realism", "July 23, 1963", "1162", "a method which pre- allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes", "Barbara Walters", "as soon as 2050", "Red River", "aircraft", "Buenos Aires", "Sub-Saharan Africa", "How I Met Your Mother", "the Carrousel du Louvre", "$1.5 million", "3 to 17", "Isabella", "Obama", "a Minnesota-based charitable organization providing Christian education, health care, nutrition and micro-enterprise opportunities to children and families in Haiti", "Preah Vihear temple", "Ralph Lauren", "Noriko Savoie", "T.I.", "a Columbian mammoth", "to stabilize Somalia and cooperate in security and military operations", "battles, political intrigue, and the characters", "oldpatricktoe-end", "Mulberry", "a long horizontal shaft"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7343885281385282}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.25, 0.25, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9926", "mrqa_squad-validation-8634", "mrqa_squad-validation-1891", "mrqa_squad-validation-10333", "mrqa_squad-validation-9665", "mrqa_squad-validation-3706", "mrqa_squad-validation-3848", "mrqa_squad-validation-10068", "mrqa_squad-validation-2564", "mrqa_squad-validation-4746", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-2234", "mrqa_naturalquestions-validation-7134", "mrqa_triviaqa-validation-712", "mrqa_searchqa-validation-8929"], "SR": 0.640625, "CSR": 0.68125, "EFR": 1.0, "Overall": 0.840625}, {"timecode": 5, "before_eval_results": {"predictions": ["visitation of the Electorate of Saxony", "United States", "11", "May", "to employ limited coercion", "1798", "3D printing technology", "Waterlogged", "Tim Allen", "wealth and income", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "40", "LeGrande", "filaments", "Rowan Atkinson", "Energiprojekt AB in Sweden", "DuMont Television Network", "1913", "Egyptian Islamic Jihad organization", "consumer prices", "petroleum", "1870", "27.7 million tons", "the remainder of the British Isles", "Jean Auguste Dominique Ingres,", "Eliot Ness", "condemned the violence as the devil's work, and called for the nobles to put down the rebels like mad dogs", "Edgar", "experience and extra responsibilities", "chameleon circuit", "All-Channel Receiver Act", "secular powers", "at the opposite end from the mouth", "areas controlled by Russia in 1914", "kinescope", "University Athletic Association", "three", "Chebyshev", "reality television", "Disco", "Christopher Lloyd Smalling", "Mary Harron", "Polk", "Minette Walters", "1983", "79 AD", "1993 to 2001", "Major League Soccer", "1669", "University of Vienna", "\"lo Stivale\" (the Boot)", "Richa Sharma", "Centennial Olympic Stadium", "Violet", "Vernier, Switzerland", "October 21, 2016", "an Indian cricketer and former captain of the Indian cricket team", "FIFA World Cup, AFC Asian Cup and East Asian Football Championship", "The conversation", "9 February 2018", "Canada", "teenage", "a gas (like water vapor) changes to make tiny liquid water droplets that hang in the air", "Detective West"], "metric_results": {"EM": 0.734375, "QA-F1": 0.803249007936508}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6846", "mrqa_squad-validation-7886", "mrqa_squad-validation-3345", "mrqa_squad-validation-5519", "mrqa_squad-validation-2322", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4614", "mrqa_triviaqa-validation-4029", "mrqa_newsqa-validation-2982", "mrqa_searchqa-validation-4118", "mrqa_naturalquestions-validation-10691"], "SR": 0.734375, "CSR": 0.6901041666666667, "EFR": 1.0, "Overall": 0.8450520833333334}, {"timecode": 6, "before_eval_results": {"predictions": ["woodblocks", "General Hospital", "circle logo", "Denver Broncos", "129 MSPs", "Lenin", "completed (or local) fields", "\"push\" motivations", "alone", "stem cells", "John Pell, Lord of Pelham Manor", "quickly", "induction motor", "religious", "pressure terms", "ten million people", "Tommy Lee Jones", "nine factors", "13.34%", "kilopond", "water level", "1981", "rules that conflict with morality", "sixteenth century", "R\u00fcdesheim", "epidemiological account", "time or space", "Jacksonville Consolidation", "Aristotle", "1724", "Emsian", "Canada", "Stanford", "Reuben Townroe", "small forward", "Alamo Bowl", "The King of Chutzpah", "Charles Russell", "German", "Minette Walters (born 26 September 1949)", "St. Patrick's Day in 1988", "Dulwich", "Michael Sheen", "Bohemia", "Sky News", "Ella Fitzgerald", "\"Confessions of a Teenage Drama Queen\"", "EBSCO Information Services", "Dutch", "Marc Bolan", "\"Frankenstein\"", "John Mills", "November 1961", "Saint-Domingue", "Airline Deregulation Act", "\"Kill Your Darlings\"", "University of Kansas", "The Land of Enchantment", "2001", "The Colossus of Rhodes", "state governments", "Billy Bob Thornton", "Charles Martel", "Saturday Night Live"], "metric_results": {"EM": 0.6875, "QA-F1": 0.746875}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9430", "mrqa_squad-validation-7614", "mrqa_squad-validation-2567", "mrqa_squad-validation-9753", "mrqa_squad-validation-5303", "mrqa_squad-validation-9098", "mrqa_squad-validation-4802", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-4387", "mrqa_triviaqa-validation-2856", "mrqa_newsqa-validation-696", "mrqa_searchqa-validation-12796"], "SR": 0.6875, "CSR": 0.6897321428571428, "EFR": 1.0, "Overall": 0.8448660714285714}, {"timecode": 7, "before_eval_results": {"predictions": ["Germany", "1985", "William Iron Arm", "Muhammad Abd al-Salaam Farag", "comedies", "lack of understanding", "British East Africa", "Pacific", "Jingshi Dadian", "economic instability", "Jean- Marc Bosman", "Ismailiyah", "Wiesner", "exponential-time", "quarterback", "ten times their own weight", "primes", "light", "success", "TGIF", "married outside their immediate French communities", "George Westinghouse", "nuclear force", "certification", "Alberto Calder\u00f3n", "Mercury", "Marconi successfully transmitted the letter S from England to Newfoundland", "private", "Cadeby", "Ten", "vice president", "Steven Gerrard", "the international community", "Hine's school", "David Mansfield", "Silvio Berlusconi", "London Heathrow's Terminal 5", "Los Angeles", "football", "sharia law", "JBS", "President Obama", "Kurdish militant group", "pilot", "Wednesday", "$50", "back at work", "flooding", "1969", "composer", "50,000", "their own personal rock star", "Bhola district", "SSM Cardinal Glennon Children's Medical Center", "people were throwing Molotov cocktails", "he has no plans to fritter his cash away", "Secretary of State Hillary Clinton", "7th century", "Wigan", "1974", "Temple Square", "Hagai Amir", "The Little Foxes", "Lord Tennyson"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6221453373015873}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.08, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.05714285714285715, 0.33333333333333337, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6034", "mrqa_squad-validation-6925", "mrqa_squad-validation-8339", "mrqa_squad-validation-4692", "mrqa_squad-validation-9614", "mrqa_squad-validation-8771", "mrqa_squad-validation-3069", "mrqa_squad-validation-10445", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_naturalquestions-validation-8664", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-4905", "mrqa_searchqa-validation-1341"], "SR": 0.546875, "CSR": 0.671875, "EFR": 0.896551724137931, "Overall": 0.7842133620689655}, {"timecode": 8, "before_eval_results": {"predictions": ["2 July 1505,", "beginning in early September and ending in mid-May", "high risk preparations and some other compounding functions", "Non Governmental and Intergovernmental Organizations", "chief electrician position", "1671", "CBS,", "12 January", "his own men", "Sonia Shankman Orthogenic School", "Innate immune systems", "service sector", "Wardenclyffe Tower project", "horizontal compression", "Edgar Scherick", "Imperial", "Moscone Center", "\"Manchu dynasty\"", "Since the 1980s,", "oxygen-16", "Gerhard. Lessing", "one advanced lay servant course, and be interviewed by the District or Conference Committee on Lay Speaking.", "city centre roads", "forbidden speech", "ca. 22,000\u201314,000 yr BP,", "two", "Lek", "New England Patriots", "work rule issues.", "28 of those now on hardcourt surfaces.", "July 4.", "Catholic League.", "more than two years,", "at the exit in question", "We Found Love", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "\"Larry King Live\"", "participate in Iraq's government.", "Addis Ababa,", "five female pastors", "military commissions", "to launch a group that will serve as an alternative to the Organization of American States.", "pay him a monthly allowance,", "Tutsi and Hutu", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "\"Draquila -- Italy Trembles.\"", "\"solitary confinement.\"", "Diego Maradona,", "17 Again", "five", "75", "Kgalema Motlanthe,", "to protect ocean ecology, address climate change and promote sustainable ocean economies.", "about 3,000 kilometers (1,900 miles)", "two courses on the Black Sea coast in Bulgaria.", "NATO fighters", "in Austin, Texas,", "two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "Royals", "Anah\u00ed", "a helicopter", "Nikkei 225 Stock Average", "Surrey", "David Bowie"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6195893705183111}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1818181818181818, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4800000000000001, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.058823529411764705, 0.2608695652173913, 0.0, 0.3333333333333333, 0.06451612903225806, 0.5, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.4444444444444445, 0.13333333333333333, 0.888888888888889, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8526", "mrqa_squad-validation-1279", "mrqa_squad-validation-3113", "mrqa_squad-validation-589", "mrqa_squad-validation-6128", "mrqa_squad-validation-7377", "mrqa_squad-validation-8084", "mrqa_squad-validation-2405", "mrqa_squad-validation-10083", "mrqa_squad-validation-5357", "mrqa_squad-validation-6671", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-4000", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-93", "mrqa_searchqa-validation-8602", "mrqa_triviaqa-validation-1"], "SR": 0.484375, "CSR": 0.6510416666666667, "EFR": 1.0, "Overall": 0.8255208333333334}, {"timecode": 9, "before_eval_results": {"predictions": ["Private Education Student Financial Assistance", "philanthropy", "Mongolia", "Daily Mail", "heard her songs; he followed the fishermen and captured the mermaid.", "magnitude", "Combined Statistical Area", "orogenic wedges", "eleven", "Grissom, White, and Chaffee", "Amtrak San Joaquins", "two", "1", "Barbara Walters", "temperate", "girls", "by citizens", "by up to 3 pence in the pound", "Gerhard", "the courts of member states and the Court of Justice of the European Union", "1887", "new laws or amendments to existing laws as a bill", "487", "Presiding Officer", "expansion", "3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "axial skeleton ( 28 in the skull and 52 in the torso )", "Thebes", "moral", "The Maidstone Studios in Maidstone, Kent,", "muttnik", "National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1986", "Charles Darwin", "the Germanic god Wodan, who was associated with the pagan midwinter event of Yule and led the Wild Hunt, a ghostly procession through the sky.", "Poems : Series 1", "1927", "merengue", "lamina dura", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "Proposition 103", "1997", "the intersection of Mud Mountain Road and Highway 410", "San Francisco Bay", "Tom Brady", "the Guardians travel throughout the cosmos as they help Peter Quill learn more about his mysterious parentage", "Duck", "three levels", "Sylvester Stallone", "once every 23 hours, 56 minutes, and 4 seconds with respect to the stars", "The Divergent Series : Ascendant was never made, due to Allegiant's poor showing at the box office", "income statement", "1960", "costumes", "altitude", "E \u00d7 12", "sow", "mouth", "John Joseph Travolta", "Allies of World War I, or Entente Powers,", "U.S. 93", "have a smile on her face when her kids were around.\"", "Randy Couture, Williams,", "Delaware"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6163475929224285}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9, 0.1818181818181818, 0.0, 0.6666666666666666, 1.0, 0.0, 0.631578947368421, 1.0, 1.0, 0.09523809523809523, 0.24000000000000002, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 1.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.125, 0.25, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.7692307692307693, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-805", "mrqa_squad-validation-2717", "mrqa_squad-validation-3922", "mrqa_squad-validation-9641", "mrqa_squad-validation-2404", "mrqa_squad-validation-9452", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-893", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-1173", "mrqa_triviaqa-validation-2329", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2729", "mrqa_newsqa-validation-4179", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-9187"], "SR": 0.484375, "CSR": 0.634375, "EFR": 0.9696969696969697, "Overall": 0.8020359848484848}, {"timecode": 10, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1735", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1877", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3503", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3994", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6984", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-4000", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-69", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16779", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2470", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-350", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-9187", "mrqa_squad-validation-10015", "mrqa_squad-validation-10052", "mrqa_squad-validation-10068", "mrqa_squad-validation-1008", "mrqa_squad-validation-10083", "mrqa_squad-validation-10103", "mrqa_squad-validation-10107", "mrqa_squad-validation-10116", "mrqa_squad-validation-10125", "mrqa_squad-validation-10186", "mrqa_squad-validation-10210", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-1030", "mrqa_squad-validation-10308", "mrqa_squad-validation-10333", "mrqa_squad-validation-10333", "mrqa_squad-validation-10344", "mrqa_squad-validation-10367", "mrqa_squad-validation-10374", "mrqa_squad-validation-104", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10466", "mrqa_squad-validation-10493", "mrqa_squad-validation-1051", "mrqa_squad-validation-1052", "mrqa_squad-validation-1068", "mrqa_squad-validation-1113", "mrqa_squad-validation-116", "mrqa_squad-validation-1165", "mrqa_squad-validation-1178", "mrqa_squad-validation-1188", "mrqa_squad-validation-1193", "mrqa_squad-validation-1200", "mrqa_squad-validation-1207", "mrqa_squad-validation-1211", "mrqa_squad-validation-1257", "mrqa_squad-validation-1269", "mrqa_squad-validation-1279", "mrqa_squad-validation-131", "mrqa_squad-validation-1330", "mrqa_squad-validation-1348", "mrqa_squad-validation-1368", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1509", "mrqa_squad-validation-1527", "mrqa_squad-validation-1536", "mrqa_squad-validation-1541", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1714", "mrqa_squad-validation-1769", "mrqa_squad-validation-1802", "mrqa_squad-validation-1891", "mrqa_squad-validation-1947", "mrqa_squad-validation-1967", "mrqa_squad-validation-2030", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2166", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-2297", "mrqa_squad-validation-2331", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2405", "mrqa_squad-validation-2409", "mrqa_squad-validation-2438", "mrqa_squad-validation-25", "mrqa_squad-validation-2554", "mrqa_squad-validation-2559", "mrqa_squad-validation-2564", "mrqa_squad-validation-2567", "mrqa_squad-validation-2576", "mrqa_squad-validation-2579", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2717", "mrqa_squad-validation-2778", "mrqa_squad-validation-2822", "mrqa_squad-validation-2827", "mrqa_squad-validation-2870", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-3050", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-313", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3261", "mrqa_squad-validation-3269", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3388", "mrqa_squad-validation-3445", "mrqa_squad-validation-3492", "mrqa_squad-validation-3603", "mrqa_squad-validation-3617", "mrqa_squad-validation-365", "mrqa_squad-validation-3699", "mrqa_squad-validation-3759", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3815", "mrqa_squad-validation-3833", "mrqa_squad-validation-3837", "mrqa_squad-validation-3844", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3922", "mrqa_squad-validation-3938", "mrqa_squad-validation-3958", "mrqa_squad-validation-3976", "mrqa_squad-validation-4030", "mrqa_squad-validation-4086", "mrqa_squad-validation-4191", "mrqa_squad-validation-4231", "mrqa_squad-validation-4232", "mrqa_squad-validation-4248", "mrqa_squad-validation-4269", "mrqa_squad-validation-43", "mrqa_squad-validation-4419", "mrqa_squad-validation-4480", "mrqa_squad-validation-4491", "mrqa_squad-validation-4560", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4746", "mrqa_squad-validation-475", "mrqa_squad-validation-4765", "mrqa_squad-validation-4836", "mrqa_squad-validation-4847", "mrqa_squad-validation-4896", "mrqa_squad-validation-4935", "mrqa_squad-validation-5009", "mrqa_squad-validation-5075", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5164", "mrqa_squad-validation-5180", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5221", "mrqa_squad-validation-5272", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5357", "mrqa_squad-validation-5363", "mrqa_squad-validation-5424", "mrqa_squad-validation-5451", "mrqa_squad-validation-5455", "mrqa_squad-validation-5471", "mrqa_squad-validation-5505", "mrqa_squad-validation-5519", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5541", "mrqa_squad-validation-5616", "mrqa_squad-validation-5651", "mrqa_squad-validation-5670", "mrqa_squad-validation-5774", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-583", "mrqa_squad-validation-5840", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-5877", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5908", "mrqa_squad-validation-5937", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-5971", "mrqa_squad-validation-5976", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6048", "mrqa_squad-validation-6083", "mrqa_squad-validation-6098", "mrqa_squad-validation-6098", "mrqa_squad-validation-6128", "mrqa_squad-validation-6158", "mrqa_squad-validation-618", "mrqa_squad-validation-6238", "mrqa_squad-validation-6294", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6381", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6506", "mrqa_squad-validation-6527", "mrqa_squad-validation-6530", "mrqa_squad-validation-6569", "mrqa_squad-validation-6580", "mrqa_squad-validation-6605", "mrqa_squad-validation-6670", "mrqa_squad-validation-6681", "mrqa_squad-validation-6707", "mrqa_squad-validation-6754", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-69", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-6996", "mrqa_squad-validation-7002", "mrqa_squad-validation-7020", "mrqa_squad-validation-7022", "mrqa_squad-validation-7034", "mrqa_squad-validation-7080", "mrqa_squad-validation-7083", "mrqa_squad-validation-7092", "mrqa_squad-validation-7094", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7303", "mrqa_squad-validation-7304", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7372", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7420", "mrqa_squad-validation-7476", "mrqa_squad-validation-7502", "mrqa_squad-validation-7614", "mrqa_squad-validation-7687", "mrqa_squad-validation-7690", "mrqa_squad-validation-7704", "mrqa_squad-validation-775", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-7886", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7981", "mrqa_squad-validation-805", "mrqa_squad-validation-8052", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8197", "mrqa_squad-validation-8247", "mrqa_squad-validation-829", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8364", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8416", "mrqa_squad-validation-8479", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8526", "mrqa_squad-validation-8546", "mrqa_squad-validation-8580", "mrqa_squad-validation-8600", "mrqa_squad-validation-863", "mrqa_squad-validation-8680", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8777", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8953", "mrqa_squad-validation-8957", "mrqa_squad-validation-8965", "mrqa_squad-validation-9002", "mrqa_squad-validation-9012", "mrqa_squad-validation-902", "mrqa_squad-validation-9023", "mrqa_squad-validation-9024", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9136", "mrqa_squad-validation-9141", "mrqa_squad-validation-9208", "mrqa_squad-validation-9254", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9337", "mrqa_squad-validation-9411", "mrqa_squad-validation-9430", "mrqa_squad-validation-9452", "mrqa_squad-validation-9457", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9527", "mrqa_squad-validation-9569", "mrqa_squad-validation-9597", "mrqa_squad-validation-9614", "mrqa_squad-validation-9615", "mrqa_squad-validation-9624", "mrqa_squad-validation-9635", "mrqa_squad-validation-9641", "mrqa_squad-validation-9665", "mrqa_squad-validation-9718", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_squad-validation-9845", "mrqa_squad-validation-985", "mrqa_squad-validation-9926", "mrqa_squad-validation-9940", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.91015625, "KG": 0.4359375, "before_eval_results": {"predictions": ["the Miller\u2013Urey experiment", "five", "Mughal emperors", "a series of strikes by coal miners and railroad workers over the winter of 1973\u201374 became a major factor in the change of government.", "\"informal\" imperialism", "HO, giving the atomic mass of oxygen as 8 times that of hydrogen, instead of the modern value of about 16", "increased settlement and deforestation", "Afranji, meaning \"Franks.\"", "Abercynon in south Wales", "drinking water", "permafrost", "10 to 15 million", "Cabot Science Library, Lamont Library, and Widener Library", "the Henry Cole wing", "the mayor (the President of Warsaw),", "The time and space hierarchy theorems", "The innate immune system", "Cow Counties", "all", "Royal Institute of British Architects", "the 6th century", "The owner", "United Parcel Service", "Ernie", "Sapporo", "The Rosetta Stone", "m\u00e0hjeung", "The Tonight Ensemble from West Side Story Essay", "the right hand side of the second line of letters", "Jap\u00b7a\u00b7nese", "William Boyd", "Hilary Mantel's Wolf Hall", "losing points during the game as long as you win the game", "the gums", "the Irishman", "Richmond, Va.", "Wawrinka", "Humphrey Bogart", "Nigel Hawthorne", "the word \"animal\",", "Auric Goldfinger", "the ship", "5", "Mary Poppins", "Neil Armstrong", "the inner bull", "the Metropolitan Borough of Oldham, in Greater Manchester, England.", "British Defence Secretary", "Old Ironsides", "Bullnose", "blue", "the skull", "mhoiz", "gold", "Brainy", "New Zealand", "Paige O'Hara", "Chris Martin", "Ishtar Gate", "neo-Nazi", "several weeks", "an animal tranquilizer", "Robin", "the mouth"], "metric_results": {"EM": 0.5, "QA-F1": 0.5858639504652435}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6206896551724138, 0.0, 0.1, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3722", "mrqa_squad-validation-9808", "mrqa_squad-validation-3464", "mrqa_squad-validation-1134", "mrqa_squad-validation-6223", "mrqa_squad-validation-962", "mrqa_squad-validation-944", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5918", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-1905", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-865", "mrqa_triviaqa-validation-6822", "mrqa_triviaqa-validation-1579", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-515", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-2154"], "SR": 0.5, "CSR": 0.6221590909090908, "EFR": 0.96875, "Overall": 0.7393536931818182}, {"timecode": 11, "before_eval_results": {"predictions": ["\"Old Briton\"", "Tiffany & Co.", "1985", "entered Europe in two waves", "4k + 1", "Samuel Reshevsky", "mujahideen Muslim Afghanistan", "July 1977", "coal", "programmes", "Stanford Stadium", "gravity", "his work", "monatomic", "MetroCentre", "SAP Center", "composite numbers (the Carmichael numbers)", "the courts of member states", "German-language publications", "27", "6", "summer", "Durham Cathedral", "Konakuppakatil Gopinathan Balakrishnan", "Burj Khalifa", "March 14, 1942", "art of the book and architecture", "Bart Howard", "over the specimen", "94", "Max Martin", "2005", "31 December 1600", "February 29", "transmission", "216", "Dr. Addison Montgomery", "old English pyrige", "food and clothing", "Kansas City Chiefs", "the Indians", "Claudia Grace Wells", "fascia surrounding skeletal muscle", "the amount of surface", "2008", "Alex Rodriguez", "Elliot Scheiner", "Jaydev Shah", "Jodie Foster", "13,000 astronomical units ( 0.21 ly )", "1978", "wisdom", "Thomas Chisholm", "Rocinante", "before the first letter of an interrogative sentence", "Wikia", "The Daily Mirror", "invoice", "Sydney", "1,500", "the underprivileged", "Alexander Pushkin", "richmond", "Time Machine"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6913070436507937}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.8333333333333333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10142", "mrqa_squad-validation-4963", "mrqa_squad-validation-9082", "mrqa_squad-validation-3450", "mrqa_squad-validation-3947", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-3602", "mrqa_triviaqa-validation-412", "mrqa_hotpotqa-validation-2913", "mrqa_newsqa-validation-3686", "mrqa_searchqa-validation-14088", "mrqa_searchqa-validation-2383"], "SR": 0.59375, "CSR": 0.6197916666666667, "EFR": 0.9615384615384616, "Overall": 0.7374379006410257}, {"timecode": 12, "before_eval_results": {"predictions": ["2014", "Muslim", "Frederick William", "herbal remedies", "Mongolian patrimonial feudalism and the traditional Chinese autocratic-bureaucratic system", "96.26%", "19th", "Raoul Pierre Pictet", "more than $45,000", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "n < p < 2n \u2212 2", "illegal", "A", "The European Court of Justice", "chlorophyll a and phycobilins", "German-language publications", "1992", "25 percent", "inwards towards the pith", "Valens and Richardson", "1976", "British Army soldiers shot and killed people while under attack by a mob", "Judiththia Aline Keppel", "Ted '' Levine", "Sebastian Lund ( Rob Kerkovich )", "a noble gas", "A rotation", "Jaydev Shah", "Polly Walker", "Raya Yarbrough", "Darlene Cates", "2014", "northern Europe", "Las Vegas", "to feel close to his son", "28 July 1914", "Jean F Kernel", "Haiti", "1948", "bearers", "Chandan Shetty", "Bill Henderson", "Liam Cunningham", "Nancy Jean Cartwright", "Max", "Jonathan Goldstein", "a premalignant flat", "in the brain", "his friends", "Ren\u00e9 Descartes", "Clarence Anglin", "Jason Lee", "October 1941", "2013", "on the continent of Antarctica", "diocese of Bath and Wells", "John Nash", "Alistair Grant", "1989", "nearly $2 billion", "Hutus", "Inkerman", "Brownsville", "Ralph Lauren"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6513764880952382}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.11111111111111112, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1111111111111111, 0.5714285714285715, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.6, 0.4, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8206", "mrqa_squad-validation-8412", "mrqa_squad-validation-6655", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-5259", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-7346", "mrqa_triviaqa-validation-6797", "mrqa_hotpotqa-validation-2319", "mrqa_newsqa-validation-3661", "mrqa_searchqa-validation-3744"], "SR": 0.578125, "CSR": 0.6165865384615384, "EFR": 1.0, "Overall": 0.7444891826923077}, {"timecode": 13, "before_eval_results": {"predictions": ["early 1990s", "the 17th century work of Galileo Galilei", "Four thousand", "P", "the seal of the Federal Communications Commission", "gentrification of older neighbourhoods", "a historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\"", "1206", "2011", "his lab", "their bright colors sometimes override the chlorophyll green", "Radio Corporation of America (RCA)", "up to three-fourths of the population of the Iranian Plateau", "The Five Doctors", "Open Door Policy", "1538", "a surgical resident at Seattle Grace Hospital", "a minimum adequate diet", "Tony Almeida", "Herbert Hoover", "St. Wenceslaus", "coal", "the Union Pacific & the Central Pacific", "to obtain their tickets in advance", "the skulls of \"Peking Man,\"", "Latin", "Folkvang", "the city of Thebes", "the grindstone", "the Osmonds", "the snake", "butterflies", "the elbow", "The Real Apprentices", "Calypso", "King John", "A Million Little pieces", "the ball of Revolution rolling", "John Lennon", "the Billy Goats Gruff", "the palindrome", "\"bacon strips\"", "Saturn", "the Urals", "Amsterdam", "Etna", "Oahu", "Richard Nixon", "\"Under the Sea\"", "an offensive formation.", "George Carlin", "Che Guevara", "a raven", "the Kurdish language of Iraq", "Nicholas Sparks", "Lizzy Greene", "Bart\u00f3k", "Ivan Owen", "English Electric Canberra", "the murder of 40-50 Karankawa people in Mexican Texas near present-day Matagorda by a party of White colonists in 1826", "Karthik Rajaram", "the Lindsey oil refinery in eastern England.", "a month of training", "processing data"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6209356003473651}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0, 0.8, 1.0, 1.0, 0.7692307692307693, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0909090909090909, 0.5, 0.4444444444444444, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10285", "mrqa_squad-validation-9575", "mrqa_squad-validation-8229", "mrqa_squad-validation-1613", "mrqa_squad-validation-6250", "mrqa_squad-validation-7792", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-16887", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-16654", "mrqa_searchqa-validation-389", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-2755", "mrqa_hotpotqa-validation-1876", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-415"], "SR": 0.53125, "CSR": 0.6104910714285714, "EFR": 1.0, "Overall": 0.7432700892857143}, {"timecode": 14, "before_eval_results": {"predictions": ["Pittsburgh Steelers", "J. S. Bach", "Denver", "from January 1964, until it achieved the first manned landing in July 1969,", "simple unicellular organisms", "The Quasiturbine", "Dave Logan", "Roger NFL", "by compressing and cooling it", "December 2014", "Ladner", "the Ming dynasty", "quarterback", "15,100", "Ronnie Hillman", "Masha Skorobogatov", "Daniel A. Dailey", "on the urinary floor", "31 December 1600", "Shenandoah National Park in the Blue Ridge Mountains of Virginia", "1916", "Milira", "in vitro", "The Walking Dead", "Ceramic art", "State Bar of Arizona", "Tbilisi, Georgia", "a candidate state", "Yondu Udonta", "religious Hindu musical theatre styles", "Rocinante", "one", "John Roberts", "Ray Charles", "British", "a routing table", "the source of the donor organ", "Kevin Spacey", "a 420A motor with an upgrade to a T3 turbo and front mount intercooler", "Lana Del Rey", "Kim Basinger", "a light source", "4,840", "Valmiki", "September 19, 2017", "President pro tempore", "February 16, 2016", "Wisconsin", "Merry Clayton", "a phenomenon known as cold shortening", "Florida", "a violation of nature", "Nepal", "Hungary", "Word Options", "Australia", "June 2, 2008", "Reverend Timothy \"Tim\" Lovejoy", "Arthur E. Morgan III,", "a Muslim", "Australia", "Oxfam", "a Follies", "Oshkosh"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6017180735930736}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.888888888888889, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.09090909090909093, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-32", "mrqa_squad-validation-6453", "mrqa_squad-validation-690", "mrqa_squad-validation-80", "mrqa_squad-validation-3473", "mrqa_squad-validation-8062", "mrqa_squad-validation-362", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-10135", "mrqa_triviaqa-validation-5650", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2435", "mrqa_searchqa-validation-11788"], "SR": 0.515625, "CSR": 0.6041666666666667, "EFR": 1.0, "Overall": 0.7420052083333334}, {"timecode": 15, "before_eval_results": {"predictions": ["chest pains", "thermodynamic theory", "The Lone Ranger", "broken arm", "Arthur Woolf", "the name was derived by association with Hugues Capet, king of France,", "Miller", "Warren Buffett", "The Malkin Athletic Center", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "planktonic", "Lucas Horenbout", "3\u20130 lead", "Spektor", "writ of certiorari", "the right of the dinner plate", "Joanne Wheatley", "flawed democracy", "Southwest Florida International Airport ( RSW )", "In 1987", "the internal auditory canal of the temporal bone", "Missi Hale", "their son Jack ( short for Jack - o - Lantern ) is born on Halloween 2023", "in a Norwegian town circa 1879", "at about 6 : 00 p.m.", "Television demonstrations", "22 days", "1937", "KU", "gastrocnemius", "great king", "The first recorded Christmas celebration was in Rome in 336", "1952", "1 atm pressure", "a major earthquake", "Isaiah Amir Mustafa", "dry lake beds northeast of Los Angeles", "The photoelectric ( optical ) smoke detector", "New York University", "mascot", "the ball is fed into the gap between the two forward packs", "on the microscope's stage", "9 February 2018", "31 October 1972", "Jesse McCartney", "4.25 inches ( 108 mm )", "Lady Gaga", "Director of National Intelligence", "April 1979", "mainland greece", "at least 18 or 21 years old ( or have a legal guardian present )", "the Royal Air Force ( RAF ) defended the United Kingdom ( UK ) against large - scale attacks by Nazi Germany's air force, the Luftwaffe", "deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "two peptide bonds", "5,874 miles (40,030 km)", "Alberich", "Debbie Reynolds", "\"The Brothers\" (2001)", "about 100 light bulbs", "58", "ulna", "2", "Monday night", "fight outside of an Atlanta strip club"], "metric_results": {"EM": 0.4375, "QA-F1": 0.571286542213906}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7368421052631579, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.25, 1.0, 0.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.6976744186046512, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.30769230769230765]}}, "before_error_ids": ["mrqa_squad-validation-3190", "mrqa_squad-validation-845", "mrqa_squad-validation-9399", "mrqa_squad-validation-4636", "mrqa_squad-validation-800", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-4064", "mrqa_naturalquestions-validation-437", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-6481", "mrqa_triviaqa-validation-2131", "mrqa_hotpotqa-validation-1447", "mrqa_newsqa-validation-1789", "mrqa_searchqa-validation-7968", "mrqa_searchqa-validation-10092", "mrqa_newsqa-validation-85"], "SR": 0.4375, "CSR": 0.59375, "EFR": 0.9444444444444444, "Overall": 0.728810763888889}, {"timecode": 16, "before_eval_results": {"predictions": ["2,000", "Michael Mullett", "high pressure shock waves that are generated during impact events", "since 2001", "double or triple non-French linguistic origins", "German", "a course of study, lesson plan, or a practical skill", "Super Bowl XXXIII", "a protest that blocked heavy traffic passing over the A13, Brenner Autobahn, en route to Italy.", "unequal", "in whole by charging their students tuition fees", "every four years", "Gatsby", "the \"Scout Swallows\"", "crawdads", "Lewis", "to the assembled", "The Age of Innocence", "Anderson Cooper", "soda", "polders", "Canada", "an asylum", "a 4.0 GPA", "Judy Garland", "yod", "airplanes", "Detroit", "a bay leaf", "John", "the opera", "a house", "carbon dioxide", "San Francisco", "the day of the Venus landing", "Saturn", "nickel", "a lichen", "Lake Baikal", "Brazil", "a road", "Laos", "Chang Apana", "Jeckle", "Erma Bombeck", "Clinton", "Didi Gregorius", "Herod", "Iran", "a serve", "Touch of Evil", "Billy Idol", "Nightingale", "the Golden Age of Murder", "Daryl Sabara", "holiday", "William Shakespeare", "Nick Berry", "a split 7\"", "around the United States and in Canada", "Turkey", "sumo wrestling", "more than 26,000", "Fat Man"], "metric_results": {"EM": 0.46875, "QA-F1": 0.541183085919928}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5263157894736842, 1.0, 0.4, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4065", "mrqa_squad-validation-1844", "mrqa_squad-validation-4332", "mrqa_squad-validation-6983", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-3450", "mrqa_searchqa-validation-3445", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-16782", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-3214", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-9973", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-15522", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-15665", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-1397", "mrqa_searchqa-validation-519", "mrqa_naturalquestions-validation-94", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-3174", "mrqa_newsqa-validation-1120", "mrqa_hotpotqa-validation-5388"], "SR": 0.46875, "CSR": 0.5863970588235294, "EFR": 1.0, "Overall": 0.738451286764706}, {"timecode": 17, "before_eval_results": {"predictions": ["forceful taking of property", "Robert Iger", "Central business districts", "a block of houses", "prime number theorem", "a nonphotosynthetic eukaryote engulfed a chloroplast-containing alga", "Santa Clara", "Trevathan", "soap opera General Hospital", "being drafted into the Austro-Hungarian Army in Smiljan", "an innate force of impetus", "a statue", "Titanic", "Queen Anne", "Heroes", "Prince of Denmarke", "beer", "Sir Anthony Eden", "Defending Your Life", "a person", "Rocky Down Mexico Way", "Santa Fe", "the Space Coast Convention Center", "Muddy Waters", "the First Message by Morse Code", "a vote to recess", "Manfred von Richthofen", "cowboys", "Michael Collins", "John J. Pershing", "a ballast", "Milwaukee", "the Jesuit", "Javier", "a novel", "an anatomical animation", "the Great U.S. Employment Sag of the 2000s", "Mount Holyoke College", "60 beats per minute", "a quacher", "a coal", "the heart", "a rabbit", "agriculture", "The Call of the Wild", "Vladimir Putin", "Hillary Clinton", "Nikola Tesla", "gingerbread", "Elza", "a treporter", "a light-gathering mirror", "Eli Manning", "statistical modeling and statistical estimation or statistical inference", "The federal government", "the Caribbean", "Doncaster Rovers", "Christies Beach", "an Anglo-Saxon tumulus (or \"barrow\")", "Monday night", "Immigration Minister Eric Besson", "needle - like teeth", "3", "relieve families who had difficulty finding jobs during the Great Depression in the United States"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5400462962962963}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.962962962962963]}}, "before_error_ids": ["mrqa_squad-validation-3106", "mrqa_squad-validation-8638", "mrqa_squad-validation-5938", "mrqa_squad-validation-1232", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-7483", "mrqa_searchqa-validation-10653", "mrqa_searchqa-validation-8777", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-15293", "mrqa_searchqa-validation-10372", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-222", "mrqa_triviaqa-validation-2095", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-3804", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-9856"], "SR": 0.40625, "CSR": 0.5763888888888888, "EFR": 1.0, "Overall": 0.7364496527777777}, {"timecode": 18, "before_eval_results": {"predictions": ["308", "Trajan's Column", "the solution", "the Danube", "conduct inquiries and scrutinise legislation", "Ralph Woodward", "major cities", "19th Century", "high density", "the Ten Commandments", "amyotrophic lateral sclerosis (ALS)", "The Jefferson Memorial", "Magic Johnson", "between 7,500 and 40,000", "The Soloist", "bronze", "Anishinaabeg", "feats of exploration", "Abdul Razzak Yaqoob", "Norwegian", "1868", "James Fitz James, 1st Duke of Berwick", "Hopeless Records", "Lorne Michaels", "Jacques Dominique Wilkins", "Ang Lee", "near North Chicago", "1971", "Jimmy Floyd Hasselbaink", "Arkansas", "(until 2009, the Reading Evening Post)", "The Ministry of Utmost Happiness", "Naruto", "northeastern", "March 30, 2025", "Miller Brewing", "Michael Lewis Greenwell", "Daimler-Benz", "1994", "John of Gaunt", "classical realism", "1,521", "a palace", "James Metcalfe Campbell Bower", "fantasy role-playing", "Leofric", "The Division of Fawkner", "Argentinian", "Westgate Las Vegas Resort & Casino", "American black bear", "Mot\u00f6rhead", "Richa Sharma", "An impresario", "Lake Michigan", "Tulsa", "local people", "Jessica Smith", "Hungary", "45", "Maryland", "OK", "Ulysses S. Grant", "The Case-Book of Sherlock Holmes", "kidnapping"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6533424908424907}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.28571428571428575, 0.33333333333333337, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9479", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-525", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-4297", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-2241", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-4001", "mrqa_newsqa-validation-4077", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-10375"], "SR": 0.546875, "CSR": 0.5748355263157895, "EFR": 0.9655172413793104, "Overall": 0.72924242853902}, {"timecode": 19, "before_eval_results": {"predictions": ["toward the end of his life", "motivated students", "native tribes", "Cuba", "Dai \u00d6n Ulus", "Parliamentary time", "Private Bill Committees", "photolysis of ozone", "Eliot Ness", "Joseph Stalin", "Bobby Eli", "Eurasian Plate", "Anglican", "10.5 %", "living - donor", "New South Wales", "silk floss", "Tom Waits", "Cherbourg in France", "Thomas Andrews", "T.J. Miller", "full '' sexual intercourse", "1963", "Paradise, Nevada", "Mike Nesmith", "Steveston Outdoor pool", "Office of Inspector General", "Skat", "1878", "Jermaine Jackson", "Scott Bakula", "October 2008", "the Two Brothers", "the theory of rasa", "Heather Stebbins", "Spanish surname", "supervillains who pose catastrophic challenges to the world", "1999", "Geraldine Margaret Agnew - Somerville", "Joe Lawrence", "October 2, 2017", "Stephen Curry", "December 15, 2016", "a star", "armored fighting vehicle", "The Stanley Hotel", "Marty Robbins", "Albert Einstein", "early 2017", "Ed Sheeran", "state ownership of the means of production", "HTTP / 1.1", "A substitute", "Midsomer Murders", "Hawaii", "Hopeless Records", "death sentence", "Seasons of My Heart", "helen shapiro", "\"To Build a Fire\"", "a cricothyroidotomy", "MBA", "Vancouver", "Hudson"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5560049019607843}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-6584", "mrqa_hotpotqa-validation-4897", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-566"], "SR": 0.46875, "CSR": 0.56953125, "EFR": 0.9411764705882353, "Overall": 0.7233134191176471}, {"timecode": 20, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-2139", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2467", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10135", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2025", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-3628", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1789", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11788", "mrqa_searchqa-validation-12143", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-16723", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-3450", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6061", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6844", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-6969", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8063", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_squad-validation-10052", "mrqa_squad-validation-10107", "mrqa_squad-validation-10125", "mrqa_squad-validation-10149", "mrqa_squad-validation-10186", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10333", "mrqa_squad-validation-10341", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10445", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10493", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-116", "mrqa_squad-validation-1193", "mrqa_squad-validation-1257", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1527", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1684", "mrqa_squad-validation-1754", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2092", "mrqa_squad-validation-2166", "mrqa_squad-validation-2288", "mrqa_squad-validation-2302", "mrqa_squad-validation-232", "mrqa_squad-validation-2322", "mrqa_squad-validation-2324", "mrqa_squad-validation-2344", "mrqa_squad-validation-2406", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2559", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2737", "mrqa_squad-validation-2778", "mrqa_squad-validation-2827", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-32", "mrqa_squad-validation-3217", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3506", "mrqa_squad-validation-3617", "mrqa_squad-validation-362", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3923", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-401", "mrqa_squad-validation-4086", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4248", "mrqa_squad-validation-4287", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4836", "mrqa_squad-validation-4974", "mrqa_squad-validation-5012", "mrqa_squad-validation-5088", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5379", "mrqa_squad-validation-5451", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5950", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6069", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6250", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6605", "mrqa_squad-validation-6671", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6843", "mrqa_squad-validation-6846", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7476", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-789", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-800", "mrqa_squad-validation-805", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8193", "mrqa_squad-validation-8197", "mrqa_squad-validation-8307", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-845", "mrqa_squad-validation-852", "mrqa_squad-validation-8580", "mrqa_squad-validation-8696", "mrqa_squad-validation-8771", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8798", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8935", "mrqa_squad-validation-8953", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9141", "mrqa_squad-validation-9254", "mrqa_squad-validation-9270", "mrqa_squad-validation-929", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9457", "mrqa_squad-validation-9479", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9630", "mrqa_squad-validation-964", "mrqa_squad-validation-9718", "mrqa_squad-validation-9766", "mrqa_squad-validation-9768", "mrqa_squad-validation-985", "mrqa_squad-validation-9968", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1751", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2106", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.80859375, "KG": 0.4328125, "before_eval_results": {"predictions": ["\" Behind the Sofa\"", "Jean Fran\u00e7ois de Troy, Jean-Baptiste Pater", "the Ministry of Justice", "During the Second World War", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "an assembly center", "Leonardo da Vinci", "end of 1350", "two", "a Taylor series", "June 12, 2017", "Prussian statesman", "Gatwick", "27 November 1956", "Dan Conner", "horror", "Billy Joel", "The Times Higher Education Guide", "The conversation", "Al D'Amato 55% to 44%", "veto power", "Lush Ltd.", "Port Macquarie", "non-alcoholic recipe", "Big & Rich", "Brea, California", "World War II", "Brazilian Jiu-Jitsu", "Biola University", "Revolution Studios", "February 22, 1968", "Erreway", "2015", "Kim Bauer", "the Joint Chiefs of Staff", "The Postal Service", "1955", "1993", "near Philip Billard Municipal Airport", "Sam Rockwell", "John Boyd Dunlop", "Iran", "The Cherokee\u2013American wars", "a creek", "Edward Trowbridge Collins Sr.", "the Province of New York", "Archie Andrews", "January 28, 2016", "a championship and an NBA Finals Most Valuable Player Award", "Steve Martin", "the Israeli Declaration of Independence in 1948", "Sleepy Hollow", "Mumbai, India", "1996", "Ithaca", "The National Council for the Unmarried Mother", "Doubting Castle", "Noida", "genocide, crimes against humanity, and war crimes", "Mrs. Elton", "Howard Hughes Jr.", "the Dairy Queen", "AC/DC", "the Iberian Peninsula"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6755332341269842}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.2222222222222222, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.4, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.8, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5489", "mrqa_squad-validation-8189", "mrqa_squad-validation-5061", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-3393", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-3529", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-5047", "mrqa_hotpotqa-validation-5727", "mrqa_naturalquestions-validation-3404", "mrqa_triviaqa-validation-5418", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-407", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11847", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-12943"], "SR": 0.5625, "CSR": 0.5691964285714286, "EFR": 1.0, "Overall": 0.7023549107142857}, {"timecode": 21, "before_eval_results": {"predictions": ["Several thousand", "Paris", "Islamic Republic", "different subject specialists", "the Council on Advanced Studies in the Social Sciences and Humanities", "inverse proportionality of acceleration to mass", "alternating current", "1002", "Guardians of the Galaxy Vol. 2", "1987", "arts manager", "technical director", "UHF channel 44", "their unusual behavior", "music", "Ireland", "Valley Falls", "Gaelic", "Ry\u016bkyuan", "Dallas", "a leg injury", "Hirsch index rating", "neo-Nazi", "Warner Animation Group", "2008", "Michael Crawford", "Isobel", "Nicholas \" Nick\" Offerman", "American", "the Dominican Republic", "29,000", "Wes Unseld", "Mr. Church", "1990", "Ronald Wilson Reagan", "Raymond Albert Romano", "\"The Dressmaker\"", "Kennedy Road", "Nazareth", "a bat, a black bird, wood pecker, turtle dove, and a blue jay", "\"From Here to Eternity\"", "Kentucky RiverBats", "\"The Royal Family\"", "Backstreet Boys", "Lynn Minmei", "Sir Matthew Arundell", "Derek Jacobi", "February 14, 1859", "Catwoman", "The Hindu Group", "Detroit Lions and the Los Angeles Rams", "Audi", "1886", "Phillip Schofield", "the Chesapeake", "Jupiter", "Seal", "CNN", "Somali", "\"Do You Want Crying\"", "Joe Jackson", "Michael Arrington", "Krishna Rajaram,", "at least 12 months"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6258413461538462}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.15384615384615383, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-7981", "mrqa_squad-validation-10424", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4843", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2473", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-2098", "mrqa_triviaqa-validation-2997", "mrqa_newsqa-validation-3178", "mrqa_searchqa-validation-6538", "mrqa_searchqa-validation-939", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-274"], "SR": 0.53125, "CSR": 0.5674715909090908, "EFR": 1.0, "Overall": 0.7020099431818181}, {"timecode": 22, "before_eval_results": {"predictions": ["1,230 kilometres (764 miles)", "anti-colonial movements", "The Middle and Modern Family", "An increase in imported cars", "immunomodulators", "inequality", "Duval County", "Univision", "Pieter van Musschenbroek", "\"Beauty and the Beast\"", "every aspect of public and private life", "Missouri Tigers", "Acela Express", "German", "Cartoon Network", "Aamir Khan", "trans-Pacific flight", "Kristina Ceyton and Kristian Moliere", "ginger Rogers", "Dan Castellaneta", "from 1995 to 2012", "Saint Petersburg Conservatory", "Harlequin", "one", "Colonel", "Ars Nova Theater", "Donna Paige Helmintoller", "Detroit, Michigan", "CBS News", "1838", "Assistant Director Neil J. Welch", "near Philip Billard Municipal Airport", "ZZ Top", "2", "The Handmaid's Tale", "Rick and Morty", "second largest", "Bamyan Province", "Melbourne", "1590", "247,597", "jurisdiction", "Yoruba people", "Madonna Louise Ciccone", "Telugu", "1800000 sqft", "Malayalam cinema", "New York State Route 907E", "Man Booker Prize for Fiction", "teenage actor or teen actor", "October 21, 2016", "Taeko Ikeda", "Claire Rhiannon Holt", "An acronym", "Thomas Chisholm", "Brazil", "Superman", "Wales", "since 1983", "three", "threatening messages", "China", "a Polaroid picture", "the Cambodian Navy"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7686383928571429}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9302", "mrqa_hotpotqa-validation-3366", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-2446", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-1384", "mrqa_triviaqa-validation-5792", "mrqa_newsqa-validation-377", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-6136"], "SR": 0.6875, "CSR": 0.5726902173913043, "EFR": 1.0, "Overall": 0.7030536684782608}, {"timecode": 23, "before_eval_results": {"predictions": ["28.5\u00b0E", "House of Hohenstaufen", "seven", "BBC Dead Ringers", "Mnemiopsis", "Dr. George E. Mueller", "a salmon", "the white Russian", "warmth for occupants.", "Bob Newhart", "constellations", "opossums", "New Zealand", "Crime and Punishment", "The Plaza Hotel", "Stephen Hawking", "Rodeo", "Hawaii", "A sustained pull applied mechanically especially to the arm, leg, or neck so as to correct fractured or dislocated bones,", "chiselling", "Gerald Geraldai", "The M1 Abrams", "The Moon", "an egg", "Henry II", "Jean Foucault", "\"Saturday Night Live\" skit in which two... Democrats said Schwarzenegger's remarks were insulting to", "Gerald E. Neugebauer", "College of William", "Who's Afraid of Virginia Woolf", "aluminum", "jamais", "Barnard College", "Marcus Aurelius Antoninus Augustus", "Gerald Goodstein", "Aragorn II", "The Delaware Blue Hen", "a stone", "George W. Bush", "a chile", "James Cook", "Bosom Buddies", "Alexander Calder", "Gerald Retriever", "Walter Crawford Kelly, Jr.", "Edith Wharton", "Rapa Nui National Park", "Nike+ iPod", "gravity", "The Earth", "chile", "a ruby", "Bay of Montevideo", "The U.S. state of Georgia", "The first Old World equid fossil was found in the gypsum quarries in Montmartre, Paris", "\"Amor, Roma\", \"race car\", \"stack cats\", \"step on no pets\", \"taco cat\", \"put it up\", \"Was it a car or a cat I saw?\"", "A, O, E, H, J, K, and L", "a chock", "Las Vegas Boulevard", "\"Slaughterhouse-Five\"", "Food and Agriculture Organization", "further reconciliation between religious and ethnic groups.", "a significant contribution to the Iraqi economy.", "2050,"], "metric_results": {"EM": 0.375, "QA-F1": 0.4547619047619047}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1045", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-4758", "mrqa_searchqa-validation-15407", "mrqa_searchqa-validation-12149", "mrqa_searchqa-validation-13231", "mrqa_searchqa-validation-9639", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-14893", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-2225", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-596", "mrqa_searchqa-validation-14432", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-4374", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-7572", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-15964", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7047", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-3087", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-1639"], "SR": 0.375, "CSR": 0.564453125, "EFR": 1.0, "Overall": 0.70140625}, {"timecode": 24, "before_eval_results": {"predictions": ["2 million", "Catholic", "The waxy cuticle of many leaves", "theta intermediary", "long-run economic growth", "the Bush administration's controversial system of military trials.\"", "two-state solution", "WFTV", "\"an Afghan patriot\"", "Juliet", "Polka Dot", "eight", "Haiti", "Lebanese", "Indian Ocean", "at least 25 dead", "18", "Libyan", "a rally.\"", "Herman Cain", "humans", "Friday,", "Saluhallen", "murder", "Swat Valley", "two years,", "228", "Keating Holland", "no special menus,\"", "Steve Williams", "computer-generated imagery", "Nazi Germany", "\"I loved the convenience [of the train]", "Most Iranians weren't even born then.", "five", "July", "Oprah: A Biography", "100 percent", "white", "Abu Sayyaf", "The Palm", "There's no chance of it being open on time.\"", "Egypt", "Jeddah, Saudi Arabia", "poor families", "Russia", "2011", "use of torture and indefinite detention", "a skull", "Iran", "a review of state government practices completed in 100 days.\"", "stripper", "Luigi Fagioli", "The chief executive of West Virginia", "Massachusetts", "phi", "the natural world and mysticism", "The Lion King", "7pm", "1966", "Wendell Erdman Berry", "Twelfth Night", "Shabbat", "yellow"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5623579545454546}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.8, 1.0, 0.7272727272727273, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.3636363636363636, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666665, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6435", "mrqa_squad-validation-8681", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2201", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1419", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-4207", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-173", "mrqa_searchqa-validation-2076"], "SR": 0.453125, "CSR": 0.56, "EFR": 1.0, "Overall": 0.700515625}, {"timecode": 25, "before_eval_results": {"predictions": ["primes", "UNESCO's World Heritage list", "lower incomes", "the Wankel engine", "five or more seats", "the Bronx", "in northwest Pakistan", "backbreaking labor", "near Grand Ronde, Oregon", "Democrats and Republicans are saying Meehan shouldn't be using a 9/11 image to make a political point.", "Fullerton, California,", "unprecedented wave of buying amid the elections.", "The minister later apologized, telling CNN his comments had been taken out of context.", "Briton Carl Froch", "10 below in Chicago,", "\"have no problems about the school, they are happy about everything.\"", "The oceans are growing crowded,", "opening the door", "The pilot, whose name has not yet been released,", "The children were Sudanese orphans that it was trying to rescue from a war-torn nation.", "the Iraqi and U.S. soldiers were attacked by small-arms, machine-gun and RPG fire", "air support", "Marie-Therese Walter", "neither Sudanese nor orphans,", "\"it should stay that way.\"", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "Swansea Crown Court,", "The Uighurs", "Michael Jackson", "Kurdish Workers' Party,", "insurgent small arms fire,", "quality of teaching and learning in American schools", "a lump in Henry's nether regions was a cancerous tumor.", "on the 12th on the Blue Monster course at Doral", "glamour and hedonism", "\"I am sick of life\"", "peace sign.", "the Southeast", "Diego Milito", "a grizzly bear", "The Human Rights Watch", "Florida", "Christopher Savoie", "rebels", "three Ghanaians, two Liberians and a Togo national", "How I Met Your Mother", "the British capital's other two airports, Stansted and Gatwick,", "Mafia", "Stratfor", "The BBC", "Blagojevich", "Ashley \"A.J.\" Jewell,", "The stratum lucidum", "Gettysburg College", "the English", "Celsius", "1", "the supreme religious leader of the Israelites", "Charles Quinton Murphy", "Rio Gavin Ferdinand", "more than 110", "Athens", "Craig Safire", "artesian"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5965266325790223}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.23529411764705882, 0.8571428571428571, 0.0, 1.0, 0.0, 0.2666666666666667, 0.0, 0.0, 0.0, 0.6666666666666666, 0.14285714285714285, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.15384615384615385, 0.4, 0.0, 0.375, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 0.5, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-561", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-1390", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-4004", "mrqa_searchqa-validation-1916", "mrqa_searchqa-validation-16961"], "SR": 0.4375, "CSR": 0.5552884615384616, "EFR": 1.0, "Overall": 0.6995733173076923}, {"timecode": 26, "before_eval_results": {"predictions": ["enhanced transit infrastructure, possible shuttles open to the public, and park space which will also be publicly accessible.", "Katy\u0144 Museum", "two", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship.", "David Anthony O'Leary", "1822", "Janet Evanovich", "Dominican", "Evgeni Platov", "Knoxville, Tennessee", "Enkare Nairobi", "Field Marshal Stapleton Cotton", "Shari Shattuck", "Si Da Ming Bu", "\"The Blue Album\"", "Apsley George Benet Cherry-Garrard", "a basilica", "\"the ultimate guide to music and more\"", "Odawa", "Columbus Crew SC", "Yoo Seung-ho", "1989 until 1994", "Phil Spector", "Wildhorn, Bricusse and Cuden", "actress", "Montreal", "Sam the Sham", "John Nicholas Galleher", "San Francisco 49ers", "Prince of Cambodia Norodom Sihanouk", "Durham, North Carolina", "Double Crossed", "19th-century", "Donald McNichol Sutherland", "a Computer reservations system", "Security Management", "14,372", "musician", "Cersei", "Fort Worth", "Dutch", "shorthand writing", "North Atlantic Treaty Organisation (NATO)", "\"The Comic Strip Presents...\"", "Tabasco", "Sunday, November 2, 2003", "1812", "1.5 million", "Neymar", "Plymouth Regional High School", "\"Tainted Love\"", "Dissection", "Cecil Lockhart", "MercyMe", "Five years later", "a ride cymbal", "Al Jazeera", "Anabaptists", "38 feet", "identity documents", "Shenzhen", "Brigham Young", "Silly Putty", "aaron"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6437596006144393}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8387096774193548, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.8, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6426", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3749", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-111", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-3889", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-2131", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-5740", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-3954", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2495", "mrqa_searchqa-validation-10970"], "SR": 0.515625, "CSR": 0.5538194444444444, "EFR": 1.0, "Overall": 0.6992795138888889}, {"timecode": 27, "before_eval_results": {"predictions": ["In low-light conditions", "1550", "41 sentences drawn from his writings, including the 95 Theses, within 60 days", "$60,000 in cash and stock", "four", "Steve Goodman", "Jonathan Goldstein", "Cristeta Comerford", "Kirstjen Nielsen", "1997", "Tristan Rogers", "Mason Alan Dinehart", "counter clockwise direction", "Paul Hogan", "A seed is a competitor or team in a sport or other tournament who is given a preliminary ranking for the purposes of the draw", "Matt Monro", "IIII", "Fa Ze Rug", "8ft", "Robert Irsay", "to manage the characteristics of the beer's head", "a moral tale", "2019", "Scott Drever", "Southern Ocean", "The Exploratorium continues to hold Pi Day celebrations", "it is an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner", "1", "Her cameo was filmed on the set of the Sex and The City", "December 1, 2017", "James Watson and Francis Crick", "Sara Gilbert", "London", "the courts", "John Travolta", "Djokovic", "Lady Olenna Tyrell", "Ant & Dec", "Nepal", "Joanne Wheatley", "UNESCO / ILO Recommendation concerning the Status of Teachers", "September 24, 2012", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F)", "summer", "between the Eastern Ghats and the Bay of Bengal", "Daniel Suarez", "1 - 2 spinal nerve segments above the point of entry", "16 August 1975", "it is an expression of at least a moderate amount of manual dexterity", "Kyla Pratt", "Lori McKenna", "Beorn", "Douglas MacArthur", "The Daily Mail's", "Scotland", "Forbes", "2006", "Fife", "April 28", "Russia and the United States", "one American diplomat to a \"prostitute\"", "unassisted triple play", "Ben Kingsley", "Conrad Hilton"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6742918941170198}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.2857142857142857, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.15384615384615385, 1.0, 0.0, 0.4, 0.0, 1.0, 0.7692307692307692, 1.0, 1.0, 0.0, 0.125, 0.0, 0.1111111111111111, 1.0, 0.10526315789473685, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.0, 0.9, 1.0, 0.11764705882352942, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.4, 0.0, 0.4, 0.9090909090909091, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2246", "mrqa_squad-validation-1313", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7208", "mrqa_triviaqa-validation-6967", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-506", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-2350", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-11955"], "SR": 0.5625, "CSR": 0.5541294642857143, "EFR": 0.9642857142857143, "Overall": 0.6921986607142857}, {"timecode": 28, "before_eval_results": {"predictions": ["Elisabeth Sladen", "1206", "Genghis Khan", "on Monday to meet with the Shanghai mayor and hold a town hall-style meeting with \"future Chinese leaders\"", "24", "Monday night where it left off in September", "a delegation of American Muslim and Christian leaders", "September 21.", "1,500", "the former Massachusetts governor in an ad Sunday in Iowa's The Des Moines Register newspaper.", "269,000", "on Saturday as Marc Lievremont's inconsistent side bounces back from two defeats to eliminate England and reach the World Cup semifinals.", "The Ski Train", "a sailboat matching the description of the missing 38-foot boat was found overturned about 5:15 p.m. Saturday,", "A large concrete block is next to his shoulder, with shattered pieces of it around him.", "for an arms embargo on Israel, currently fighting militant Palestinian group Hamas,", "dogs who walk on ice in Alaska.", "his father's parenting skills.", "an investigation and take steps to help the Awa", "Haiti's equivalent of Mardi Gras,", "President Obama", "militant group issued a thinly threat Monday against an upcoming world football tournament that is tentatively scheduled to take place in the west African nation later this year.", "9,500 energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals. The LEDs will use the same amount of electricity as about 10 toasters,", "it is currently home to 15 African and Asian elephants.", "to secure more funds", "short- and medium-range missile tests,", "Karl Eikenberry", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "A member of the group dubbed the \"Jena 6\"", "state", "Anil Kapoor", "Illlinois.", "greeted with general astonishment in Seoul,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Nazi Germany", "has been widely praised and adopted as a model around the world.", "gun", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "don't believe the U.S. assertion that the system is needed to guard against imminent threats from Iran or North Korea.", "David McKenzie", "citizenship because he was depriving his wife of the liberty to come and go with her face uncovered,", "heavy brush", "through a facility in Salt Lake City, Utah,", "that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis,", "remains unknown,", "I wondered what will they do now?\"", "Alwin Landry's supply vessel Damon Bankston", "along the equator between South America and Africa.", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case is recovering well and wants to see the ocean and a pop concert,", "an empty tub, his face blue and purple and a chain around his neck,", "in a tenement in the Mumbai suburb of Chembur,", "in California, Texas and Florida, with the rest scattered through the South, Midwest and West.", "Rigg", "the efferent nerves that directly innervate muscles", "from the port of Nueva Espa\u00f1a to the Spanish coast", "The Duchess of Devonshire", "Bahrain", "Tokyo International Airport", "Danny Glover", "William Shakespeare", "British", "an assault rifle", "can you make the most of the opportunity to choose elective classes in areas of interest", "Turtle Wax"], "metric_results": {"EM": 0.3125, "QA-F1": 0.47177164389940124}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.25, 0.6666666666666666, 0.2, 0.0, 1.0, 0.0, 0.23076923076923078, 0.0, 0.6666666666666666, 0.25, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4615384615384615, 1.0, 0.8333333333333333, 1.0, 0.0909090909090909, 0.08333333333333333, 1.0, 0.1111111111111111, 0.0, 1.0, 0.9333333333333333, 1.0, 0.0, 0.5, 0.8750000000000001, 0.0, 0.0, 0.923076923076923, 0.13333333333333336, 1.0, 0.5, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-301", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-2338", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-7484", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-1993", "mrqa_searchqa-validation-9212", "mrqa_searchqa-validation-6288"], "SR": 0.3125, "CSR": 0.5457974137931034, "EFR": 1.0, "Overall": 0.6976751077586206}, {"timecode": 29, "before_eval_results": {"predictions": ["Disney\u2013ABC Domestic Television", "2011", "Soviet", "naltrexone", "sense of smell", "(Thelma) Ferris", "Florence", "Wrigley's", "Oprah Winfrey", "2004", "a Great Dane", "Director General of the Security Service", "Caracas", "Rock Follies of \u201977", "the show Pickwick", "Nobel Prize in Literature", "Celtic", "Northwestern University", "the best value diamond for your money", "The Star Spangled Banner", "(a)(adj)", "Ibrox Stadium", "New York City", "Stirling Moss", "Michael Caine", "Llyn Padarn", "Little Dorrit", "a Roman historian", "apples", "Chekhov's", "Chris Evans", "Robert Burns", "Declaration of Independence", "lome", "a condor", "Belgium", "Pilgrim's Progress", "Plato", "Fulham Football Club", "a type of lead and consequently was called black lead or plumbago.", "Australia", "Alaska", "(Am I My Brother's Keeper)", "Sanl\u00facar de Barrameda", "Michelin Man", "Tesco", "Z", "a bear suit", "Clio Awards", "Pygmalion", "Watford Football Club", "Trainspotting", "English", "eliminate or reduce the trade barriers among all countries in the Americas", "Vicente Fox", "Central-Eastern Europe", "1919", "August 24, 1983", "ultimately ensuring that all prescription drugs on the market are FDA approved,", "International Polo Club", "more than 200.", "Sappho", "the White House", "Steely Dan"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6017193969979295}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 0.6666666666666666, 0.4, 1.0, 0.4, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.4, 0.8695652173913044, 1.0, 0.0, 1.0, 0.5, 0.07142857142857142, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7651", "mrqa_triviaqa-validation-2203", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-4186", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-566", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-5646", "mrqa_triviaqa-validation-4615", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-1069", "mrqa_triviaqa-validation-874", "mrqa_triviaqa-validation-6429", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-855", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-3697", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-4547", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-4122"], "SR": 0.484375, "CSR": 0.54375, "EFR": 1.0, "Overall": 0.697265625}, {"timecode": 30, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2030", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-3249", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3655", "mrqa_hotpotqa-validation-3701", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-5047", "mrqa_hotpotqa-validation-505", "mrqa_hotpotqa-validation-5213", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5645", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-988", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4308", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-994", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2553", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-919", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-14268", "mrqa_searchqa-validation-14735", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-1664", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_squad-validation-10052", "mrqa_squad-validation-10125", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10308", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-1159", "mrqa_squad-validation-1193", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-1368", "mrqa_squad-validation-1503", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2166", "mrqa_squad-validation-2324", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2778", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-3259", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3831", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3916", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-4065", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4191", "mrqa_squad-validation-4248", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4746", "mrqa_squad-validation-4836", "mrqa_squad-validation-5009", "mrqa_squad-validation-5088", "mrqa_squad-validation-5108", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5180", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5521", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5964", "mrqa_squad-validation-6001", "mrqa_squad-validation-6069", "mrqa_squad-validation-6082", "mrqa_squad-validation-6158", "mrqa_squad-validation-6256", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6592", "mrqa_squad-validation-6605", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-709", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7476", "mrqa_squad-validation-7485", "mrqa_squad-validation-7502", "mrqa_squad-validation-7578", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-8159", "mrqa_squad-validation-8213", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8580", "mrqa_squad-validation-8681", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8935", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9141", "mrqa_squad-validation-9270", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9510", "mrqa_squad-validation-9569", "mrqa_squad-validation-964", "mrqa_squad-validation-9759", "mrqa_squad-validation-9766", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1329", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-576", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-855", "mrqa_triviaqa-validation-874", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-996"], "OKR": 0.828125, "KG": 0.42890625, "before_eval_results": {"predictions": ["1947", "Madame de Pompadour", "$125 per month", "helen shapiper", "Newbury", "doubtington", "brass instruments", "Genesis", "Lisieux", "Astor", "diolchwch", "Canned Heat", "John Huston", "Daniel Casey", "Christopher Lee", "Matthew", "CSIRO", "Patrick Kielty", "Norfolk Island", "d'Ivoire", "chicoas\u00e9n", "Worcester Cathedral", "Bonnie and Clyde", "Albert Finney", "Carrefour", "ken Russell", "in the garden of Gethsemane", "John Galliano", "jaguar", "Bartlett Sher", "mickey Mouse", "sun protection factor", "Plato", "Bugsy Malone", "1812", "jere", "basil", "mEXICO", "cfs", "lyonesse", "raclette", "the AllStars", "pennsylvania", "Copenhagen", "nihon", "gymnastics", "George Walker Bush", "Nicky Henderson", "poker pro James \u201cFlushy\u201d Dempsey", "Zachary Taylor", "old Ironsides", "to make wrinkles in the face", "Total Drama World Tour", "General George Washington", "Real Madrid", "two", "Balvenie Castle", "John Kavanagh", "threatening messages", "London", "fight outside of an Atlanta strip club", "a lemur", "Communists", "misfortune"], "metric_results": {"EM": 0.546875, "QA-F1": 0.596474358974359}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.30769230769230765, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5491", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2299", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-715", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4169", "mrqa_triviaqa-validation-4499", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3285", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-1468", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-3517", "mrqa_hotpotqa-validation-1351", "mrqa_hotpotqa-validation-3032", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-85", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-3674"], "SR": 0.546875, "CSR": 0.5438508064516129, "EFR": 1.0, "Overall": 0.7011920362903226}, {"timecode": 31, "before_eval_results": {"predictions": ["nine", "Lower Rhine", "Pet Shop Boys", "rugby", "geologist", "finland", "Mikhail Baryshakov", "a doodle", "finford wford", "joan crawford", "joan Paul Gaultier", "javelin", "silvery blue", "cymru", "French", "Vatican City", "buttock", "david", "mexico", "Alan B'Stard", "david david shannon", "Pablo Picasso", "apples", "440 hertz", "Spanish", "finland", "a karst", "mexico", "mexico", "united states", "sense of taste", "finland", "Boojum", "woran", "finland", "gethundo", "finland finford", "woran", "finland", "ailing fish", "finado Tuerto, Argentina", "Charlie Chaplin", "The Perfect Storm by Sebastian Junger", "middies", "joan willow", "the Queen of England", "Newbury", "ishmael", "vice-admiral", "1936", "gethse", "joan crawford", "Rachel Kelly Tucker", "the BBC", "pilgrimages to Jerusalem", "co-op of grape growers", "tim Allen", "Anatoly Lunacharsky", "an empty water bottle down the touchline following a disallowed goal for Arsenal.", "Japanese officials", "The Rev. Alberto Cutie", "customs Union", "glenn crawford", "Saudi Arabia"], "metric_results": {"EM": 0.25, "QA-F1": 0.3552026098901099}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.5714285714285715, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-4054", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-4765", "mrqa_triviaqa-validation-5237", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-1633", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-1261", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-5259", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-7645", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-5549", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-6730", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5728", "mrqa_newsqa-validation-317", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-12801"], "SR": 0.25, "CSR": 0.53466796875, "EFR": 0.9583333333333334, "Overall": 0.6910221354166667}, {"timecode": 32, "before_eval_results": {"predictions": ["TeacherspayTeachers.com", "provisional elder", "yarn", "Prince Harry", "Mujib", "b\u00e9la Bart\u00f3k", "roosevelt", "a person trained for travelling in space", "Salt Lake City", "four feet", "teacher", "brazil", "Canada", "Peter Nichols", "American Family Publishers", "seven", "Microsoft", "brigit Forsyth", "Celsius", "the Poincar\u00e9 conjecture", "delimos", "leicestershire", "Boris Johnson", "HMS Conqueror", "Tamar", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "golf ball", "the inner ear", "Gloucestershire", "sweden", "swimming", "george cukor", "Ishmael", "blue", "russia", "Prokofiev", "Cyclone", "Tom Hanks", "heavy breeds", "hooliganism", "Thank you", "magnetism", "second year", "william Neil Connor", "russia", "charles chaplin", "horse-racing", "\"Slow\"", "spain", "dragon", "peregrines", "2008", "Siddharth Arora / Vibhav Roy", "Tim Rice", "five", "\"The Walking Dead\"", "1979", "1999", "capital murder and three counts of attempted murder", "Apple employees", "mental health", "marston moor", "it's a month after her last race and a few even months for hormonal and mood changes", "Athol Fugard"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6067708333333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.5, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10088", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-2608", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-1113", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-1838", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-7671", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-2258", "mrqa_triviaqa-validation-6678", "mrqa_triviaqa-validation-6230", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-4171", "mrqa_triviaqa-validation-1919", "mrqa_triviaqa-validation-6726", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-5666", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-6815", "mrqa_naturalquestions-validation-10161", "mrqa_newsqa-validation-1889", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-6553"], "SR": 0.546875, "CSR": 0.5350378787878788, "EFR": 1.0, "Overall": 0.6994294507575758}, {"timecode": 33, "before_eval_results": {"predictions": ["1,100", "since 2001", "horseshoe", "rogers", "table tennis", "Tintin", "Bart\u00f3k", "6-4,", "Harold Shipman", "Undertones", "michael hordern", "rogers", "yeast", "michael maxing", "permian", "Thank you", "nipples", "queen Mary", "lola", "muscle tissue", "Surrealism", "shinto", "sewing machines", "Morgan Spurlock", "john Buchan", "seals", "The Scots", "new governor", "shorthand", "the Altamont Speedway", "fourteen", "jack Sprat", "Operation Dynamo", "the Allstars", "john", "praseodymium", "tla\u010denica or \u0161vargla", "norway", "an antipasto, sometimes herbs, and may at times be topped with onion, cheese and meat.", "chairman of President George W. Bush's Council of Economic Advisers", "Wicked Witch", "rogers", "the Observer", "newton", "Turnbull & Asser", "bullfighting", "Arthur C. Clarke", "john rogers", "entropy", "Chad", "books", "Taggart", "on the microscope's stage", "norway", "Norway", "Virgin", "Philadelphia Naval Shipyard", "West African descendants", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "cross-country skiers", "was \"a smoking gun of confirmation of Brazil's effort to engage in operations to overthrow the government of Chile and a discussion of collusion with the United States.\"", "an aerial", "Hill Street Blues", "white"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5960679945054945}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5384615384615384, 1.0, 0.2051282051282051, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-7142", "mrqa_triviaqa-validation-368", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-7043", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-3060", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-2220", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1538", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-11071"], "SR": 0.53125, "CSR": 0.5349264705882353, "EFR": 0.9, "Overall": 0.6794071691176471}, {"timecode": 34, "before_eval_results": {"predictions": ["second Gleichschaltung", "two of Tesla's uncles", "Renault", "kinks", "Massachusetts", "lyonesse", "6", "paul david hewson", "ink", "Toy Story 2", "the wren", "The Lion", "intestines", "Independence Day", "Charlie Brooker", "Oxford", "number 13", "argentina", "Stephen Hawking", "Florentino", "Wat Tyler", "Tony Meo", "black Wednesday", "vomiting or excessive sedation", "Ennio Morricone", "NBA", "author", "Benjamin Franklin", "horseradish", "richmond", "1066", "carmen carmen", "$1", "Glasgow", "Sigourney Weaver", "port", "checkers", "Norman Mailer", "action Soldier", "jura", "edward carmenc", "chatsworth house", "pongo", "quant", "Scooby-Doo", "Pennine Way", "carmen sherbert", "Lorne Greene", "glenn greens dev devry", "frans hals", "charles", "margarita carmencansino", "the University of Oxford", "Matt Monro", "William Chatterton Dix", "Belarus", "Big Fucking German", "Welterweight", "Harrison Ford", "The EU naval force", "Miami Beach, Florida,", "ice age", "the 39 Steps", "rambert"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6013174019607843}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-3695", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-1015", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-6587", "mrqa_triviaqa-validation-5744", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-4239", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6896", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-4210", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-1891", "mrqa_searchqa-validation-2191"], "SR": 0.5625, "CSR": 0.5357142857142857, "EFR": 0.9285714285714286, "Overall": 0.6852790178571428}, {"timecode": 35, "before_eval_results": {"predictions": ["Phil Simms", "2013", "Russ Conway", "green", "Amsterdam Stock Exchange", "in York", "london", "The one and only Wonderbra", "john tommie Connor", "jons Jacob Berzelius", "Saint Aidan", "a hundred and one hundred men", "soybeans", "Annie Leibovitz", "Pinot Noir", "Seal", "london", "king arthur VI", "dennis whittington", "peacock", "pisces", "Ishmael", "smell", "Brad Pitt", "Eleanor Rigby", "The Simpsons", "One Direction", "yellows", "Cornell University", "nipper", "vinegar joe", "london", "Follicle-stimulating hormone", "paramita", "dennis taylor", "Normandie", "Thursday", "london", "racecar", "Costa Concordia", "london", "Gary Gibbon", "london", "a fly", "horseshoes", "king andrew", "george", "the Oil Capital of Europe", "orange", "a crow", "Yassir Arafat", "Black Sea", "Jennifer O'Neill", "49 cents", "1", "Sim\u00f3n Bol\u00edvar", "Wings of Desire", "actor and former fashion model", "the Airbus A330-200", "The account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "they can demonstrate they have been satisfactorily treated for at least 12 months.", "Roland Garros", "rhinoplasty", "global village"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5182291666666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-168", "mrqa_triviaqa-validation-1757", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-6664", "mrqa_triviaqa-validation-3593", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-5010", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-121", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-5128", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-276", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11091"], "SR": 0.4375, "CSR": 0.5329861111111112, "EFR": 1.0, "Overall": 0.6990190972222222}, {"timecode": 36, "before_eval_results": {"predictions": ["independent schools", "cymbals", "Djibouti and Yemen", "the Nile", "Barcelona", "Meg Ryan", "Caernarfon", "norman nodel", "Mt. McKinley", "New Hampshire", "rodents", "crow", "The Flintstones", "fire", "gelatine", "Ecuador", "Cyprus", "coffin-maker", "homelessness", "Dublin", "Sin City", "walker", "rent doesn't include additional costs such as insurance or business rates", "Google", "lulu", "Pembrokeshire Coast National Park", "Tripoli's", "Jack Johnson", "Dreamgirls", "Opus Dei", "Belize", "Civil Law", "The Mary Whitehouse Experience", "a Liberator", "Farlake", "a horse", "Dubai", "Sydney", "orange", "Lehman Bros International", "no relationship", "Ordovices", "Robert Devereux,", "dombey and Son", "mexico", "pascal", "jean galsworthy", "davy", "george", "Amsterdam", "davy", "24", "Ford", "between 27 July and 7 August 2021", "94", "Dan Castellaneta", "La Liga", "environmental videos", "five minutes before commandos descended from ropes that dangled from helicopters,", "St. Louis, Missouri,", "cheese", "jzebel", "goodson", "city"], "metric_results": {"EM": 0.5, "QA-F1": 0.5561197916666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.625, 0.8, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-5503", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-7626", "mrqa_triviaqa-validation-6236", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7186", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-7594", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-562", "mrqa_naturalquestions-validation-5647", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-621", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-12212"], "SR": 0.5, "CSR": 0.5320945945945945, "EFR": 1.0, "Overall": 0.698840793918919}, {"timecode": 37, "before_eval_results": {"predictions": ["uncivilized", "del niro", "8", "M\u00e1laga Airport", "rabbit", "Tiananmen Square", "noises off", "juliannon dowes", "Till Death Us Do Part", "javier Bardem", "1720", "australia", "indysmenorrhea", "peace palace", "red", "cuble line", "julay Vtoroy", "Sally Ride", "g\u00e9rard Depardieu", "Jimmy Carter", "glade", "herpes zoster virus", "small faces", "zoom lens", "Angela Merkel", "Lynde", "chronic effects", "henry asquith", "spectator", "2", "narnia", "Zipporah", "Vancouver Island", "purdy", "juliet", "six", "sash", "dior", "Stockholm syndrome", "st helens", "beta", "henry rogers", "Basil Fawlty Towers", "blue", "frederick west", "Boreas", "World Heavyweight title", "stringed instruments", "g Gaga", "chardonnay", "colleen McCullough", "770,000 and 1.7 million nerve fibers", "2026", "Kevin Zegers", "50th anniversary of the founding of the National Basketball Association", "Cherokee National Holiday", "empire of Rome", "1927", "indabella", "the man facing up, with his arms out to the side.", "muskrat", "Versace", "trisha yearwood", "burt Bacharach"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4845052083333333}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.375, 0.4, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-7331", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-5452", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-5616", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-5223", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-856", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-3899", "mrqa_triviaqa-validation-6605", "mrqa_triviaqa-validation-6813", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-6638", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-1958", "mrqa_newsqa-validation-1855", "mrqa_searchqa-validation-12111", "mrqa_naturalquestions-validation-6125"], "SR": 0.40625, "CSR": 0.528782894736842, "EFR": 0.9473684210526315, "Overall": 0.6876521381578946}, {"timecode": 38, "before_eval_results": {"predictions": ["71", "(Cecil) Rhodes", "constant", "religion", "a triangle", "jedoublen/jeopardy", "when they were my brothers", "Anne", "root beer", "cheerios", "Venus", "duc de Berry", "shirley", "secretary of state", "density", "Berlusconi", "twelve", "Barack Obama", "stockholm", "toaster", "empanada", "Macau", "60 miles per hour", "jaffa", "Dan Marino", "constantine", "viola", "duchy of Luxembourg", "constantinephilia", "suntory", "9", "hugh hefner", "20th", "german wounds", "bix", "yellow", "ian Dyas", "obelisk", "gothic", "concave", "Caspian Sea", "gretchen wilson", "Scooby-Doo", "Dean Cain", "hypnotic", "South Dakota", "constantine germanbin", "50", "when he made his name", "war of the Worlds", "Peter Pan", "subduction zone", "Erica Rivera", "Julia Roberts", "wuthering Heights", "Craggy Island", "Ionian Seas", "argentine", "Belgian", "Esteban Ocon", "Roberto Micheletti,", "green grump", "116", "Oahu"], "metric_results": {"EM": 0.4375, "QA-F1": 0.47135416666666663}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-3544", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-8074", "mrqa_searchqa-validation-15961", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-13762", "mrqa_searchqa-validation-16868", "mrqa_searchqa-validation-8389", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-7647", "mrqa_searchqa-validation-10062", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-844", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-11967", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-5544", "mrqa_searchqa-validation-9662", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-11364", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-3914", "mrqa_hotpotqa-validation-4023", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-857", "mrqa_hotpotqa-validation-4625"], "SR": 0.4375, "CSR": 0.5264423076923077, "EFR": 1.0, "Overall": 0.6977103365384615}, {"timecode": 39, "before_eval_results": {"predictions": ["inequality in wealth and income", "Princess Aisha bint Hussein", "Baugur Group", "Westchester County", "City Mazda Stadium", "841", "American", "Lonestar", "Kaep", "The WB supernatural drama series \"Charmed\"", "Southern Rhodesia", "Alonso L\u00f3pez", "made into a TV series for the BBC channel CBeebies", "Voni Morrison", "Galleria Vittorio Emanuele II", "October Sky", "Shelley Regner", "neuro-orthopaedic", "the City of Westminster", "British", "Albert", "6,241", "Dan Bilzerian", "the Space Shuttle \"Discovery\"", "an American business magnate, investor, and philanthropist.", "Crackle", "Kristy Lee Cook", "Perth", "Love Streams", "1935", "5.3 million", "New York City", "Dara Torres", "The dyers of Lincoln", "actress", "a fictional world", "May 5, 2015", "Red and Assiniboine Rivers", "methylenedioxy meth", "Neymar", "The Jefferson Memorial", "Strange Interlude", "Bothtec", "2,099", "a body of water", "35,402", "2004 Paris Motor Show", "1996", "33", "1999", "Guns N' Roses", "5", "John McConnell", "George Harrison", "Bruno Mars", "Wyre", "The Italy national football team", "Heshmatollah Attarzadeh", "a one-shot victory in the Bob Hope Classic", "700", "The Untouchables", "Monopoly", "a cookie jar", "June 2002."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6796130952380952}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.28571428571428575, 0.16666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.8, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-3495", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2707", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-5074", "mrqa_triviaqa-validation-1097", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1230"], "SR": 0.578125, "CSR": 0.527734375, "EFR": 1.0, "Overall": 0.69796875}, {"timecode": 40, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1616", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3708", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-612", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12704", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15107", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-2178", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7865", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9870", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10149", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1844", "mrqa_squad-validation-1967", "mrqa_squad-validation-2049", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3428", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3815", "mrqa_squad-validation-3836", "mrqa_squad-validation-3837", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4135", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-503", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5338", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-5859", "mrqa_squad-validation-5893", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6435", "mrqa_squad-validation-6506", "mrqa_squad-validation-6671", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7002", "mrqa_squad-validation-7193", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7704", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8084", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8935", "mrqa_squad-validation-902", "mrqa_squad-validation-9254", "mrqa_squad-validation-9300", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9479", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3640", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3874", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4239", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4527", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6152", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-657", "mrqa_triviaqa-validation-6762", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7400", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-996"], "OKR": 0.80078125, "KG": 0.45546875, "before_eval_results": {"predictions": ["2006", "Ghana's Asamoah Gyan", "Dublin", "Ribhu Dasgupta", "George Gordon Byron, 6th Baron Byron,", "\"The Deer Hunter\"", "Irish", "sulfur mustard H or HD blister gas", "12\u201318", "Mike Biden", "Jeffrey William Van Gundy", "People!", "Ballarat Bitter", "the Bank of China Building", "1919", "26,000", "TransAd Adelaide", "Gillian Leigh Anderson", "Steven John Carell", "December 16, 1959", "Lauren Alaina", "the National Football League (NFL)", "1943", "Fountains of Wayne", "Saint Paul, Minnesota", "(16 March 1885, Toscolano \u2014 20 January 1943, Barbarano-Sal\u00f2)", "the southern portion of Carroll County, the eastern portion of Grafton County, and the northern portions of Strafford County and Merrimack County", "on October 20, 2017", "Zimbabwe", "Larry Eustachy", "Barbara Niven", "Friedrich Nietzsche", "Rabat", "Straits of Gibraltar", "American burlesque", "Russell T Davies", "Jay Schottenstein", "600", "April 30, 1982", "Martin \"Marty\" McCann", "Nine Inch Nails", "Labour Party", "Orlando\u2013Kissimmee\u2013Sanford, Florida Metropolitan Statistical Area", "Prussia", "Boston", "Bambi, a Life in the Woods", "from 1993 to 1996", "Watertown, New York", "green and yellow", "ice hockey", "in the 1995 teen drama \"Kids\"", "3 September", "the Gaget, Gauthier & Co. workshop", "200 to 500 mg up to 7 mg", "Paul Gauguin", "Ynys M\u00f4n", "Saturday Night Live", "an antihistamine", "Chrysler", "outside his house in Najaf's Adala neighborhood", "a panic's in thy breastie", "The Backstreet Boys", "inertia", "hemoglobin"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5520213293650793}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 0.5, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.8571428571428571, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-5509", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-507", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-883", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2527", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-325", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-4639", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-2482", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-8555", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-5417", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-1606", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-10053"], "SR": 0.40625, "CSR": 0.5247713414634146, "EFR": 1.0, "Overall": 0.698782393292683}, {"timecode": 41, "before_eval_results": {"predictions": ["his sons and grandsons", "Rabband", "a huge mass of something", "a fisheye lens", "berenstain Bears", "Parris Island", "bagels", "Eddie Fisher", "Rabbit Ringwald", "Mick Jagger", "White blood cells", "Al Capone", "Matsu", "Stardust", "a Book of Operas", "Who's Afraid of the Big Bad Wolf", "Rabb the Eskimo", "Medical Malpractice", "Rabbly bear", "Kareem Abdul-Jabbar", "The Police", "Bravo", "Henry Clay Frick", "Mikhail Gorbachev", "a telephone call", "Mayor Antonio Villaraigosa", "Jeopardy", "the West Gate", "Gregory Maguire", "a Pin", "Franklin D. Roosevelt", "Pisces", "the Golden Fleece", "Alzheimer's disease", "Chuck Yeager", "3", "\"PANT\"s", "ajax", "Vermont", "vilebrequin", "a rocket launcher", "zenith", "Whig", "Vietnam", "Ectoplasm", "Alberto Fujimori", "Old North Church", "binocular", "I Can't Help Myself", "Legally Blonde", "Scorpio", "Abbasid Caliphate", "Kaley Christine Cuoco", "the synthesis of adenosine triphosphate ( ATP )", "Salix", "Pebble", "the Bible", "Crown Holdings Incorporated", "New Orleans Saints", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "would allow employees to signal support for unionizing by openly signing a card demanding it.", "Aldgate East", "because the Indians don't want to get involved in the armed struggle.", "crossword puzzle"], "metric_results": {"EM": 0.453125, "QA-F1": 0.49531249999999993}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-13266", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-9985", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-6579", "mrqa_searchqa-validation-9793", "mrqa_searchqa-validation-1329", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-1899", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-12679", "mrqa_searchqa-validation-16343", "mrqa_searchqa-validation-12899", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-1035", "mrqa_triviaqa-validation-987"], "SR": 0.453125, "CSR": 0.5230654761904762, "EFR": 1.0, "Overall": 0.6984412202380953}, {"timecode": 42, "before_eval_results": {"predictions": ["to Westminster", "Ratatouille", "Ecclesiastes", "Catherine de' Medici", "the Bohemia", "Ecuador", "Microsoft", "May 12, 1907", "binocular", "If you look at things one at a time,", "London", "Little Boy Blue", "cotton", "Alexander", "Seinfeld", "John Paul Jones", "the sound barrier", "Spider-Man", "Camelot", "Hudson Bay", "Hamlet", "an axiom", "Patrick\\'s", "a cereal", "George III", "Saul", "Illinois", "Heart of Darkness", "Rastafari", "Beverly Cleary", "Neapolitan", "Heartbreak Hotel", "andorra", "a pillar", "Pisa", "Blues Brothers", "Bangkok", "Jerry Maguire", "Russia", "Burt Lancaster", "deagonals", "Communist Party", "a sacristy", "Israel", "the Moor", "Alabama", "Making the Band 3", "Martinique", "a deodorant", "The Pathfinder", "the Lion King", "Florida", "Norman", "After tentatively courting each other in `` Entropy ''", "dragonfly", "is our children learning", "firethorn", "alt-right", "Australian", "Hawaii House of Representatives", "Negotiators for Zelaya and Roberto Micheletti,", "People around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "a Daytime Emmy Lifetime Achievement Award", "Australia"], "metric_results": {"EM": 0.5, "QA-F1": 0.559375}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9392", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-1830", "mrqa_searchqa-validation-13742", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-854", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-1179", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-314", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-8416", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-1452", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10320", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-616", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-755", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-1351"], "SR": 0.5, "CSR": 0.5225290697674418, "EFR": 1.0, "Overall": 0.6983339389534884}, {"timecode": 43, "before_eval_results": {"predictions": ["jellyfish", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups", "the Primal rib", "early as January 3, and as late as February 12", "Monk's Caf\u00e9", "Christopher Lloyd", "Morgan Freeman", "1910", "over 74", "John Vincent Calipari", "Floyd", "2018 and 2019", "James Madison", "The bills taken up under legislative power of parliament are treated as passed provided majority of members present at that time approved the bill either by voting or voice vote", "1917", "Icarus", "Brad Johnson", "Red Sea", "1982", "1956", "Hagrid", "Monk's", "Fulton, Arkansas", "Pete Seeger", "depression", "Sam", "the first heart sound ( S )", "Kevin Sumlin", "Bonnie Lipton", "American drama film", "two", "stomach of leg", "passwords, commands and data", "Roy Eberhardt moves to Florida and into the town of Coconut Cove", "senior enlisted sailor", "March 9, 2018", "741 weeks", "Bi Sheng", "cadmium", "1961", "2 September 1990", "chertendron", "cephalopoda", "1939", "March 26, 1973", "Stephen Foster", "Charles Carson", "Barry and Robin Gibb", "Brazilian state of Mato Grosso", "The White House Executive Chef", "3.9 and 5.5 mmol / L ( 70 to 100 mg / dL )", "Mike Danger", "conductor", "bridge", "Manchester United", "Skipton", "Andrew Lloyd Webber, Jim Steinman, Nigel Wright", "Elena Kagan", "a gym", "peace with Israel", "Ernesto \"Che\" Guevara", "pinnipeds", "Heather Locklear", "The Arizona Health Care Cost Containment System"], "metric_results": {"EM": 0.359375, "QA-F1": 0.492350965007215}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.9523809523809523, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.5, 1.0, 0.4, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.7777777777777778, 0.25, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 0.0, 1.0, 0.3636363636363636, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-2630", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-4874", "mrqa_newsqa-validation-4074", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-8799", "mrqa_hotpotqa-validation-1803"], "SR": 0.359375, "CSR": 0.5188210227272727, "EFR": 0.9512195121951219, "Overall": 0.6878362319844789}, {"timecode": 44, "before_eval_results": {"predictions": ["Confucianism", "April 2011", "James Intveld", "frontal lobe", "Haliaeetus", "black and yellow", "a low concentration in pigmentation", "Carol Ann Susi", "the septum", "a premalignant flat ( or sessile ) lesion of the colon", "Arnold Schoenberg", "The `` main line '' or `` first line ''", "asexually", "the Wednesday after Thanksgiving", "Deposition", "Andy Cole and Shearer", "George Strait", "boiling water reactor", "to solve its problem of lack of food self - sufficiency", "October 2012", "201", "Rafael Nadal", "the International Border", "Melissa Disney", "twice", "Gene MacLellan", "the fingers on either side of the mouth", "the world", "frozen carbon dioxide accumulates as a comparatively thin layer about one metre thick on the north cap in the northern winter only", "Michael Schumacher", "on the microscope's stage", "living - donor", "North Atlantic Ocean", "John Hancock", "silk floss tree", "around 3,000 - 5,000 program - erase cycles", "Triple threat", "Clarence Darrow", "alpha efferent neurons", "teaching elocution", "the optic disc to the optic chiasma", "Butter Island off North Haven, Maine in the Penobscot Bay", "a combination of genetics and the male hormone dihydrotestosterone", "British Columbia, Canada", "3.5, a fluorapatite - like remineralized veneer is formed over the remaining surface of the enamel", "Halliwell, French, Timomatic and Sandilands", "Frankie Valli", "908 mbar ( hPa ; 26.81 inHg )", "1939", "Pyeongchang County, Gangwon Province, South Korea", "Utah, Arizona, Wyoming, and Oroville, California", "Quito", "Lidice, Czechoslovakia", "Augustus Caesar", "Ars Nova Theater", "French", "1902", "Najaf.", "a monthly allowance,", "(3 degrees Fahrenheit),", "silver", "Caesar", "Possession is nine-tenths of the law", "last summer."], "metric_results": {"EM": 0.484375, "QA-F1": 0.6083164231601732}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.25, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2727272727272727, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.3333333333333333, 0.33333333333333337, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.33333333333333337, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8160", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-9196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-8391", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-6013", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-2863", "mrqa_newsqa-validation-1076", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-6194"], "SR": 0.484375, "CSR": 0.5180555555555555, "EFR": 0.9393939393939394, "Overall": 0.685318023989899}, {"timecode": 45, "before_eval_results": {"predictions": ["immunoglobulins and T cell receptors", "New Delhi", "two Frenchmen", "a mid-size four - wheel drive luxury Mercedes -Benz GL - Class", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "compasses", "Lake Wales", "an unmasked and redeemed Anakin Skywalker", "Megan Park", "Tom Hanks", "Tulsa, Oklahoma", "Broken Hill and Sydney", "John Goodman", "from the right side of the heart to the lungs", "during the day", "California", "11 January 1923", "southwestern Colorado and northwestern New Mexico", "Ann Gillespie", "Pontic Mountains in Turkey", "Master Christopher Jones", "931 BCE", "Claudia Grace Wells", "Jerry Leiber and Mike Stoller", "2002 Mitsubishi Lancer OZ Rally", "Natural - language processing", "Sir Alex Ferguson", "1875", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "2011", "September 1973", "Cairo, Illinois", "a comic book series", "Abanindranath Tagore CIE", "Coldplay", "the retina", "Empiricism", "1,149 feet ( 350 m )", "Lana Del Rey", "Romancing the Stone", "six degrees of freedom", "December 1886", "Ludacris", "A costume", "10 pesos ( approximately $ 18 )", "Frankie Muniz", "Richard Schiff", "the somatic nervous system and the autonomic nervous system", "Andy Cole", "1966", "Shenzi", "an abnormal visual condition that makes colorless objects appear tinged with color", "Perth Racecourse", "brambles", "England", "two", "6,241", "Roy Foster", "share personal information.\"It's very new and involves repairing my leaky valve using a clip device, without open heart surgery, so that my heart will function better,\"", "Stephen Tyrone Johns", "Around the", "flatware", "the Golden Hind", "Charles Swinburne"], "metric_results": {"EM": 0.40625, "QA-F1": 0.568158960292581}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.5714285714285715, 0.0, 0.3636363636363636, 0.0, 0.0, 0.8, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 1.0, 0.2666666666666667, 0.4, 1.0, 1.0, 1.0, 0.6, 0.0, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.5, 0.13793103448275865, 1.0, 0.33333333333333337, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6677", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-6506", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5970", "mrqa_triviaqa-validation-5041", "mrqa_hotpotqa-validation-5438", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-2549", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-6979", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-2528"], "SR": 0.40625, "CSR": 0.515625, "EFR": 1.0, "Overall": 0.6969531250000001}, {"timecode": 46, "before_eval_results": {"predictions": ["The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316,", "2.5 % of the total", "an ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "Cliff Richard", "McKim Marriott", "The British Indian Association", "foreign investors", "Glen W. Dickson", "British and French Canadian fur traders", "a line of committed and effective Sultans", "Jules Shear", "from 13 to 22 June 2012", "Tandi", "H.L.A. Hart", "Janie Crawford", "West Norse sailors", "the following year", "2017", "in the front of the body", "Dottie West", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "in Christianity", "it failed to enforce its rule, and its vast territory was divided into several successor polities", "Buffalo Lookout", "Aristotle", "until the nation's capital in Washington, D.C.", "John Donne", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Cristeta Comerford 2005 -- present", "Paspahegh Indians", "3 - largest economy by purchasing power parity ( PPP )", "Arnold Schoenberg", "Identification of alternative plans / policies", "The Outback", "quartz or feldspar", "Wisconsin", "85 %", "Long Island", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "seven", "NFL commissioner Roger Goodell", "The 133rd overall episode overall", "Juliet", "a certified question or proposition of law from one of the United States Courts of Appeals", "gathering money from the public", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "2018", "the contestant", "San Jose, California", "Nicklaus", "Indo - Pacific", "Who's the cat that won't cop out", "Denise van Outen", "West Virginia", "Syracuse", "Girls' Generation", "Manchester, England", "Authorities in Fayetteville, North Carolina,", "nearly three out of four Americans are scared about the way things are going in the country today.", "If a security officer were to pull a gun on an armed individual in a mall, it could result in \"the gunfight at the 'OK corral,'", "Jericho", "James Garfield Davis", "Catherine", "Ashley \"A.J.\" Jewell,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5785753990267903}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.2222222222222222, 0.0, 0.08333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.9090909090909091, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5283018867924527, 0.25, 0.72, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.22222222222222224, 1.0, 0.11764705882352941, 0.47058823529411764, 0.4615384615384615, 1.0, 0.06451612903225806, 0.0, 1.0, 0.05714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.4666666666666667, 0.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-259", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-171", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-6804", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-3226", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-2578", "mrqa_triviaqa-validation-3771", "mrqa_hotpotqa-validation-4117", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-982", "mrqa_searchqa-validation-16615"], "SR": 0.421875, "CSR": 0.5136303191489362, "EFR": 0.9459459459459459, "Overall": 0.6857433780189764}, {"timecode": 47, "before_eval_results": {"predictions": ["mumps", "The Last King of Scotland", "Kazakhstan", "Wilkie Collins", "pericardium", "Heathwaite House", "Burma", "Christian", "a falcon", "South Park", "Shylock", "Canada", "Phil Spector", "Champagne Cosmopolitan Cocktail", "\"Tiptoe Through the Tulips\"", "the Surrealist movement", "bemidji", "Roddy Doyle", "geography", "Talon Vise", "Berlin", "Charlie Chan", "Wanderers", "Pinwright's Progress", "a hat", "Lady Gaga", "a duck", "Christian Wulff", "the Kinks Are the Village Green Preservation Society", "the Queen of Comedy", "Debbie Rowe", "Sir Herbert Kitchener", "a centaur", "iodine deficiency", "36", "michael Foot", "(Sir Walter) Scott", "Shropshire", "George Bernard Shaw", "table tennis", "(Lord) Woolton", "the Gulf Stream", "Fleet Street", "Purley", "Gandalf", "(1894)", "Motown", "Quebec", "Pope Benedict XVI", "a dove", "Cable", "translocation Down syndrome", "disagreements involving slavery and states'rights", "$2 million in 2011", "Amy Poehler", "Eric Allan Kramer", "Koninklijke Ahold N.V.", "via YouTube days after 35 bodies were found in two trucks during rush hour in the city of Boca del Rio.", "Karl Kr\u00f8yer", "debris", "Sweden", "Dehlia Draycott", "schizophrenia", "it's hot out there today so remember to keep them"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5497023809523809}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09523809523809523, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-4823", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-2153", "mrqa_triviaqa-validation-3098", "mrqa_triviaqa-validation-2077", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-3444", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-1527", "mrqa_triviaqa-validation-7274", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-4286", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-6090", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-961", "mrqa_triviaqa-validation-304", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9093", "mrqa_hotpotqa-validation-4091", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-111", "mrqa_searchqa-validation-11594", "mrqa_searchqa-validation-14439"], "SR": 0.515625, "CSR": 0.513671875, "EFR": 1.0, "Overall": 0.6965625}, {"timecode": 48, "before_eval_results": {"predictions": ["taxonomy", "eight", "April 1st", "June 1992", "won", "Katharine Hepburn", "Kimberlin Brown", "March 31, 2017", "vincent gogh", "New York City", "George Strait", "John Adams", "a major fall in stock prices", "On the west", "Charles Path\u00e9", "Phillip Paley", "generally directed towards officers and agencies of the U.S. federal government", "18", "2007", "Thomas Jefferson", "Abraham Gottlob Werner", "IMS", "the 18th century", "Lesley Gore", "sometime between 124 and 800 CE", "thirteen", "mongrel female", "Teri Hatcher", "John Quincy Adams", "August 1991", "Uralic languages", "dromedary", "Bhupendranath Dutt", "2011", "a substance that fully activates the receptor that it binds to )", "Bill Russell", "Battle of Antietam", "sport utility", "Hunter Tylo", "Buffalo Bill", "the early 3rd century", "James Rodr\u00edguez", "around 10 : 30am", "Jack Barry", "White Sox", "45 %", "to condense the steam coming out of the cylinders or turbines", "Bill Russell", "1984", "the problems and / or goals", "the second planet from the Sun", "goodge street", "cilla black", "epernay", "Hanako to Anne", "Taoiseach", "Los Angeles Dance Theater", "Kurdistan Freedom Falcons", "AbdulMutallab", "hank moody", "a typewriter", "calico", "mandolin", "Gary Player"], "metric_results": {"EM": 0.609375, "QA-F1": 0.677073826058201}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.12500000000000003, 0.22222222222222224, 1.0, 1.0, 0.07407407407407408, 1.0, 1.0, 0.0, 0.0, 0.0, 0.16, 1.0, 0.4799999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-183", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-4655", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3102"], "SR": 0.609375, "CSR": 0.515625, "EFR": 1.0, "Overall": 0.6969531250000001}, {"timecode": 49, "before_eval_results": {"predictions": ["Napoleon", "iron", "Patrick Warburton", "the condemned during execution rehearsals and sells snacks to prisoners and guards", "pneumonoultramicroscopicsilicovolcanoconiosis", "Joseph M. Scriven", "Andrew Garfield", "July 4, 1776", "Keith Thibodeaux", "Jesus Christ", "Charles Path\u00e9", "eleven", "President alone, and the latter grants judicial power solely to the federal judiciary", "Johannes Gutenberg", "O'Meara", "first published in the United States by Melvil Dewey in 1876", "marley & Me", "fourth season", "four seasons", "Confederate", "the probability of rejecting the null hypothesis given that it is true", "slavery", "Lou Stallman", "employer", "The Outback", "its vast territory was divided into several successor polities", "Louis XV", "in Jacksonville", "its genome", "Beorn", "the ability to influence somebody to do something that he / she would not have done ''", "nationalists of the Union proclaimed loyalty to the U.S. Constitution", "in response to the Weimar Republic's failure to continue its reparation payments in the aftermath of World War I", "Indirect rule", "Zachary John Quinto", "the governor of West Virginia", "Wednesday, September 21, 2016", "ninth", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "milling process", "in North America", "1939", "1992", "Meister Brau Brewing", "Felix Baumgartner", "Donald Fauntleroy Duck", "c. 3000 BC", "Bart Howard", "Hope chooses Liam and they become engaged", "1966", "inwards towards the pith, and secondary phloem growth outwards to the bark", "Child of the 1980's", "Leeds", "Portugal", "Coleman Hawkins", "Samuel Joel \"Zero\" Mostel", "Ellesmere Port, United Kingdom", "the Genocide Prevention Task Force", "A grizzly bear", "U.S. Navy", "deborah", "Treasure Island", "Pablo Picasso", "kite surfers"], "metric_results": {"EM": 0.5, "QA-F1": 0.6312677054319495}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.8148148148148148, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.8837209302325582, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-1290", "mrqa_naturalquestions-validation-6888", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-8254", "mrqa_triviaqa-validation-460", "mrqa_triviaqa-validation-3952", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2428", "mrqa_searchqa-validation-9956", "mrqa_searchqa-validation-15480", "mrqa_newsqa-validation-1446"], "SR": 0.5, "CSR": 0.5153125000000001, "EFR": 0.96875, "Overall": 0.6906406250000001}, {"timecode": 50, "UKR": 0.689453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1373", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10692", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5387", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8709", "mrqa_naturalquestions-validation-8819", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5744", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99"], "OKR": 0.8515625, "KG": 0.490625, "before_eval_results": {"predictions": ["Jason", "Trainspotting", "Chess Records", "netherlands", "spark", "Concorde", "jazeera", "French", "netherd", "netherlands", "2007", "Goldfinger", "midway", "Flower", "Ford", "Dengue fever", "japan", "Ted Turner", "nosebleed fear", "primula veris", "Mount Everest", "Strangeways", "Carthage", "River Wensum", "Robben Island", "netherlands", "Taekwondo", "hanao", "apple", "Wimbledon", "Mandela", "George Orwell", "Andrew Jackson", "Muriel Spark", "Table Tennis", "jumbles Reservoir", "DeLorean", "six", "Perseus", "Yakutat", "netherlands", "coffee house", "muscle", "transuranic", "John Buchan", "Tesco", "Lolita", "jukebox", "the Indus valley", "netherlands", "Pickwick", "Nancy Jean Cartwright", "Watson and Crick", "Authority", "his superhero roles", "German", "Che Guevara", "CNNI's Michael Holmes", "five", "Jet Republic", "netherlands", "Justin Bieber", "Murder by Death", "Senate Democrats"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6588541666666666}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-139", "mrqa_triviaqa-validation-6499", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-6476", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-204", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-5363", "mrqa_triviaqa-validation-3982", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-112", "mrqa_triviaqa-validation-590", "mrqa_naturalquestions-validation-3468", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2031", "mrqa_searchqa-validation-5409", "mrqa_searchqa-validation-9297", "mrqa_newsqa-validation-1550"], "SR": 0.59375, "CSR": 0.5168504901960784, "EFR": 1.0, "Overall": 0.7096982230392157}, {"timecode": 51, "before_eval_results": {"predictions": ["Flatbush Zombies", "British", "Arthur Schnitzler's 1926 novella \"Traumnovelle\" (\"Dream Story\")", "Denmark", "Bad Meets Evil", "Bellagio and The Mirage", "Mr. Basketball", "more than 20", "Guthred", "The New Yorker", "Jeffrey Jones", "St. Louis Cardinals", "NXT Tag Team Championship", "Lee Byung-hun", "24", "February 1", "capital crimes or capital offences", "Let Me Be the One", "March", "John Stallworth", "Cate Blanchett", "California", "Atlas ICBM", "Republican", "Kim So-hyun", "Rolling Stones", "22,500 acres", "Trey Parker", "Kew", "Albany", "Fainaru Fantaj\u012b Tuerubu", "Wembley Stadium", "Shameless", "Raden Panji", "skiing and mountaineering", "Indian", "Comedy Film Nerds", "cruiserweight", "five", "Leofric", "Sasquatch", "March 17, 2015", "yeeun", "5249", "fourth", "seven", "28 November 1973", "La Nouba", "Londonderry", "Leonardo DiCaprio", "jewelry designer", "Steve Russell", "from `` Wat '', or `` Wa'ter '', an old pronunciation of Gaultier or Walter, and similarly derived from the surname Watson", "while studying All My Sons by Arthur Miller, a play about a man whose choice to send out faulty airplane parts for the good of his business and family caused the death of twenty one pilots during World War II", "Pegasus", "sunday", "barildon-born Perry", "15-year-old's", "July 23.", "Baghdad", "Buena", "circumference", "T.S. Eliot", "Boris Becker"], "metric_results": {"EM": 0.390625, "QA-F1": 0.517471293403697}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true], "QA-F1": [0.22222222222222224, 1.0, 0.6, 0.0, 0.4, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.10526315789473684, 0.9859154929577464, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-3270", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-3712", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-248", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-3752", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-1886", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3651", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-7514", "mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-4557", "mrqa_searchqa-validation-6192", "mrqa_searchqa-validation-834"], "SR": 0.390625, "CSR": 0.5144230769230769, "EFR": 1.0, "Overall": 0.7092127403846153}, {"timecode": 52, "before_eval_results": {"predictions": ["Frank Ocean", "Town of Brookhaven", "2010", "Ryukyuan people", "Robert L. Stone", "Spanish", "The King of Hollywood", "five times", "1968", "Charles Eug\u00e8ne Jules Marie Nungesser", "Kim Yoon-seok and Ha Jung-woo", "Jennifer Grey", "1978", "M2M", "Mark Neveldine and Brian Taylor", "Starship Planet", "Beauty and the Beast", "\"Odorama\", whereby viewers could smell what they saw on screen through scratch and sniff cards", "The 8th Habit", "Larnelle Steward Harris", "Total Nonstop Action Wrestling", "Lambic", "Bit Instant", "Tom Jones", "Leon Czolgosz", "\"Secrets and Lies\"", "Hard rock", "Ludwig van Beethoven", "Peter Kay's Car Share", "Orph\u00e9e et Eurydice", "Dirt track racing", "Frederick Barbarossa", "Karakalpaks", "Walldorf, Baden-W\u00fcrttemberg", "Han Solo", "Campbellsville University", "Shinjuku", "1933", "Delphi Lawrence", "Philadelphia", "December 13, 2015", "The New York Stock Exchange (abbreviated as NYSE and nicknamed \"The Big Board\")", "Paradise, Nevada", "Russell T Davies", "four", "Marlborough", "Alan Young", "the 2018\u201319 UEFA Europa League group stage", "Argentinian", "76,416", "Burning Man", "the President of the United States", "Ra\u00fal Eduardo Esparza", "October 30, 2017", "Live and Let Die", "The Village Vomit", "on Mars", "one of South Africa's most famous musicians,", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\" and says \"it should stay that way.\"", "a hockey mask", "Yes", "East Germany", "heart"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7269531250000001}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.09523809523809523, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-1263", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-7133", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-2661", "mrqa_searchqa-validation-15932", "mrqa_triviaqa-validation-3362"], "SR": 0.640625, "CSR": 0.5168042452830188, "EFR": 1.0, "Overall": 0.7096889740566038}, {"timecode": 53, "before_eval_results": {"predictions": ["doubles", "New York", "Long Island", "muezzin", "car", "a binder", "James Hogg", "Sarajevo", "Darby and Joan", "The Hurt Locker", "Full Metal jacket", "Blur", "chicken Marengo", "Nelson Mandela", "Sir Herbert Kitchener", "white", "a bodice", "grizzly", "buk'wus", "Society of Jesus", "rowing", "the end of March 1939", "Nowhere Boy", "Donald Trump", "Gorbachev", "Popeye", "John Key", "Charlie Brooker", "Northwestern University", "Gulf of Mexico", "her lover, the iron man of the ancient world", "Aceso", "dynamite", "the Beatles", "Jean Alexander", "Lew Hoad", "David Hockney", "La Toya Jackson", "Jimmy Carter", "canada", "the Czech Republic", "Edinburgh", "Today", "carrie", "Bolton", "Norwegian", "Super Bowl", "\"Stutter Rap (No Sleep til Bedtime)\"", "Vladimir Putin", "Belle", "a bear (losing) market", "Augustus Waters", "1967", "silk, hair / fur ( including wool )", "Ghana", "Peter Kay's Car Share", "Miller Brewing", "The son of Gabon's former president", "more than two years,", "auction off one of the earliest versions of the Declaration of Independence,", "the tartan of the Argyle clan", "an interest", "carrie perlin", "Unseeded Frenchwoman"], "metric_results": {"EM": 0.53125, "QA-F1": 0.589069976076555}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7368421052631577, 0.4, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-5000", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-1004", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-5848", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-2385", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5531", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-5344", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-3499", "mrqa_newsqa-validation-3285"], "SR": 0.53125, "CSR": 0.5170717592592593, "EFR": 1.0, "Overall": 0.7097424768518519}, {"timecode": 54, "before_eval_results": {"predictions": ["ford administration", "the great gatsby", "ford administration", "San Francisco", "first World War", "germany", "prince andrew", "duchess", "germany", "ford administration", "atlantic", "Skylab", "John Poulson", "ford administration", "shoes", "the great depression", "corsets", "Queen Anne", "atlantic", "dicken\\'s Dream", "Swansea City", "argon", "the silurian", "meatloaf", "non-Orthodox synagogues", "j.M.W.", "the Lone Gunmen", "czarina Maria", "Duncan Jones", "at the north-west corner of the central business district", "wonderwall", "basketball", "carburetor", "knivskjellodden", "corsets", "germany", "Jean- Martin Charcot", "winged horse", "Charlie Chaplin, Jr.", "bathe", "dora peggotty", "Scotland", "germany", "Arthur C. Clarke", "Neil Armstrong", "power outage", "Napoleon Bonaparte", "index fingers", "Blenheim Palace", "Rihanna", "at Chernobyl nuclear power plant", "28 July 1914 to 11 November 1918", "Fix You", "the spectroscopic notation for the associated atomic orbitals", "Copa Airlines", "my Beautiful Dark Twisted Fantasy", "Soha Ali Khan Khemu", "for vitamin injections that promise to improve health and beauty.", "debris", "digging ditches", "Typhoid Mary", "germany", "the Edict of Nantes", "Austin and Pflugerville"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5500405844155845}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-1536", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-7740", "mrqa_triviaqa-validation-2312", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-5896", "mrqa_triviaqa-validation-2243", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-4795", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-7712", "mrqa_hotpotqa-validation-303", "mrqa_newsqa-validation-3330", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-2415", "mrqa_naturalquestions-validation-3995"], "SR": 0.46875, "CSR": 0.5161931818181817, "EFR": 0.9705882352941176, "Overall": 0.7036844084224599}, {"timecode": 55, "before_eval_results": {"predictions": ["aviva plc", "Venezuela", "Mozart", "france", "mark of the city", "Catherine Cookson", "almonds", "Barack Obama", "Geneva", "shatner", "Peter Paul Rubens", "Persian Gulf", "durlach", "ascot", "seine river", "mary peppins", "sheryl Crow", "winnie Mae", "Spain", "come quietly", "Gibeon", "graphite", "wellington", "Moby Dick", "The Scream", "gingerbread", "boddington beer", "king John of England", "wellbeing", "raspberries", "USVI", "surfer", "oakum", "blancmange", "rochdale", "Penhaligon", "Black September", "7,926 miles", "Germany", "shoe", "oxide", "ferry", "Professor Brian Cox", "Meow Mix", "8", "kidneys", "Bolivia", "synagogue", "Jordan", "Hans Lippershey", "india", "Christina Giles", "statistical advantage for the casino that is built into the game", "the top of the cab can be crushed or sliced off as it swings round violently and tries to fold under the trailer", "Teriade", "Japan", "the National Aviation Hall of Fame class of 2001", "touma", "Madeleine K. Albright", "three empty vodka bottles,", "pinniped", "grotesque", "Liam Neeson", "a centaur"], "metric_results": {"EM": 0.578125, "QA-F1": 0.624727564102564}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08000000000000002, 1.0, 0.0, 0.7692307692307693, 0.4, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-4616", "mrqa_triviaqa-validation-5454", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1324", "mrqa_triviaqa-validation-3231", "mrqa_triviaqa-validation-6866", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3275", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-3897", "mrqa_triviaqa-validation-2442", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6931", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5510", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3067", "mrqa_searchqa-validation-4686", "mrqa_searchqa-validation-2027"], "SR": 0.578125, "CSR": 0.5172991071428572, "EFR": 1.0, "Overall": 0.7097879464285715}, {"timecode": 56, "before_eval_results": {"predictions": ["joshua coltrane", "j Judy gumm", "bruck", "friedrich Nietzsche", "Ben Affleck", "jamaican", "magical mystery tour", "brazil", "Cyclopes", "purple", "1929", "Oklahoma", "florence", "robbie coltrane", "georgia coltrane", "Antoine Lavoisier", "30th anniversary", "merkat", "tara", "Henri Rousseau", "atlantic", "the Beatles", "rijn", "robbie coltrane", "united plants", "paddington bear", "tjorn", "the motorway", "tidal Bay", "spike", "Alastair Cook", "sorawak", "cribbage", "1960s", "united states", "LMFAO", "robbie coltrane", "kinks", "Tony Blackburn", "spain", "rebecca", "united states", "pink Floyd", "robbie coltrane", "Spider-Man", "robbie coltrane", "the American Revolutionary War", "Tokyo", "spike", "mono", "Augustus Caesar", "the south coast of eastern New Guinea", "Lady Gaga", "If wanting to feel broken - hearted, the female protagonist sings that she feels happy to have left her lover, who did not recognize the potential for a happy life with her", "\"Secrets and Lies\"", "October 3, 2017", "Moe \"Moe\" Greene", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "Unseeded", "Kingman Regional Medical Center", "norbert trotrane", "the fairway", "the IRA", "2018"], "metric_results": {"EM": 0.53125, "QA-F1": 0.54375}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true], "QA-F1": [0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-223", "mrqa_triviaqa-validation-454", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-346", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-3252", "mrqa_triviaqa-validation-188", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-4757", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-6935", "mrqa_triviaqa-validation-2743", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-3716", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-9821", "mrqa_hotpotqa-validation-4161", "mrqa_newsqa-validation-3287", "mrqa_searchqa-validation-16213"], "SR": 0.53125, "CSR": 0.5175438596491229, "EFR": 1.0, "Overall": 0.7098368969298245}, {"timecode": 57, "before_eval_results": {"predictions": ["george tector Gorch", "Illinois", "Edward Hopper", "robocop", "morocco", "Quentin Blake", "Melanie Ward", "mardi gras", "new york", "Hamlet", "Jose Antonio Reyes", "james bernard", "Piccadilly line", "morocco", "Jordan", "Tangled", "Brothers in Arms", "united states", "crossword puzzle", "Sheree Murphy", "morocco", "Robin Ellis", "tomato Basil", "davy crockett", "war and peace", "paphos", "three", "east of Eden", "de quincey", "zaragoza", "morocco", "argentina", "king eddy", "france", "scrooge", "bridge", "elliptical", "k Koblenz", "Bruce Springsteen", "blood", "zips", "amisbach", "Roman history", "mj\u00f6llnir", "Admiral Vernon", "florence", "woodstock", "jays", "nijinsky", "whittingham", "drogba", "Chairman of the Monetary Policy Committee", "B.J. Thomas", "8 bytes", "Prince Amedeo,", "San Antonio", "hulder", "the Obama administration", "Republicans", "27 Awa", "Manhattan Island", "peter henry", "obstructive sleep", "the Eagles"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5880208333333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.5, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1493", "mrqa_triviaqa-validation-3686", "mrqa_triviaqa-validation-4243", "mrqa_triviaqa-validation-910", "mrqa_triviaqa-validation-6237", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-3645", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1563", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7378", "mrqa_triviaqa-validation-6841", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-3218", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-2177", "mrqa_naturalquestions-validation-9571", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-2399", "mrqa_newsqa-validation-1042", "mrqa_searchqa-validation-10934", "mrqa_searchqa-validation-16820", "mrqa_searchqa-validation-384"], "SR": 0.53125, "CSR": 0.5177801724137931, "EFR": 1.0, "Overall": 0.7098841594827586}, {"timecode": 58, "before_eval_results": {"predictions": ["the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "10th Cavalry Regiment", "Hong Kong First Division League club Happy Valley", "John Robert Cocker", "Taylor Swift", "mountaineer", "\"Lonely\"", "Garrett Morris", "October 5, 1937", "1692", "Dizzy Dean", "Target Corporation", "British Labour Party", "Bandai", "Bill Ponsford", "Patricia Neal", "Code 02Pretty pretty", "every Rose has its Thorn", "Cleveland Browns", "Jacking", "27 December 1901", "my Beautiful Dark Twisted Fantasy", "Broadcasting House in London", "20", "Alticor", "Congo River", "Minneapolis", "Alemannic German", "illnesses", "sex website", "1967", "1967", "The fictional character Spider-Man", "Lawrence of Arabia", "The Fault in Our Stars", "Gareth Jones", "head of the Cabinet of Bluhme I", "J35-A-23", "\"The Young and the Restless\"", "balloon Street, Manchester", "Somerset County, Pennsylvania", "southern Italy", "Psych", "\"Portal\" and \"Metroid\" series", "Iran", "Veneto", "Empire Falls", "Fitzroya cupressoides", "Vernon L. Smith", "Daniel Hale \"Dan\" Rowan", "Bohemia, New York", "March 18, 2005", "1978", "Austria - Hungary", "Switzerland", "treaty of Waitangi", "1978-1988", "a U.S. military helicopter", "African National Congress Deputy President Kgalema Motlanthe,", "new DNA evidence", "blue whale", "( Christopher) Darden", "out-of-print books", "bullfight"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5720486111111112}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.5, 0.5, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-4494", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-243", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-1960", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2924", "mrqa_hotpotqa-validation-2004", "mrqa_naturalquestions-validation-6040", "mrqa_triviaqa-validation-5517", "mrqa_newsqa-validation-1382", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-1481", "mrqa_triviaqa-validation-6175"], "SR": 0.453125, "CSR": 0.5166843220338984, "EFR": 1.0, "Overall": 0.7096649894067797}, {"timecode": 59, "before_eval_results": {"predictions": ["the first integrated circuit", "Oracle Corporation", "Levittown", "Mako", "seven", "Ashanti Region", "1934", "Cheshire", "1980", "cricket fighting", "Dachshund", "wild boar, and red, fallow and roe deer", "Duncan Kenworthy", "the Stern-Plaza in Potsdam", "Netherlands", "Continental Army", "Norse mythology", "Henry Lau", "1", "Russia", "the Catholic Church in Ireland", "people working in film and the performing arts", "Lykan HyperSport", "1989", "Gareth Barry", "1999", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "Margarine Unie", "David Naughton", "A123 Systems, LLC", "Ian Fleming", "Minnesota", "14", "an anvil", "50 Greatest Players in National Basketball Association History", "James G. Kiernan", "Dizzy Dean", "Magnus Carlsen", "BBC Focus", "Towards the Sun", "1958", "World War II", "Jenn Brown", "\"Glee\"", "Purdue University", "Indianapolis", "a hard rock/blues rock band", "Greater Manchester, England", "Raabta", "Marxist and a Leninist", "Johan Leysen", "Laura Jane Haddock", "when the cell is undergoing the metaphase of cell division ( where all chromosomes are aligned in the center of the cell in their condensed form )", "2011", "a camelopardalis", "ghee", "Wagner", "to step up.\"", "Baitullah Mehsud,", "Andrew Morris,", "Home on the Range", "one meter", "Donnie Wahlberg", "1918"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7187596819030642}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-4420", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-1148", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-1006", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-1134", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-1098", "mrqa_searchqa-validation-12933", "mrqa_newsqa-validation-2789"], "SR": 0.59375, "CSR": 0.51796875, "EFR": 1.0, "Overall": 0.709921875}, {"timecode": 60, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4534", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-4728", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-637", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-768", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-921", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-10651", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4312", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2415", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1403", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1768", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2243", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3432", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5228", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6098", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6536", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7606", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-997"], "OKR": 0.830078125, "KG": 0.5015625, "before_eval_results": {"predictions": ["September, Bianchi's death", "five minutes before commandos descended", "Keith Hackett", "going through a metamorphosis from blobs of orange to art as night falls.", "causing enormous suffering and massive displacement,\"", "They are co-chairs of the Genocide Prevention Task Force.", "allegedly involved in forged credit cards and identity theft led authorities to a $13 million global crime ring,", "4.6 million", "Ferraris, a Lamborghini and an Acura NSX", "Vicente Carrillo Leyva,", "fossil", "Communist Party of Nepal (Unified Marxist-Leninist)", "581 points", "Molotov cocktails, rocks and glass.", "1994,", "10 years", "Gulf", "25 percent", "Orbiting Carbon Observatory", "then-Sen. Obama", "Claude Monet", "4,000", "apartment building", "Pittsburgh native", "Herman Thomas", "Daytime Emmy Lifetime Achievement Award", "Johannesburg", "Barack Obama's", "dual nationality", "Knox's parents", "Cash for Clunkers program", "It will be the golfer's first public appearance since his November 27 car crash outside his home near Orlando, Florida.\"", "couple's surrogate", "prostate cancer", "Zimbabwe", "Britain.", "thunderstorms", "10 percent", "eight or nine young girls, some younger then 18,", "Jaipur", "cancer", "Alan Graham", "salutes the \"People of Palestine\" and calls on them to fight back against Israel in Gaza.", "forgery and flying without a valid license,", "georgeWashington", "Sean,", "three out of four questioned say that things are going well for them personally.", "poems", "Fourth time lucky", "think are the best.", "Graeme Smith", "2009", "William Jennings Bryan", "the nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome, which contains enzymes used for penetrating the female egg.", "Humphries", "Mozambique Channel", "Ede & Ravenscroft", "Adelaide", "punk rock", "Great Northern Railway", "Otis Elevator", "Iberian Peninsula", "George Balanchine", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5791846001221002}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true], "QA-F1": [0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-3297", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-4092", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-56", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-2468", "mrqa_hotpotqa-validation-3984", "mrqa_searchqa-validation-15121"], "SR": 0.453125, "CSR": 0.516905737704918, "EFR": 1.0, "Overall": 0.7118967725409836}, {"timecode": 61, "before_eval_results": {"predictions": ["Donald Duck", "Iran's parliament speaker", "Department of Homeland Security Secretary Janet Napolitano", "18", "India", "World leaders", "Mafia", "managing his time.", "his club", "we seek a new way forward, based on mutual interest and mutual respect.", "$50", "collaborating with the Colombian government,", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "1,500", "Jada,", "200", "Karen Floyd", "space shuttle Discovery,", "Brazil", "EU naval force", "set up the philanthropic Bill & Melinda Gates Foundation in 1994.", "Harrison Ford", "Sunday", "28", "The Falklands, known as Las Malvinas", "New York City Mayor Michael Bloomberg", "Department of Homeland Security Secretary Janet Napolitano", "two", "Too many glass shards left by beer drinkers in the city center,", "30-minute", "338", "UNICEF", "eight", "Daniel Radcliffe", "Department of Homeland Security Secretary Janet Napolitano", "Portuguese water dog", "Al Nisr Al Saudi", "lightning strikes", "if he did cheat on you", "Afghan lawmakers", "the Dalai Lama", "Colombia", "nine-wicket", "nearly 100", "1616.", "to sniff out cell phones.", "Casey Anthony,", "people look at the content of the speech, not just the delivery.", "2005", "act, and it doesn't matter that they're non-state actors.\"", "a point for Bayern Munich as the German Bundesliga leaders were held to a 1-1 draw by Cologne on Saturday.", "2018", "players who passes a screening committee ( which removes from consideration players of clearly lesser qualification )", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "St Paul's Cathedral", "15", "Latium", "University of Vienna", "Dutch", "Naomi Campbell", "Earhart", "Optus", "Saturday Night Live", "the federal government"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5491860510651629}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.2857142857142857, 0.4444444444444445, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.16, 0.21052631578947367, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-2145", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4915", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-880", "mrqa_triviaqa-validation-3747", "mrqa_hotpotqa-validation-3500", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-7641", "mrqa_naturalquestions-validation-6092"], "SR": 0.484375, "CSR": 0.5163810483870968, "EFR": 0.9696969696969697, "Overall": 0.7057312286168134}, {"timecode": 62, "before_eval_results": {"predictions": ["Golden Valley, Minnesota,", "an Emmy and four", "small forward", "Molly Hatchet", "Harriet Tubman", "the Mach number (M or Ma)", "eight", "November 13, 2015", "Al Capone", "Atomic bomb", "St Augustine's Abbey", "Vilyam \"Willie\" Genrikhovich Fisher", "minister and biographer", "Carl Michael Edwards II", "Henry Luce", "chicken dance", "Standard Oil", "over 1.6 million passengers", "British Labour Party", "September 8, 2017", "Obafemi Akinwunmi Martins", "Charles Edward Stuart", "HackThis Site", "Steve Carell", "Saint Motel", "Melissa Rauch", "Flyweight", "Levon Helm", "Jean Acker", "the attack on Pearl Harbor", "Fountains of Wayne", "Nick Offerman", "Sam Raimi, Ivan Raima, and Tom Spezialy", "SAS", "Double Crossed", "Edmonton, Alberta", "8,211", "KXII", "Wikimedia Foundation", "Greek-American", "Mika H\u00e4kkinen", "Debbie Isitt", "Los Angeles", "1999", "Outside", "Food and Agriculture Organization", "Wojtek", "Edward I", "Los Angeles", "Cold Spring", "New York City", "Speaker of the House of Representatives", "13", "the fairy tale `` The Little Mermaid '' by Hans Christian Andersen", "an arrowhead", "Corin Redgrave", "a son of Amram and Jochebed,", "UNICEF", "Bob Bogle,", "she was humiliated by last month's incident, in which she was forced to inappropriately remove the piercings", "In the Heat of the Night", "mead", "David", "sixth"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7019997318105871}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3157894736842105, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1420", "mrqa_hotpotqa-validation-5444", "mrqa_hotpotqa-validation-4237", "mrqa_hotpotqa-validation-4866", "mrqa_hotpotqa-validation-4539", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-821", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2993", "mrqa_hotpotqa-validation-298", "mrqa_naturalquestions-validation-839", "mrqa_triviaqa-validation-6039", "mrqa_triviaqa-validation-5231", "mrqa_newsqa-validation-390", "mrqa_naturalquestions-validation-5292"], "SR": 0.59375, "CSR": 0.517609126984127, "EFR": 1.0, "Overall": 0.7120374503968254}, {"timecode": 63, "before_eval_results": {"predictions": ["African National Congress", "Ronald Cummings", "five", "Bob Bogle,", "Bob Bogle,", "for the creation of an Islamic emirate in Gaza,", "suppress the memories and to live as normal a life as possible;", "Caster Semenya", "Kandi Burruss,", "the foyer of the BBC building in Glasgow, Scotland", "Kgalema Motlanthe,", "as he tried to throw a petrol bomb at the officers,", "Eikenberry", "9 p.m. ET", "Asia qualifying Group 2", "Elena Kagan", "Harrison Ford", "Soddy-Daisy, Tennessee,", "Salt Lake City, Utah,", "eight in 10", "your ex's loved ones ask why", "2-1", "racial intolerance.", "a spurned suitor.", "23 million square meters (248 million square feet)", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "nine newly-purchased bicycles", "landed in Cameroon,", "Akshay Kumar", "August 19, 2007.", "a strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television.", "starting a dialogue while maintaining sanctions,", "either grant full health-care coverage, which would require an act of Congress, White House spokesman Robert Gibbs said.", "the test results", "are more often women, have fewer kids and more often have college educations.\"", "dancing with the Stars.\"", "Brown-Waite", "are \"totaled,\"", "40 militants and six Pakistan soldiers dead,", "strife in Somalia, where riots continued in the capital city of Mogadishu for the second day Tuesday.", "thousands of women protest child trafficking and shout anti-French slogans", "colonel in the Rwandan army,", "September 6, 1918,", "in the mouth.", "cancer", "Susan Atkins", "137", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "pulling on the top-knot of an opponent, and has gained a reputation as the enfant-terrible of sumo.", "the end of a biology department faculty meeting at the University of Alabama in Huntsville,", "former U.S. secretary of state.", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "kerogen Type III", "the United States", "hallmarks", "Quentin Tarantino", "uric goldfinger", "Thomas Mawson", "Al D'Amato", "Peel Holdings", "Java", "aaron", "ecliptic", "Harriet M. Welsch"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5427670957633772}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.2608695652173913, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.08695652173913043, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4285714285714285, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 0.0, 0.0, 1.0, 0.7894736842105263, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-2287", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-10403", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-6865", "mrqa_searchqa-validation-2650", "mrqa_searchqa-validation-15537", "mrqa_searchqa-validation-5471"], "SR": 0.46875, "CSR": 0.516845703125, "EFR": 1.0, "Overall": 0.711884765625}, {"timecode": 64, "before_eval_results": {"predictions": ["\"The straight man, stooge, or deadwood\"", "Great British Bake Off", "Garym Havelock", "Corporal", "Fiji", "Natty Bumppo", "morle Oberon (who had been cast as Claudius\u2019s wife Valeria Messalina)", "bambara", "Aleister Crowley", "\"5 questions,\" \"moment for Us,\" \"Dance, Dance, Dance,\" and \"Your Moment of Zen.\"", "\"Barefoot Bandit\"", "Ytterby", "mammals", "neon", "Boston Braves", "yokai", "Blanche", "1826", "chile", "Lisieux, Normandy", "Royal Ascot", "morlene Dietrich", "charles cairoli", "sunil Gavaskar", "The Blue Danube", "florida", "William Caxton", "Buzz Aldrin", "highball", "bituminous", "Dutch", "Reform Club", "amicus", "Saint Cecilia", "the Netherlands", "the Joker", "pistil", "The World as Will", "Thomas Cranmer", "the Mad Hatter", "Nick Clegg", "Norfolk, Virginia", "Mr. Spock", "the uppermost buttock", "Nikola Tesla", "Adrian Edmondson", "Persian Empire", "the innermost digit of the forelimb", "Boyle\u2019s law", "Antonio Vivaldi", "Bachelor of Science", "lamina dura", "July 2014", "The Internet protocol suite ( UDP / IP )", "Bath, Maine", "Princess Jessica", "raw materials, of work-in-process inventory, and of finished goods", "Dr. Jennifer Arnold and husband Bill Klein,", "\"I've got a good group of Marines that are behind me, so I'm real excited about the deployment,\"", "\"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "Spmi", "Private Benjamin", "the Rhine & the Main", "\"All the Boys Love Mandy Lane\" (2006)"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4796955128205128}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8000000000000002, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 0.08, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-6992", "mrqa_triviaqa-validation-1020", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-6566", "mrqa_triviaqa-validation-5536", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-84", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-195", "mrqa_triviaqa-validation-4258", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-1949", "mrqa_triviaqa-validation-31", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-5009", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-1171", "mrqa_hotpotqa-validation-1001", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-2668", "mrqa_searchqa-validation-7466", "mrqa_hotpotqa-validation-652"], "SR": 0.40625, "CSR": 0.5151442307692308, "EFR": 1.0, "Overall": 0.7115444711538461}, {"timecode": 65, "before_eval_results": {"predictions": ["hemlock", "james bond", "roosevelt", "eyes", "sierra leone", "david hockney", "sierra leone", "preston", "stanley castle", "ascot", "USS Maine", "Coalbrookdale", "borgia family", "Periodic", "dennis taylor", "bread", "india", "j Jakarta", "spike milligan", "mitsubishi", "the Panama Canal", "1960", "feet", "apples", "lug", "paul rowe", "Hamelin", "Harold Godwinson", "Kuwait", "leicestershire", "sprint", "green", "The Grapes of Wrath", "Coldplay", "pamphlets, posters, ballads", "Rugrats", "Chancellor of the Exchequer", "bottle ring", "fat", "Austria", "sergeant", "cuckoo", "paul boyle", "Markus Aemilius Lepidus", "business", "lite", "stanley da ponte", "Nikita Khrushchev", "Ivy Carter", "sugar", "molecular structure of nucleic acids", "Schwarzenegger", "Wakanda", "lamina dura", "stand-up comedian, actor, musician, and author", "Robert L. Stone", "Haitian Revolution", "sierra leone", "to launch a group that will serve as an alternative to the Organization of American States.", "64,", "kolkata", "Aunt Bee's Delightful Desserts", "Anaheim", "Old English pyrige ( pear tree )"], "metric_results": {"EM": 0.484375, "QA-F1": 0.578723473084886}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.28571428571428575, 0.0, 0.2857142857142857, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.34782608695652173, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-922", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-1899", "mrqa_triviaqa-validation-4758", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-5514", "mrqa_triviaqa-validation-990", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-5504", "mrqa_triviaqa-validation-4521", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-4255", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6015", "mrqa_hotpotqa-validation-4757", "mrqa_newsqa-validation-3226", "mrqa_newsqa-validation-2224", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-16126", "mrqa_searchqa-validation-884"], "SR": 0.484375, "CSR": 0.5146780303030303, "EFR": 1.0, "Overall": 0.711451231060606}, {"timecode": 66, "before_eval_results": {"predictions": ["Air NEXUS card", "hillsborough", "Buddhism", "Andrew Jackson,", "The Bad Beginning", "dennis crawford", "Red sea", "red", "sesame seeds", "grizzly bear", "adoine Lavoisier", "Swiss", "purney", "battles of the Roses", "terence Edward \" Terry\" Hall", "acetone", "San Francisco", "Paris", "sewing machines", "arthur", "adanagan", "eye", "mozak", "lolita", "peter Principle", "video", "Frank McCourt", "jacky horner", "mark-girl", "blanc manger", "mark", "dauphin", "stand-up", "Happy Birthday to You", "1992", "Pride and Prejudice", "william golding", "ad", "\"bad\"", "Mr. Brainwash", "calypsos", "\"round-eyed\"", "phrenology", "mary Tudor", "peterry", "apple", "driver", "Joan Rivers", "Mr. Humphries", "katherine mansfield", "enduring Slogans", "1987", "1996", "Ella Mitchell", "Valleyfair", "Lerotholi Polytechnic", "3730 km", "mild to moderate depression", "Saturday", "\"green-card warriors\"", "Bering Sea", "charlie of Mecklenburg-Strelitz", "\"The Sacred Wood\"", "Former Beatles"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4869791666666667}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-3268", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6164", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-6825", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-5391", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-1067", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-758", "mrqa_newsqa-validation-150", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-16370", "mrqa_searchqa-validation-12734"], "SR": 0.4375, "CSR": 0.5135261194029851, "EFR": 0.9722222222222222, "Overall": 0.7056652933250415}, {"timecode": 67, "before_eval_results": {"predictions": ["Yuri Andropov", "katherine wollet", "john Mortimer", "Camino Franc\u00e9s", "fox", "Victoria Coren Mitchell", "sound and light", "latte", "tomato", "fred", "wrought iron", "Columba", "peter", "1215", "1937", "12", "Margaret Beckett", "king george IV", "british", "Venice", "adnams", "Massachusetts", "nikkei", "Nutbush, TN,", "robert shumann", "jape", "NASCAR", "Jordan", "b.B. King, Dionne Warwick", "llanfairfechan", "Battle Marengo", "darshaan", "und", "Nicobar", "Blake Griffin", "par-5", "Oklahoma", "Jason Bourne", "Venus", "argon", "protein gluttony", "antelope", "Nevada", "SW19", "Jews of Iberia", "Eva Marie", "gymnastics", "Burma", "gower", "london", "archenemy Black Jack", "Justin Bieber", "2017 season", "eleven", "Club Deportivo Castell\u00f3n, S.A.D.", "East Kn Boyle", "Jan Kazimierz", "2.5 million copies", "Vivek Wadhwa,", "Wednesday.", "parody", "deltas", "Lake Victoria", "air support"], "metric_results": {"EM": 0.5, "QA-F1": 0.5967261904761905}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-565", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-6399", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-4669", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-2021", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-5176", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-506", "mrqa_triviaqa-validation-7710", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3917", "mrqa_searchqa-validation-11044"], "SR": 0.5, "CSR": 0.5133272058823529, "EFR": 1.0, "Overall": 0.7111810661764706}, {"timecode": 68, "before_eval_results": {"predictions": ["fort boyard", "richard seddon", "16", "archers", "st james cephalonia", "Top Cat", "fotheringhay Castle", "tungsten", "New Zealand", "Fenn Street School", "Kristiania", "South Pacific", "kurt hummel", "mozart", "thalia", "paddy mcinness", "woodstock", "Mel Blanc", "Chicago", "jack", "dog sport", "alfresco", "Sarajevo", "Hokkaido", "Norman Mailer", "david boyard", "florence", "apple", "braille", "computer", "stockholm", "george w", "Switzerland", "st. Mark's", "pressure", "Tower of London", "daniel ostroff", "peter boyard", "dr ichak adizes", "1936", "honda", "anoushka", "Dunfermline", "cribbage", "midtown", "fort boyard", "eighth", "osmium", "pear", "white Star Line", "elton john", "peptide bond", "William the Conqueror", "castle", "Gregory Carlton \" Greg\" Anthony", "\"K.Y. or Kentucky\" Jones, D.V.M.,", "Lowe's Companies, Inc.", "India", "Cash for Clunkers", "after Michael Jackson's death", "Nassau", "Maurice Jarre", "degauss", "10 Years"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6875}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-4762", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-3851", "mrqa_triviaqa-validation-5077", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-1862", "mrqa_triviaqa-validation-5611", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-536", "mrqa_triviaqa-validation-7144", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-2760", "mrqa_naturalquestions-validation-3016", "mrqa_hotpotqa-validation-3119", "mrqa_hotpotqa-validation-1129", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-3611", "mrqa_searchqa-validation-8095"], "SR": 0.59375, "CSR": 0.5144927536231885, "EFR": 1.0, "Overall": 0.7114141757246377}, {"timecode": 69, "before_eval_results": {"predictions": ["victorias", "Ronald Searle", "dennis taylor", "Loki", "The Avengers", "snakes", "insulin", "Lilac", "sandra bullock", "frontonasal prominence", "Andes", "banshee", "Hawaii", "seattle-elevation", "heraldry", "The Good Life", "japan", "Ireland", "Sherlock Holmes", "Ida Noddack", "Dudley Do- right", "vindaloo", "senegal", "ABBA", "mark Twain", "Holly Johnson", "steels", "khaki", "birmingham", "joseph w", "dunfermline athletic football club", "4", "joseph caiaphas", "penrhyn", "australia", "African violet", "usselves alone", "james Dean", "Eva Herzigov\u00e1", "drizzle", "chiropractic", "The Wicker Man", "struce Larsson", "jules bow", "butterfly", "boston university", "first website", "Croatia", "cete", "greyfriars", "Mr. chips", "John Locke", "September in the semi-finals", "John Barry", "comic", "Disha Patani", "USS \"Enterprise\"", "Charles Lock", "Kenneth Cole", "the Ku Klux Klan", "annular", "Cher", "the Black Sea", "Crank Yankers"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6536458333333334}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2531", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-4485", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1322", "mrqa_triviaqa-validation-2810", "mrqa_triviaqa-validation-6999", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-5851", "mrqa_triviaqa-validation-335", "mrqa_triviaqa-validation-6456", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-9560", "mrqa_newsqa-validation-3529", "mrqa_searchqa-validation-9522", "mrqa_searchqa-validation-14235"], "SR": 0.59375, "CSR": 0.515625, "EFR": 1.0, "Overall": 0.711640625}, {"timecode": 70, "UKR": 0.69921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3054", "mrqa_hotpotqa-validation-306", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-242", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3680", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-975", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1461", "mrqa_squad-validation-147", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2564", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3473", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3923", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5884", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6670", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-6981", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7083", "mrqa_squad-validation-7094", "mrqa_squad-validation-7339", "mrqa_squad-validation-78", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-9002", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9344", "mrqa_squad-validation-9411", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1514", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1801", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4615", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5117", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6285", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.80859375, "KG": 0.4859375, "before_eval_results": {"predictions": ["australian", "silurian", "r Ricky Gervais", "lord snooty", "lite", "french", "pear", "lyon", "gold", "Tina Turner", "Sparks", "nissan", "washing", "mexico", "Benjamin Britten", "Eric Coates", "st Pancras", "beer", "Toronto", "the Cevennes", "lady gaga", "john skelton", "cc", "1979", "Donald Trump", "oscure", "Tomorrow Never Dies", "tea", "toptenz", "Melbourne", "bullfighting", "Autobahn", "Kiss Me, Kate", "watches", "Hindenburg", "Andre Agassi", "lady penelope creighton- Ward", "Tangled", "spain", "nell raggett", "stockings", "ooperatiou", "smallpox", "her mother", "Leicester City", "violin", "nipples", "lady Thatcher", "the Temple of Artemis", "abietic acid", "Achille Lauro", "Frank Langella", "anion", "a woman who had a sexual relationship with Paul", "Hilo", "Objectivism", "16,116", "president", "Daniel Radcliffe", "Eleven", "nell harriman", "bone marrow", "Miss Smilla", "France"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6165134803921568}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8235294117647058, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5262", "mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-5348", "mrqa_triviaqa-validation-6736", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-5521", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-2633", "mrqa_triviaqa-validation-7099", "mrqa_triviaqa-validation-2648", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-1591", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-2372", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-2351", "mrqa_hotpotqa-validation-4382", "mrqa_newsqa-validation-3688", "mrqa_searchqa-validation-15744", "mrqa_searchqa-validation-11038"], "SR": 0.5625, "CSR": 0.5162852112676056, "EFR": 1.0, "Overall": 0.7020070422535211}, {"timecode": 71, "before_eval_results": {"predictions": ["31536000 seconds", "suez Canal", "robert boyle", "manwick upon Tyne", "Paris catacombs", "john poulson", "breadfruit", "1963", "Richard Strauss", "Leonard Nimoy", "sandi Tok svig", "jules hardouin-Mansart", "robert davis", "bette davis", "Edinburgh International Festival", "australian", "Arabah", "doris gumm", "Ut\u00f8ya island", "lesley Garrett", "Ty Hardin", "b\u00e4umer", "2240 pounds", "dolomites", "Bristol Aeroplane", "charlie parker", "anita Brookner", "a keyhole", "endometriosis", "james stewart", "d", "eight", "Pizza Express", "Lilo & Stitch", "Hugh Quarshie", "Billie Holiday", "khadi boli", "st leger", "Eric Morley", "sandstone Trail", "Assault on Precinct 13", "james chastain", "a barge", "Yemen", "antelope", "relativistic mass", "james bond", "muskets", "bajan", "malted barley", "hoagland", "Jonny Buckland", "Liam Garrigan", "Janie Crawford", "teenitans go!", "Milk Barn Animation", "nursery rhyme", "the Russian air force", "President Nicolas Sarkozy and Canada's Prime Minister Stephen Harper", "100% of its byproducts", "we're Here", "Ivan III Vasilyevich", "kayak", "Wisconsin"], "metric_results": {"EM": 0.359375, "QA-F1": 0.47118055555555555}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.5555555555555556, 1.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-2746", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3511", "mrqa_triviaqa-validation-7290", "mrqa_triviaqa-validation-3506", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-4467", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-4979", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7629", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-5018", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-564", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5346", "mrqa_newsqa-validation-2497", "mrqa_searchqa-validation-14649", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-3830"], "SR": 0.359375, "CSR": 0.5141059027777778, "EFR": 0.926829268292683, "Overall": 0.6869370342140921}, {"timecode": 72, "before_eval_results": {"predictions": ["Cambridge", "contact lenses", "Poland", "washington", "apples", "high jump", "a horizontal desire", "Hungary", "port Talbot", "Gargantua", "sweatshirts", "Not So Much a Programme, More a Way of Life", "australian", "charlie", "smell", "lord nixon", "judy holliday", "cedars", "michael connelly", "Yellowstone", "blue ivy", "edward", "Israel", "blackburn rovers", "1943", "Elizabeth Taylor", "Mercedes-Benz", "d Dublin", "australian", "jimmy hargreaves", "antonia fraser", "peter stuyvesant", "south africa", "libero armani", "Berlin", "mark Twain", "surfer", "The Good Life", "quito", "Sensurround", "roland", "sandown", "goat", "lady", "germany", "bb", "phoenician", "mental floss", "kajagoogoo", "Carly Simon", "robbie holliday", "March 12, 2013", "a wood block struck by a rubber mallet", "into the intermembrane space", "Ronnie Schell", "La Familia Michoacana", "Starlite", "Djibouti.", "cervical cancer", "along the equator between South America and Africa.", "Warsaw", "City Slickers", "ex", "Richa Sharma"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6565972222222223}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1196", "mrqa_triviaqa-validation-7446", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-4148", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-249", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-4498", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-5471", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-180", "mrqa_hotpotqa-validation-1782", "mrqa_newsqa-validation-1370", "mrqa_newsqa-validation-2784"], "SR": 0.59375, "CSR": 0.5151969178082192, "EFR": 1.0, "Overall": 0.7017893835616438}, {"timecode": 73, "before_eval_results": {"predictions": ["edward and the Devil", "c\u00e9vennes", "Lilo & Stitch", "Tacitus", "george best", "loki", "Bagram Collection Point", "pink", "charlie chaplin", "ostrich", "republic", "Louren\u00e7o Marques", "fabric", "swaziland", "cartoons", "Dracula", "jaws", "dodo", "Imola", "albus white", "brazil", "philippine", "united states", "worcester Cathedral", "curvature", "Superman", "wales", "c Copenhagen", "Madison Square Garden", "The Equals", "baffin", "woodstock", "molybdenum", "permian", "Hungary", "apollon", "Matterhorn", "hallmarks", "tide-wise", "Genesis", "trumpet", "south Carolina", "Ourselves alone", "james chadwick", "coffee house", "Apocalypse Now", "fasting", "boris Becker", "althorp", "Pyrenees", "Noah", "Richmond, BC", "October 1, 2015", "Flamborough Head", "25 November 2015", "Jesper Myrfors", "Vancouver", "Rod Blagojevich,", "Mary Procidano,", "opium", "War of the Spanish Succession", "Minnesota", "quid", "stella"], "metric_results": {"EM": 0.625, "QA-F1": 0.7098958333333333}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-80", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-1967", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-605", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-1499", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-3373", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-5018", "mrqa_newsqa-validation-3632", "mrqa_searchqa-validation-7502", "mrqa_searchqa-validation-16012"], "SR": 0.625, "CSR": 0.5166807432432432, "EFR": 0.9166666666666666, "Overall": 0.685419481981982}, {"timecode": 74, "before_eval_results": {"predictions": ["jamaica", "steptoe and son", "jean Baptiste le Roy", "norway", "halloween", "Compiegne", "Samson", "thailand", "element 92 (uranium)", "western Caribbean", "bathtub", "john Napier", "silvergrass", "macbeth", "Eton College", "geomagnetic", "phoebus", "keeper of the Longstone (Fame Islands) lighthouse", "ritchie ltd", "russellius", "titania", "Sphinx", "earl and stitchz", "william morris", "pennsylvania state university", "Father Brown", "Henry Ford", "aircraft", "hydrogen", "rudolph", "rudolph", "Alison Krauss", "raspberries", "4", "neurons", "copenhagen", "banjo", "cricketer", "time bandits", "hague", "one Foot in the Grave", "harry mccain", "copper", "e. T. A. Hoffmann", "the speed camera", "food", "blue", "passport", "florence", "Fancy Dress Shop", "jabba the hutt", "the 15th century", "1937", "turkey", "business", "boxer", "The Handmaid's Tale", "Stanford University", "Sgt. Barbara Jones", "because the Indians were gathering information about the rebels to give to the Colombian military.", "George Babbitt", "Emanuel Swedenborg", "nod", "Norway"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5921875}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-1912", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-4433", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-3070", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-4603", "mrqa_triviaqa-validation-53", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-4192", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-5806", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-957", "mrqa_naturalquestions-validation-2830", "mrqa_hotpotqa-validation-2388", "mrqa_newsqa-validation-2236"], "SR": 0.515625, "CSR": 0.5166666666666666, "EFR": 0.967741935483871, "Overall": 0.6956317204301075}, {"timecode": 75, "before_eval_results": {"predictions": ["rugby", "hyperbole", "North by Northwest", "Danelaw", "mahatma Gandhi", "for Gallantry", "filibustering", "colette", "willow", "eurozone", "Separate Tables", "Percy Spencer", "Ulysses S. Grant", "1929", "caribbean", "Antarctica", "The Hurt Locker", "Gen. Douglas MacArthur", "carl lacey", "zager and Evans", "c\u00e9vennes", "us Can't Dance", "sirhan Sirhan", "handball", "JeSuis Charlie", "gumm sisters", "dark blood", "lowestoft", "washington", "teaching evolution in violation of a Tennessee state law.", "lulu", "erinyes", "faggots", "Godwin Austen", "Angus Deayton", "david bowie", "chester", "tchaikovsky", "faversham", "Jimmy Knapp", "alexius", "norway", "caribbutz", "butcher", "edward Woodward", "priesthood", "violins", "charlie", "eucalyptus", "1883", "Herald of Free Enterprise", "3000 BC", "lacteal", "Renishaw Hall", "Jefferson Memorial", "at least 96", "New Orleans, Louisiana", "JBS Swift Beef Company, of Greeley, Colorado,", "Silicon Valley.", "10-person", "leon to kill Czar Alexander III", "atenolol", "ono", "oracles"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7000270562770563}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-3649", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2328", "mrqa_naturalquestions-validation-10408", "mrqa_hotpotqa-validation-4154", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-8292"], "SR": 0.640625, "CSR": 0.518297697368421, "EFR": 1.0, "Overall": 0.7024095394736842}, {"timecode": 76, "before_eval_results": {"predictions": ["sandra day o'Connor", "a propeller", "Joe Louis", "George Clooney", "Wyeth", "Louvre", "feminism", "potatoes", "Wallace and Gromit", "a cone", "Mozambique", "blue Nile", "troy", "\"Timber!\"", "reptiles", "auction", "cornstarch", "Imaginext", "goods", "Profumo", "Finland", "Making the Band", "Abraham Lincoln", "Colorado", "chiaroscuro", "onomatopoeia", "library of congress", "New Guinea", "ederick bloch", "Georgetown University", "insomnia", "difference", "half", "Andrew Samonsky", "Kennebunkport", "a room with a view", "eyes", "africa", "ingenue", "Notre-Dame de Paris", "human subjects", "peppermint", "I am the adman, goo goo g'joob", "Iberian", "bionic", "tEN 43", "baccarat", "Drums Along the Mohawk", "Wallis Warfield Simpson", "grapevione", "4-ish (American)", "December 14, 2017", "Virginia Dare", "a loanword of the Visigothic word guma `` man ''", "seppuku", "Panama Canal", "will Smith", "1950", "\"Loch Lomond\"", "An aircraft", "Russia and China", "Alwin Landry's supply vessel Damon Bankston", "The Obama administration", "funchal"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6190476190476191}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.0, 0.4, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-16515", "mrqa_searchqa-validation-5372", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-5110", "mrqa_searchqa-validation-8417", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-13605", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12916", "mrqa_searchqa-validation-9899", "mrqa_searchqa-validation-11504", "mrqa_searchqa-validation-4056", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-13006", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10756", "mrqa_searchqa-validation-635", "mrqa_searchqa-validation-5273", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-3521", "mrqa_searchqa-validation-11797", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-3019", "mrqa_triviaqa-validation-5219", "mrqa_hotpotqa-validation-2065"], "SR": 0.515625, "CSR": 0.518262987012987, "EFR": 1.0, "Overall": 0.7024025974025974}, {"timecode": 77, "before_eval_results": {"predictions": ["Graceland", "Bob Fosse", "(Guglielmo) Marconi", "mexico", "Wynton Marsalis", "Volleyball", "Havana", "Edwin Hubble", "Einstein", "Lhasa", "U.S. Census Bureau", "New Kids on the Block", "Manila", "Lady Chatterley", "molasses", "Hard Knock Life", "a crumpet", "(Douglas) MacArthur", "Bill Clinton", "(Ode to Aphrodite)", "Ethiopia", "Texas", "Heart of Darkness", "The Hippocratic Oath", "the Taliban", "Solidarity", "Kookaburra", "the Hastings", "(Tom) Tom", "Craftsman", "Shift", "W.H. Auden", "Chuck Berry", "Cal Ripken Jr.", "the piano's diaphragm", "the Marquis de Sade", "glass", "a tornado", "the joker", "New Zealand", "a glove", "Jutland", "Kindergarten Cop", "tentacles", "Titanic", "San Francisco", "Gulliver's Travels", "a carriage", "Billy Bathgate", "Richmond", "steel", "1988", "Cheryl Campbell", "Ohio newspaper", "Joe Brown", "funchal", "Virgil", "Timothy Dowling", "WANH", "Avoca Lodge", "Campbell Brown", "staff sergeant", "\"procedure on her heart,\"", "55th district"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6515624999999999}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14018", "mrqa_searchqa-validation-7461", "mrqa_searchqa-validation-2395", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-1785", "mrqa_searchqa-validation-16708", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-1687", "mrqa_searchqa-validation-6045", "mrqa_searchqa-validation-1661", "mrqa_searchqa-validation-1963", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14160", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-11728", "mrqa_searchqa-validation-388", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-12470", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-10670", "mrqa_naturalquestions-validation-6665", "mrqa_hotpotqa-validation-5500", "mrqa_newsqa-validation-563", "mrqa_newsqa-validation-2547", "mrqa_hotpotqa-validation-5006"], "SR": 0.59375, "CSR": 0.5192307692307692, "EFR": 1.0, "Overall": 0.7025961538461538}, {"timecode": 78, "before_eval_results": {"predictions": ["Flint, Michigan.", "a president who understands the world today, the future we seek and the change we need.", "5:20 p.m.", "Former Mobile County Circuit Judge Herman Thomas", "The Hubble Space Telescope", "Daniel Radcliffe", "Graham's wife", "Tennessee.", "Intensifying", "asphyxiation and had two broken bones in his neck,", "Dubai", "gun", "Larry King", "Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "the world shut down every year.", "repression and dire economic circumstances.", "African National Congress", "204,000", "Haiti.", "police", "Zuma", "TV", "US Airways Flight 1549", "the estate with its 18th-century sights, sounds, and scents.", "injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "an auxiliary lock", "The Rosie Show", "Ninety-two", "Tuesday,", "St Petersburg and Moscow,", "6-1", "your period is over, maybe the week after your period was done when the breasts are not going to be tender.", "Diego Milito\\'s", "\"Three Little Beers,\"", "love.", "Sen. Evan Bayh", "to block the release of photos showing prisoners allegedly being abused by U.S. personnel in Iraq and Afghanistan,", "Friday.", "a judge to order the pop star\\'s estate to pay him a monthly allowance,", "consumer confidence", "Afghanistan and India", "Hugo Chavez", "Expedia", "these planning processes are urgently needed and have been a long time in coming.", "bronze medal in the women's figure skating final,", "nine", "JBS Swift Beef Company, of Greeley, Colorado,", "as many as 250,000 unprotected civilians", "state senators who will decide whether to remove him from office", "Friday,", "Jaime Andrade", "headdresses", "in cases of the federal death penalty, the power to seek the death penalty rests with the Attorney General", "the Sons of Liberty", "Arkansas", "Adam Smith", "the ozone", "Manchester Victoria station", "communist", "1896", "the ceiling", "Gulliver\\'s Travels", "John Molson", "Sir Adrian Boult"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5551366315376864}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false], "QA-F1": [0.0, 0.9565217391304348, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 0.6399999999999999, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.45454545454545453, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.23076923076923078, 0.0, 0.7058823529411764, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.22222222222222224, 0.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 0.0, 0.1, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-806", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-4205", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-560", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-642", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-7615", "mrqa_hotpotqa-validation-375", "mrqa_searchqa-validation-3681", "mrqa_searchqa-validation-15735", "mrqa_triviaqa-validation-5099"], "SR": 0.40625, "CSR": 0.5178006329113924, "EFR": 0.9736842105263158, "Overall": 0.6970469686875417}, {"timecode": 79, "before_eval_results": {"predictions": ["an Italian and six Africans", "Daniel Radcliffe", "an independent homeland since 1983.", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Samoa", "BET", "basic infrastructure", "Adam Yahiye Gadahn,", "mental health and recovery.", "75.", "co-wrote", "2005.", "12", "Lana Clarkson", "Iran", "attempted burglar", "70,000", "severe flooding", "56,", "frozen world located in the Gaslight Theater.", "AbdulMutallab", "anesthetic and sedative.", "Saturday's Hungarian Grand Prix.", "Sub-Saharan Africa", "Manny Pacquiao", "Aung San Suu Kyi", "Aniston, Demi Moore and Alicia Keys", "modern and classic designs", "Michael Schumacher", "Austin Wuennenberg,", "1983", "the U.S. Holocaust Memorial Museum,", "the college campus.", "African-American", "\"fusion teams,\"", "misdemeanor", "Thursday", "the man facing up, with his arms out to the side.", "golf", "stopping militant rocket fire into Israel.", "prostate cancer,", "\"Iraqi Sadrists march in the Najaf funeral procession of Sayyed Saudi al-Nuri,", "$1.45 billion", "people look at the content of the speech, not just the delivery.", "\"Wax on, wax off", "strategy, plans and policy", "walk", "\"Golden City,\"", "regulators in the agency's Colorado office", "Angela Merkel", "\"Nothing But Love\"", "number of times a pitcher pitches in a season", "Audrey II", "access to US courts", "troposphere", "arthur ashe", "Hippo", "2004", "Tim \"Ripper\" Owens", "Brad Silberling", "Mother Vine", "mars", "the Capitol", "during World War II"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7254243718238284}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 0.0, 0.2, 1.0, 0.1818181818181818, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2835", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-1706", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7987", "mrqa_triviaqa-validation-4441", "mrqa_searchqa-validation-15009"], "SR": 0.671875, "CSR": 0.5197265625, "EFR": 1.0, "Overall": 0.7026953125}, {"timecode": 80, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-512", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11854", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5646", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7035", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-984"], "OKR": 0.78515625, "KG": 0.51640625, "before_eval_results": {"predictions": ["Tyler \"Ty\" Mendoza", "Anne of Green Gables", "four", "professional footballer", "five", "Julie Taymor", "Greg Anthony", "Drifting", "Logar", "Rebirth", "President's Volunteer Service Award", "\"Big\" Harpe,", "Bedknobs and Broomsticks", "Dougray Scott, Jessica De Gouw and Martin McCann", "Yubin, Yeeun", "Herbert Ross", "Elena Verdugo", "melodic hard rock", "49 matches", "Chancellor of Austria", "Taylor Swift", "SARS", "son of writer William F. Buckley Jr. and socialite Patricia Buckley", "1345 to 1377", "Austro-Hungarian Army", "Noel (who had previously only sung lead on B-sides)", "India Today", "North Dakota", "2006", "\"Histoires ou contes du temps pass\u00e9\"", "Charles Reed Bishop", "mixed martial arts", "Yarrow and Stookey", "Mathieu Kassovitz", "Dame Eileen Atkins", "Summerlin, Nevada", "Jean-Marc Vall\u00e9e", "Klasky Csupo", "1950s", "Prussia", "Newfoundland and Labrador", "Tom Kartsotis", "Knowlton Hall", "shock cavalry", "Manhattan", "Professor Frederick Lindemann,", "dementia", "Sandy Bentley", "seven", "January 2001", "\"Vision of Love\"", "5 - 7 teams", "40 %", "March 1, 2018", "piano", "Pink Panther", "G\u00e9rard Depardieu", "is a businessman, team owner, radio-show host and author.", "137", "14", "Rhizo", "Emperor Maximillian", "Lake Michigan", "The Full Monty"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6749391233766233}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.16666666666666669, 0.8571428571428571, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.4, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-1975", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-3898", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-399", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-4131", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-8301", "mrqa_naturalquestions-validation-190", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-1295", "mrqa_searchqa-validation-1349", "mrqa_searchqa-validation-6921"], "SR": 0.5625, "CSR": 0.5202546296296297, "EFR": 1.0, "Overall": 0.704988425925926}, {"timecode": 81, "before_eval_results": {"predictions": ["Amazing Videos", "I Love The White Shadow", "Hungary", "the HIV/AIDS", "Nepal", "singluar", "Sanjaya", "Fauvism", "Dresden", "Turkish", "the Shirley Temple Story", "flavor Flav", "Mike Nichols", "backcountry", "blue blood", "acetylene", "32", "Harriet the Spy", "Illinois", "a drop of Roses lime", "Amsterdam", "Grover Cleveland", "Clyde", "James Naismith", "Godwinson", "North Carolina", "Job", "the Moon", "the Buzz", "pickles", "Stand by Me", "lead", "Nokia", "Bernard Malamud", "Cyprus", "the foot-marker", "Neil Diamond", "Munich", "Babe Ruth", "wildebeest", "Sicilian pizza", "the Caribbean", "tuna", "Arts and Crafts", "lime", "Subclue 2", "Uvula", "Biloxi", "Treasure Island", "Valiant", "a hope chest", "specific brain regions", "Robber baron", "NFL owners", "Jane Seymour", "Guy", "george bernard shaw", "Brian Yorkey", "Tom Ewell", "Wolfgang Amadeus Mozart", "House and Senate Republicans", "seven", "16", "cancerous tumor."], "metric_results": {"EM": 0.65625, "QA-F1": 0.70625}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5160", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-6682", "mrqa_searchqa-validation-6307", "mrqa_searchqa-validation-7539", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-14093", "mrqa_searchqa-validation-13991", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-1314", "mrqa_searchqa-validation-7545", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-15450", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-16773", "mrqa_searchqa-validation-7174", "mrqa_naturalquestions-validation-3840", "mrqa_newsqa-validation-1546"], "SR": 0.65625, "CSR": 0.5219131097560976, "EFR": 1.0, "Overall": 0.7053201219512195}, {"timecode": 82, "before_eval_results": {"predictions": ["Jaws 2", "the leg", "Ovid", "Jimmy", "Yeoman", "Tudor", "Australia", "rum", "sheep", "Washington, D.C.", "lily", "Hammurabi", "the Isle of Wight", "gung ho", "Dale Earnhardt", "Johns Hopkins", "Hashemite Kingdom of Jordan", "Lindsay Davenport", "North Africa", "timpani", "Stephen Hawking", "James Madison", "an beam of light", "Disturbia", "Michael Moore", "The Indianapolis 500", "I, Daniel Blake", "tapping & decided to make it her specialty", "8", "Johannesburg", "carbon", "the Philistines", "deep brain stimulation", "Louis Chevy", "Morocco", "I melt for no one", "Hieronymus Bosch", "Neil Diamond", "Cardinal Richelieu", "Malaysia", "bionic", "Hamlet", "Lance Armstrong", "chicken Fried Steak", "Edith Wharton", "the Berlin Wall", "Uranus", "Frank", "telephone operator", "a bonnet", "Henry Moore", "wintertime", "the Great Crash", "Elena Anaya", "Brisbane Road", "Vinegar Joe", "austria", "Girl Meets World", "three", "\"Pour le M\u00e9rite\"", "The meter reader who led authorities last week to remains believed to be those of Caylee Anthony", "New York-based Human Rights Watch", "used-luxury", "Don Draper"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7323784722222222}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-734", "mrqa_searchqa-validation-7268", "mrqa_searchqa-validation-802", "mrqa_searchqa-validation-8093", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-15533", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-13445", "mrqa_searchqa-validation-14350", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-319", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8837", "mrqa_triviaqa-validation-5807", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2964"], "SR": 0.65625, "CSR": 0.5235316265060241, "EFR": 1.0, "Overall": 0.7056438253012048}, {"timecode": 83, "before_eval_results": {"predictions": ["Atlanta", "Dmitri Mendeleev", "calligraphy", "Kennedy", "Maria Sharapova", "Chile", "glow", "John Waters", "Aristophanes", "the Clean Air Act", "freelance", "Yahoo", "Thurman Munson", "a barrel", "Chippewa", "Rooster Cogburn", "5", "Richard Burton", "gears", "meringue", "The Dying Swan", "the Big Bang", "winter", "Alyssa Milano", "Tahiti", "Herbert Hoover", "Keith Urban", "isosceles triangle", "smoking", "Neil Armstrong", "The Netherlands", "Kelly Clarkson", "Michael Douglas", "aquiline", "Troy weight", "Neil Simon", "Candidate", "trespass", "Ronald Reagan", "Patrick Henry", "the light bulb", "war", "viola", "Ostriches", "I love rock and roll", "the American Mind", "America", "Ziploc", "Hannibal", "Anne Wiggins Brown", "Beethoven", "Gene Barry", "Tachycardia", "American comedy - drama film directed by Fred Schepisi", "the American Civil War", "mexico", "Denise Richards", "India Today", "Distinguished Service Cross", "non-binary", "raping and murdering a woman in Missouri.", "Jeanne Tripplehorn", "Zelaya and Roberto Micheletti,", "New York Islanders"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7880208333333334}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8000000000000002, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4451", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-7651", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-9041", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-16490", "mrqa_searchqa-validation-3952", "mrqa_searchqa-validation-3237", "mrqa_searchqa-validation-6308", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-4354", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-435"], "SR": 0.734375, "CSR": 0.5260416666666667, "EFR": 0.9411764705882353, "Overall": 0.6943811274509804}, {"timecode": 84, "before_eval_results": {"predictions": ["Hoffmann", "Ford", "an enzyme", "Grover\\'s Corner", "President Lincoln\\'s", "topaz", "Universal City", "surrender", "Kathleen Winsor", "subtraction", "Harpy", "Macon, Georgia", "the state whose capital is Annapolis", "fur", "Titan", "the crossword clue", "quick pick", "Fulgencio Batista", "a glaciers", "macram", "Toy Story", "a major boxing event", "the Ark of the Covenant", "the French Legion of Honour", "the Granite State", "the Emperor", "a Klondike", "a dove", "the Jet Propulsion Laboratory", "Francis Scott Key", "Eminem", "Tarzan of the Apes", "Diebold", "Mozzarella", "Puncak Jaya", "Queen Latifah", "the Liberty Bell", "a margherita", "a saint", "Clarence Thomas", "the day of Mars", "naqur", "whimper", "Prison Break", "the Iberian Peninsula", "the ceiling", "a kart", "Kilimanjaro", "the koala", "the Circus World Museum", "Extradition", "1979", "Lou LaRue", "Steve Lukather", "pasta", "his finger", "Robert Schumann", "Pacific Place", "1941", "Lebrey Otis Hampton", "help the convicts find calmness in a prison culture fertile with violence and chaos.", "Polo", "Friday,", "2009"], "metric_results": {"EM": 0.421875, "QA-F1": 0.513095238095238}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.2, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-10313", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-3217", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-14803", "mrqa_searchqa-validation-12869", "mrqa_searchqa-validation-2179", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-12518", "mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-828", "mrqa_searchqa-validation-9830", "mrqa_searchqa-validation-4630", "mrqa_searchqa-validation-8767", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-12943", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-7275", "mrqa_searchqa-validation-14709", "mrqa_searchqa-validation-10541", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-5264", "mrqa_triviaqa-validation-7214", "mrqa_triviaqa-validation-7611", "mrqa_hotpotqa-validation-3149", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-1008", "mrqa_naturalquestions-validation-1856"], "SR": 0.421875, "CSR": 0.5248161764705883, "EFR": 0.972972972972973, "Overall": 0.7004953298887123}, {"timecode": 85, "before_eval_results": {"predictions": ["15", "cancer", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "Picasso's muse and mistress, Marie-Therese Walter.", "a motor scooter", "Jobs", "supermodel", "South Africa captain Graeme Smith", "Missouri.", "AbdulMutallab", "to kill members of the Zetas, a ruthless cartel whose area of influence includes the eastern state of Veracruz.", "the equator,", "at least 13", "detainees of Immigration and Customs Enforcement accuse the agency in a lawsuit of forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "10 a.m.", "acid attack by a spurned suitor.", "Bowie", "accepts defeat Sunday in legislative elections in Buenos Aires.", "a number of calls,", "summer", "Tuesday afternoon,", "Peppermint oil, soluble fiber, and antispasmodic drugs", "Molotov cocktails, rocks and glass.", "Sen. Evan Bayh of Indiana and Virginia Gov. Tim Kaine", "last April,", "2008", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "three", "the L'Aquila earthquake, which killed nearly 300 people and devastated the city when it struck last year,", "At least 22", "Cash for Clunkers", "$50", "cancerous tumor.", "American", "CNN's \"Piers Morgan Tonight\"", "several weeks,", "$1.4 million,", "next year", "died at Fort Bragg on June 12.", "the 11th year in a row.", "explore the world on smaller scales than any human invention has explored before.", "Saturn owners", "three empty vodka bottles,", "eggs", "Alfredo Astiz, a former Navy captain whose boyish looks and deceitful ways earned him the nickname the \"Blonde Angel of Death.\"", "drug cartels", "some of the Awa before killing them with knives.", "two remaining crew members", "Filmmaker Sabina Guzzanti", "400", "Robert Gates", "the 50 states of the United States", "Jennifer Hampton", "to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "161-D-1", "Gaston Leroux", "Volkswagen", "\u00c6thelwald Moll", "Robert \"Bobby\" Germaine, Sr.", "green", "epiphyte", "Re- Animator", "feet", "Girls' Generation"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6236404220779221}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.36363636363636365, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6363636363636364, 1.0, 0.9523809523809523, 0.6666666666666666, 1.0, 0.0, 1.0, 0.19047619047619044, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.19999999999999998, 1.0, 0.5454545454545454, 1.0, 0.8, 0.0, 0.0, 0.1818181818181818, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3186", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-164", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-3537", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9476"], "SR": 0.46875, "CSR": 0.5241642441860466, "EFR": 1.0, "Overall": 0.7057703488372093}, {"timecode": 86, "before_eval_results": {"predictions": ["cement (or concrete)", "the Cayman Islands", "Ferrari", "coercive", "a sonnet", "waived", "China", "loverly", "economics", "Graceland", "funnel", "Beverly Hills", "coffee", "a live young chicken", "gasoline cars", "Isaac Newton", "Billy Budd", "John Milton", "Communist", "Gene Krupa", "diamonds", "Cain", "Smashing Pumpkins", "cruller", "I", "\"Ma\" Barker", "Northanger Abbey", "Wyatt Earp", "\"Star Trek\"", "Mensa", "febreze", "a portrait", "a mutton", "Philip Seymour Hoffman", "a judgment", "Wayne Gretzky", "amu", "Michael Irvin", "Gap", "salt", "Tower Hill", "Arbor Day", "Westinghouse Electric Company", "a bowl", "The Fugitive", "Sisyphus", "Java", "Ponce de Len", "the Doppler shift", "the Renaissance", "the First Barbary War", "Pakistan", "961", "Reba McEntire and Linda Davis", "a dodo", "Northumberland", "Louis XVI", "July 16, 1971", "pastels and oil painting", "9", "monarchy's", "Africa", "Donald Trump", "Jean-Claude Camille Fran\u00e7ois Van Varenberg"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6753348214285714}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16965", "mrqa_searchqa-validation-8424", "mrqa_searchqa-validation-13637", "mrqa_searchqa-validation-15329", "mrqa_searchqa-validation-1300", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-5358", "mrqa_searchqa-validation-12385", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-14681", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-6245", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-13204", "mrqa_searchqa-validation-2161", "mrqa_searchqa-validation-5579", "mrqa_searchqa-validation-15543", "mrqa_searchqa-validation-9723", "mrqa_searchqa-validation-5867", "mrqa_naturalquestions-validation-3672", "mrqa_hotpotqa-validation-680", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1586", "mrqa_hotpotqa-validation-4514"], "SR": 0.609375, "CSR": 0.5251436781609196, "EFR": 1.0, "Overall": 0.7059662356321839}, {"timecode": 87, "before_eval_results": {"predictions": ["John Adams, Benjamin Franklin, Alexander Hamilton, John Jay, Thomas Jefferson, James Madison, and George Washington", "no more than 4.25 inches ( 108 mm )", "the uterus", "empire", "eleven", "the fourth ventricle", "Cody Fern", "five", "in 2007 and 2008", "New York City", "joy", "E longitude, in Mirzapur, Uttar Pradesh,", "rabanada", "Johannes Gutenberg", "256 - acre ( 1.04 km ; 0.400 sq mi ) campus", "Rocky Dzidzornu", "Jennifer Grey", "Experimental neuropsychology", "the slopes of Mt. Hood in Oregon", "April 3, 1973", "Sarah Silverman", "administrative supervision over all courts and the personnel thereof", "1997", "Emma Watson", "near Flamborough Head", "is designed to be shown in tandem with the video for Andr\u00e9 3000's `` Hey Ya! ''", "the President of India", "John J. Flanagan", "the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "the House of Representatives", "Session Initiation Protocol", "bowel obstruction, short bowel syndrome, gastroschisis, prolonged diarrhea regardless of its cause, high - output fistula, very severe Crohn's disease or ulcerative colitis, and certain pediatric GI disorders", "presbyters", "Tessa Peake - Jones", "Katherine Allentuck", "September 19 - 22, 2017", "the 2001 -- 2002 season", "1773", "Randy VanWarmer", "senators", "Teri Garr", "reducing trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors", "13", "10.5 %", "Anne Murray", "the length of suspended roadway between the bridge's towers", "Brad Dourif", "during a game in 1993", "Sanchez Navarro", "in the 1960s", "23 September 1889", "My Fair Lady", "Armageddon", "thais", "\u00c6thelstan", "Eugene", "Highwayman", "Israel", "Former Mobile County Circuit Judge Herman Thomas", "the abduction of minors.", "Dag Hammarskjld", "Erin Go Bragh", "Art Garfunkel", "three"], "metric_results": {"EM": 0.625, "QA-F1": 0.7040907557354926}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9473684210526316, 0.0, 1.0, 0.2631578947368421, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2777777777777778, 1.0, 1.0, 0.0, 0.0, 1.0, 0.46153846153846156, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-6157", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4571", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-7138", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-4707", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3080", "mrqa_searchqa-validation-3012"], "SR": 0.625, "CSR": 0.5262784090909092, "EFR": 0.875, "Overall": 0.6811931818181819}, {"timecode": 88, "before_eval_results": {"predictions": ["2013", "Triple Alliance of Germany, Austria - Hungary, and Italy", "John von Neumann", "Joanne Wheatley", "Alicia Vikander", "the Federated States of Micronesia and the Indonesia ( which consists of thousands of islands )", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "Mickey Rourke", "4.37 light - years ( 1.34 pc ) from the Sun", "9 hours from Coordinated Universal Time ( UTC \u2212 09 : 00 )", "Jack Lord", "12 to 36 months old", "1988", "Malayalam", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "the Old Testament", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "Krypton", "Shawn Wayans", "a crust of mashed potato", "in Christian eschatology", "digestive systems", "state workforce agencies", "end - user", "1973", "reactor core", "Nancy Jean Cartwright", "Germany", "total cost ( TC )", "Justin Timberlake", "1978", "William Whewell", "1956", "G minor", "Thomas Middleditch", "Taittiriya Samhita", "Fix You", "2016", "during initial entry training", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "Andy Kim", "2003", "Kirsten Simone Vangsness", "Ludacris", "vehicles inspired by the Chrysler that are suitable for use on rough terrain", "tissues in the vicinity of the nose", "Yuzuru Hanyu", "Felicity Huffman", "the RAF", "1974", "National Industrial Recovery Act ( NIRA )", "Venus", "Bob Marley & the Wailers", "ronseal", "Gwyneth Paltrow, Ewan McGregor, Olivia Munn, Paul Bettany and Jeff Goldblum", "the 8th and 16th centuries", "Blue Origin", "2,700-acre", "to provide security as needed.", "forcibly injecting them with psychotropic drugs", "Akron", "words", "the Cherokee", "the elections are slated for Saturday."], "metric_results": {"EM": 0.484375, "QA-F1": 0.6140059683383948}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 0.2, 1.0, 1.0, 0.5714285714285715, 0.0, 0.48275862068965514, 1.0, 0.6, 0.5714285714285715, 0.0, 0.5714285714285715, 1.0, 1.0, 0.5245901639344263, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.5, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2272", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-2908", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-6001", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-141", "mrqa_newsqa-validation-2756", "mrqa_searchqa-validation-11445", "mrqa_newsqa-validation-1133"], "SR": 0.484375, "CSR": 0.5258075842696629, "EFR": 0.9696969696969697, "Overall": 0.7000384107933264}, {"timecode": 89, "before_eval_results": {"predictions": ["Paul Lynde", "naos", "1902", "420", "the fourth ventricle", "Dr. Emmett Brown", "Steve Russell", "if the occurrence of one does not affect the probability of occurrence of the other", "full '' sexual intercourse", "the Archies", "the Panamanian government", "to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "Native American nation", "Jay Baruchel", "Plank", "In the early 20th century", "in 1651 by Thomas Hobbes in his Leviathan", "in the red bone marrow of large bones", "John C. Reilly", "Janie Crawford", "New England", "3", "supervillains who pose catastrophic challenges to the world", "in 1932", "March 15, 1945", "reproductive", "the NFL", "Biotic -- Biotic resources are obtained from the biosphere ( living and organic material ), such as forests and animals, and the materials that can be obtained from them", "1975", "the British colonists", "March 1995", "Austin, Texas", "Muhammad", "July 21, 1861", "Harry Potter and the Deathly Hallows", "1980 Summer Olympics boycott was one part of a number of actions initiated by the United States to protest the Soviet invasion of Afghanistan", "9 or 10 national ( significant ) numbers after the `` 0 '' trunk code", "David Joseph Madden", "December 12, 2017", "8 December 1985", "The British Indian Association", "in human physiology", "on the microscope's stage", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F ) at Vostok Station", "The uvea", "a result of a `` no - compete '' clause he was unable to wrestling", "1971", "Moton Field, the Tuskegee Army Air Field", "2002 Mitsubishi Lancer OZ Rally", "Beijing", "off - road vehicles", "Nowhere Boy", "The Beatles", "the kidney", "Luigi Segre", "Ronald Lyle \" Ron\" Goldman", "Adrian Lyne", "543", "eight", "stay on track and get me through prison,\"", "tanks", "the Army of the Potomac", "Horn", "Basketball"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7334363187584727}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.17391304347826084, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.07999999999999999, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.8421052631578948, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-699", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-9003", "mrqa_triviaqa-validation-519", "mrqa_hotpotqa-validation-2410", "mrqa_newsqa-validation-2027", "mrqa_searchqa-validation-1282"], "SR": 0.640625, "CSR": 0.5270833333333333, "EFR": 0.9565217391304348, "Overall": 0.6976585144927536}, {"timecode": 90, "UKR": 0.66015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3812", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8093", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.791015625, "KG": 0.4921875, "before_eval_results": {"predictions": ["Leif Eriksson", "Tiananmen", "Colonel Sebastian", "December 7, 1941", "Milli Vanilli", "Chrysler", "shakyamuni", "Real Madrid", "York", "Norman Hartnell", "A Beautiful Mind", "Red Admiral", "The Meadows", "the Tropic of Capricorn", "Verona", "Ishmael", "Let It Snow!", "Macbeth", "throw", "physics", "poland", "Easter", "Peter Sellers", "febrile", "Milton Keynes", "comets", "1954", "China", "wimpole", "fishes", "Independence Day", "English", "Keane", "Nicolas Sarkozy", "The Princess bride", "mercury", "Jack Ruby", "bacon", "website", "Helen Gurley Brown", "New Zealand", "Groucho Marx", "Exile", "1664", "Shanghai", "Stieg Larsson", "five", "Saskatchewan", "Priam", "Denise Van Outen", "an argument", "19 state rooms", "nasal septum", "September 1493", "Dallas", "four months in jail", "Christmas Day, 2009", "L'Aquila", "Janet and La Toya", "that he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted.", "2", "rain", "I Will Remember You", "Bavarian"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6625}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-2675", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-3122", "mrqa_triviaqa-validation-4163", "mrqa_triviaqa-validation-5187", "mrqa_triviaqa-validation-7317", "mrqa_triviaqa-validation-2576", "mrqa_triviaqa-validation-7092", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-2330", "mrqa_triviaqa-validation-4760", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-3004", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-5474", "mrqa_hotpotqa-validation-4546", "mrqa_newsqa-validation-2839", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-3057"], "SR": 0.59375, "CSR": 0.5278159340659341, "EFR": 1.0, "Overall": 0.6942350618131868}, {"timecode": 91, "before_eval_results": {"predictions": ["diddle", "lusitania", "sebastian moran", "japan", "blind side", "korea", "Imola", "two", "Herald of Free Enterprise", "bridge", "norway", "le Leicester", "dr tasel", "gilt bronze", "yellow", "Burkina Faso", "mortadella", "london", "phil archer", "The Telegraph", "palladium", "leander", "a militia", "aluminium oxide", "dorset", "1802", "Mussolini", "portugal", "wildeve", "eye", "Donald Trump", "ruritania", "a scarlet tanager", "(William) Churchill", "lily alan", "greece", "flofeld", "mozart", "short neck", "portugal", "one hundred pennies", "alberta", "de goya", "brawn", "zipporah", "carousel", "Michael Hordern", "Mary Poppins", "The Quatermass Experiment", "Benjamin Disraeli", "a cappella", "M\u00f6ssbauer spectrometer", "cases that have not been considered by a lower court may be heard by the Supreme Court in the first instance under what is termed original jurisdiction", "2014", "Cielos del Sur S.A.", "tragedy", "Eastern College Athletic Conference", "in the emergency room at LakeWest Hospital in Cleveland,", "\"The Cycle of Life\"", "July 23.", "a tartar sauce", "enron", "mercury", "The 2010 eruptions of Eyjafjallaj\u00f6kull"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5770585317460317}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-4425", "mrqa_triviaqa-validation-6622", "mrqa_triviaqa-validation-684", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-3194", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-3425", "mrqa_triviaqa-validation-7358", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-409", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-1023", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8092", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-2585", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2013", "mrqa_naturalquestions-validation-5300"], "SR": 0.515625, "CSR": 0.5276834239130435, "EFR": 1.0, "Overall": 0.6942085597826086}, {"timecode": 92, "before_eval_results": {"predictions": ["stanley tromp", "indonesia", "bulgaria", "diamondbacks", "Dan Dare", "rudolph", "fat", "Singapore", "birds", "Stephenie Meyer", "Hebrew", "heisenberg", "carlsberg", "Cumberland", "Billy Connolly", "spanish", "kiel Canal", "australia", "faggots", "philadelphia", "Jeffery Deaver", "John Flamsteed", "pangram", "croquet", "kinks", "Spearchucker", "reservoirs", "Botticelli", "giraffe", "arthur", "Renaissance", "south africa", "blackfriars", "philadelphia", "pet sounds", "Jeremy Thorpe", "haute", "arthur", "paris", "pouch", "kiki", "sarah mitchell", "sheep", "st. Petersburg", "stronaut", "poirot", "three", "pariolis", "Harley", "stanley macLaine", "kipps: The Story of a Simple Soul", "in the National Basketball Association", "the NFL", "Identity Theory", "Melbourne Storm", "A Rush of Blood to the Head", "1993", "1994", "Derek Mears", "11 healthy eggs", "a span", "Lake Pontchartrain", "on a raised platform", "on 4,000 credit cards and the company's \"private client\" list,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6427083333333333}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, false, false], "QA-F1": [0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-606", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-3842", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-6743", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-5230", "mrqa_triviaqa-validation-525", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-4934", "mrqa_triviaqa-validation-1601", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-1507", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4023", "mrqa_searchqa-validation-4717", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-6744", "mrqa_newsqa-validation-3021"], "SR": 0.5625, "CSR": 0.5280577956989247, "EFR": 0.9285714285714286, "Overall": 0.6799977198540706}, {"timecode": 93, "before_eval_results": {"predictions": ["k", "h Hercules", "spain", "japan", "philip marlowe", "Elektra King", "thumper", "Atomic Kitten", "peter davison", "sinus node", "red", "oscar", "Dutch", "indonesia", "spain", "spain", "paul reubens", "gluteal region", "Majorca (Mallorca)", "12", "carry on Cleo", "doretta mavrakis", "h Hector BERLIOZ", "arthurian", "spain", "st aidan", "john Virgo", "Richard Seddon", "mole", "stiefbeen", "Prince Edward, Earl of Wessex", "parma", "cryonic suspension", "human", "puffer fish", "sodor", "the Porteous Riots", "willie nelson", "giovanni bologna", "wittle", "Sicilia", "jupiter", "travolta", "spain van der Straeten Ponthoz", "eleanora Olive", "australia", "Essex County Cricket Club", "k Kaiser Chiefs", "Israelites", "nicolas cage", "philip catelinet", "long - standing policy of neutrality was tested on many occasions during the 1930s", "April 1917", "Saint Alphonsa", "Takura Tendayi", "Whitesnake", "310", "Robert Barnett,", "voice-assistant software", "the conversion -- which had been planned for years -- to accommodate people like Richter who had not been able to update their TVs.", "an orchid", "UNESCO", "lira", "Joanna Moskawa"], "metric_results": {"EM": 0.4375, "QA-F1": 0.483309766214178}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.5882352941176471, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.07692307692307693, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2634", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-4113", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-4429", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4135", "mrqa_triviaqa-validation-5975", "mrqa_triviaqa-validation-4323", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-6771", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-389", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-4211", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-3276", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-1424"], "SR": 0.4375, "CSR": 0.527094414893617, "EFR": 0.9722222222222222, "Overall": 0.6885352024231678}, {"timecode": 94, "before_eval_results": {"predictions": ["the key of Eb", "Frida Khalo", "the Kite Runner", "Pope John Paul II", "Louisa May Alcott", "Rock Island", "Turandot", "the Bolsheviks", "cloning", "Signs", "Edward", "wild driving", "The Police", "a Carrots", "Manhattan", "Rehab", "a ballpoint pen", "tap", "Ernie Banks", "Christopher Columbus", "Olivia Newton-John", "the great white way", "shrewd", "virginia", "peter shaffer", "the Ubangi River", "(William) Harvey", "reptile", "gizzard", "Bangkok", "the Reform Party", "Catwoman", "bats", "Puccini", "Omaha", "the Monitor", "magnesium", "silver", "save the original Star Trek", "the Takana", "the Silk Road", "dreams", "Google", "Jack Ruby", "\"Hairspray\"", "Duncan", "a palace", "an ABO", "Italy", "green", "Engelbert Humperdinck", "1940", "three", "when calls the heart is a Canadian - American television drama series", "the Velvet Revolution", "taka", "Hampton Court Palace", "20th Century Fox", "three", "Donald Wayne Johnson", "Marcell Jansen", "28 of them seriously enough to go to a hospital,", "African-Americans", "rocket"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7213541666666666}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9550", "mrqa_searchqa-validation-10265", "mrqa_searchqa-validation-5390", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-14368", "mrqa_searchqa-validation-12939", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-3284", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-5868", "mrqa_naturalquestions-validation-4594", "mrqa_hotpotqa-validation-4423", "mrqa_hotpotqa-validation-4578", "mrqa_newsqa-validation-2068"], "SR": 0.703125, "CSR": 0.5289473684210526, "EFR": 1.0, "Overall": 0.6944613486842105}, {"timecode": 95, "before_eval_results": {"predictions": ["Bobby Jones", "\"Rikki-Tavi\"", "Staten Island", "MEXICO", "Peter Paul Rubens", "(Tom) Parker", "Belgium", "zendo", "cholesterol", "his only begotten Son", "the Dolphins", "a surface-to-air missile", "a knife", "Ms. VALSECA", "Northern Exposure", "Pocahontas", "Easy Rider", "the East River", "Blessed", "Ned Kelly", "Batavia", "the Cherokee", "Jim Bunning", "brood", "John F. Kennedy", "Arby\\'s", "Einstein", "a bacterio", "VICTOR HUGO", "fudge", "\"G\"s", "a cattle prod", "Robert Fulton", "stimulation", "an egg", "Ken Russell", "The Crucible", "the Laborers' International Union", "zenith", "apogee", "Calais", "semaphore", "a reverse", "Coors Field", "\"TARzan and the Jewels of Opar\"", "\"All for Our Country\"", "agatherer", "the former president of Liberia", "3.14159", "Rings Twice", "Kansas City", "In 1979", "Mockingjay -- Part 2 ( 2015 )", "Massachusetts", "Illinois", "South Africa", "Darth Vador", "George Adamski", "five", "Marine Corps", "two", "\"black box\"", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "black"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6343276515151515}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-117", "mrqa_searchqa-validation-4090", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-6979", "mrqa_searchqa-validation-664", "mrqa_searchqa-validation-4716", "mrqa_searchqa-validation-16678", "mrqa_searchqa-validation-7412", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-9198", "mrqa_searchqa-validation-3964", "mrqa_searchqa-validation-11984", "mrqa_naturalquestions-validation-5799", "mrqa_naturalquestions-validation-1427", "mrqa_triviaqa-validation-3778", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3198"], "SR": 0.546875, "CSR": 0.5291341145833333, "EFR": 1.0, "Overall": 0.6944986979166666}, {"timecode": 96, "before_eval_results": {"predictions": ["Flickr", "Eric Angat", "air", "Leontyne Price", "a dragonfly", "King Charles I", "Casey Kasem", "San Juan", "sheep", "The witches of Eastwick", "Joseph Smith", "the Rose", "May", "a chocolate chip", "Cyrano de Bergerac", "Alaska", "birds", "the European Union", "Giuseppe Verdi", "Louis C.K.", "the Kremlin", "Muqtada al-Sadr", "Frogs", "heracles", "a clerk", "Scotland", "ACL", "the Sacred Cod", "Agatha Christie", "get your house back", "Esther", "cat scratch fever", "New Kids on the Block", "Mosul", "country", "The Crucible", "Abraham Lincoln", "a parasol", "Moriarty", "Simon Cowell", "potassium", "Lenin", "a fruitcake", "nests", "the Firebird", "Kansas", "a radical", "Air France", "Louis Brandeis", "plaque", "David", "the 1979 -- 80 season", "Dollree Mapp", "Tom Brady", "Dick Whittington", "leeds", "gloster", "1828", "Elijah Wood", "Dizzy Dean", "The Ski Train", "Hezbollah.", "a rapist", "skirts"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7046875}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-3073", "mrqa_searchqa-validation-13498", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-1921", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-862", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-12287", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4540", "mrqa_searchqa-validation-16138", "mrqa_searchqa-validation-15257", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9745", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-6912", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-3093", "mrqa_hotpotqa-validation-4621", "mrqa_newsqa-validation-1402", "mrqa_newsqa-validation-1176"], "SR": 0.609375, "CSR": 0.5299613402061856, "EFR": 1.0, "Overall": 0.6946641430412371}, {"timecode": 97, "before_eval_results": {"predictions": ["All-New Blue Ribbon", "a pig", "Fear of Flying", "War Admiral", "Abraham Lincoln", "the B horizon", "McDuck", "Czechoslovakia", "Roussimoff", "Maria Callas", "Buddhism", "Roosevelt", "inundation", "Cold Mountain", "murder mysteries", "boxing", "A Night at the Roxbury", "King Henry II", "Claddagh", "Keith Richards", "the Hydra", "Walter Cronkite", "Central Park Zoo", "Marcia Clark", "the Lincoln Tunnel", "anbatross", "Bob Fosse", "assertion", "Georgia", "Nixon", "Madame Tussaud", "\"Cloverfield\"", "Shakespeare", "Mother Jones", "Dr. King", "HatfieldMcCoy", "Walter Scott", "Pig Latin", "the Nile", "the Department of Transportation", "a mutton", "Latin", "New Wave", "Patrick Ewing", "Prague", "Genesis of Germs", "New Orleans", "Parody", "ccoli", "arteries", "Carol Burnett", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "in a legal case in certain legal systems", "9 February 2018", "18", "Malcolm Thorpe", "blue", "A Song of Ice and Fire", "British Labour Party", "Balvenie Castle", "Animal Planet", "St. Louis, Missouri.", "the strawberry,\"", "ITV"], "metric_results": {"EM": 0.53125, "QA-F1": 0.632986111111111}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true], "QA-F1": [0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-15628", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-5057", "mrqa_searchqa-validation-3194", "mrqa_searchqa-validation-13412", "mrqa_searchqa-validation-15943", "mrqa_searchqa-validation-16528", "mrqa_searchqa-validation-8337", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6243", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-4255", "mrqa_searchqa-validation-16590", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-8347", "mrqa_searchqa-validation-13547", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-2490", "mrqa_triviaqa-validation-1589"], "SR": 0.53125, "CSR": 0.5299744897959184, "EFR": 1.0, "Overall": 0.6946667729591838}, {"timecode": 98, "before_eval_results": {"predictions": ["Franklin, Indiana", "Nelson County", "47", "Sir Hiram Stevens Maxim", "15 October 1988", "5 February 1976", "Boston Celtics", "Hermione Baddeley", "45,698", "Andries Jonker", "Ashanti Region of Ghana", "Groupe PSA", "Texas Tech", "brigadier general", "Omega SA", "South Australia", "\"Apatosaurus\"", "Resorts World Genting", "the Beatles", "British", "1950", "six", "Future", "London", "Cuyler Reynolds", "New York City", "Toxics Release Inventory", "Figaro", "South African", "Alleyne v. United States", "Bambi: Eine Lebensgeschichte aus dem Walde", "close to 50 million", "Whoopi Goldberg", "Transporter 3", "Disco", "Frederick I", "Afghanistan", "Thrillers", "Antonio Lippi", "March 30, 2025", "English", "Azeroth", "Isabella II", "McG", "Vitor Vieira Belfort", "11 November 1821", "villanelle", "Emilia-Romagna", "Who's That Girl", "Alan Young", "Cristiano Ronaldo", "Virginia Dare", "David Tennant", "inefficient", "Simeon Williamson", "Frobisher Bay", "Margaret Smith Court", "Alwin Landry's", "World leaders", "the Sri Lankan cricket team in the Pakistani city of Lahore.", "a mainsail", "a banana", "Tallahassee", "Brooke Wexler"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6382812499999999}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-4963", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1669", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-3578", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-4088", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-2364", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-3420", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-1102", "mrqa_searchqa-validation-5295", "mrqa_searchqa-validation-3489"], "SR": 0.546875, "CSR": 0.530145202020202, "EFR": 1.0, "Overall": 0.6947009154040404}, {"timecode": 99, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5687", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-1037", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-1440", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15285", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15680", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3057", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4454", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1734", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2791", "mrqa_triviaqa-validation-2844", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.794921875, "KG": 0.51328125, "before_eval_results": {"predictions": ["Robert Arthur Mould", "1926", "Antonio Lippi", "the 1993 election", "The Allies of World War I", "Logan International Airport", "1979", "Best Sound", "The Suite Life of Zack & Cody", "Switzerland", "2017", "Stern-Plaza", "Pakistan", "jazz homeland section of New Orleans", "Michael Schumacher", "Darkroom", "1972", "Royce da 5'9\" (Bad) and Eminem (Evil)", "evangelical Christian periodical", "Lionel Eugene Hollins", "water", "Harlem neighborhood of New York City", "Sophia Charlene Akland Monk", "Love Actually", "Commanding General", "George Orwell", "five", "20 March to 1 May 2003", "1993", "imp My Ride", "1886", "the Swiss Confederation has adopted various provisions of European Union law in order to participate in the Union's single market", "Syracuse", "Godspell", "1755", "The Big Bang Theory", "1966", "Johnny Herbert", "Annales de chimie et de physique", "British", "punk rock", "Atlantic Ocean", "Adelaide Laetitia \" Addie\" Miethke", "Red", "Ted Bundy", "Matt Kemp", "Epic Records", "Roslyn Castle", "2.1 million members", "the Rothschild banking dynasty", "Corendon Airlines", "Claudia Grace Wells", "Hugo Weaving", "Freedom Day", "Misery", "Henry Hudson", "geometry", "Adidas", "whether to close some entrances, bring in additional officers, and make security more visible.", "the 3rd Platoon,", "Lebanon", "cherries", "(William) Patrick", "bullnose"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7236877705627704}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-5238", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-1711", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-3154", "mrqa_hotpotqa-validation-1822", "mrqa_hotpotqa-validation-735", "mrqa_naturalquestions-validation-9150", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1859", "mrqa_searchqa-validation-16845", "mrqa_searchqa-validation-14664"], "SR": 0.640625, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.7073437499999999}]}