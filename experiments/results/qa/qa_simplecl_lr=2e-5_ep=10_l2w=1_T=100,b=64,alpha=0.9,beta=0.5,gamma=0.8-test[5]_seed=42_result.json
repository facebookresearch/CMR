{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/simplecl/qa_simplecl_lr=2e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=42', diff_loss_weight=1.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=2e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=2e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=42_result.json', stream_id=5, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 1640, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a plug valve", "1550", "French Louisiana west of the Mississippi River", "2012", "carbon dioxide", "the Lisbon Treaty", "all colors", "in the chloroplasts of C4 plants", "An attorney", "democracy", "The Greens", "third", "Enthusiastic teachers", "expositions", "no French regular army troops were stationed in North America", "estates of the Holy Roman Empire", "Stromatoveris", "2011", "Louis Pasteur", "the owner", "Time Lord", "mosaic floors", "economic", "1893", "environmental factors like light color and intensity", "Gandhi", "deforestation", "Middle Rhine Valley", "pump this into the mesoglea", "low-light conditions", "No Child Left Behind", "one way streets", "\u20ac25,000 per year", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "unbalanced torque", "Ulaanbaatar", "power", "very weak", "Judith Merril", "Gender pay gap", "the Ilkhanate", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "University of Chicago campus", "3D printing technology", "1957", "2000", "a certain number of teacher's salaries are paid by the State", "the Dutch Republic", "San Jose Marriott", "April 20", "the Commission", "evacuate the cylinder", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "Hurricane Beryl", "temperature and light", "terra nullius", "growth", "human", "the \u2018combs\u2019", "1978", "non-Catholics", "Sanders", "vice president", "700 employees"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8576388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10143", "mrqa_squad-validation-8841", "mrqa_squad-validation-2145", "mrqa_squad-validation-739", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-7449", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-9764", "mrqa_squad-validation-7051"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["1986", "Peridinin", "standardized", "50% to 60%", "Stromatoveris", "lower incomes", "Fort Duquesne", "Katharina von Bora", "Miller", "women", "Three's Company", "Frank Marx", "architect or engineer", "$2 million", "superintendent of New York City schools", "Santa Clara, California", "Kingdom of Prussia", "Asian Economic Tigers", "Palestine", "Aristotle and Archimedes", "in the chloroplasts of C4 plants", "Outlaws", "increased blood flow into tissue", "Edgar Scherick", "14th to the 19th century", "Gibraltar and the \u00c5land islands", "Evangelical Lutheran Church", "oxygen", "the BBC National Orchestra of Wales", "Thanksgiving", "the founding of new Protestant churches", "Associating forces with vectors", "Venus", "those who proceed to secondary school or vocational training", "zoning and building code requirements", "Ikh Zasag", "Central Bridge", "modern-day Canada", "William Tyndale", "1935", "seven", "Grumman", "1191", "Maciot de Bethencourt", "Euclid", "case law by the Court of Justice", "long, slender tentacles", "mesoglea", "1970s", "white", "misguided", "2014", "Reconstruction and the Gilded Age", "European Parliament and the Council of the European Union", "members in good standing with the college", "Manakin Episcopal Church", "Nicholas Stone", "ongoing tectonic subsidence", "the release of her eponymous debut album the following year", "a form of business network", "The Senate ( north ) wing was completed in 1800", "The euro is the result of the European Union's project for economic and monetary union", "Djokovic", "\"colorful\" mercenary group fought for Padua, Florence & other Italian city-states"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8498368818681319}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2, 0.15384615384615385, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7332", "mrqa_squad-validation-8459", "mrqa_squad-validation-10339", "mrqa_squad-validation-3021", "mrqa_squad-validation-3946", "mrqa_squad-validation-1906", "mrqa_squad-validation-5588", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1187", "mrqa_searchqa-validation-2579"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 2, "before_eval_results": {"predictions": ["Lek", "prohibited emigration", "The Private Education Student Financial Assistance", "lower-paid", "Labor", "time and storage", "special efforts", "rhetoric", "British", "a year", "Genghis Khan", "a supervisory church body", "77", "a cubic interpolation formula", "King Sigismund III Vasa", "1835", "the exploitation of the valuable assets and supplies of the nation that was conquered", "poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies", "five", "liquid oxygen", "Gosforth Park", "Metropolitan Police Authority", "18 February 1546", "1996", "1.7 billion years ago", "Mike Carey", "coal", "18 February", "Stanford University", "1976", "LOVE Radio", "political assassins", "Khasar", "Sky Digital", "99.4", "about a third", "the issue of laity having a voice and vote in the administration of the church", "1995", "Endosymbiotic gene transfer", "avionics, telecommunications, and computers", "linebacker", "feed water", "Sir Edward Poynter", "oxygen", "August 1967", "Velamen parallelum", "terrorist organisation", "three", "Lowry Digital", "the worst-case time complexity", "24 September 2007", "33", "Buffalo Lookout", "Missouri", "The User State Migration Tool", "1773", "Onsan", "October 6, 2017", "11 p.m. to 3 a.m", "Haliaeetus", "Sir Henry Bartle Frere", "Calpurnia's son", "through the weekend", "Tom Hanks"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8567708333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7357", "mrqa_squad-validation-1672", "mrqa_squad-validation-6664", "mrqa_squad-validation-6171", "mrqa_squad-validation-7871", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-430"], "SR": 0.84375, "CSR": 0.8333333333333334, "EFR": 1.0, "Overall": 0.9166666666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["female", "1884", "Sayyid Abul Ala Maududi", "a family member", "James E. Webb", "Lutheran and Reformed", "phycoerytherin", "understaffed", "swimming-plates", "10 July 1856", "130 million cubic foot", "teleforce", "Heinrich Himmler", "34\u201319", "Baptism", "Decision problems", "without markings", "1957", "The Day of the Doctor", "Muhammad Khan", "Sun Life Stadium", "the Council", "February 9, 1953", "March", "sea gooseberry", "1961", "Trio Tribe", "Dai Setsen", "the Late Medieval Catholic Church", "January 1979", "phagocytic", "Rankine cycle", "$2.2 billion", "Seine", "theory of general relativity", "15 February 1546", "Marquis de la Jonqui\u00e8re", "BBC Dead Ringers", "Kenyans for Kenya", "Fresno", "Saudi", "the Presiding Officer", "an intuitive understanding", "default emission factors", "Inherited wealth", "Michael P. Millardi", "Goldman Sachs", "the BRAAVOO website", "Room Kids and Bookcase Closet", "the central U.S. state", "praying in Latin", "the children were nestled all snug in their beds", "the U.N. organization", "the Leyden jar", "a list of the subjects that candidates", "the last two of these had libretti by Gaetano Rossi", "Droll Stories: Collected from the Abbeys of Touraine", "70%", "the November 26, 2014", "joining our Keepsake Ornament Club", "the British", "early 1960s", "April 1917", "poor hygiene"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7183035714285715}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-8595", "mrqa_squad-validation-7525", "mrqa_squad-validation-2595", "mrqa_squad-validation-5262", "mrqa_squad-validation-3863", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-10156"], "SR": 0.671875, "CSR": 0.79296875, "EFR": 0.9523809523809523, "Overall": 0.8726748511904762}, {"timecode": 4, "before_eval_results": {"predictions": ["boom-and-bust cycles", "Prince of P\u0142ock", "hormones", "1840", "occupational stress", "in the parts of the internal canal network under the comb rows", "in no way contributes to faith", "Tesla Electric Company", "African-American", "Thomson", "1905", "\"Nun komm, der Heiland\"", "John Fox", "in all health care settings", "cut in half", "the study of rocks", "colonies", "lower wages", "geophysical surveys", "Huguenots", "\"degrees of privilege\"", "2,900 kilometres (1,802 mi)", "Elie Metchnikoff", "an algorithm", "Immediately after Decision Time", "Confucian propriety and ancestor veneration", "25-minute", "eight", "elude host immune responses", "Pusey Library", "inequality", "designs", "cytokines", "requiring his arrest", "wide sidewalks", "other members", "Air Force", "an occupancy permit", "reactive allotrope of oxygen", "Nederrijn", "a multi-cultural city", "pump water out of the mesoglea", "Tim Johnson", "Australia", "elected from the citizens of the jurisdiction in which they serve", "a mathematical model", "Henry Purcell", "Ram Nath Kovind", "embryo", "Cheap trick", "Sandy Knox and Billy Stritch", "Hudson Bay", "Lee Freedman", "a bow bridge with 16 arches shielded by ice guards", "from 1922 to 1991", "Nicole Gale Anderson", "1", "sedimentary", "Mrs. Wolowitz", "plate tectonics", "Columbia", "Isabella II", "Kris Allen", "UNESCO"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7357165404040404}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2463", "mrqa_squad-validation-2456", "mrqa_squad-validation-6319", "mrqa_squad-validation-7338", "mrqa_squad-validation-2943", "mrqa_squad-validation-6957", "mrqa_squad-validation-3497", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-4002", "mrqa_triviaqa-validation-5855", "mrqa_searchqa-validation-172"], "SR": 0.65625, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 5, "before_eval_results": {"predictions": ["former flooded terraces", "beginning of the 20th century", "1974", "ABC", "dictatorial", "Ben Johnston", "quantum mechanics", "Book of Exodus", "Synthetic aperture radar", "Mission Impossible", "patients' prescriptions and patient safety issues", "No, that's no good", "1697", "3 January 1521", "magma", "a \"principal hostile country\"", "Jan Hus", "Newton", "Croatia", "2011", "Swynnerton Plan", "machine gun", "Theatre Museum", "August 10, 1948", "they are distinct or equal classes", "the 2004 Treaty establishing a Constitution for Europe", "Serge Chermayeff", "Thomas Edison", "Mnemiopsis", "the flail of God", "Woodward Park", "Melbourne Cricket Ground", "Wednesdays", "most common", "up to a thousand times as many", "tears and urine", "six", "plants and algae", "Republic Day", "1913", "Yuzuru Hanyu", "Konakuppakatil Gopinathan Balakrishnan", "1942", "March 2016", "Texas, Oklahoma, and the surrounding Great Plains", "a balance sheet", "Mayor Hudnut", "1995", "William the Conqueror", "1922", "an anembryonic gestation", "Bemis Heights", "9pm ET ( UTC - 5 )", "twice", "S\u00e9rgio Mendes", "Lituya Bay in Alaska", "Sarah", "routing protocols", "The euro", "Latin ultra, \"beyond violet\"", "2000", "KCNA", "Chin", "Rodgers & Hammerstein"], "metric_results": {"EM": 0.625, "QA-F1": 0.6955538799968148}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3076923076923077, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5818", "mrqa_squad-validation-1827", "mrqa_squad-validation-1566", "mrqa_squad-validation-10388", "mrqa_squad-validation-3770", "mrqa_squad-validation-1780", "mrqa_squad-validation-3985", "mrqa_squad-validation-4572", "mrqa_squad-validation-6439", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-2749", "mrqa_newsqa-validation-2404", "mrqa_searchqa-validation-14371"], "SR": 0.625, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 6, "before_eval_results": {"predictions": ["four", "2 million", "ranges from 53% in Botswana to -40% in Bahrain", "Pliocene", "relationship between teachers and children", "LeGrande", "sixth", "Jason Bourne", "7.8%", "60,000", "University of Chicago College Bowl Team", "decline of organized labor", "Santa Clara Marriott", "oxygen chambers", "two", "two catechisms", "Cologne", "1991", "Silk Road", "Surveyor 3", "145", "growth and investment", "the centers were computer service bureaus, offering batch processing services", "Vampire bats", "antiforms", "still be standing", "weight", "as \"Genghis Khan's Mongolia\"", "oil was priced in dollars, oil producers' real income decreased", "Beyonc\u00e9 and Bruno Mars", "a university or college", "More than 1 million", "pseudorandom number generators", "Japan", "Coriolis force", "Mickey Rourke", "May 2016", "Nicklaus", "Superstition Mountains", "Panama Canal Authority", "silk, hair / fur ( including wool ) and feathers", "Convention", "two", "Sebastian Vettel", "April 10, 2018", "Gorakhpur", "How I Met Your Mother", "directly elected", "December 15, 2016", "James Hutton", "Jourdan Miller", "1991", "Mandy '' Moore", "Denmark", "Broken Hill and Sydney", "159", "China (formerly the Republic of China ), Russia ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "Judiththia Aline Keppel", "medellin", "Crown Holdings Incorporated", "Expedia", "the Large Orbiting Telescope or Large Space Telescope", "an underground parking garage near the L.A. County Museum of Art", "us to step up"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7172361492673993}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.07692307692307693, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.25, 0.25, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7445", "mrqa_squad-validation-6965", "mrqa_squad-validation-5435", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_squad-validation-6228", "mrqa_squad-validation-3718", "mrqa_squad-validation-9161", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-6106", "mrqa_searchqa-validation-10372", "mrqa_newsqa-validation-1510"], "SR": 0.640625, "CSR": 0.7276785714285714, "EFR": 0.8695652173913043, "Overall": 0.7986218944099379}, {"timecode": 7, "before_eval_results": {"predictions": ["Director", "travel literature, cartography, geography, and scientific education", "oxygen chambers", "Graham Gano", "Two", "In 1066", "2008", "Mojave Desert", "Operating System Principles", "St. Lawrence and Mississippi watersheds", "27%", "4000", "Rhine Gorge", "Helicoid stromal thylakoids", "highest", "impact process effects", "schools in some Asian, African and Caribbean countries", "Warner Bros. Presents", "pharmacists", "high-voltage", "4:51", "Kabaty Forest", "the seal of the Federal Communications Commission", "strong sedimentation", "The European Commission", "SAP Center in San Jose", "respiration", "352 votes", "eliminate the accusing law", "October 6, 2004", "The Day of the Doctor", "Pakistan", "November 1999", "September 6, 2019", "English", "During the fourth season", "just once", "Nick Kroll", "comedy web television series", "Billy Gibbons", "an apprentice", "the brain", "31", "1970s", "the U.S. State Department", "Art Carney", "accomplish the objectives of the organization", "in the water", "December 1922", "Category 4", "September 2017", "3 September", "silk floss tree", "Terrell Owens", "since 3, 1, and 4", "five", "Dolph Lundgren", "Hampton Court Palace", "Sela Ward", "Alice Horton", "Amateur Radio", "Benjamin Britten", "an isosceles triangle", "the NOW Magazine"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6903521825396826}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4629", "mrqa_squad-validation-1938", "mrqa_squad-validation-6409", "mrqa_squad-validation-451", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2112", "mrqa_triviaqa-validation-6548"], "SR": 0.65625, "CSR": 0.71875, "EFR": 0.9545454545454546, "Overall": 0.8366477272727273}, {"timecode": 8, "before_eval_results": {"predictions": ["cytokines", "William Pitt", "North Carolina and New Mexico", "the p-adic norm", "Hassan al Banna", "Gottfried Fritschel", "the Eighth Doctor", "singular plastoglobulus", "pound-force", "the Ming dynasty", "Dorothy and Michael Hintze", "The Small Catechism", "36%", "Italian sculptors", "April 20", "biomass", "their belief in the validity of the social contract", "K MJ-TV", "Foreign Protestants Naturalization Act", "mainly in the southern and central parts of France", "10%", "not designed to fly through the Earth's atmosphere", "Metro Trains Melbourne", "BBC 1", "$2 million", "Vince Lombardi Trophy", "Galileo", "in linked groups or chains", "meaning", "The Tiber", "1885", "James Madison", "Ryan Pinkston", "federal republic", "lacteal", "21 June 2007", "foreign investors", "Comanche", "8ft", "Bartolomeu Dias", "William Wyler", "1961", "March 1930", "Julie Adams", "Thomas Jefferson", "Majo to Hyakkihei 2", "January 1, 2016", "USCS or USC", "Bud Light", "Sunday night", "Billy Hill", "Mara", "Malina Weissman", "September 6, 2019", "1773", "A lacteal", "April 26, 2005", "Jennifer Eccles and her terrible freckles", "Albert", "a bullet has reportedly entered his head through his right eye, and he remains in a coma in a grave condition", "Croatia", "Drew Kesse", "six", "they were part of a group of 20 similar cars making an annual road trip"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6474020337301587}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.7777777777777778, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.1904761904761905, 0.0, 0.0, 0.0, 0.375]}}, "before_error_ids": ["mrqa_squad-validation-8958", "mrqa_squad-validation-7876", "mrqa_squad-validation-8786", "mrqa_squad-validation-5724", "mrqa_squad-validation-6706", "mrqa_squad-validation-4715", "mrqa_squad-validation-2975", "mrqa_squad-validation-3848", "mrqa_squad-validation-8769", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-8339", "mrqa_triviaqa-validation-4881", "mrqa_hotpotqa-validation-2800", "mrqa_newsqa-validation-3043", "mrqa_searchqa-validation-2141", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-3476"], "SR": 0.578125, "CSR": 0.703125, "EFR": 0.9629629629629629, "Overall": 0.8330439814814814}, {"timecode": 9, "before_eval_results": {"predictions": ["in an attempt to emphasize academics over athletics", "3,600", "nine", "individual states and territories", "30%\u201350%", "one of his wife's ladies-in-waiting", "liquid", "Dirichlet's theorem on arithmetic progressions", "Europe", "the cell membrane", "The Master is the Doctor's archenemy", "Laverne & Shirley", "carbohydrates", "his butchery is exaggerated, while his positive role is underrated", "Jean Ribault", "March 2011", "Continental Edison Company in France", "2010", "more equality in the income distribution", "X reduces to Y", "38", "1887", "1469", "a \"world classic of epoch-making oratory.\"", "up to half", "one octave lower than the lowest pitched four strings of an", "WD-40", "a barbie doll type", "Georgie Porgie", "an alcoholic beverage distilled at a high proof from a fermented baijiu (China)", "William Shaksper", "The Fray", "Venus", "Helen Hayes MacArthur", "Canberra", "The Awl", "Alexander Graham Bell", "Anna Pavlova", "a person who computes premium rates, dividends, risks, etc.", "Joe Torre", "an animated cartoon series created by Bob Clampett, who had previously worked for Warner", "The Chicago Cubs", "jennifer Kilgour", "Capricorn", "an enlarged heart", "an an ENTP, INTJs are my Power pairing, and there's no doubt that I've.... and so outside forces will give the INTJ feedback that they aren't", "an...", "a huge lake located in east central Africa along the equator", "an article", "a fireplace fire to warm you up this winter", "Andrew Jackson", "Madonna", "fauves", "a sailfish", "an updo", "Laurie", "Egypt", "James Hutton", "Morocco", "Stuart Neame", "Total Nonstop Action Wrestling", "a single-piece band from Portland, Oregon", "China and Japan", "The United Nations is calling on NATO to do more to stop the Afghan opium trade"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6042668269230769}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false], "QA-F1": [0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7943", "mrqa_squad-validation-8969", "mrqa_squad-validation-7700", "mrqa_squad-validation-6229", "mrqa_squad-validation-1240", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-6421", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-2179"], "SR": 0.53125, "CSR": 0.6859375, "EFR": 0.9333333333333333, "Overall": 0.8096354166666666}, {"timecode": 10, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8454", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-3245", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-5035", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-10000", "mrqa_squad-validation-10115", "mrqa_squad-validation-10136", "mrqa_squad-validation-1017", "mrqa_squad-validation-10181", "mrqa_squad-validation-10184", "mrqa_squad-validation-10217", "mrqa_squad-validation-10263", "mrqa_squad-validation-10281", "mrqa_squad-validation-10290", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-10361", "mrqa_squad-validation-10369", "mrqa_squad-validation-1038", "mrqa_squad-validation-10410", "mrqa_squad-validation-10454", "mrqa_squad-validation-10496", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1181", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1226", "mrqa_squad-validation-1240", "mrqa_squad-validation-1254", "mrqa_squad-validation-1269", "mrqa_squad-validation-1371", "mrqa_squad-validation-1499", "mrqa_squad-validation-1521", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1651", "mrqa_squad-validation-166", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1748", "mrqa_squad-validation-1780", "mrqa_squad-validation-1787", "mrqa_squad-validation-1848", "mrqa_squad-validation-1863", "mrqa_squad-validation-1892", "mrqa_squad-validation-1924", "mrqa_squad-validation-1938", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-1998", "mrqa_squad-validation-2019", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-2108", "mrqa_squad-validation-2145", "mrqa_squad-validation-2209", "mrqa_squad-validation-2233", "mrqa_squad-validation-2241", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-247", "mrqa_squad-validation-2521", "mrqa_squad-validation-2545", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2642", "mrqa_squad-validation-27", "mrqa_squad-validation-2751", "mrqa_squad-validation-2820", "mrqa_squad-validation-2885", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-3039", "mrqa_squad-validation-305", "mrqa_squad-validation-3076", "mrqa_squad-validation-3144", "mrqa_squad-validation-3164", "mrqa_squad-validation-317", "mrqa_squad-validation-3184", "mrqa_squad-validation-322", "mrqa_squad-validation-3230", "mrqa_squad-validation-3270", "mrqa_squad-validation-334", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3376", "mrqa_squad-validation-3380", "mrqa_squad-validation-3392", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3497", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3687", "mrqa_squad-validation-3703", "mrqa_squad-validation-3718", "mrqa_squad-validation-374", "mrqa_squad-validation-3769", "mrqa_squad-validation-3770", "mrqa_squad-validation-381", "mrqa_squad-validation-3824", "mrqa_squad-validation-3829", "mrqa_squad-validation-3842", "mrqa_squad-validation-3848", "mrqa_squad-validation-3852", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3917", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3985", "mrqa_squad-validation-3986", "mrqa_squad-validation-3998", "mrqa_squad-validation-4000", "mrqa_squad-validation-4009", "mrqa_squad-validation-402", "mrqa_squad-validation-4031", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4181", "mrqa_squad-validation-4187", "mrqa_squad-validation-4213", "mrqa_squad-validation-4291", "mrqa_squad-validation-4312", "mrqa_squad-validation-4348", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4452", "mrqa_squad-validation-4467", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-451", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4539", "mrqa_squad-validation-4557", "mrqa_squad-validation-4557", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4838", "mrqa_squad-validation-491", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5019", "mrqa_squad-validation-5064", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-516", "mrqa_squad-validation-5262", "mrqa_squad-validation-5396", "mrqa_squad-validation-5436", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5493", "mrqa_squad-validation-5527", "mrqa_squad-validation-5546", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5602", "mrqa_squad-validation-5631", "mrqa_squad-validation-5664", "mrqa_squad-validation-5677", "mrqa_squad-validation-57", "mrqa_squad-validation-5726", "mrqa_squad-validation-5750", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5806", "mrqa_squad-validation-5818", "mrqa_squad-validation-5852", "mrqa_squad-validation-5860", "mrqa_squad-validation-5865", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6031", "mrqa_squad-validation-6066", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6176", "mrqa_squad-validation-6206", "mrqa_squad-validation-6222", "mrqa_squad-validation-6229", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6319", "mrqa_squad-validation-6330", "mrqa_squad-validation-6347", "mrqa_squad-validation-6353", "mrqa_squad-validation-6355", "mrqa_squad-validation-6409", "mrqa_squad-validation-6439", "mrqa_squad-validation-6502", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6611", "mrqa_squad-validation-6649", "mrqa_squad-validation-6664", "mrqa_squad-validation-6694", "mrqa_squad-validation-6790", "mrqa_squad-validation-6815", "mrqa_squad-validation-6838", "mrqa_squad-validation-6875", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6898", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7036", "mrqa_squad-validation-7039", "mrqa_squad-validation-7064", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7228", "mrqa_squad-validation-7260", "mrqa_squad-validation-7261", "mrqa_squad-validation-7297", "mrqa_squad-validation-7332", "mrqa_squad-validation-7338", "mrqa_squad-validation-7357", "mrqa_squad-validation-7364", "mrqa_squad-validation-7368", "mrqa_squad-validation-7380", "mrqa_squad-validation-739", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7445", "mrqa_squad-validation-7457", "mrqa_squad-validation-7470", "mrqa_squad-validation-7492", "mrqa_squad-validation-7503", "mrqa_squad-validation-7525", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-762", "mrqa_squad-validation-7693", "mrqa_squad-validation-7700", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7775", "mrqa_squad-validation-7781", "mrqa_squad-validation-7785", "mrqa_squad-validation-779", "mrqa_squad-validation-7863", "mrqa_squad-validation-7871", "mrqa_squad-validation-7917", "mrqa_squad-validation-7943", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8016", "mrqa_squad-validation-8043", "mrqa_squad-validation-8093", "mrqa_squad-validation-8125", "mrqa_squad-validation-8154", "mrqa_squad-validation-8177", "mrqa_squad-validation-8184", "mrqa_squad-validation-8192", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8365", "mrqa_squad-validation-8414", "mrqa_squad-validation-8449", "mrqa_squad-validation-8459", "mrqa_squad-validation-8471", "mrqa_squad-validation-8484", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8568", "mrqa_squad-validation-8585", "mrqa_squad-validation-8661", "mrqa_squad-validation-8670", "mrqa_squad-validation-8670", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-8841", "mrqa_squad-validation-888", "mrqa_squad-validation-8904", "mrqa_squad-validation-8925", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8958", "mrqa_squad-validation-8985", "mrqa_squad-validation-908", "mrqa_squad-validation-9095", "mrqa_squad-validation-9161", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9403", "mrqa_squad-validation-9405", "mrqa_squad-validation-9446", "mrqa_squad-validation-9464", "mrqa_squad-validation-9556", "mrqa_squad-validation-957", "mrqa_squad-validation-9594", "mrqa_squad-validation-9615", "mrqa_squad-validation-9669", "mrqa_squad-validation-9716", "mrqa_squad-validation-9717", "mrqa_squad-validation-9764", "mrqa_squad-validation-9814", "mrqa_squad-validation-9816", "mrqa_squad-validation-9876", "mrqa_squad-validation-9907", "mrqa_squad-validation-9928", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7463"], "OKR": 0.91015625, "KG": 0.44609375, "before_eval_results": {"predictions": ["Northern Europe and the Mid-Atlantic", "$2 million", "an fish larvae and organisms that would otherwise have fed the fish", "Chris Keates", "its many castles and vineyards", "Selmur Productions catalog", "Antigone", "3.5 million", "Denver Broncos", "1997", "A \u2192 G deamination gradients", "since 2001", "A", "1767", "Narrow alleys", "another problem", "economic", "John and Benjamin Green", "1530", "installed electrical arc light based illumination systems", "two", "poor", "Irish Hospitals' Sweepstakes", "Pearl Jam", "Grey's Anatomy", "black", "Bruce Springsteen", "Wounded Knee", "Maria Callas", "Henry Moore", "Boston Red Sox", "Charlotte", "an eagle", "Narcissus", "Fred Williamson", "the Orange River", "tapestry", "the Holy Grail", "Smashing Pumpkins", "C.C.", "Ludwig Van Beethoven", "Lake Victoria", "a small twice-a-day (seidiurnal) component to the... atmosphere", "Dr Pepper", "Fredericks", "Sarah Orne Jewett", "Velvet Revolver", "(Britain)", "air", "You Bet Your Life", "China", "Nova Scotia", "Kenny G", "lion", "Franklin Pierce", "The Final Jeopardy answer", "Michael Schumacher", "a four - page pamphlet in 1876", "pool", "Queen Margaret College", "Joely Kim Richardson", "Hugh Dowding", "NATO fighters", "Congress"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6265997023809524}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.13333333333333333, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.26666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8990", "mrqa_squad-validation-5887", "mrqa_squad-validation-6655", "mrqa_squad-validation-9959", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3530", "mrqa_searchqa-validation-11388", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-8760", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-6909", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-6181", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6896", "mrqa_hotpotqa-validation-1843"], "SR": 0.578125, "CSR": 0.6761363636363636, "EFR": 1.0, "Overall": 0.7627272727272728}, {"timecode": 11, "before_eval_results": {"predictions": ["Horn of Africa", "Grumman", "submit cheerfully to the highest penalty that can be inflicted upon me for what in law is a deliberate crime", "1671", "St. Johns River", "the AS-205 mission was canceled", "The President of the Council and a Commissioner can sit in on ECB meetings, but don't have voting rights", "Ismailiyah, Egypt", "phycobilisomes", "lupus erythematosus", "December 1878", "bars, caf\u00e9s and clubs", "both PNU and ODM camps", "T(n) = O(n2)", "Bill Clinton", "qu", "International Crops Research Institute for the Semi-Arid Tropics", "straight line", "Scandinavia", "autoimmune", "January 26, 1996", "Ontogenetic depth", "Seoul", "2005", "April 12, 2011", "May 21, 2000", "100 metres", "January 2016", "seven", "Samuel Beckett", "l Loch Duich, Loch Long and Loch Alsh", "Sonic Mania", "Homeland", "Carson City", "League of the Three Emperors", "Barack Obama", "Brian A. Miller", "Washington, D.C.", "December 13, 2015", "Front Row", "before 1638", "Vixen", "Revolution Studios", "Mach number", "1990", "Michael A. Cremo", "Gangsta's Paradise", "The A41", "Mary Astor", "five", "Indiana", "Esteban Ocon", "ABC", "the British military", "National Lottery", "2018", "Alan Menken", "Milton Friedman", "Billy Wilder", "twice", "a full garden and pool, a tennis court, or several heli-pads", "(Northern Bahr el Ghazal and Western Upper Nile", "a pillar of salt", "Yahya Khan"], "metric_results": {"EM": 0.625, "QA-F1": 0.6701252052545156}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.06896551724137931, 0.0, 1.0, 0.4, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9912", "mrqa_squad-validation-6759", "mrqa_squad-validation-3113", "mrqa_squad-validation-3954", "mrqa_squad-validation-4150", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-10161", "mrqa_newsqa-validation-3227", "mrqa_searchqa-validation-11991", "mrqa_naturalquestions-validation-3485"], "SR": 0.625, "CSR": 0.671875, "EFR": 0.9583333333333334, "Overall": 0.7535416666666668}, {"timecode": 12, "before_eval_results": {"predictions": ["a Wi-Fi or Power-line connection", "water pump", "the Tesla coil", "1946", "21 to 11", "Parliamentary Bureau", "Japan and Latin America", "force the Huguenots to convert", "Arizona Cardinals", "842 pounds", "1540s", "John Fox", "American Indians in the colony of Georgia", "orbit the Moon", "poison", "quickly", "innate immune system", "March 1896", "The Lightning thief", "James `` Jamie '' Dornan", "W. Edwards Deming", "usually in May", "biochemistry", "$603.4 million", "current day Poole Harbour towards mid-Channel", "Accounting Standards Board ( ASB )", "Germany", "General George Washington", "following graduation", "Djokovic", "Longliners", "1961", "dome", "1998", "Procol Harum", "Sheev Palpatine", "Dan Rooney", "punk rock", "septum", "The White House Executive chef", "vaskania", "the church at Philippi", "10 May 1940", "Brenda", "bohrium", "generally in a way considered to be unfair", "cartilage", "loss", "Spanish American wars of independence", "Tristan Rogers", "Owen Vaccaro", "Walter Brennan", "1872", "Mike Alstott", "1992", "King Richard II of England", "Austria and Switzerland", "Chuck vs. First Class", "Dana Scully", "French Open", "Monday", "a nineteenth-century man who lived on the land", "blinking his left eye", "Carr Inlet"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5489831349206349}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 0.0, 0.2857142857142857, 0.20000000000000004, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-978", "mrqa_squad-validation-3130", "mrqa_squad-validation-3811", "mrqa_squad-validation-9863", "mrqa_squad-validation-8164", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-2813", "mrqa_triviaqa-validation-4886", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-5292", "mrqa_newsqa-validation-1360", "mrqa_searchqa-validation-13332", "mrqa_searchqa-validation-9822", "mrqa_searchqa-validation-10098"], "SR": 0.46875, "CSR": 0.65625, "EFR": 0.9411764705882353, "Overall": 0.7469852941176471}, {"timecode": 13, "before_eval_results": {"predictions": ["Protestantism", "Extension", "Riverside", "interactions", "Hamburg merchants and traders", "Department of Justice", "water flow through the body cavity", "67.9", "Fort Duquesne", "Sports Programs, Inc.", "quality rental units", "Pittsburgh Steelers", "Edward Teller", "the geographical area it covers as well as the frequency of meeting", "stay", "Andrew Lortie", "Animals are divided by body plan into vertebrates and invertebrates", "Thirty years after the Galactic Civil War", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "eight years", "the original Star Trek television series", "Longline fishing", "various locations in Redford's adopted home state of Utah", "Stephen A. Douglas", "April 2011", "around 1940", "Paradise, Nevada", "Herman Hollerith", "Dr. Rajendra Prasad", "Ron Harper", "hairpin corner", "over two days", "the IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "when the cell is undergoing the metaphase of cell division", "it activates a relay which will handle the higher current load", "Donald Trump", "Liam Cunningham", "the spectroscopic notation for the associated atomic orbitals", "Veronica", "moral tale", "a revolution or orbital revolution", "Sauron's", "Gustav Bauer", "2002", "Mohammad Reza Pahlavi", "Virginia Beach", "two tectonic plates move towards each other at a convergent plate boundary", "Jourdan Miller", "10,605", "84", "1773", "Jesse McCartney", "73", "Mars Hill, 150 miles ( 240 km ) to the northeast", "2005", "Catherine Zeta-Jones", "Michael Crawford", "247,597", "10,000", "the two bodies", "the Mormon Tabernacle Choir", "carbon dioxide", "Pickwick (based on Charles Dickens' novel The Pickwick Papers)", "Florida"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6473800505050505}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.19999999999999998, 1.0, 0.0, 0.8, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 0.2222222222222222, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10475", "mrqa_squad-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-8699", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-4328"], "SR": 0.5625, "CSR": 0.6495535714285714, "EFR": 0.9642857142857143, "Overall": 0.7502678571428572}, {"timecode": 14, "before_eval_results": {"predictions": ["a Tulku", "the Quaternary", "the Treaty of Aix-la-Chapelle", "Brad Nortman", "Behind the Sofa", "BBC Dead Ringers", "1206", "Louis Pasteur", "The Brain of Morbius (1976)", "the depths of the oceans and seas", "118", "a mainline Protestant Methodist denomination", "Albert Einstein", "Vince Lombardi Trophy", "death in body and soul, if only as highwaymen and murderers", "Candice Susan Swanepoel", "AT&T", "Australian", "German", "Chris Anderson", "just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "1949", "Red", "Australian", "Humphrey Goodman", "Jena Malone", "John M. Dowd", "twelfth", "Republican", "New York", "Molly Hatchet", "a tragedy", "cricket fighting", "14th Street", "bass", "Brad Wilk", "2012", "New Orleans, Louisiana", "Robert \"Bobby\" Germaine, Sr.", "May 4, 1924", "Australian", "1966", "2012", "1926", "27th congressional district", "Santiago Herrera", "mother goddess", "the EA-18G Growler carrier-based electronic warfare jet aircraft", "1892", "Ludwig van Beethoven", "Sam Phillips", "Manchester United", "Turkmenistan", "1942", "October 6, 2017", "a given temperature", "wolf", "Ganges", "January 24, 2006", "South Africa, seeking a better life", "a pager", "Baltimore", "In 1917", "in the Blue Ridge Mountains of Virginia"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6850958777151211}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.10526315789473684]}}, "before_error_ids": ["mrqa_squad-validation-10168", "mrqa_squad-validation-8229", "mrqa_squad-validation-7688", "mrqa_squad-validation-2383", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-10613", "mrqa_newsqa-validation-1879", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-1813"], "SR": 0.5625, "CSR": 0.64375, "EFR": 1.0, "Overall": 0.75625}, {"timecode": 15, "before_eval_results": {"predictions": ["1937", "teachers in Scotland can be registered members of trade unions with the main ones being the Educational Institute of Scotland and the Scottish Secondary Teachers' Association.", "June 4, 2014", "Journey's End", "a stronger, tech-oriented economy in the Bay Area and an emerging Greater Sacramento region.", "John Houghton", "heterokontophyte", "isomorphic", "Tenggis", "128,843", "by a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections.[citation needed]", "56.2%", "20\u201318", "Chaplain to the Forces", "KlingStubbins", "Edward James Olmos", "Named in honour of Louis Mountbatten, 1st Earl mountbatten of Burma", "Alcorn, Mississippi", "The Guest", "The Light in the Piazza", "Philadelphia, Pennsylvania", "12 members", "The A41", "Royce da 5'9\" (Bad) and Eminem (Evil)", "\"Pimp My Ride\"", "1998", "casting, job opportunities, and career advice", "Mary Harron", "Flashback: The Quest for Identity", "Eenasul Fateh", "Chicago", "Australia", "2014", "the Second World War", "Lismore, New South Wales", "rural areas", "teenage actor or teen actor", "Summerlin, Clark County, Nevada", "Lester Ben \"Benny\" Binion", "YG Entertainment", "water", "Noel Gallagher", "the \"Pour le M\u00e9rite\"", "Trey Parker and Matt Stone", "Riot Act", "Aqua", "various registries.", "four operas", "Christy Walton", "Commanding General", "Hechingen", "N.I.B.", "footballer", "8,211", "Kristin Beth Baxter", "the cell nucleus", "a Bristol Box Kite", "1961", "a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "South Dakota State Penitentiary", "Douglas Fir", "kiss a fool", "a striking blow to due process and the rule of law", "Philip Markoff"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6170011123136123}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.29629629629629634, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2727272727272727, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.28571428571428575, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.4, 0.0, 0.8, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2094", "mrqa_squad-validation-2835", "mrqa_squad-validation-1791", "mrqa_squad-validation-6279", "mrqa_squad-validation-4298", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-5320", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2558", "mrqa_triviaqa-validation-7461", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-13595", "mrqa_searchqa-validation-1757"], "SR": 0.46875, "CSR": 0.6328125, "EFR": 1.0, "Overall": 0.7540625000000001}, {"timecode": 16, "before_eval_results": {"predictions": ["the Central Secretariat (Zhongshu Sheng) to manage civil affairs", "Puritanism", "James Wolfe", "March 1974", "2003", "Frederick II the Great", "Lower taxes, increased economic development, unification of the community, better public spending and effective administration by a more central authority", "Armenians vassal-states of Sassoun and Taron", "redistributive taxation", "Seattle Seahawks", "paid professionals", "a polynomial-time reduction", "revelry", "Krishna Rajaram", "an bathing-suit-clad woman", "British broadcasters", "1-0", "Kim Il Sung", "second-degree aggravated battery", "former House Speaker Newt Gingrich", "a UPS delivery box", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "be silent", "200", "2,000", "several weeks", "auction off one of the earliest versions of the Magna Carta later this year", "burns over about two-thirds of his body", "Michael Jackson", "Caylee", "10 below", "female veterans and veterans with children", "Manmohan Singh", "jazz", "1983", "cancer", "Kenyan forces", "Casalesi Camorra clan", "videtaping", "Appathurai", "Eintracht Frankfurt", "opium poppies", "The Bronx County District Attorneys Office", "1,073", "Arthur E. Morgan III", "Al Alberto Espinoza", "Las Vegas", "Pakistan", "an American freighter captain", "the chief executive officer, the one on the very top", "18", "\"Draquila -- Italy Trembles.\"", "India", "Mumbai", "Miami Heat", "a combination of genetics and the male hormone dihydrotestosterone", "Senegal", "Windermere", "Field of Dreams", "Bill Paxton", "a star", "American Airlines LAS-PHL", "Sex Pistols", "an ambitious Jewish boy growing up in a poor neighborhood in Montreal"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5674827776390277}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false], "QA-F1": [0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5454545454545454, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5714285714285714, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8294", "mrqa_squad-validation-7147", "mrqa_squad-validation-7296", "mrqa_squad-validation-1136", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-2900", "mrqa_triviaqa-validation-4966", "mrqa_hotpotqa-validation-5742", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-4356"], "SR": 0.46875, "CSR": 0.6231617647058824, "EFR": 1.0, "Overall": 0.7521323529411765}, {"timecode": 17, "before_eval_results": {"predictions": ["higher economic inequality", "private individuals, private organizations or religious groups", "high schools", "a glass case suspended from the lid", "phagocytic cells", "2000", "five", "weight", "Leukocytes", "3D printing technology", "Ong Khan", "colonel in the Rwandan army", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "Wigan Athletic", "Vertikal-T", "Graeme Smith", "228", "the Bush administration's controversial system of military trials", "a Florida girl who disappeared in February", "St. Francis De Sales Catholic Church", "The Tinkler", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "air support", "power lines downed by Saturday's winds,", "African National Congress", "United States", "The oldest documented bikinis", "at the age of 23", "Adam Yahiye Gadahn,", "Gov. Mark Sanford", "150", "anti- strikers", "the equator", "Chinese President Hu Jintao", "183", "alert patients of possible tendon ruptures and tendonitis", "They're big, strong, and fierce", "Cirque du Soleil's", "\"Goldstone Report\"", "11th year in a row", "theft in Switzerland", "fastest circumnavigation of the globe in a powerboat", "Guinea, Myanmar, Sudan and Venezuela", "Austin Wuennenberg", "Diversity", "forcibly drugging deportees", "in the Oaxacan countryside of southern Mexico", "buckling under pressure from the ruling party.", "a polo match", "more than 100", "bribing other wrestlers to lose bouts", "Alfredo Astiz,", "MacFarlane", "convert single - stranded genomic RNA into double - stranded cDNA", "Harrison Ford", "Andes", "\"The Snowman\"", "2009", "Candid Microphone", "Marilyn Monroe", "The Who", "The Band Concert", "Mexican chain"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5986896900959401}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.25, 0.2666666666666667, 1.0, 0.4, 0.0, 1.0, 0.7272727272727273, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.8333333333333334, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4864864864864865, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7136", "mrqa_squad-validation-2000", "mrqa_squad-validation-7845", "mrqa_squad-validation-6477", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1123", "mrqa_naturalquestions-validation-1974", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1649"], "SR": 0.515625, "CSR": 0.6171875, "EFR": 1.0, "Overall": 0.7509375}, {"timecode": 18, "before_eval_results": {"predictions": ["Jason Bourne", "in the condenser", "1999,", "mesoglea", "a body of treaties and legislation,", "liquid", "socially owned", "Mark Twain's", "in amylopectin starch granules that are located in their cytoplasm,", "Tower District", "\"a violent and brutal extremist group with a number of individuals affiliated with al Qaeda.", "pro-democracy activists clashed Friday with Egyptian security forces in central Cairo,", "at a construction site in the heart of Los Angeles.", "to overthrow the socialist government of Salvador Allende in Chile,", "discovered a 70 ft, three-masted ship built from pine and oak, that could carry 40 men.", "a rally at the State House next week", "2,000", "Michael Schumacher", "Ventures", "seven", "hanging a noose in a campus library,", "resigned", "\"I'm just getting started.\"", "14,", "diplomatic relations", "hand-painted Swedish wooden clogs", "Daniel Radcliffe", "Muslim", "five searches", "mother", "$10 billion", "Zoe's Ark", "Galveston, Texas,", "9-week-old", "a form of liquid morphine used by terminally ill patients will remain on the market even though it is an \"unapproved drug,\"", "Lucky Dube,", "children's books", "James Newell Osterberg", "At least 40", "NATO", "Lindsey Vonn", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "a grizzly bear", "Lechuza Caracas", "celebrity-inspired names", "The cervical cancer vaccine, approved in 2006, is recommended for girls around 11 or 12.", "poor families", "At least 88", "creation of an Islamic emirate in Gaza,", "an \"unnamed international terror group\"", "Nicole", "Manchester City", "Taher Nunu", "the sixth series", "initially absent from the original game release, but were added in the January 2017 patch.", "30", "a sweater", "Waylon Albright \"Shooter\" Jennings", "people working in film and the performing arts,", "Coppertone", "school holidays", "Marlborough, New Hampshire", "1968", "The Krypto Report"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5751669057045895}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.4, 0.9411764705882353, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.2564102564102564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.375, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7516", "mrqa_squad-validation-1235", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2471", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-5209", "mrqa_hotpotqa-validation-2986", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-3428"], "SR": 0.4375, "CSR": 0.6077302631578947, "EFR": 1.0, "Overall": 0.749046052631579}, {"timecode": 19, "before_eval_results": {"predictions": ["the blood\u2013brain barrier,", "an Executive Committee,", "New Orleans", "the death of Elisabeth Sladen", "annual NFL Experience", "English and Swahili,", "61%", "plastoglobulus, sometimes spelled plast\u00f6obule(s)", "three", "Turkey", "Wombat", "KENNY", "gestation", "Peyton Place", "Hope Diamond", "gin", "Pilate", "enamel", "bone", "Tagline", "Battle of Hastings", "the Caspian Sea", "a bone", "\"The 1,001 Nights\"", "Gannett", "\"Won't Get Fooled Again\"", "\"Don Juan De Marco\"", "Fes", "FIFA World Cup Final", "Interlaken", "Mystic Pizza", "Princeton", "\"Every Breath You Take\"", "\"The Nation's Largest Libraries", "Malay Peninsula", "Herman Wouk", "Frederick IV,", "Benjamin Franklin", "poetry", "Napoleon Bonaparte", "sauropods", "unassisted", "thermodynamics", "Derek Smalls", "Dalits", "Harry Houdini", "Mary Hatcher", "Double Vision", "Sporcle", "Lust for Life", "carrigaline", "James Ross Clemens", "a hole-in-one", "1991", "to universalize the topic of the song into something everyone could relate to and ascribe personal meaning to in their own way", "cuban cigars", "\"Thrilla in Manila\"", "North America, Australia, and India", "841", "Ike", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "Harlem River", "sovereign states", "near the city of Cairo, Illinois"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5617716827745898}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true], "QA-F1": [0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.13953488372093023, 1.0, 0.5, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.08333333333333334, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6490", "mrqa_squad-validation-8786", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-16558", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-2769", "mrqa_newsqa-validation-3214", "mrqa_naturalquestions-validation-3559"], "SR": 0.453125, "CSR": 0.6, "EFR": 0.9714285714285714, "Overall": 0.7417857142857143}, {"timecode": 20, "UKR": 0.76953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-1252", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1967", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-3519", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-782", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-920", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11270", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2607", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4469", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-971", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10143", "mrqa_squad-validation-10168", "mrqa_squad-validation-10241", "mrqa_squad-validation-10266", "mrqa_squad-validation-10370", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1141", "mrqa_squad-validation-115", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1254", "mrqa_squad-validation-127", "mrqa_squad-validation-1288", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1747", "mrqa_squad-validation-1765", "mrqa_squad-validation-1827", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-218", "mrqa_squad-validation-22", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2379", "mrqa_squad-validation-2383", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-2538", "mrqa_squad-validation-2545", "mrqa_squad-validation-257", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2886", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-305", "mrqa_squad-validation-3052", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3567", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3864", "mrqa_squad-validation-3917", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3975", "mrqa_squad-validation-3986", "mrqa_squad-validation-3994", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4187", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4883", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5097", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-5237", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5604", "mrqa_squad-validation-5677", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5859", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6347", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6594", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7034", "mrqa_squad-validation-7039", "mrqa_squad-validation-7051", "mrqa_squad-validation-71", "mrqa_squad-validation-7125", "mrqa_squad-validation-7136", "mrqa_squad-validation-7192", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7449", "mrqa_squad-validation-7521", "mrqa_squad-validation-7576", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7708", "mrqa_squad-validation-7751", "mrqa_squad-validation-7814", "mrqa_squad-validation-7863", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7881", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8471", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8670", "mrqa_squad-validation-8710", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9095", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9367", "mrqa_squad-validation-9405", "mrqa_squad-validation-942", "mrqa_squad-validation-9594", "mrqa_squad-validation-9614", "mrqa_squad-validation-9669", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-4881", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-7496"], "OKR": 0.880859375, "KG": 0.49296875, "before_eval_results": {"predictions": ["the main contractor", "widespread education", "300", "an attack on New France's capital, Quebec", "two-thirds", "bubbles of inert gas, mostly nitrogen and helium, forming in their blood", "1979", "Parliament Square, High Street and George IV Bridge in Edinburgh", "1959", "the Lincoln Laboratory", "grizzly bear", "Dracula", "Sid Vicious", "Nitrous oxide", "the Tchaikovsky 1812 Overture", "Frederic Remington", "lowlands", "Arkansas", "an object-oriented programming", "15", "the Uniform Code of Military Justice", "the Whig", "ER", "a genie", "food Network", "\"The Princess Diaries\"", "Arkansas", "Mao Zedong", "a person who learns and uses five or more languages", "a genie", "Wells Fargo", "the Sundance Kid", "money changers", "amber", "Holly Golightly", "Umbria", "a Roth IRA", "Quentin Tarantino", "the Palatine Hill", "Kentucky", "an axiom", "Daylight Saving Time", "a miniskirt", "airplane", "Scooter Libby", "an abundance of", "a genie", "Equatorial Guinea", "a fat prisoner", "bowling", "Walter Reed", "Aerobic", "Anaheim", "Steve Hale", "epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results ( including peer review and occasional systematic review )", "Belgium", "David Jason", "137th", "Merck", "semiconductors", "Moscow", "France", "Hagrid", "Phil Mickelson"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6572916666666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4918", "mrqa_searchqa-validation-4028", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-16826", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-12996", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-9183", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-4992", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-4118", "mrqa_triviaqa-validation-436"], "SR": 0.59375, "CSR": 0.5997023809523809, "EFR": 1.0, "Overall": 0.7486123511904762}, {"timecode": 21, "before_eval_results": {"predictions": ["lesson plan", "laws of physics", "1893", "Welsh", "pastors and teachers", "criminal", "a monthly subscription", "15,000 BC", "novella", "President of the United States", "above the light source and under the sample in an upright microscope", "November 3, 2007", "1939", "April 1917", "1959", "orphanage", "September 19 - 22, 2017", "tolled ( quota ) highways", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "Bobby Eli", "Arunachal Pradesh", "Dick Rutan and Jeana Yeager", "Paracelsus", "January 2004", "members of the gay ( LGBT ) community", "annually in late January or early February", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Jerry Lee Lewis", "push the food down the esophagus", "Splodgenessabounds", "offensive player's feet are slightly wider than shoulder width and slightly on the balls of their feet, their knees flexed, with both hands on the basketball in front of them or almost resting on their thigh", "Edd Kimber", "omitted", "diastema ( plural diastemata )", "Eddie Murphy", "television", "high officials", "flour and water", "the National Football League ( NFL )", "Gupta Empire", "card verification value ( CVV )", "T - Bone Walker", "Ray Charles", "Francis Hutcheson", "1937", "Cairo, Illinois", "Barbara Windsor", "the British", "Gladys Knight & the Pips", "Executive Residence of the White House Complex", "the eighth episode of Arrow's second season", "judges", "Kanawha River", "middle-distance runner", "isosceles", "1898", "James I", "WFTV", "tennis", "Las Vegas", "Austria", "women and breast cancer", "Harry Nicolaides", "\"He's crying like a baby,\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.6462603751436472}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6428571428571429, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4444444444444444, 0.9428571428571428, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.35294117647058826, 0.0, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7692307692307692, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-5041", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-6087", "mrqa_triviaqa-validation-4115", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-4715", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1985"], "SR": 0.578125, "CSR": 0.5987215909090908, "EFR": 0.8518518518518519, "Overall": 0.7187865635521886}, {"timecode": 22, "before_eval_results": {"predictions": ["literacy and numeracy, craftsmanship or vocational training, the arts, religion, civics, community roles, or life skills.", "bark of mulberry trees.", "drama", "1806", "distributive efficiency", "on issues related to the substance of the statement.", "in 23 cities", "Continental drift", "Frank Oz", "introduced in 1975 by Barbara Castle, introducing for the first time a universal payment, paid for each child, doubling the number of children within its scope", "775", "Kimberlin Brown", "in Ephesus in AD 95 -- 110", "the status line", "about 26,000 light - years from the Galactic Center, on the inner edge of the Nebula Arm, one of the spiral - shaped concentrations of gas and dust", "permanently absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "handheld subscriber equipment", "Weston - super-Mare, which stood in for Clevedon", "a type II endoprotease, cleaves the C peptide - A chain bond", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Javier Fern\u00e1ndez", "Qutab Ud - Din - Aibak", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Coton in the Elms", "an ascender", "Wakanda", "1992", "ATP, generated by the root respiration", "shortwave radio", "a place of trade, entertainment, and education", "Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively,", "Robert Hooke", "molecular clouds in interstellar space", "Alicia Vikander as Tomb Croft", "a neighbour", "the name announcement of Kylie Jenner's first child", "approximately 5 liters", "somatic cell nuclear transfer ( SCNT )", "Betty", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "June 8, 2009", "head - up display", "a presidential representative democratic republic, whereby the President of El Salvador is both head of state and head of government, and of an Executive power is exercised by the government", "Ferm\u00edn Francisco de Lasu\u00e9n", "moral", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "the 1994 season", "of Spanish / Basque origin", "Laura Jane Haddock", "Atlanta", "2002", "the nasal septum", "a coffee house", "Chief Inspector of Prisons", "Cheshire", "#364", "24800 mi", "liberal revolutions of 1848", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Matamoros, Mexico", "The incident Sunday evening forced the closure of a terminal for hours while authorities rescreened thousands of passengers.", "The X-Files", "biometrics", "a motto"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6835937737867885}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false], "QA-F1": [0.23529411764705882, 1.0, 0.6666666666666666, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.08333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6486486486486486, 0.9600000000000001, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.7407407407407407, 1.0, 0.0, 0.2857142857142857, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8000000000000002, 0.0, 0.7999999999999999, 0.4, 0.888888888888889, 0.14285714285714288, 0.9090909090909091, 1.0, 1.0, 0.2666666666666667, 1.0, 0.6666666666666666, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1802", "mrqa_squad-validation-5608", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-7124", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-7662"], "SR": 0.484375, "CSR": 0.59375, "EFR": 0.9393939393939394, "Overall": 0.7353006628787879}, {"timecode": 23, "before_eval_results": {"predictions": ["around 100,000 soldiers", "an extensive neoclassical centre referred to as Tyneside Classical", "algebraic", "Smiljan", "Persia", "ABC-DuMont", "a long flat bottom in between", "the First World War", "John Constable", "Charlie Harper", "lingual", "Duncan", "Everton", "September", "cogito ergo sum", "Bull Moose Party", "Augusta", "Demi Moore", "the College of Cardinals", "Cornell", "Robert Stroud", "Alice in Alice", "a drink containing stimulant drugs, chiefly caffeine,", "\"The Blind Side\"", "11", "17 pink \"double-word\" squares", "Achille Lauro", "Quentin Tarantino", "Bert Jones", "Swansea", "Wyatt", "Chuck Hagel", "Hispaniola", "Bangladesh", "an argument", "Sean Maddox", "sixteen pieces", "Bath", "tinctures", "Andy Murray", "Independence Day", "a phantom 8-ender", "the Hanseatic League", "Crusades", "Geoffrey", "Thundercats", "comic Cuts", "The European Council", "Volkswagen", "King George IV", "an owl", "China", "Edward Seton", "Thomas Jefferson", "the central plains", "William Adelin", "Barbary pirates", "Sir William Collins", "to hold onto his land", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf, police said.", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "port", "a Lone Star", "Bahadur Shah Zafar"], "metric_results": {"EM": 0.546875, "QA-F1": 0.64375}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false], "QA-F1": [0.8, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9333333333333333, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6205", "mrqa_squad-validation-5180", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-6584", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-7212", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-1683", "mrqa_naturalquestions-validation-1782", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2371", "mrqa_searchqa-validation-13686"], "SR": 0.546875, "CSR": 0.591796875, "EFR": 0.9655172413793104, "Overall": 0.7401346982758621}, {"timecode": 24, "before_eval_results": {"predictions": ["Napoleon", "The Victorian Alps in the northeast", "skin damage", "three", "European Parliament and the Council of the European Union", "Steve McQueen", "\u00c9dith Piaf", "guitar", "Midtown", "eagle", "the finest luxury shoes and boots from the finest of Spanish cordovan leather", "boxer", "Geneva", "Call for the Dead", "Woodrow Wilson", "Menorca", "Wales", "eaglebank thistle", "bulldog Drummond", "distance selling", "Edward VI", "raw linseed", "mercury", "trumpet", "architecture", "jaws", "Iain Banks", "Spain", "gluten", "Van Eyck", "yvonne", "dalton", "charlie", "wyatt Hayworth", "World War II", "the Battle of Thermopylae", "george george Goncharov", "Yosemite", "The Sandstone Trail", "duncan", "8 minutes", "radionuclides", "eagle", "golden ditties", "West Point", "Saint Cecilia", "algebra", "ichak adizes", "Whittle", "eagle", "Not So Much a Programme", "Chester", "the Brazilian state of Mato Grosso to its confluence with the Paran\u00e1 River north of Corrientes and Resistencia", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "2005", "Wes Craven", "1698", "President Bill Clinton", "an Airbus A320-214", "a broken pelvis", "246", "hurricane", "The Treasure of the Sierra Madre", "smallpox"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6361545138888889}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.625, 1.0, 1.0, 1.0, 0.8, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2911", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-3769", "mrqa_triviaqa-validation-5981", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-4863", "mrqa_triviaqa-validation-4073", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-6358", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-2426", "mrqa_hotpotqa-validation-3034", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-1857"], "SR": 0.515625, "CSR": 0.58875, "EFR": 0.9354838709677419, "Overall": 0.7335186491935485}, {"timecode": 25, "before_eval_results": {"predictions": ["Thoreau", "Thomas Piketty", "1,548", "zoning and building code requirements", "Science and Discovery", "New York", "tuppenny tube", "Vietnam", "a non-speaking character", "bluebird", "boat", "300", "1894", "manhattan", "jon pert pert pert", "jodie Foster", "Billie Holiday", "The National Council for the Unmarried Mother", "Phil Mickelson", "Jean-Paul Sartre", "Len Deighton", "dress", "Alex Garland", "L. Pasteur", "Dionysus", "Benjamin Disraeli", "Johannesburg", "George Washington", "Chicago", "The Frighteners", "points", "ocellaris", "David Cameron", "Newfoundland and Labrador", "Eddie Cochran", "Alessandro di Guericke", "Andre 3000", "Wanderers", "sunshine after afternoon", "the Biafra secession", "Anna Mae Bullock", "Flint", "Cuba", "doves", "Heston Blumenthal", "Harold Godwinson", "jon pertries", "ritchie Valens", "Peterborough United", "carWale", "Bristol", "rabelais", "Krypton", "Cyanea capillata", "if the concentration of a compound exceeds its solubility", "morning news", "Alexander Ragnew", "Paul John", "the underprivileged", "Jennifer Aniston, Demi Moore and Alicia Keys", "80", "Maldives", "Matt Leinart", "Max Planck"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5439503205128206}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.923076923076923, 0.5, 0.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-3418", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-154", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-3582", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-1387", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2965", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4832", "mrqa_hotpotqa-validation-3714", "mrqa_newsqa-validation-439", "mrqa_searchqa-validation-13257"], "SR": 0.453125, "CSR": 0.5835336538461539, "EFR": 1.0, "Overall": 0.7453786057692308}, {"timecode": 26, "before_eval_results": {"predictions": ["paid professionals", "Basel", "\"we want to practice Christian love toward them and pray that they convert,\"", "Informal rule", "animals", "raven", "argon", "John Logie Baird", "london", "pickwick", "Titanic", "john morris", "taekwondo", "Spain", "Rome", "lola", "skull", "almanack", "bury", "oxygen", "magnesium", "Roy johnson", "Venus", "Ben Watson", "French", "Jupiter", "if\u2013", "ummi puckett", "moron", "god", "Australia", "meninges", "loney tunes", "Ely", "Netherlands", "Vladivostok", "Boiling", "ticks", "phoenicia", "Norwegian", "Gulf of Aden", "man", "lichfield", "lithium", "seema", "tin man", "jostling one another's umbrellas in a general infection of ill temper", "tempera", "Brazil", "peacock", "Hong Kong", "Chile", "6ft 1in", "Judiththia Aline Keppel", "94 by 50 feet", "Eli john godman", "photographs, film and television", "March 17, 2015", "AbdulMutallab", "blind", "three", "island hymn", "jedoublen/jeopardy", "May"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6080171130952381}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.375, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2368", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-5478", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-7030", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-1605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-3288", "mrqa_naturalquestions-validation-6106", "mrqa_hotpotqa-validation-3563", "mrqa_newsqa-validation-1640", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-2854"], "SR": 0.5625, "CSR": 0.5827546296296297, "EFR": 0.9285714285714286, "Overall": 0.7309370866402116}, {"timecode": 27, "before_eval_results": {"predictions": ["CBS", "15th", "60%", "Xbox One", "two", "agent Mark Steinberg said Woods would not answer questions", "the National Restaurant Association", "to \"wipe out\" the United States", "blew himself up", "an older generation", "wildland firefighters", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "\"Americans always believe things are better in their own lives than in the rest of the country,\"", "Elena Kagan", "Mandi Hamlin", "750", "a long-range missile in the near future", "claire Clarkson", "Expedia", "Mildred", "severe", "jobs", "Swat Valley", "Parade Lane mosque", "Santaquin City, Utah", "Sunday", "four", "a residential area in East Java", "South Africa", "nearly $2 billion", "Six members of Zoe's Ark", "2002", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "\"It has never been the policy of this president or this administration to torture.\"", "Former Mobile County Circuit Judge Herman Thomas", "Cairo", "mayor of Seoul from 2002 to 2004", "kerstin Fritzl", "they don't feelMisty Cummings has told them everything she knows.", "hung from the beach to offshore buoys", "Melbourne", "into the Southeast", "Sunday", "$273 million", "Salt Lake City, Utah", "millionaire's surtax", "an animal tranquilizer", "Section 60", "NATO's International Security Assistance Force", "Jaime Andrade", "1994", "dance Your Ass Off", "Santiago Ram\u00f3n y Cajal", "Los Angeles", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south", "the Magic Circle", "Newcastle-on-Tyne, England", "sleepless", "Lin-Manuel Miranda", "15,024", "novelist and poet", "mantle", "beaker", "marshmallows"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5889069829626895}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.7272727272727272, 1.0, 0.0, 0.0, 0.15384615384615385, 0.1739130434782609, 1.0, 1.0, 1.0, 0.25, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.5714285714285715, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6, 0.0, 0.15384615384615383, 1.0, 0.18181818181818182, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-4411", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1864"], "SR": 0.484375, "CSR": 0.5792410714285714, "EFR": 0.8787878787878788, "Overall": 0.72027766504329}, {"timecode": 28, "before_eval_results": {"predictions": ["Denver's Executive Vice President of Football Operations and General Manager", "illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins", "Pittsburgh", "Cress", "interstellar space", "Phil Johnston", "the highway between the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "Ram Nath Kovind", "Senator Joseph McCarthy", "100", "members of the gay ( LGBT ) community", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "Wembley Stadium", "more than a million members", "1775", "Continental drift", "Julie Adams", "a combination of genetics and the male hormone dihydrotestosterone", "Jonathan Cheban", "Norman Greenbaum", "De Wayne Warren", "Thirty years after the Galactic Civil War", "restoring someone's faith in love and family relationships", "Darth Tyranus", "April 15, 2018", "April 17, 1982", "Speaker of the House of Representatives", "London, United Kingdom", "a legal case in certain legal systems", "the Near East", "two senators, regardless of its population, serving staggered terms of six years", "Club Bijou on Chapel Street", "pre-Columbian times", "the central plains", "China (formerly the Republic of China ), Russia ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "a type of party, common mainly in contemporary Western culture, where guests dress up in costumes", "into the gastrointestinal tract through a series of ducts", "Kenny Anderson", "beneath the liver", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "Nathan Hale", "Jesse Frederick James Conaway", "the naos", "defense against rain rather than sun", "the port of Veracruz", "September 19, 2017", "West Egg on prosperous Long Island in the summer of 1922", "it was first published on November 12, 1976 by Ballantine Books", "Butter Island off North Haven, Maine in the Penobscot Bay", "wintertime", "Tony Rydinger", "Laura Williams and Sally Dworsky", "Flanagan and Allen", "Florida", "Mediterranean", "Manor of the More", "Craig William Macneill", "Democratic Unionist Party", "military personnel", "1946", "The Orchid thief", "was considered Hebrew", "\"the establishment in Palestine of a national home\" for the Jewish people", "Gordon"], "metric_results": {"EM": 0.453125, "QA-F1": 0.557750611209396}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.47058823529411764, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.823529411764706, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7368421052631577, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.4, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-386", "mrqa_squad-validation-6848", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-6349", "mrqa_triviaqa-validation-7330", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-2150", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1570", "mrqa_searchqa-validation-4495", "mrqa_searchqa-validation-15496"], "SR": 0.453125, "CSR": 0.5748922413793103, "EFR": 0.8857142857142857, "Overall": 0.7207931804187192}, {"timecode": 29, "before_eval_results": {"predictions": ["Antigone", "Meuse, through the Hollands Diep and Haringvliet estuaries", "In 1806", "New Delhi", "MacFarlane", "Super Bowl XXXIX", "Hon July Moyo", "many forested parts of the world", "Narendra Modi", "living and organic material", "Aaron Harrison", "The White House Executive chef", "Michael Crawford", "9 February 2018", "the red bone marrow of large bones", "Drew Barrymore", "Pangaea", "Jonathan Breck", "dermis", "Joe Pizzulo and Leeza Miller", "Ming", "201", "Chuck Noland", "Montreal Canadiens", "Britney Spears", "Waylon Jennings", "Nancy Jean Cartwright", "Coldplay", "Smyrna ( Revelation 2 : 8 - 11 )", "New York Yankees", "1996", "when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "United States customary units", "Joe Spano", "Michael Moriarty", "Rock Island, Illinois", "2002", "September 1959", "Neil Patrick Harris", "Bonnie Lipton", "the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "0.05 ( 5 % )", "Poems : Series 1", "Rebekah", "the central plate", "rizal", "Ernest Rutherford", "Napoleon Bonaparte", "the 13th century", "Yosemite National Park", "Norman Pritchard", "2014", "\"business\"", "the Big Bang", "King Henry VI", "October 13, 1980", "250cc world championship", "Niihau", "Justice Department motion filed last week in support of the Defense of Marriage Act", "Bronx.", "almost 9 million", "\"Tennessee Waltz\"", "abacus", "Cyrus"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6796490200790911}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.25, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 0.33333333333333337, 1.0, 1.0, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.08695652173913043, 0.6666666666666666, 0.24000000000000002, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 0.0, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9225", "mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-2382", "mrqa_naturalquestions-validation-3760", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-5106", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-2195", "mrqa_newsqa-validation-1426"], "SR": 0.53125, "CSR": 0.5734375, "EFR": 0.9666666666666667, "Overall": 0.7366927083333333}, {"timecode": 30, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-392", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-974", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-13874", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15740", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-1649", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9269", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10181", "mrqa_squad-validation-10268", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1765", "mrqa_squad-validation-1791", "mrqa_squad-validation-1848", "mrqa_squad-validation-1890", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-2019", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2243", "mrqa_squad-validation-2411", "mrqa_squad-validation-2456", "mrqa_squad-validation-247", "mrqa_squad-validation-2545", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2742", "mrqa_squad-validation-305", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3507", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3718", "mrqa_squad-validation-3770", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-386", "mrqa_squad-validation-3863", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3986", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-4046", "mrqa_squad-validation-4054", "mrqa_squad-validation-4175", "mrqa_squad-validation-4213", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4629", "mrqa_squad-validation-4883", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5097", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5588", "mrqa_squad-validation-5692", "mrqa_squad-validation-5724", "mrqa_squad-validation-5781", "mrqa_squad-validation-5818", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-6019", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6353", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6543", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6965", "mrqa_squad-validation-6973", "mrqa_squad-validation-6999", "mrqa_squad-validation-7039", "mrqa_squad-validation-71", "mrqa_squad-validation-7192", "mrqa_squad-validation-7368", "mrqa_squad-validation-7426", "mrqa_squad-validation-7521", "mrqa_squad-validation-7612", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7814", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-829", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9176", "mrqa_squad-validation-9196", "mrqa_squad-validation-942", "mrqa_squad-validation-9445", "mrqa_squad-validation-957", "mrqa_squad-validation-9614", "mrqa_squad-validation-9764", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4465", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7461"], "OKR": 0.810546875, "KG": 0.46328125, "before_eval_results": {"predictions": ["flammable cabin and space suit materials", "1992", "at least four passengers", "Genesis", "real estate investment trusts (REITs)", "bayou", "carat", "Mission: Impossible", "presbyter", "Edinburgh", "Teha'amana", "Galpagos", "Mark Twain", "Battle of Chancellorsville", "Wrigley Field", "Wii", "Suez Canal", "Dave Matthews Band", "henry hODGEPODGE", "teeth to shift,", "Don't Smoke in Bed", "Kinko's", "jedoublen/jeopardy", "photon", "tongue bones", "Cherokee", "nekropolis", "Eleanor Roosevelt", "Grand Central Oyster Bar", "fortune", "pimpernel", "bamboos", "Isaac Newton,", "Unabomber", "Narnia", "Freud", "Burma Ruby stone", "librettos", "Scriblerus", "Tracy Letts", "Ikerit Jaswal", "bos taurus", "wyatt e. Grady", "Botswana", "Susan B. Anthony dollar", "Mattel", "Little Red Riding Hood", "spoon splint", "melanin", "Slavic", "spinal cord", "Chanukah", "infection, irritation, or allergies", "Castleford", "usually in May", "wycombe", "Sahara Desert", "henry henry", "You're Next", "Headless Body in Topless Bar", "political correctness", "Hanin Zoabi, a member of the Israeli parliament,", "Princess Diana", "homicide"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5650297619047618}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-3542", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-12684", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-7004", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-16480", "mrqa_searchqa-validation-5586", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-4851", "mrqa_searchqa-validation-382", "mrqa_naturalquestions-validation-2666", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-6237", "mrqa_newsqa-validation-1291"], "SR": 0.484375, "CSR": 0.5705645161290323, "EFR": 1.0, "Overall": 0.7161441532258064}, {"timecode": 31, "before_eval_results": {"predictions": ["the West", "Begter", "the International Series were held in London", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor )", "the University of Oxford", "July 1, 1923", "used as a pH indicator, a color marker, and a dye", "winter", "the \u01c3ke e : \u01c0xarra \u01c1ke", "Khasi and Jaintia Hills", "the largest part of the brain", "Janie Crawford, an African - American woman in her early forties,", "Elliot Scheiner, and RobJac", "the straight - line distance from A to B", "During the reign of King Beorhtric of Wessex ( 786 -- 802 )", "the Department of Health and Human Services, Office of Inspector General", "The primary and secondary recipients can not see the tertiary recipients", "from the `` round '', the rear leg of the cow", "1957", "Ella Mitchell", "An error does not count as a hit but still counts as an at bat for the batter", "Andreas Vesalius ( / v\u026a\u02c8se\u026ali\u0259s / ; 31 December 1514 -- 15 October 1564 )", "Moscazzano", "Kristy Swanson", "detritus from the settlement of the sedimentation", "Asuka", "Jay Baruchel", "the mainland of the Australian continent", "a revolution or orbital revolution", "Houston Astros", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "The optic nerve carries the ganglion cell axons to the brain, and the blood vessels that supply the retina", "in the fascia surrounding skeletal muscle", "Pangaea", "2017", "near the inner rim of the Orion Arm, within the Local Fluff of the Local Bubble, and in the Gould Belt", "Ricky Nelson", "the player shouts in order to attract the caller's attention", "Debbie Gibson", "Hagrid", "a written compendium of Rabbinic Judaism's Oral Torah", "a mark that reminds of the Omnipotent Lord, which is formless", "the first week of April", "Algeria", "the King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "Harlem River", "1998", "R.E.M.", "332", "above the light source and under the sample in an upright microscope, and above the stage and below the light sources in an inverted microscope", "Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "Illinois", "Northumberland", "Northern Ireland", "Travis County", "Boston, Massachusetts", "Adam Dawes", "Republicans", "Zed", "in a tenement in the Mumbai suburb of Chembur, with eight people living together in a single room.", "a dummy", "Aristotle", "nothing gained"], "metric_results": {"EM": 0.5, "QA-F1": 0.6547095904232583}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.5, 0.4, 1.0, 0.0, 0.18181818181818182, 0.16666666666666666, 0.3636363636363636, 0.0, 0.0, 0.3636363636363636, 0.7499999999999999, 0.0, 0.8, 1.0, 1.0, 0.2978723404255319, 0.3636363636363636, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08333333333333334, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.9473684210526315, 0.2666666666666667, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-9791", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-5599", "mrqa_triviaqa-validation-3940", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3518"], "SR": 0.5, "CSR": 0.568359375, "EFR": 0.875, "Overall": 0.690703125}, {"timecode": 32, "before_eval_results": {"predictions": ["magnitude and direction", "2003", "Tara Lipinski", "National Aviation Hall of Fame", "Giotto di Bondone", "1985", "more than 26,000", "Lakshmibai", "the Championship", "French", "2009", "wargame", "Bonobo", "singer", "Greg Gorman and Helmut Newton", "Shameless", "stolperstein", "1901", "Carl Zeiss AG", "YouTube", "Bambi, a Life in the Woods", "Robert \"Bobby\" Germaine, Sr.", "2004", "IndyCar", "one", "\"Twice in a Lifetime\"", "the Sun", "Greg Hertz", "Srinagar", "\"The Walking Dead\"", "Ted Nugent", "jewelry designer", "Gust Avrakotos", "Maleficent", "Coll\u00e8ge de France", "Miami-Dade County", "Marty Ingels", "1945", "Edward R. Murrow", "Conservatorio Verdi", "Mindy Kaling", "June 10, 1982", "beer and soft drinks", "Liga MX", "Donald Duck", "The School Boys", "Lord Chancellor of England", "Taoiseach", "The English Electric Canberra", "Richa Sharma", "48,982, making Southaven the third largest city in Mississippi", "The Sound of Music", "53", "Michigan State Spartans", "Frank Langella", "an elephant", "a cuckoo", "Hydrochloric", "London's Heathrow airport", "homicide", "maintain an \"aesthetic environment\" and ensure public safety,", "Marshal Ptain", "a Stryker", "Hannah Montana"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6755189255189256}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-444", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-1030", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-136", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-3926", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3726", "mrqa_searchqa-validation-4641", "mrqa_searchqa-validation-4628"], "SR": 0.609375, "CSR": 0.5696022727272727, "EFR": 1.0, "Overall": 0.7159517045454545}, {"timecode": 33, "before_eval_results": {"predictions": ["CEPR", "special university classes, called Lehramtstudien", "CTV", "13\u20133", "American", "July 25 to August 4", "1958", "Norway", "twenty-three", "Crips", "The Crowned Prince of the Philadelphia Mob", "the Kentucky Derby", "Charles Edward Stuart", "historic buildings, arts, and published works", "August 9, 2017", "Batman", "Tennessee", "G\u00e9rard Depardieu", "books, films and other media", "King Duncan", "Europop", "1835", "Mayor Ed Lee", "Ghana", "Norwegian", "Dutch", "1976", "January 23, 1898", "light quadricycles", "30.9%", "Charlyn Marie \" Chan\" Marshall", "1968", "76,416", "Father Dougal McGuire", "June 17, 2007", "Deputy F\u00fchrer", "The United States of America (USA)", "The Ryukyuan people", "coaxial", "November 15, 1903", "international producers", "1961", "1952", "Bengali", "relationship with Apple co-founder Steve Jobs", "Pablo Escobar", "ZZ Top", "Larry Wayne Gatlin", "Russian Empire", "Bi-fuel", "the Blue Ridge", "King of Cool", "a subduction zone", "Barry Bonds", "Owen Vaccaro", "Machu Picchu", "Exile", "a downtown restaurant in New York\u2019s Greenwich Village", "normal maritime traffic", "U Win Tin", "$1.45 billion", "onomatopoeia", "Singapore", "the femur"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7596354166666668}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.7499999999999999, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2043", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-3703", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-742", "mrqa_searchqa-validation-15477"], "SR": 0.640625, "CSR": 0.5716911764705883, "EFR": 0.9565217391304348, "Overall": 0.7076738331202047}, {"timecode": 34, "before_eval_results": {"predictions": ["John Elway", "The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object", "over 20 million", "A simple iron boar crest", "Vienna", "period dependent", "the Harpe brothers", "Bill Clinton", "Dirk Werner Nowitzki", "Detroit, Michigan,", "Bury St Edmunds, Suffolk, England", "novelty songs, comedy, and strange or unusual recordings", "Mahoning County", "16 November 1973", "\"There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "Bohemia", "New York", "The Washington Post", "400 MW", "Mauritian", "Household Words", "Gatwick Airport", "Kagoshima Airport", "Minette Walters", "CTV Television Network", "Firestorm", "2013", "Leslie Edwin Miles", "40 Days and 40 Nights", "James Tinling", "2014", "Louis King", "gull-wing doors", "Terry Malloy", "Landing Barge", "Attack the Block", "House of Commons", "Hessians", "Battle of Chester", "Wayne County, Michigan", "Samoa", "mistress of the Robes", "Duchess Eleanor of Aquitaine", "Barack Obama", "August 17, 2017", "Guardians of the Galaxy Vol. 2", "Director of Central Intelligence", "1963", "The Bologna Process", "Paris", "Nebraska Cornhuskers women's basketball team", "Salman Rushdie", "the Internal Revenue Service", "the Hongwu Emperor of the Ming Dynasty", "commemorating fealty and filial piety", "Mexico", "Arkansas", "throat", "1979", "his father", "$8.8 million", "Red Heat", "Miriam Makeba", "a mesio-occlusal cavity"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7179513888888889}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.16, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10316", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-1797", "mrqa_hotpotqa-validation-712", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-2316", "mrqa_newsqa-validation-501", "mrqa_searchqa-validation-9394"], "SR": 0.640625, "CSR": 0.5736607142857143, "EFR": 1.0, "Overall": 0.7167633928571429}, {"timecode": 35, "before_eval_results": {"predictions": ["CBS", "Mount Kenya", "Brava, Cape Verde", "1908", "3 May 1958", "from 1986 to 2013", "Ronald Wilson Reagan", "Chiltern Hills", "Ted", "Bay of Fundy", "CD Castell\u00f3n", "2001", "Sean Yseult", "American country music", "Previously she was a member of the Hawaii House of Representatives from 1990-96", "Operation Watchtower", "Paul W. S. Anderson", "15 February 1970", "Yasiin Bey", "Shooter Jennings", "Cincinnati", "\"Bad Moon Rising\"", "Bjki Farr", "381.6 days", "Atomic Kitten", "Trey Parker and Matt Stone", "Matt Gonzalez", "The Gold Coast", "1898", "King \u00c6thelred the Unready", "PlayStation 4", "Malta", "1966", "Key West", "Europe", "Black Mountain College", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "an Italian former professional footballer", "Comedy Central sketch-comedy series \"Chappelle's Show\"", "Prince George's County", "Pittsburgh, Pennsylvania", "1891", "L\u00edneas A\u00e9reas", "Gainsborough Trinity", "Los Angeles", "October 13, 1980", "water sprite", "India", "Syracuse University", "FIFA Women's World Cup", "Orange County", "76,416", "in various submucosal membrane sites of the body", "Quantitative psychological research", "Will", "Deep Blue", "Albert Reynolds", "George Washington", "U.S. senators", "California-based Current TV", "two", "one bath", "The Lost Boys", "a succotash"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6442302489177489}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.0, 0.2857142857142857, 1.0, 0.5714285714285715, 1.0, 0.8, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4666666666666667, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5670", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-257", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-10259", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-2595", "mrqa_searchqa-validation-16021"], "SR": 0.5625, "CSR": 0.5733506944444444, "EFR": 1.0, "Overall": 0.7167013888888889}, {"timecode": 36, "before_eval_results": {"predictions": ["between June and September", "\"This is not something that anybody can reasonably anticipate,\" said Don Greene, a former FBI agent who has written a book on mall security.\"", "a music video on his land.", "a bank", "July for A Country Christmas", "The Casalesi Camorra clan", "Tulsa, Oklahoma.", "380,000", "Old Trafford", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's war zone.", "\"Home\"", "Matthew Perry and Leslie Mann", "Kabul", "wrote the word \"pig\" in blood on the door of the home", "three-time road race world champion", "Annie Duke", "to step up attacks against innocent civilians.", "The bill orders immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally.", "producing rock music with a country influence", "The Kirchners", "The move frees up a place for another non-European Union player in Frank Rijkaard's squad.", "root out terrorists within its borders.", "violent separatist campaign", "Olympia", "3,000", "closing these racial gaps", "The Joy Behar Show.", "22", "3-0", "150", "the two remaining crew members from the helicopter", "North Korea", "more than 30", "The man who was killed had been part of a hunting party of three men, said Mike Weland, a spokesman for Boundary County, Idaho,", "Both", "23 million square meters (248 million square feet)", "Now Zad in Helmand province, Afghanistan.", "Virgin America", "fuel economy and safety", "American Civil Liberties Union", "summer", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "3rd District of Utah", "mental health and recovery.", "56", "The people kill him with the blocks, because the people are angry. They are not hungry, they are angry,\" Mano told CNN's Lisa Desjardins.", "Frank Ricci", "the Ku Klux Klan", "90", "Cash for Clunkers", "Argentina", "1997", "As of July 2017, there were 103 national parks encompassing an area of 40,500 km ( 15,600 sq mi ), comprising 1.23 % of India's total surface area", "Carolyn Sue Jones", "vanilla", "Hercules", "a map of world climates", "Gian Carlo Menotti", "a silent, giant Native American character in the film \"One Flew Over the Cuckoo's Nest\"", "a semi-nomadic, semi-agricultural tribe", "Suzanne Valadon", "Daisy Miller", "Detaiils", "Apollo"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6241931767866449}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.5714285714285715, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.2666666666666667, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 1.0, 0.923076923076923, 0.10526315789473682, 0.0, 1.0, 0.375, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.17391304347826084, 1.0, 0.15384615384615385, 1.0, 1.0, 0.8, 1.0, 1.0, 0.9565217391304348, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.4, 1.0, 0.1, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-978", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-2051", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-533", "mrqa_naturalquestions-validation-1028", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4389", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-2623"], "SR": 0.515625, "CSR": 0.5717905405405406, "EFR": 0.9354838709677419, "Overall": 0.7034861323016565}, {"timecode": 37, "before_eval_results": {"predictions": ["five", "the state's attorney", "President Abdullah Gul,", "Ed McMahon,", "they are co-chair of the Genocide Prevention Task Force.", "aboard the ship's 28-foot lifeboat,", "clogs", "upper respiratory infection.", "seven", "tells stories of different women coping with breast cancer in five vignettes.", "Bobby Jindal", "Dr. Jennifer Arnold and husband Bill Klein,", "Facebook and Google,", "Alwin Landry", "Venus Williams", "National Intelligence Service, and Defense Minister Kim Kwan Jim", "President Robert Mugabe", "a female soldier,", "his brother, Julio Cesar Godoy Toscano,", "a $158 green skirt and $298 bead and rhinestone cardigan", "$17,000", "his vice president", "Al-Aqsa mosque", "\"momentous discovery\"", "a three-story residential building in downtown Nairobi.", "Robert Barnett,", "Asian qualifying Group 2", "Matthew Fisher", "his government \"to illegally siphon revenue and foreign exchange from the Zimbabwean people,\" as well as one individual.\"", "Ben Roethlisberger", "They're all American citizens born here in the Pacific.", "Pew Research Center", "Sgt. Jason Bendett", "Brazil", "his private information", "$24.1 million", "a jury", "south of Atlanta", "on Sunday.", "President Robert Mugabe", "13.", "Osama bin Laden's sons", "for security reasons and not because of their faith.", "\"We tortured (Mohammed al+) Qahtani,\"", "autonomy.", "Arctic north of Murmansk down to the southern climes of Sochi", "Long Island convenience store", "Ma Khin Khin Leh,", "400 years", "The 19-year-old woman", "Washington State's decommissioned Hanford nuclear site,", "from the breast or lower chest of beef or veal", "winter", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Elberta", "bullfight", "the waltz", "1887", "Atlantic Coast Conference", "uncle", "Pennsylvania", "\"Rabbit\"", "Brunswick", "Labrador retrievevers."], "metric_results": {"EM": 0.484375, "QA-F1": 0.6293574481074481}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6153846153846153, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-2905", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-9856", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-5703", "mrqa_searchqa-validation-12609", "mrqa_triviaqa-validation-3505"], "SR": 0.484375, "CSR": 0.5694901315789473, "EFR": 0.9696969696969697, "Overall": 0.7098686702551834}, {"timecode": 38, "before_eval_results": {"predictions": ["vector quantities", "Transport Workers Union leaders", "March 24,", "Eleven", "Enrique Torres", "Pakistani officials,", "$7.8", "Stratfor's", "Madeleine K. Albright", "Clinton", "his company Polo", "German Foreign Ministry,", "10,000 refugees,", "intravenous vitamin \"drips\"", "Red Lines", "in body bags on the roadway near the bus,", "40", "Flemish tapestries in an east-facing sitting room called the Morning Room.", "Islamic militants", "in the heart of Los Angeles.", "October 29 and November 5", "Sunni Arab and Shiite tribal leaders", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "an antihistamine and an epinephrine auto-injector", "North Korea", "Hong Kong from other parts of Asia, such as India and mainland China,", "two Israeli soldiers,", "the release of the four men", "ties,", "Sheikh Sharif Sheikh Ahmed", "Saturday's Hungarian Grand Prix.", "power-sharing talks", "from her small home in gritty Soweto township.", "in Amstetten,", "12-1", "Africa", "they can demonstrate they have been satisfactorily treated", "Zimbabwe's main opposition party", "FBI.", "first grand Slam,", "\"it should stay that way.\"", "CNN", "Barack Obama", "strength of its brand name and the diversity of its product portfolio,", "U.S. State Department and British Foreign Office", "Monday", "Fiona MacKeown", "sculptures", "Pakistan's High Commission in India", "Bryant Purvis", "morphine elixir", "In 1871", "Kenneth Kaunda", "psychology", "all animals,", "Treaty of Utrecht", "Massachusetts", "James Harrison", "Ford Island", "Tomorrowland", "Tiger Woods", "Wall Street", "January 15", "Sesame Street"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5937551725235548}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 0.8333333333333333, 0.4, 1.0, 0.11764705882352941, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-334", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-237", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-1060", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-6409", "mrqa_searchqa-validation-2728", "mrqa_searchqa-validation-13907"], "SR": 0.453125, "CSR": 0.5665064102564102, "EFR": 0.9714285714285714, "Overall": 0.7096182463369963}, {"timecode": 39, "before_eval_results": {"predictions": ["Andrew Lortie", "people are paid for each specific, short-term task that they do and don't have conventional contracts of employment", "cone\u00ef\u00bf\u00bdin", "high cooking", "Silver Hatch", "peripheral nerves", "Ethiopia", "blue Butterflies", "two young people that fall in love, but are kept apart by their friends/families.", "4G WiFi device the Osprey 2", "a person trained for travelling in space.", "China currently has two special administrative zones \u2013 also known as SAR, Hong Kong and Macau,", "Alastair Cook", "Enterprise", "Three interior die-cut holes", "Asia", "hip", "Frank Sinatra", "meninges", "English", "Charles Brandon", "Munich,", "Henry Mancini,", "Fred Astaire", "five", "Tuesday, September 21, 2010", "Sudan", "Low Countries", "\"Sierra One from Sierra Oscar\"", "The Bible", "TV personality", "Jamaica", "Tornado,", "police drama", "S\u00e3o Vicente Island", "pancreas", "puff", "football", "Antoine Lavoisier", "Leon", "Neuna", "societies or amalgamations of persons", "Pet Shop Boys", "Sir John Houblon,", "Algiers", "Frank Doel,", "Aabaptists", "Crispin", "Hebrew", "John Virgo.", "herpes virus,", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods )", "Garfield Sobers", "in the mountains outside City 17", "Krusty the Clown", "Johannes Vermeer", "O.T. Genasis", "Climatecare,", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Schalke", "Lost in America", "9:21 AM CDT", "Soviet Union", "Senate Democrats"], "metric_results": {"EM": 0.421875, "QA-F1": 0.46799759676597913}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-970", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-86", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3132", "mrqa_searchqa-validation-6304", "mrqa_newsqa-validation-1550"], "SR": 0.421875, "CSR": 0.562890625, "EFR": 0.972972972972973, "Overall": 0.7092039695945946}, {"timecode": 40, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1389", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10606", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-1264", "mrqa_searchqa-validation-12829", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16453", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10263", "mrqa_squad-validation-10317", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10369", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-127", "mrqa_squad-validation-1408", "mrqa_squad-validation-1430", "mrqa_squad-validation-1453", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2094", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3124", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3444", "mrqa_squad-validation-3497", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3946", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-4715", "mrqa_squad-validation-491", "mrqa_squad-validation-4918", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5664", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6706", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7147", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7296", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7751", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8154", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-8841", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-568", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-899"], "OKR": 0.857421875, "KG": 0.465625, "before_eval_results": {"predictions": ["chameleon circuit", "Jake La Motta", "danish", "Joshua", "pangram", "Let Die", "lassie", "salford", "Brazil", "Robert Hooke", "Hadrian", "John", "Sony Interactive Entertainment", "king Henry I of England", "green,", "1215", "Elijah's Chariot", "Robinson Crusoe", "Charles Dickens", "Brussels", "Egypt", "a neutron", "earache", "New York Yankees", "Four Tops", "hudd", "July 20, 1969", "9 gallons", "bali", "lilac", "Hilary Swank", "redbird", "dove", "a mole", "John McCarthy", "springtime for Hitler", "two", "lV", "shaft", "hindfoot", "The Daily Mirror", "zak Starkey", "horse", "indus", "southerly", "Vitcos", "Paul McCartney", "Madness", "Jane Eyre", "Kansas", "Australia", "A marriage officiant", "Ricky Nelson", "Abenaki and Mi'kmaq", "Port Moresby, Papua New Guinea", "A bass", "Security Management", "eight", "Russia", "help women \" learn how to dance and feel sexy,\"", "Malacca", "smith", "livin", "Octopus"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5753906249999999}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.8750000000000001, 0.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-571", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-1746", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-56", "mrqa_triviaqa-validation-2632", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6915", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-774", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-9938", "mrqa_searchqa-validation-5984", "mrqa_naturalquestions-validation-6903"], "SR": 0.46875, "CSR": 0.5605945121951219, "EFR": 0.9705882352941176, "Overall": 0.721236549497848}]}