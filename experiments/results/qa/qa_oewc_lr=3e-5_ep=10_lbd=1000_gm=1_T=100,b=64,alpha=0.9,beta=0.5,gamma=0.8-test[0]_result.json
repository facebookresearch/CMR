{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=1000.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4160, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["67.9", "Mike Carey", "rapidly raising population and traffic in cities along SR 99, as well as the desirability of Federal funding", "their greatest common divisor is one", "the Official Report", "immunoinformatics", "lupus erythematosus", "Kublai Khan", "New Testament", "1926", "other ctenophores", "60%", "he was illiterate in Czech", "architect or engineer", "British", "Gateshead Council", "Book of Genesis", "Shing-Tung Yau", "complexity classes", "Mexico", "cabinet", "after its 1977 merger with Radcliffe College", "The Master", "chastity", "Mark Woods", "one another", "100% oxygen", "Steam engines", "the wedding banquet", "the Ohio Company of Virginia", "CBS and NBC", "aircraft manufacturing", "a school or other place of formal education", "Times Square Studios", "Normans and Norman", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "decreases", "Duke Richard II of Normandy, and King Ethelred II of England", "Royal Shakespeare Company", "books and articles", "between 1835 and 1842", "Edmonton, Canada", "rises in sea levels", "it is neither zero nor a unit", "University of Chicago College Bowl Team", "algorithms have been written that solve the problem in reasonable times in most cases", "13.34%", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "49\u201315", "The Mongols' extensive West Asian and European contacts", "east-west", "the kip", "Croatia", "railway locomotives", "the law is no longer to be taught to Christians but belonged only to city hall", "Josh Norman", "1964 and 1968", "expelled Jews", "Elton Rule", "gravel", "11:28", "Super Bowl XLVII", "a fee per unit of information transmitted", "seven"], "metric_results": {"EM": 0.875, "QA-F1": 0.9101325145442792}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1187", "mrqa_squad-validation-2853", "mrqa_squad-validation-6986", "mrqa_squad-validation-2704", "mrqa_squad-validation-1891", "mrqa_squad-validation-1161", "mrqa_squad-validation-1089", "mrqa_squad-validation-2473"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["\"Fall of the Eleventh\"", "The input string", "European Parliament", "the Meuse", "overseas colonies", "Kings Canyon Avenue and Clovis Avenue", "entertainment", "Astra 2A", "poison", "atoon", "Orange", "the BBC", "Dolby Digital", "shocked", "Executive Vice President of Football Operations", "Thomas Edison", "was particularly forceful, stating that British colonists would not be safe as long as the French were present.", "least prejudiced", "inferior", "Johann Tetzel", "miniature cydippids", "main porch", "the courts of member states", "Shah's decision to divide his army into small groups concentrated in various cities", "1st century BC", "Iberia", "Silas B. Cobb", "Aristotle", "The Swahili", "the sheepshanks Gallery", "1968", "heavy/highway, heavy civil or heavy engineering", "type III secretion system", "Commission v Italy", "Gary Kubiak", "The European Court of Justice", "oxygen compounds", "1933", "Endosymbiotic gene transfer", "proplastids", "Triassic Period of the Mesozoic Era", "Conrad of Montferrat", "sacramental union", "reciprocating", "the Arabs and much of the rest of the Third World", "mineral deposits", "Grand Canal d'Alsace", "Vince Lombardi Trophy", "\u00dcberseering BV v Nordic Construction GmbH", "first 15 years", "Neoclassical economics", "Variable lymphocyte receptors", "Exploration", "Elder", "internal migration and urbanisation", "high risk preparations and some other compounding functions", "water level", "two", "James", "A driver's license is an official document permitting a specific individual to operate one or more types of motorized vehicles", "radioisotope thermoelectric generator", "Mickey Mantle", "A lymphocytes is one of the subtypes of white blood cell in a vertebrate's immune system", "Amsterdam"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7073051948051948}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7689", "mrqa_squad-validation-1767", "mrqa_squad-validation-3922", "mrqa_squad-validation-6029", "mrqa_squad-validation-2672", "mrqa_squad-validation-6211", "mrqa_squad-validation-1902", "mrqa_squad-validation-375", "mrqa_squad-validation-10186", "mrqa_squad-validation-6163", "mrqa_squad-validation-3511", "mrqa_squad-validation-8927", "mrqa_squad-validation-9325", "mrqa_squad-validation-3370", "mrqa_squad-validation-7635", "mrqa_squad-validation-2315", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-9342", "mrqa_newsqa-validation-1945"], "SR": 0.671875, "CSR": 0.7734375, "EFR": 0.9523809523809523, "Overall": 0.8629092261904762}, {"timecode": 2, "before_eval_results": {"predictions": ["six quadrangles", "1883\u201384", "1870 to 1939", "2010", "unit-dose, or a single doses of medicine", "2003", "Budapest Telephone Exchange", "induction motor", "markets", "to avoid being targeted by the boycott", "socialist realism", "Ten", "city's tax base dissipated", "4000", "St. Bartholomew's Day massacre", "early twentieth century homes, many of which have been restored in recent decades", "Thesis 86", "five or more seats", "in a glass case", "1,320 kilometres (820 miles)", "force-free magnetic fields", "a Tatar chieftain, Tem\u00fcjin-\u00fcge", "The Three Doctors", "San Francisco Bay Area's Levi's Stadium", "1", "St. Lawrence and Mississippi watersheds", "2010", "cholera", "168,637", "four", "faith", "25", "river Deabolis", "ten", "to spearhead the regeneration of the North-East", "the late 1920s", "2007", "1936", "1968", "the European Parliament and the Council of the European Union", "typhus, smallpox and respiratory infections", "California", "Spanish", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "the sheepshanks Gallery", "Four thousand", "Prime ideals", "Stairs", "a lack of understanding of the legal ramifications, or due to a fear of seeming rude", "MHC class I molecules", "the Great Fire of London", "in his lab and elsewhere", "Rudy Clark", "Art Carney", "Ren\u00e9 Verdon", "honey bees may be the state's most valuable export", "July 4, 1776", "May 5, 1904", "The Lykan Hypersport is a Lebanese limited production supercar built by W Motors, a United Arab Emirates based company, founded in 2012 in Lebanon with the collaboration of Lebanese, French and Italian engineers", "E \u00d7 12, A \u00d7 9, I \u00d7 9, O \u00d7 8, N \u00d7 6, L \u00d7 4, S \u00d7 4", "the most recent technological change to the u.s. economy was?", "Trace Adkins", "he checked himself into a Los Angeles mental institution in an effort to kick the habit", "refugees agency said on Tuesday.Islamist fighters exchange gunfire with government forces in Mogadishu on July 3."], "metric_results": {"EM": 0.734375, "QA-F1": 0.8028958559230144}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.12903225806451613, 0.0, 0.0, 1.0, 0.761904761904762, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9855", "mrqa_squad-validation-6526", "mrqa_squad-validation-1277", "mrqa_squad-validation-7246", "mrqa_squad-validation-5751", "mrqa_squad-validation-4669", "mrqa_squad-validation-133", "mrqa_squad-validation-4546", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-9392", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-3164"], "SR": 0.734375, "CSR": 0.7604166666666666, "EFR": 1.0, "Overall": 0.8802083333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["the Hamiltonian path problem", "Thomas Edison", "systematic economic inequalities", "July 31, 1995", "vitamin D", "Thomas Edison", "1987", "Aboriginal", "Leukocytes", "research, exhibitions and other shows", "Erg\u00e4nzungsschulen", "along the coast", "a \"principal hostile country\"", "17", "The Book of Discipline", "67.9", "Turkey", "over 100%", "Saffir-Simpson Scale", "New Orleans", "Nearly 3,000", "Earth", "Bruno Mars", "Merwede", "Antigone", "Soviet Union", "the Dodge D-50", "over the age of 18", "various causes", "polynomial-time reduction", "the WMO", "several years", "Danny Trevathan", "Albert of Mainz", "patient care rounds drug product selection", "Thomas Coke", "Colony of Victoria Act 1855", "Anglo-Saxon populations", "John Debney", "Geordie", "the Ancient Greeks", "Budget cuts", "Systemic acquired resistance (SAR)", "one darkened lens; the picture would look normal to those viewers who watched without the glasses.", "the Legislative Assembly", "the desire to prevent things that are indisputably bad", "Any member", "Tyrion", "Manchuria", "Kenneth Cook ( 2018 )  Samuel F. Herd ( 1999 )   Geoffrey Jackson ( 2005)", "to capitalize on her publicity", "1937", "Wah - Wah ''", "James W. Marshall", "1979", "January 12, 2017", "a loop", "Kyrie Irving", "Charles Carson", "Morgan Freeman", "do n 't tell mom the babysitter's dead", "The Seven Cities of Cibola", "in the keyboard", "The fibula is a long, thin bone running parallel to the tibia"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7734221283420368}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false], "QA-F1": [0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789473, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.4615384615384615, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.19999999999999998]}}, "before_error_ids": ["mrqa_squad-validation-1760", "mrqa_squad-validation-3770", "mrqa_squad-validation-100", "mrqa_squad-validation-9157", "mrqa_squad-validation-1436", "mrqa_squad-validation-1764", "mrqa_squad-validation-7713", "mrqa_squad-validation-7838", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-849", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-6338"], "SR": 0.734375, "CSR": 0.75390625, "EFR": 0.8823529411764706, "Overall": 0.8181295955882353}, {"timecode": 4, "before_eval_results": {"predictions": ["immune surveillance", "Arizona Cardinals", "10,000", "3 January 1521", "The Scotland Act 1998", "AC", "Chuck Howley", "1\u20133 \u03bcm thick", "United States Air Force", "Meiji Restoration", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "842 pounds", "June 4, 2014", "recast as decision problems", "Gateshead Council", "more than 70", "Gamal Abdul Nasser", "the Bible", "completes replication with a rolling circle mechanism", "inner core", "Hangzhou", "genetic branches", "12 December 1963", "UNESCO World Heritage Site", "January 27, 1967", "cortisol and catecholamines", "constituency seats", "drummes", "Tower Theatre", "Min system", "to pressure the lazy, inspire the bored, deflate the cocky", "North American Aviation", "banded iron formations", "Arizona Cardinals", "Super Bowl XXXVIII", "major business districts", "HO", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "two", "destruction of the forest", "the Commission", "net mechanical energy", "Josh Cantrell", "Saul", "winter", "southwestern Colorado and northwestern New Mexico", "Paul to the Philippians", "Carol Worthington", "1997", "accomplish the objectives of the organization", "3 October 1990", "The federal government received only those powers which the colonies had recognized as belonging to king and parliament", "Jerry Leiber and Mike Stoller", "Heroes and Villains", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "positive, zero, or negative scalar quantity", "June 12, 2018", "Tim McGraw and Kenny Chesney", "N 80.000 \u00b0 W \ufeff", "James Martin Lafferty", "La Dame aux cam\u00e9lias", "C7 Stingrays", "InStyle", "\"blank slate\" without rules for"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7055803571428572}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5714285714285715, 0.4, 1.0, 0.3333333333333333, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-635", "mrqa_squad-validation-8750", "mrqa_squad-validation-6327", "mrqa_squad-validation-3811", "mrqa_squad-validation-1649", "mrqa_squad-validation-8683", "mrqa_squad-validation-4949", "mrqa_squad-validation-1920", "mrqa_squad-validation-73", "mrqa_squad-validation-2632", "mrqa_squad-validation-6614", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-5451", "mrqa_triviaqa-validation-4831", "mrqa_hotpotqa-validation-5838", "mrqa_newsqa-validation-1003", "mrqa_searchqa-validation-5338"], "SR": 0.59375, "CSR": 0.721875, "EFR": 0.9230769230769231, "Overall": 0.8224759615384616}, {"timecode": 5, "before_eval_results": {"predictions": ["orientalism", "Director", "the east end", "NL and NC", "1534", "April 1887", "\u00d6gedei Khan", "nearby open spaces", "the West Side", "John Pell, Lord of Pelham Manor", "Dai Setsen", "821,784", "two political parties would share power equally", "buoyancy", "ancestors", "divergence problem", "Islamist", "satellite television", "ordered the deportation of the French-speaking Acadian population from the area", "justifying grace", "131", "co-chair", "Maria Sk\u0142odowska-Curie Institute of Oncology", "winter of 1973\u201374", "P", "BAFTA Television Award", "Off-Off Campus", "the General Conference", "Super Bowl XLIV", "Sam Chisholm", "the mother", "LDS Church", "the most cost efficient bidder", "non-governmental agencies", "non-Catholics", "southern and central parts of France", "not been renewed", "Luther's education", "1998", "administrative supervision over all courts and the personnel thereof", "XXXX", "Thunder Road", "Sylvester Stallone", "present ( 2016 -- 2018, contemporaneous with airing ) and a storyline taking place at a set time in the past", "Mohammad Reza Pahlavi", "Thomas Mundy Peterson", "in the bible", "two goods", "season seven", "TLC", "the town of Acolman, just north of Mexico City,", "1987", "Asuka", "4 September 1936", "Amy Wong", "2002", "Pasek and Paul", "U.S. service members who have died without their remains being identified", "mostly by women, and rings can feature diamonds or other gemstones. In some cultures men and women wear matching rings, and engagement rings may also be used as wedding rings", "George Bernard Shaw", "Mary Porter ( Keyes) Babcock", "Bryant Purvis", "deuce", "40,400 members"], "metric_results": {"EM": 0.703125, "QA-F1": 0.728125}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3492", "mrqa_squad-validation-10273", "mrqa_squad-validation-9416", "mrqa_squad-validation-8579", "mrqa_squad-validation-1759", "mrqa_squad-validation-7819", "mrqa_squad-validation-6752", "mrqa_squad-validation-1008", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-10093", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-1989"], "SR": 0.703125, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 6, "before_eval_results": {"predictions": ["monophyletic", "memory consumption", "1971", "rapid combustion", "Stanford", "a continuous supply of gaseous oxygen to be pumped through a pipeline", "chloroplast", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "Citadel Media", "lands west of the Appalachian Mountains", "two", "ten", "treats the death of a heretic", "average workers", "Catholic", "late 1886", "potentially dangerous", "Golovin", "an upper limit", "near the center of the chloroplast", "the last 7000 years", "3.6%", "treats him to be suspicious of even the greatest thinkers and to test everything himself by experience", "C. J. Anderson", "Robert R. Gilruth", "1469", "the border with Somalia and Ethiopia", "$216,000", "slightly more than atmospheric pressure", "with observations", "6800", "after their second year", "produces no sugar", "Giuliano da Sangallo", "two", "Chuck Connors", "treats", "Lincoln Logs", "a tin star", "in the muscle tissue", "Connochaetes", "treats", "Hamlet", "Plymouth Rock", "President Abraham Lincoln", "Martina Hingis", "treats", "TESLAR Satellite", "treats", "Gentlemen Prefer Blondes", "treats", "treats the number of bonds formed by most elements in their compounds", "Bouvier", "Paris", "Titanic", "treats", "New Zealand", "Warren Ladd", "January 15, 2010", "Mark Neveldine and Brian Taylor", "four", "treats", "Edward Kenway ( Matt Ryan )", "1770 BC"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5983077686202686}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.25, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-1665", "mrqa_squad-validation-3676", "mrqa_squad-validation-8643", "mrqa_squad-validation-2754", "mrqa_squad-validation-10316", "mrqa_squad-validation-8900", "mrqa_squad-validation-2079", "mrqa_squad-validation-8399", "mrqa_squad-validation-1330", "mrqa_squad-validation-3479", "mrqa_squad-validation-8529", "mrqa_squad-validation-8924", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-5627", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1643", "mrqa_naturalquestions-validation-8909", "mrqa_searchqa-validation-9828", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-8659"], "SR": 0.515625, "CSR": 0.6897321428571428, "EFR": 1.0, "Overall": 0.8448660714285714}, {"timecode": 7, "before_eval_results": {"predictions": ["a free state", "Oireachtas funds", "May 21, 2013", "Warsaw", "communications between Yuan dynasty and its ally and the novel and the increased use of the written vernacular", "n > 3", "a fire that started as a kitchen fire", "the frequency and severity of micrometeorite impacts", "Charles Avison", "colonel", "Pedro Men\u00e9ndez de Avil\u00e9s", "a mainline Protestant Methodist denomination", "Capital Cities Communications", "up to 40 km wide", "meritocracy", "a very reactive allotrope of oxygen", "bitstrings", "markets", "a Gender pay gap in favor of males in the labor market", "2014", "By 9000 BP", "Neoclassical economics", "d'Hondt method", "made tape recordings of the show", "the Kenyan coastal town of Kilifi", "first set of endosymbiotic events", "Abe Silverstein", "embroidery", "three", "1823", "British Sky Broadcasting Group plc", "Pylos and Thebes", "Supreme Court Judge", "the north wing of the State Capitol in Saint Paul", "Syracuse", "artist and graffiti writer", "General Manager", "Bergen", "John Lennon", "David Irving", "a album", "Croatan, Nantahala, and Uwharrie", "15,000 people", "2016 World Indoor Championships", "fighter ace credited with 35 victories", "psilocybin", "Washington, D.C.", "Na Na", "a state, which was both free from slavery, and ruled by non-whites and former captives", "Jena Malone", "3 June 1865", "National Hockey League", "one", "the Big 12 Conference", "Walldorf", "a Douglas-Long Beach built B-17G-95-DL", "nursery rhyme", "Necator americanus and Ancy lostoma duodenale", "Hamelin", "11 healthy eggs", "bushes", "a neurological disorder causing an attractive urge to move the body while relaxing or trying to get to sleep", "C. S. Forester", "6 verses"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6619086642524142}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.7692307692307693, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8066", "mrqa_squad-validation-7233", "mrqa_squad-validation-5351", "mrqa_squad-validation-10174", "mrqa_squad-validation-9195", "mrqa_squad-validation-3497", "mrqa_squad-validation-7447", "mrqa_squad-validation-9436", "mrqa_squad-validation-9532", "mrqa_squad-validation-7647", "mrqa_squad-validation-8453", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-5346", "mrqa_naturalquestions-validation-6200", "mrqa_searchqa-validation-7219", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-221"], "SR": 0.53125, "CSR": 0.669921875, "EFR": 1.0, "Overall": 0.8349609375}, {"timecode": 8, "before_eval_results": {"predictions": ["a rock concert", "50-yard line", "a decrease in the price of skilled labor", "12 December 1964", "speed-up theorem", "Schr\u00f6dinger equation", "Guinness World Records", "nominate speakers", "Louis Agassiz", "Annual Status of Education Report", "locomotion", "geophysical surveys", "The later accidental introduction of Beroe", "socially", "one of the most common forms of school discipline", "the Romantic Rhine", "British colonists", "adjustable spring-loaded valve", "complicated", "Super Bowl L", "7\u20134\u20132\u20133 system", "much higher school fees", "WatchESPN", "Economist", "Richard Lindzen", "The revocation forbade Protestant services, required education of children as Catholics, and prohibited emigration", "Political Islam", "between 25-minute episodes", "Figaro", "Mazda", "US Naval Submarine Base New London submarine school", "Secretary of Defense", "Nanyue", "Vyd\u016bnas", "1989", "American", "Anne of Green Gables", "Fainaru Fantaj\u012b Tuerubu", "Reverend Lovejoy", "his death", "Walldorf", "Kings Point, New York", "Bill Miner", "Agent 99", "Outside", "Christopher Nolan", "Umina Beach", "Chicago", "Thriller", "1992", "Let's Make Sure We Kiss Goodbye", "Andrzej Go\u0142ota", "Alonso L\u00f3pez", "post\u2013World War II", "Waylon Albright", "The New Yorker", "Type 10", "their need to repent in time", "gautama", "Jared Polis", "gregory", "Speaker of the House of Representatives", "1947, 1956, 1975, 2015 and 2017", "49"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5893773723257418}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.5217391304347826, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7436", "mrqa_squad-validation-8369", "mrqa_squad-validation-10386", "mrqa_squad-validation-4327", "mrqa_squad-validation-2085", "mrqa_squad-validation-477", "mrqa_squad-validation-7131", "mrqa_squad-validation-3124", "mrqa_squad-validation-9608", "mrqa_squad-validation-7707", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-2481", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-2976", "mrqa_newsqa-validation-3169", "mrqa_searchqa-validation-14617", "mrqa_naturalquestions-validation-5865"], "SR": 0.515625, "CSR": 0.6527777777777778, "EFR": 1.0, "Overall": 0.8263888888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["Konwiktorska Street", "glass", "39", "1760", "CALIPSO", "the state", "The European Court of Justice", "if the head of government of a country were to refuse to enforce a decision of that country's highest court", "lymphokines", "solar power", "specific terminology has no more (or no less) meaning than the individual orator intends it to have", "Finsteraarhorn", "1015 kelvins", "Aaron Spelling", "1770", "833,500", "1851", "Canada", "Northern Chinese", "1865", "use the proceedings as a forum to inform the jury and the public of the political circumstances surrounding the case and their reasons for breaking the law via civil disobedience", "Gap", "\"missile gap\"", "Kensington", "Port of Long Beach", "an eccentric U.S. saloon-keeper", "animated", "close to 50 million", "George Clooney", "Matt Groening", "Hern\u00e1n Jorge Crespo", "William Finn", "Kenny Young", "Alistair Grant", "literary", "The Rebirth", "sulfur mustards", "Hearts", "Christian Maelen", "a scholar during the Joseon Dynasty who begins to write erotic novels, and becomes the lover of the King's favorite concubine", "The Terminator", "the Saint Petersburg Conservatory", "Michael Phelps", "Bolton, England", "Quasimodo", "Cuban", "Cleveland Browns", "Dhivehi Raa'jeyge Jumhooriyya", "\u00c9cole des Beaux-Arts", "the Kentucky Music Hall of Fame", "American 3D computer-animated comedy", "Agra", "Polka", "Esteban Ocon", "actress", "Kassie \"Kassie\" DePaiva", "Jaydev Shah", "Stephen Lang", "Doris Lessing", "April", "voluntary manslaughter", "Sodra nongovernmental organization", "Tutankhamun", "consonants"], "metric_results": {"EM": 0.625, "QA-F1": 0.707176681570064}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.08, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 0.0, 1.0, 0.3076923076923077, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3173", "mrqa_squad-validation-6814", "mrqa_squad-validation-5621", "mrqa_squad-validation-6837", "mrqa_squad-validation-5294", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-4944", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-9926"], "SR": 0.625, "CSR": 0.65, "EFR": 1.0, "Overall": 0.825}, {"timecode": 10, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-2362", "mrqa_hotpotqa-validation-2481", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-3898", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-476", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4903", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5162", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5512", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5701", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-965", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6604", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-9536", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9842", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-9926", "mrqa_squad-validation-10001", "mrqa_squad-validation-10059", "mrqa_squad-validation-10063", "mrqa_squad-validation-1008", "mrqa_squad-validation-10123", "mrqa_squad-validation-10148", "mrqa_squad-validation-10174", "mrqa_squad-validation-10181", "mrqa_squad-validation-10186", "mrqa_squad-validation-10273", "mrqa_squad-validation-10316", "mrqa_squad-validation-10386", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-1078", "mrqa_squad-validation-1089", "mrqa_squad-validation-1161", "mrqa_squad-validation-1177", "mrqa_squad-validation-1177", "mrqa_squad-validation-1182", "mrqa_squad-validation-1187", "mrqa_squad-validation-1219", "mrqa_squad-validation-1254", "mrqa_squad-validation-1277", "mrqa_squad-validation-133", "mrqa_squad-validation-134", "mrqa_squad-validation-1356", "mrqa_squad-validation-1423", "mrqa_squad-validation-1432", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1593", "mrqa_squad-validation-1613", "mrqa_squad-validation-1614", "mrqa_squad-validation-1640", "mrqa_squad-validation-1649", "mrqa_squad-validation-1665", "mrqa_squad-validation-1678", "mrqa_squad-validation-168", "mrqa_squad-validation-1681", "mrqa_squad-validation-175", "mrqa_squad-validation-1759", "mrqa_squad-validation-1767", "mrqa_squad-validation-1779", "mrqa_squad-validation-1815", "mrqa_squad-validation-185", "mrqa_squad-validation-1859", "mrqa_squad-validation-1891", "mrqa_squad-validation-1898", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-1980", "mrqa_squad-validation-2079", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2144", "mrqa_squad-validation-215", "mrqa_squad-validation-2186", "mrqa_squad-validation-2197", "mrqa_squad-validation-2200", "mrqa_squad-validation-2214", "mrqa_squad-validation-2248", "mrqa_squad-validation-2272", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-242", "mrqa_squad-validation-2473", "mrqa_squad-validation-2490", "mrqa_squad-validation-2568", "mrqa_squad-validation-2586", "mrqa_squad-validation-2612", "mrqa_squad-validation-2632", "mrqa_squad-validation-2666", "mrqa_squad-validation-2672", "mrqa_squad-validation-270", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-271", "mrqa_squad-validation-2725", "mrqa_squad-validation-2765", "mrqa_squad-validation-2775", "mrqa_squad-validation-2807", "mrqa_squad-validation-2811", "mrqa_squad-validation-2853", "mrqa_squad-validation-2873", "mrqa_squad-validation-2893", "mrqa_squad-validation-2950", "mrqa_squad-validation-2975", "mrqa_squad-validation-2986", "mrqa_squad-validation-3007", "mrqa_squad-validation-3076", "mrqa_squad-validation-3096", "mrqa_squad-validation-3124", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3140", "mrqa_squad-validation-3143", "mrqa_squad-validation-3173", "mrqa_squad-validation-3275", "mrqa_squad-validation-3319", "mrqa_squad-validation-3327", "mrqa_squad-validation-3370", "mrqa_squad-validation-3407", "mrqa_squad-validation-3464", "mrqa_squad-validation-3479", "mrqa_squad-validation-3492", "mrqa_squad-validation-3497", "mrqa_squad-validation-3511", "mrqa_squad-validation-3537", "mrqa_squad-validation-3550", "mrqa_squad-validation-3581", "mrqa_squad-validation-3676", "mrqa_squad-validation-3723", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3788", "mrqa_squad-validation-38", "mrqa_squad-validation-3802", "mrqa_squad-validation-3811", "mrqa_squad-validation-3813", "mrqa_squad-validation-3842", "mrqa_squad-validation-3852", "mrqa_squad-validation-3871", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3923", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-3945", "mrqa_squad-validation-402", "mrqa_squad-validation-4034", "mrqa_squad-validation-4179", "mrqa_squad-validation-420", "mrqa_squad-validation-4227", "mrqa_squad-validation-4246", "mrqa_squad-validation-4292", "mrqa_squad-validation-4327", "mrqa_squad-validation-4430", "mrqa_squad-validation-4437", "mrqa_squad-validation-4473", "mrqa_squad-validation-4484", "mrqa_squad-validation-4607", "mrqa_squad-validation-4612", "mrqa_squad-validation-4636", "mrqa_squad-validation-4660", "mrqa_squad-validation-4737", "mrqa_squad-validation-4750", "mrqa_squad-validation-476", "mrqa_squad-validation-4882", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-4981", "mrqa_squad-validation-5007", "mrqa_squad-validation-5019", "mrqa_squad-validation-5030", "mrqa_squad-validation-5042", "mrqa_squad-validation-5085", "mrqa_squad-validation-5135", "mrqa_squad-validation-5147", "mrqa_squad-validation-5196", "mrqa_squad-validation-5198", "mrqa_squad-validation-5276", "mrqa_squad-validation-5294", "mrqa_squad-validation-5325", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5351", "mrqa_squad-validation-5389", "mrqa_squad-validation-5434", "mrqa_squad-validation-5531", "mrqa_squad-validation-5621", "mrqa_squad-validation-5634", "mrqa_squad-validation-5671", "mrqa_squad-validation-5699", "mrqa_squad-validation-5724", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-5788", "mrqa_squad-validation-5797", "mrqa_squad-validation-5869", "mrqa_squad-validation-5875", "mrqa_squad-validation-5947", "mrqa_squad-validation-5961", "mrqa_squad-validation-6029", "mrqa_squad-validation-6089", "mrqa_squad-validation-611", "mrqa_squad-validation-6159", "mrqa_squad-validation-6163", "mrqa_squad-validation-624", "mrqa_squad-validation-6318", "mrqa_squad-validation-6341", "mrqa_squad-validation-635", "mrqa_squad-validation-6351", "mrqa_squad-validation-6381", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6546", "mrqa_squad-validation-6555", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6594", "mrqa_squad-validation-6628", "mrqa_squad-validation-6636", "mrqa_squad-validation-6648", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6716", "mrqa_squad-validation-6752", "mrqa_squad-validation-679", "mrqa_squad-validation-6814", "mrqa_squad-validation-682", "mrqa_squad-validation-6837", "mrqa_squad-validation-6838", "mrqa_squad-validation-6873", "mrqa_squad-validation-6877", "mrqa_squad-validation-6924", "mrqa_squad-validation-6960", "mrqa_squad-validation-6978", "mrqa_squad-validation-6981", "mrqa_squad-validation-6986", "mrqa_squad-validation-7126", "mrqa_squad-validation-7131", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7168", "mrqa_squad-validation-7194", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7277", "mrqa_squad-validation-729", "mrqa_squad-validation-73", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7384", "mrqa_squad-validation-7436", "mrqa_squad-validation-7447", "mrqa_squad-validation-7476", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7585", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7652", "mrqa_squad-validation-7656", "mrqa_squad-validation-7689", "mrqa_squad-validation-7707", "mrqa_squad-validation-7713", "mrqa_squad-validation-773", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-7819", "mrqa_squad-validation-7838", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-8028", "mrqa_squad-validation-8043", "mrqa_squad-validation-8049", "mrqa_squad-validation-8066", "mrqa_squad-validation-8118", "mrqa_squad-validation-8139", "mrqa_squad-validation-816", "mrqa_squad-validation-824", "mrqa_squad-validation-8253", "mrqa_squad-validation-8273", "mrqa_squad-validation-8283", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8386", "mrqa_squad-validation-8399", "mrqa_squad-validation-8453", "mrqa_squad-validation-850", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8505", "mrqa_squad-validation-8523", "mrqa_squad-validation-8529", "mrqa_squad-validation-8555", "mrqa_squad-validation-8579", "mrqa_squad-validation-8601", "mrqa_squad-validation-8603", "mrqa_squad-validation-8643", "mrqa_squad-validation-8680", "mrqa_squad-validation-8683", "mrqa_squad-validation-8750", "mrqa_squad-validation-8801", "mrqa_squad-validation-889", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8924", "mrqa_squad-validation-8927", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-8969", "mrqa_squad-validation-8987", "mrqa_squad-validation-9048", "mrqa_squad-validation-9097", "mrqa_squad-validation-9135", "mrqa_squad-validation-9157", "mrqa_squad-validation-9165", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9328", "mrqa_squad-validation-9367", "mrqa_squad-validation-9416", "mrqa_squad-validation-9436", "mrqa_squad-validation-9459", "mrqa_squad-validation-9470", "mrqa_squad-validation-9531", "mrqa_squad-validation-9543", "mrqa_squad-validation-9553", "mrqa_squad-validation-9559", "mrqa_squad-validation-9608", "mrqa_squad-validation-9764", "mrqa_squad-validation-9787", "mrqa_squad-validation-9819", "mrqa_squad-validation-9868", "mrqa_squad-validation-9900", "mrqa_squad-validation-9901", "mrqa_squad-validation-9943", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-471", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5627", "mrqa_triviaqa-validation-5960", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6577", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7683"], "OKR": 0.900390625, "KG": 0.36171875, "before_eval_results": {"predictions": ["hydrogen and helium", "a supervisory church body", "AC", "Jerricho Cotchery", "Isaac Komnenos", "a certain number of teacher's salaries", "cylinders and valve gear", "ABC1", "ATP energy", "San Andreas fault system", "Hulu", "two", "social unrest and violence", "Toyota Corona, the Toyota Corolla, the Datsun B210", "Metro Light Rail system", "receiver", "A Turing machine", "Luther", "killer T cells", "1978", "Kenneth Swezey", "1804", "seven", "ankle-length skirts", "David Hilbert", "India", "Julian Sawalha", "an abandoned studio", "anaerobe", "Kiss Me Kate", "Ngate Prison", "Mumbai", "surrealism", "Jordan", "Nang Klao Rama III", "Bertrand Russell", "A-ha", "N Africa", "Ida Lupino", "Charlie Brooker", "racing", "Nugget", "aniline dyes", "Kenya", "tennis", "\"Appaloosa\"", "an Irishman", "Norns", "steel", "Old Trafford", "the people you know and love", "an orphan", "The Small House of Uncle Thomas", "Lisbeth Salander", "Standard Oil Company", "Miranda v. Arizona", "Janis Joplin", "in 1983 and 1984", "over 1.6 million", "Mark Neveldine and Brian Taylor", "Alina Cho", "an der Elster", "a talking stuffed bear", "paul mcc McCartney"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5209821428571428}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false], "QA-F1": [0.42857142857142855, 0.0, 0.6666666666666666, 0.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3667", "mrqa_squad-validation-2468", "mrqa_squad-validation-1402", "mrqa_squad-validation-780", "mrqa_squad-validation-7036", "mrqa_squad-validation-5025", "mrqa_squad-validation-3708", "mrqa_squad-validation-336", "mrqa_squad-validation-6579", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-921", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-1160", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-2372", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4982", "mrqa_triviaqa-validation-4005", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-1461", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-7737", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9135", "mrqa_hotpotqa-validation-1526", "mrqa_newsqa-validation-2422", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-10099"], "SR": 0.421875, "CSR": 0.6292613636363636, "EFR": 0.9459459459459459, "Overall": 0.719416461916462}, {"timecode": 11, "before_eval_results": {"predictions": ["euphoric", "center of the curving path", "polynomial time", "1206", "Dutch East India Company", "Newton", "after the Franco-German War", "chloroplasts and other plastids", "June 4, 2014", "energy", "December 2014", "18 February 1546", "19", "meritocracy", "photooxidative damage", "tangential force", "St John the Baptist", "more or less rounded, sometimes nearly spherical and other times more cylindrical or egg-shaped", "adenosine triphosphate", "Johann Eck", "marx", "the American Civil War", "a woman\u2019s desire to smoke, and maybe this will help me", "king David", "egypt", "pheromones", "John Flamsteed", "egypt", "Anita Roddick", "River Cart", "Tamar", "nizhny Novgorod", "The Word", "The Left Book Club", "tchaikovsky", "in the jungle", "egypt", "spa", "butterfly", "Tarzan", "James Hanratty", "Middlesbrough", "william holders", "egypt", "cumberland", "jumper", "Milton Keynes", "November", "koftas", "Coventry to Leicester Motorway", "egypt", "\"Little Arrows\"", "legion", "Saint Vitus", "egypt", "from shore to shore", "an Ohio newspaper", "Channel 4", "Cartoon Network Too", "pattern matching", "to wear Islamic dress", "egypt", "egypt", "anti-trust"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5212436868686869}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8229", "mrqa_squad-validation-361", "mrqa_squad-validation-4469", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-828", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-3045", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-5755", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-6665", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-18", "mrqa_searchqa-validation-12699", "mrqa_searchqa-validation-5935", "mrqa_newsqa-validation-3918"], "SR": 0.484375, "CSR": 0.6171875, "EFR": 1.0, "Overall": 0.7278125}, {"timecode": 12, "before_eval_results": {"predictions": ["Marburg Colloquy", "a negative long-term impact", "with known magnitudes of force", "greater Southern California Megaregion", "Ed Whitfield", "Basel", "Levi's Stadium", "Miasma theory", "a freshwater lake", "Monte Gargano", "50-yard line", "National Galleries of Scotland", "Arts & Entertainment Television (A&E)", "gravity", "Thomas Sowell", "fans", "pathogen attack", "a not-for-profit United States computer networking consortium", "1850", "between Glen Miller Road in Trenton and the Don Valley Parkway / Highway404 Junction in Toronto", "Spanish explorers", "eight", "New York University", "Waylon Jennings", "Orange", "Melissa Disney", "five", "1990", "Palmer Williams Jr. as Floyd", "1980s and'90s", "Tyler, Ali, and Lydia", "May 18, 2018", "2015", "Afghanistan, Uzbekistan, Tajikistan, Turkmenistan, Myanmar, to Malaysia, Singapore, peninsular Thailand, Indo - China and China", "$75,000", "272", "Hudson Bay", "an expression of unknown origin", "accomplish the objectives of the organization", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "1776", "insignia", "a normally inaccessible mini-game", "Charles Darwin", "heat transfer", "dromedary", "2016", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Richard Carpenter", "T.S. Eliot", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Joe Spano", "at the 1964 Republican National Convention in San Francisco", "north end", "Paul Lynde", "pasta \"al dente,\" which means \"to the tooth,\" meaning that it still has a little bite", "Ross Bagdasarian", "Gracie Mansion", "Peter Seamus O'Toole", "Arthur E. Morgan III", "President Felipe Calderon", "Wayne's World", "Patrick", "Arkansas"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7054253636688981}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.8, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.896551724137931, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8205128205128205, 0.6666666666666666, 0.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.8, 0.4, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5213", "mrqa_squad-validation-10321", "mrqa_squad-validation-4877", "mrqa_squad-validation-10352", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-627", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-1575", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-3299", "mrqa_newsqa-validation-872", "mrqa_searchqa-validation-15790"], "SR": 0.5625, "CSR": 0.6129807692307692, "EFR": 0.9642857142857143, "Overall": 0.7198282967032967}, {"timecode": 13, "before_eval_results": {"predictions": ["phlogiston theory of combustion and corrosion", "1974", "Andrew Alper", "shaping ideas about the free market", "learning", "The Prospect Studios", "Milton Friedman Institute", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "1972", "polytechnics became new universities", "conservation of momentum", "October 1973", "Frontex", "a dispute over control of the confluence of the Allegheny and Monongahela rivers", "720p high definition", "1924", "Purple drank", "Apple Lisa", "the Dutch Empire", "part of a bet from a gambler", "October 21, 2016", "Donald Duck", "February 12, 2014", "Black Mountain College", "An aircraft", "University of Vienna", "Sunday, November 2, 2003", "Golden Calf", "2012 Summer Olympics", "February 13, 1946", "Les Clark", "the static test pressure that a sample of newly manufactured watches were exposed to", "Nathan Rothschild", "The Grandmaster", "Montana State University", "37", "Bombay Talkie", "a Rugby Sevens competition for the twelve Aviva Premiership clubs that will play the following season", "Oliver Platt", "Supergirl", "amy MacGregor", "22 November 17615 July 1816", "Mauritian", "mixed martial arts", "Cape Cod", "Mike Farrell", "Humberside Airport", "Henry Luce", "energy trading company", "George I", "Melanie Owen", "Book of Judges", "12", "35", "the length of suspended roadway between the bridge's towers", "part of Shakespeare's most frequently quoted passages", "HMS Thunderbolt", "vinegar Joe", "Ralph Lauren", "Lashkar-e-Jhangvi", "Detroit", "The Band Concert", "Larry King", "terminal brain cancer"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6070188492063492}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7763", "mrqa_squad-validation-5889", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-3057", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-405", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-2844", "mrqa_triviaqa-validation-4212", "mrqa_newsqa-validation-1095", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-15877", "mrqa_newsqa-validation-2128"], "SR": 0.578125, "CSR": 0.6104910714285714, "EFR": 0.9629629629629629, "Overall": 0.7190658068783069}, {"timecode": 14, "before_eval_results": {"predictions": ["Chu'Tsai", "second-largest global producer", "17 seconds left on the clock", "Ed Mangan", "suspended", "send aid", "nearly two-thirds of the nation's milk", "a dam turbine", "SAP Center in San Jose", "Niels Jerne", "Central Bridge", "Vistula River", "an enzyme called rubisco", "Spanish moss", "Louis Pasteur", "\"The Tales of Hoffmann\"", "Lake Placid, New York", "four", "Elton John", "1978", "Victorian England", "Indian", "polyphony", "1970", "Tom Kitt", "Outside", "Forbes", "acidic bogs", "Tampa", "Saoirse Ronan", "the media conglomerate founded by his grandfather, James M. Cox", "University of Kansas", "the Prescription Drug User Fee Act", "S\u00f8nderjyskE Ishockey", "Foxborough", "abstract hip hop", "Stalybridge, Greater Manchester", "John Delaney", "\"The Omega Man\"", "1955", "2016", "2 March 1972", "Londonderry", "11 November 1869", "Sam Bettley", "Larry Drake", "Wilhelmus Simon Petrus Fortuijn", "Eisstadion Davos", "sex comedy film about a high school graduate who goes on a road trip to have sex with a girl he met online", "Toxics Release Inventory", "St Augustine's Abbey", "a fictional character in the \"Star Wars\" franchise", "Dan Fogelman", "La Scala, Milan", "its genome", "1998", "niacin", "Israel", "Kim Clijsters", "Rev. Alberto Cutie", "the City of Light", "William", "a small closet in a room, with shelves to receive cups, dishes", "milk chocolate"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5782061688311688}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.2857142857142857, 0.5714285714285715, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.5, 0.09090909090909091, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-269", "mrqa_squad-validation-9695", "mrqa_squad-validation-2939", "mrqa_squad-validation-457", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-414", "mrqa_hotpotqa-validation-2574", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-5714", "mrqa_newsqa-validation-1150", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-16559", "mrqa_searchqa-validation-13152", "mrqa_searchqa-validation-6300"], "SR": 0.484375, "CSR": 0.6020833333333333, "EFR": 1.0, "Overall": 0.7247916666666667}, {"timecode": 15, "before_eval_results": {"predictions": ["11", "1968", "teacher who stays with them for most of the week and will teach them the whole curriculum", "Westinghouse Electric", "nine months", "more than 70 pioneers", "Inherited wealth may help explain why many Americans who have become rich may have had a \"substantial head start\"", "Super Bowl XXXIII", "Alan Turing", "North America", "Conservative", "contrasts with the newer areas of tract homes", "Clair Cameron Patterson", "Esp\u00edrito Santo Financial Group", "Europe", "Scotiabank Saddledome", "Ralph Stanley", "S Pictures' \"Veyyil\"", "PewDie Pie", "Seoul, South Korea", "Pittsburgh", "2012", "Chinese Coffee", "John Gotti", "$7.3 billion", "She most recognized achievement is her translation of and commentary on Isaac Newton's book \"Principia\"", "Umina Beach, New South Wales", "Bonny Hills", "\"Naked\"", "Edinburgh", "Anthony Ray Lynn", "Levi Weeks", "the Mayor of the City of New York", "soccer", "Memphis Minnie", "The Five", "James Mitchum", "Candice Susan Swanepoel", "Bhaktivedanta Manor", "Thomas Christopher Ince", "a American football coach", "U.S.", "south", "historic buildings, arts, and published works", "June 24, 1935", "James K. Polk", "March 31, 1944", "Linux Format", "Na Na", "Koch Industries", "Rymill Park", "Eliot Cutler", "Nina (Portman) is a perfect fit", "Thomas Jefferson", "Marie Van Brittan Brown", "Billy Idol", "Private Leonard \"Gomer\" Pyle", "Bombay Stock Exchange", "eight or nine", "Hussein's Revolutionary Command Council", "The Neapolitan type", "Anthony Fokker", "Jean Baptiste Say", "objects appear to be abnormally colored"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7033854166666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.1, 1.0, 1.0, 0.8571428571428571, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1892", "mrqa_squad-validation-1640", "mrqa_squad-validation-7456", "mrqa_squad-validation-4671", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-5730", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-5125", "mrqa_hotpotqa-validation-1192", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5715", "mrqa_searchqa-validation-8148", "mrqa_searchqa-validation-1728", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-581"], "SR": 0.609375, "CSR": 0.6025390625, "EFR": 1.0, "Overall": 0.7248828125}, {"timecode": 16, "before_eval_results": {"predictions": ["Aston Webb", "Jin", "in his lab", "1253", "the clinical pharmacy movement initially began inside hospitals and clinics", "RNA silencing", "50 members", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "Yes\u00fcgei", "Soviet Union declared itself anti-imperialist", "the American Philosophical Society", "Albert Einstein", "Bristol", "Javier Bardem", "steam engine", "Ben Whishaw", "2", "Tears for Fears", "Richard Noble", "Skylab", "Philistine", "Spain", "milk", "Z", "rue", "FreeBooter", "the proclaimer, glorifier and celebrator of history, great deeds and accomplishments", "bats", "Styal", "The wolf huffing and huff", "sergeant", "Margaret Beckett", "Brad Pitt", "a hair follicle", "Canada", "The nine of Scotland", "a marble campanile, or bell tower, for Pisa\u2019s cathedral", "hud\u00f4r", "Sesame Street", "Big Brother", "Avro Lancaster", "Leeds", "Jack Dee", "Joseph Priestley", "white", "a full member of the alliance", "St Asaph", "Fernando Torres", "ADNAMS", "hartman", "aircraft carrier", "leicestershire", "Wyoming", "Xenophon", "O'Meara", "3 total", "Paper Trail", "2013", "Mexico", "Newcastle retained fourth place with a 3-1 victory", "Hugh Grant", "pink", "a working-class young man who spends his weekends dancing and drinking at a local Brooklyn discoth\u00e8que", "1 draw"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6997395833333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6547", "mrqa_squad-validation-7907", "mrqa_squad-validation-2254", "mrqa_squad-validation-10128", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-5830", "mrqa_triviaqa-validation-5856", "mrqa_triviaqa-validation-5844", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-6246", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-6996", "mrqa_triviaqa-validation-7155", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3685", "mrqa_naturalquestions-validation-8087", "mrqa_newsqa-validation-2467", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3874"], "SR": 0.609375, "CSR": 0.6029411764705883, "EFR": 1.0, "Overall": 0.7249632352941177}, {"timecode": 17, "before_eval_results": {"predictions": ["pattern recognition receptors", "in the northern Mokot\u00f3w", "in the courtyard adjoining the Assembly Hall", "six", "1527", "Zhongdu", "tea drinking", "Chicago Bears", "Bill Clinton", "differences in value added by labor, capital and land", "geochemical evolution of rock units", "an assemblage", "birdie", "red", "Pink Floyd", "Constantinople", "tendon", "10th wedding anniversary", "Mesozoic Era", "can we get along", "ethyl mercaptan", "Coral Reef", "1849", "Clyde", "New Orleans", "a bugle", "liver", "right-to-left", "nonfiction", "Roger Williams", "mask", "$200", "Mediolanum", "butter", "before a woman goes into labor", "peck", "white blood cells", "Green Lantern", "Zeus", "New York Giants", "prince Bernhard", "Yves Saint Laurent", "the Trucial States", "Macy's", "Trieste, Kumrovec and Zagreb", "Carmen", "mammals", "Jim Brady", "bali", "corn", "the Crimean War", "Oresteia", "pesto", "providing telecommunication services to enterprises and offices", "1834", "Panurge", "The Savoy", "Leon Marcus Uris", "S6", "Nazi Party members", "open heart surgery", "Ritchie Cordell", "the tax rate paid by a small business", "The Inn at Newport Ranch"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5297371031746032}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38095238095238093]}}, "before_error_ids": ["mrqa_squad-validation-1042", "mrqa_squad-validation-9400", "mrqa_squad-validation-5429", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-15310", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-4065", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-4154", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-15504", "mrqa_searchqa-validation-14923", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-8417", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-11410", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-1387", "mrqa_hotpotqa-validation-2978", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-2546", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-2250"], "SR": 0.46875, "CSR": 0.5954861111111112, "EFR": 1.0, "Overall": 0.7234722222222223}, {"timecode": 18, "before_eval_results": {"predictions": ["Apollo Applications Program", "5.3%", "seizures", "Mercury", "a setup phase in each involved node", "Infrastructure", "MHC class I molecules", "the European Court of Human Rights", "it is neither zero nor a unit", "neuronal dendrites", "The Lost Symbol", "Robert", "November 26", "The Al Nisr Al Saudi", "two paintings", "Marcus Schrenker", "too many glass shards", "Chancellor Angela Merkel", "a level of autonomy that will allow them to protect and preserve their culture, religion and national identity", "drug trafficking", "steamboat", "Osama bin Laden's sons", "2002", "Yusuf Saad Kamel", "general secretary", "byproducts emitted during the process of burning and melting raw materials", "June 2004", "Transit Workers Union", "introduce legislation Thursday to improve the military's suicide-prevention programs", "Eintracht Frankfurt", "Jacob", "5 1/2-year-old", "canyon", "video", "Bob Dole", "Kenyan Defense Minister Yusuf Haji", "the new kid on the block in the modern art scene", "U.S. senators", "March 1st", "Chesley \"Sully\" Sullenberger", "Nigeria, Africa's largest producer", "Thursday", "housing, business and infrastructure repairs", "A third beluga whale", "Martin \"Al\" Culhane", "Washington", "broken glass and the Jaws of Life", "Dubai", "India", "the cancellation of more than 650 flights at London's Heathrow airport", "38", "Juan Martin Del Potro.", "the four women who Krazy-Glued a cheater's penis to his stomach", "most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor", "fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "The Comedy of Errors", "Xenophon", "devotional", "14th Street", "Lord Byron", "a ready stance", "Clark Irwin", "a commode", "an adhesive"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5625228937728938}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5333333333333333, 0.0, 0.0, 0.2857142857142857, 0.0, 0.28571428571428575, 0.2857142857142857, 0.0, 0.5, 0.28571428571428575, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-961", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-1482", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1184", "mrqa_naturalquestions-validation-4365", "mrqa_searchqa-validation-15244", "mrqa_searchqa-validation-9803", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-8188", "mrqa_searchqa-validation-15495"], "SR": 0.484375, "CSR": 0.5896381578947368, "EFR": 1.0, "Overall": 0.7223026315789474}, {"timecode": 19, "before_eval_results": {"predictions": ["most common", "April 1, 1963", "Islamism", "he did not want disloyal men in his army.", "conservation of momentum", "farther west", "154", "17 February 1546", "1996", "1960", "Friday", "Anil Kapoor.", "2005 for vote-tampering.", "2008", "Fred Bright, the district attorney in Milledgeville,", "President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "CNN's Moscow-based Senior International Correspondent Matthew Chance", "11th year in a row", "suspended", "romantic e-mails", "the Nazi war crimes suspect who had been ordered deported to Germany,", "Lashkar-e-Tayyiba (LeT), an Islamic militant group based in Pakistan.", "of the Movement for Democratic Change,", "Steven Chu", "Wednesday at the age of 95.", "discovery\" for the museum \"of the last 90 years.\"", "Abu Sayyaf", "\"To look at him initially he was very intimidating,\"", "2,000 euros ($2,963)", "buckling under pressure from the ruling party.", "Vertikal-T", "The Everglades, known as the River of Grass,", "a striking blow to due process and the rule of law", "federal officers' bodies", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "at the bottom of the hill surviving on leaves and water from a nearby creek,\"", "her mother", "South African police have opened a criminal investigation into allegations that a dorm parent mistreated students at the school.", "hundreds of people joined a campus rally to oppose racial intolerance.", "girls", "blind Majid Movahedi, the man who blinded her", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "suicide car bombing", "Andrew Morris,", "2007", "\"Wolfman,\" starring Benicio del Toro, grossed an estimated $30.6 million for a per-theater average of $9,497.\"", "Vernon Forrest", "The man ran away,", "Jason Chaffetz", "St. Louis,", "Alicia Keys", "Oxbow", "Zhanar Tokhtabayeba,", "1038", "Johannes Gutenberg", "husbands", "a crystal ball", "senior men's Lithuanian national team", "two years", "Michael", "James Corden", "spectacled bear", "Burrito", "Constellation family"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5807841897685648}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.1904761904761905, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 0.5333333333333333, 0.0, 0.0, 1.0, 0.33333333333333337, 0.15384615384615383, 0.0, 0.125, 1.0, 1.0, 0.4, 0.2857142857142857, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.2, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.0, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9228", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-440", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-4927", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-9391"], "SR": 0.46875, "CSR": 0.58359375, "EFR": 1.0, "Overall": 0.72109375}, {"timecode": 20, "UKR": 0.765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1334", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1484", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1696", "mrqa_hotpotqa-validation-1750", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1840", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1912", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2456", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2574", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3057", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4780", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5838", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-748", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-4876", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9545", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3297", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3365", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-872", "mrqa_searchqa-validation-11130", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-14923", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-16559", "mrqa_searchqa-validation-1926", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5531", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-7219", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-9391", "mrqa_searchqa-validation-9683", "mrqa_squad-validation-100", "mrqa_squad-validation-10001", "mrqa_squad-validation-10059", "mrqa_squad-validation-10063", "mrqa_squad-validation-1008", "mrqa_squad-validation-10186", "mrqa_squad-validation-10256", "mrqa_squad-validation-10273", "mrqa_squad-validation-10316", "mrqa_squad-validation-10346", "mrqa_squad-validation-10352", "mrqa_squad-validation-10370", "mrqa_squad-validation-10386", "mrqa_squad-validation-10399", "mrqa_squad-validation-1042", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-10475", "mrqa_squad-validation-10484", "mrqa_squad-validation-1078", "mrqa_squad-validation-1177", "mrqa_squad-validation-12", "mrqa_squad-validation-1207", "mrqa_squad-validation-1219", "mrqa_squad-validation-1254", "mrqa_squad-validation-1263", "mrqa_squad-validation-133", "mrqa_squad-validation-1330", "mrqa_squad-validation-134", "mrqa_squad-validation-1402", "mrqa_squad-validation-1432", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1565", "mrqa_squad-validation-1612", "mrqa_squad-validation-1640", "mrqa_squad-validation-168", "mrqa_squad-validation-1764", "mrqa_squad-validation-1813", "mrqa_squad-validation-185", "mrqa_squad-validation-185", "mrqa_squad-validation-1891", "mrqa_squad-validation-1980", "mrqa_squad-validation-2018", "mrqa_squad-validation-2085", "mrqa_squad-validation-2085", "mrqa_squad-validation-2109", "mrqa_squad-validation-215", "mrqa_squad-validation-2167", "mrqa_squad-validation-2214", "mrqa_squad-validation-2314", "mrqa_squad-validation-2370", "mrqa_squad-validation-246", "mrqa_squad-validation-2500", "mrqa_squad-validation-2559", "mrqa_squad-validation-2586", "mrqa_squad-validation-2666", "mrqa_squad-validation-2672", "mrqa_squad-validation-269", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-2754", "mrqa_squad-validation-2853", "mrqa_squad-validation-2913", "mrqa_squad-validation-2939", "mrqa_squad-validation-2964", "mrqa_squad-validation-2975", "mrqa_squad-validation-3096", "mrqa_squad-validation-3124", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3140", "mrqa_squad-validation-3143", "mrqa_squad-validation-3275", "mrqa_squad-validation-3319", "mrqa_squad-validation-336", "mrqa_squad-validation-3370", "mrqa_squad-validation-3407", "mrqa_squad-validation-3492", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-350", "mrqa_squad-validation-3535", "mrqa_squad-validation-3550", "mrqa_squad-validation-36", "mrqa_squad-validation-3667", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3802", "mrqa_squad-validation-3811", "mrqa_squad-validation-3813", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3904", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-4227", "mrqa_squad-validation-4246", "mrqa_squad-validation-4469", "mrqa_squad-validation-4473", "mrqa_squad-validation-4546", "mrqa_squad-validation-4591", "mrqa_squad-validation-4636", "mrqa_squad-validation-4669", "mrqa_squad-validation-468", "mrqa_squad-validation-4737", "mrqa_squad-validation-4754", "mrqa_squad-validation-4799", "mrqa_squad-validation-4860", "mrqa_squad-validation-4877", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5007", "mrqa_squad-validation-5019", "mrqa_squad-validation-5135", "mrqa_squad-validation-5294", "mrqa_squad-validation-5325", "mrqa_squad-validation-5351", "mrqa_squad-validation-5613", "mrqa_squad-validation-5699", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5716", "mrqa_squad-validation-5721", "mrqa_squad-validation-5724", "mrqa_squad-validation-5797", "mrqa_squad-validation-5875", "mrqa_squad-validation-5933", "mrqa_squad-validation-5961", "mrqa_squad-validation-6025", "mrqa_squad-validation-6089", "mrqa_squad-validation-6116", "mrqa_squad-validation-6157", "mrqa_squad-validation-6159", "mrqa_squad-validation-6163", "mrqa_squad-validation-6341", "mrqa_squad-validation-635", "mrqa_squad-validation-6502", "mrqa_squad-validation-6526", "mrqa_squad-validation-6579", "mrqa_squad-validation-6614", "mrqa_squad-validation-6628", "mrqa_squad-validation-6669", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6705", "mrqa_squad-validation-6803", "mrqa_squad-validation-6873", "mrqa_squad-validation-6986", "mrqa_squad-validation-7036", "mrqa_squad-validation-7168", "mrqa_squad-validation-719", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7277", "mrqa_squad-validation-7294", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7384", "mrqa_squad-validation-7428", "mrqa_squad-validation-7456", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7585", "mrqa_squad-validation-76", "mrqa_squad-validation-7652", "mrqa_squad-validation-7671", "mrqa_squad-validation-7689", "mrqa_squad-validation-7707", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-780", "mrqa_squad-validation-7805", "mrqa_squad-validation-7838", "mrqa_squad-validation-7909", "mrqa_squad-validation-791", "mrqa_squad-validation-8028", "mrqa_squad-validation-8043", "mrqa_squad-validation-8045", "mrqa_squad-validation-8073", "mrqa_squad-validation-8283", "mrqa_squad-validation-8386", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8529", "mrqa_squad-validation-8555", "mrqa_squad-validation-8603", "mrqa_squad-validation-8680", "mrqa_squad-validation-8754", "mrqa_squad-validation-8830", "mrqa_squad-validation-8834", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8905", "mrqa_squad-validation-891", "mrqa_squad-validation-8927", "mrqa_squad-validation-8931", "mrqa_squad-validation-8939", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-8987", "mrqa_squad-validation-9200", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9302", "mrqa_squad-validation-9322", "mrqa_squad-validation-9328", "mrqa_squad-validation-9436", "mrqa_squad-validation-95", "mrqa_squad-validation-9531", "mrqa_squad-validation-9532", "mrqa_squad-validation-9543", "mrqa_squad-validation-9695", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_squad-validation-9931", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-1873", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3278", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4850", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5025", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5960", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6246", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-828"], "OKR": 0.896484375, "KG": 0.4765625, "before_eval_results": {"predictions": ["2002.", "1,548", "American Institute of Electrical Engineers", "a 3\u20130 lead on a Brandon McManus 34-yard field goal.", "20", "Virgin Media", "in a number of stages", "light energy", "128", "power directly or elect representatives from among themselves to form a governing body,", "Terrence \"Uncle Terry\" Richardson", "Australian Defence Force", "Moon shot: The Inside Story of America's Race to the Moon", "Algirdas", "1970", "Vilnius", "Rounders", "music of pre-Hispanic and contemporary music of the Andes", "Robert John Day", "Rio Gavin Ferdinand", "National Lottery", "Battle of Prome", "M2M", "Austria", "Citizens for a Sound Economy", "right-hand", "Gatwick", "Australian", "Darkroom", "House of Commons", "La Familia Michoacana", "five", "1983", "James Packer", "Northern Ireland", "Erich Maria Remarque", "Best Musical", "Floyd Mutrux and Colin Escott.", "Fred \"Sonic\" Smith", "the German Luftwaffe", "North America", "A Little Princess", "1943", "TD Garden", "The scarp", "Vixen", "September 6, 2010", "The Ryukyuan people", "Chicago", "J35-A-23", "Axl Rose", "Prudential Center in Newark, New Jersey.", "Homer Hickam, Jr.", "CBS", "Diary of a Wimpy Kid : The Long Haul", "Daltonism", "July 28, 1948.", "right-wing extremist groups.", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "Beatles", "a key", "\"Goldstone Report\"", "future relations between the Middle East and Washington.", "650"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6317057291666667}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.2, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0625, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-800", "mrqa_squad-validation-4010", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-5692", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1812", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-2486", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-5804", "mrqa_triviaqa-validation-3423", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-9122", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-571"], "SR": 0.515625, "CSR": 0.5803571428571428, "EFR": 0.967741935483871, "Overall": 0.7373541906682027}, {"timecode": 21, "before_eval_results": {"predictions": ["1622", "wars", "all age groups", "cabin depressurization", "largest gold rushes the world has ever seen.", "1985", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol.", "Sultan Selim II", "cruiserweight", "A Hard Day's Night", "best known as the editor of the works of Francis Bacon.", "Ben Elton", "the West Cheshire Association Football League", "KB", "1,467 rooms", "pioneering New Zealand food writer", "the New Executive Office Building", "Olivia Newton-John, (born 26 September 1948)", "John R. Leonetti", "The Legend of Sleepy Hollow", "best-known novels are \"Money\" (1984) and \"London Fields\" (1989)", "Harmony Korine", "137th", "Texas Tech University", "(Croatan, Nantahala, and Uwharrie)", "703 rooms", "King George IV and the Duke of Wellington", "Rochdale", "most influenced by Tudor music and English folk-song", "Northern Ireland", "Pylos and Thebes", "Larry Gatlin & the Gatlin Brothers Band.", "the Beatles", "Lady Frederick Windsor", "1994", "2 November 1902", "most time in space (381.6 days)", "the NYPD's 83rd Precinct", "Patterns of Sexual Behavior", "Peel Holdings", "about 560", "wrestler, actor, and hip hop musician", "Theodor W. Adorno", "Gianna", "The City of Newcastle", "John Christopher Lujack Jr.", "gender queer", "Kramer", "Rod Smith (politician)", "Minette Walters", "2004", "Tamil", "1,382 inhabitants", "73", "Barbara Windsor", "Stephenie Meyer", "Jane Austen", "Pakistan's intelligence agency", "Michelle Rounds", "A Doll's House", "Florida", "most obscure animal combinations", "auction houses", "Mickey's"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6634548611111111}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 0.0, 1.0, 0.0, 1.0, 0.0, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6, 1.0, 0.7692307692307693, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.923076923076923, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2987", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-334", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-984", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-5295", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-3707", "mrqa_hotpotqa-validation-5052", "mrqa_naturalquestions-validation-2159", "mrqa_triviaqa-validation-6256", "mrqa_newsqa-validation-1218", "mrqa_triviaqa-validation-2227", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-7724"], "SR": 0.53125, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.743359375}, {"timecode": 22, "before_eval_results": {"predictions": ["$960 billion", "the European Court of Human Rights", "In 1536, he began to suffer from kidney and bladder stones, and arthritis, and an ear infection ruptured an ear drum.", "NP", "a band of visionary zealots, the so-called Zwickau prophets,", "1948", "New Orleans,", "mafia clan", "inveigle", "Vaccines", "1984", "semi-autonomous", "Alfred Gilbert", "insulin", "bullfight", "17 pink", "12", "The Pennine Way", "Muriel Spark", "basil", "La Mancha", "Martin Van Buren", "Bonnie and Clyde", "three", "Hillary Clinton", "Gettysburg", "Tom Hanks", "six dots", "\"sound and light\"", "Panamanian politics was clouded in mid-1982 by the ouster of Colonel Florencio Fl\u00f3rez Aguilar in a shakeup of the National Guard in a surprise announcement by President Aristides Royo", "hedgehogs", "Harrods", "Usain Bolt", "Mead", "To Kill a Mockingbird", "inveigle", "Sudan", "inveigle", "Russia,", "Amy", "Steve Jobs", "Christmas", "Jaipur", "in love with a young man called Jenik", "Physical in the sun", "David Hockney", "a compact bone", "Barnaby Rudge", "inveigle", "1861", "Thomas Jefferson", "Ceredigion", "around 10 : 30am", "2014 Winter Olympics in Sochi, Russia", "in Seattle", "Capture of the Five Boroughs", "79 AD", "Wilmette", "Trevor Rees,", "last week", "in the name of Jesus Christ to free political prisoners and \"open your borders so that we may bring food, provisions, medicine, necessities, and assistance to those who are struggling to survive.\"", "Sounder", "Persians", "4 peck"], "metric_results": {"EM": 0.4375, "QA-F1": 0.48084783272283277}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.4799999999999999, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.07692307692307693, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2505", "mrqa_squad-validation-2302", "mrqa_triviaqa-validation-3577", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2366", "mrqa_triviaqa-validation-6301", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-6681", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-3310", "mrqa_triviaqa-validation-3887", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-4040", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6192", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-6458", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-7443", "mrqa_hotpotqa-validation-1874", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3239", "mrqa_searchqa-validation-14932", "mrqa_searchqa-validation-2394"], "SR": 0.4375, "CSR": 0.5720108695652174, "EFR": 0.9722222222222222, "Overall": 0.736580993357488}, {"timecode": 23, "before_eval_results": {"predictions": ["1759-60", "tyrosinase", "Jadaran", "Israelis", "Konstantin Mereschkowski", "disease", "gounod and Reyer", "Vienna", "March 10, 1997", "pouched", "meat", "Benedictine Order", "cop", "1985", "The Cavern Club", "John Steinbeck", "copenhagen", "beta", "spain", "John Peel", "may have been lost due to clerical reforms imposed by Joseph II", "Cheshire", "Rebecca Adlington (born 17 February 1989 in Mansfield, Nottinghamshire, England) is a British freestyle swimmer", "silks", "power station", "Notts County", "The Tigris", "John Fitzgerald Kennedy", "Palazzo Rio, or Palace River,", "crocodile", "silicon, phosphorus, sulfur, and manganese", "island countries", "spain", "davers", "Frank Harris", "Charles Atlas", "Alex Kramer, did the newly introduced viewers' phone-in question section from 17 November 2008 until 20 March 2009,", "harrow", "Isle of Wight", "violin", "Anthony Joshua", "elephant", "whitsunday", "Dick Turpin", "poland", "restless legs syndrome", "spleen", "Olympics", "Jimmy Knapp", "poland", "Gargantua", "Late Ordovician period", "Pebe Sebert and Hugh Moffatt", "October 29, 2015", "in the season - five premiere episode `` Second Opinion ''", "Leslie Knope", "First Balkan War", "Get Him to the Greek", "next year", "heavy turbulence", "workers agreed to stave off the strike.", "Patrick Bouvier", "Chuck Schumer", "spain"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5412459935897436}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.923076923076923, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6108", "mrqa_squad-validation-8488", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-6876", "mrqa_naturalquestions-validation-3440", "mrqa_naturalquestions-validation-2818", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-5271", "mrqa_newsqa-validation-1893", "mrqa_searchqa-validation-7791", "mrqa_searchqa-validation-7996"], "SR": 0.46875, "CSR": 0.5677083333333333, "EFR": 1.0, "Overall": 0.7412760416666666}, {"timecode": 24, "before_eval_results": {"predictions": ["a comb jelly", "grubs", "gaseous oxygen", "no known case of any U.S. citizens buying Canadian drugs for personal use with a prescription", "9th century", "Paul Gauguin", "Gerald R. Ford", "venice", "lyce Fontanes", "I Don't Want to Miss a Thing", "the first Sunday", "3 characters", "saddle oxfords", "NAFTA", "the earth", "Clarissa Dalloway", "lyndon johnson", "venial", "Toronto", "america", "the beaver", "venice", "Ricky Martin", "Alfred Nobel", "Smith & Wesson", "700 nm", "Emma Watson", "Pan Am", "Cardinal Richelieu", "Robert E. Lee", "mansard roof", "OK", "Nikita Khrushchev", "koolsla", "detective novelist", "diamonds", "Sonny", "morphine", "mars", "Ice Age", "a hand", "lyndon johnson", "lyndon farr", "an American Tail", "pelican", "Corporal", "mountains", "habsburg", "a Canadian hockey player", "bathsheba", "valley", "$1000", "Wembley Stadium", "Mike Higham", "MGM Resorts International", "Olympic Games", "bats", "halogens", "Ferdinand Magellan", "January 24, 2012", "Famous Ghost Stories", "a man who said he had found it in the desert five months before.", "haitians", "leaders of more than 30 Latin American and Caribbean nations"], "metric_results": {"EM": 0.359375, "QA-F1": 0.416015625}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-9119", "mrqa_squad-validation-6383", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1375", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-16821", "mrqa_searchqa-validation-12291", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-11213", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-1004", "mrqa_searchqa-validation-16836", "mrqa_searchqa-validation-14524", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-7094", "mrqa_searchqa-validation-9261", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-8944", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-263", "mrqa_searchqa-validation-6770", "mrqa_searchqa-validation-3135", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-10911", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-2047", "mrqa_triviaqa-validation-629", "mrqa_hotpotqa-validation-4468", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2225"], "SR": 0.359375, "CSR": 0.559375, "EFR": 1.0, "Overall": 0.739609375}, {"timecode": 25, "before_eval_results": {"predictions": ["Khitan rulers", "over 100%", "vaccination", "natural grass", "three", "Zuma", "\"full civil equality,\"", "hooked up with Mildred, a younger woman of about 80, in March.", "a bag", "A large concrete block is next to his shoulder, with shattered pieces of it around him.", "gun charges", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "\"falling space debris,\"", "the man was dead", "three different videos", "September 21.", "stand down.", "hot and humid", "KBR", "the sun, and solar power", "\"Twilight\"", "private client", "Venus Williams", "Booches Billiard Hall", "Peshawar", "23-year-old", "three", "creating and distributing affordable, durable and solar-powered laptops to the world's poorest children.", "sent private cables to Obama last week, urging the president not to rush to send more troops to Afghanistan", "Sovereign Wealth Funds", "Besson", "Department of Homeland Security Secretary Janet Napolitano", "prison inmates.", "a biological match.", "death squad killings", "hours", "a full garden and pool, a tennis court, or several heli-pads", "Bill", "Debbie Bego", "Caylee Anthony", "completely changed the business of music,", "Raiders of the Lost Ark", "five days a week", "secure more funds", "upper respiratory infection", "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Brown-Waite", "Shenzhen", "a botched burg that left an off-duty New York police officer dead.", "Scudetto", "Zimbabwe's main opposition party said Sunday.", "In the year 2026", "Mitch Murray, who offered it to Adam Faith and Brian Poole", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Captain Mark Phillips", "Naomi Watts", "Bruce Alexander", "vice president", "Pittsburgh Steelers", "the attack on Pearl Harbor", "The Man Trap", "Chris Matthews", "books"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5552513583763584}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.47619047619047616, 0.0, 0.13333333333333333, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.14814814814814814, 0.0, 1.0, 0.4, 0.4444444444444445, 0.6666666666666666, 0.0, 0.42857142857142855, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.8, 0.3076923076923077, 0.9189189189189189, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-1137", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-7896", "mrqa_triviaqa-validation-1508", "mrqa_hotpotqa-validation-350", "mrqa_searchqa-validation-16581"], "SR": 0.4375, "CSR": 0.5546875, "EFR": 1.0, "Overall": 0.738671875}, {"timecode": 26, "before_eval_results": {"predictions": ["David G. Booth", "pr\u00e9tendus r\u00e9form\u00e9s", "true Islamic system", "$20,000", "its intention to set up headquarters in Dublin.", "required to submit label safety changes and the medication guide", "Itawamba County School District", "Jada,", "Elisabeth,", "about 20 miles off the Mexican coast,", "it is provocative action,\"", "waterboarding at least 266 times on two top al Qaeda suspects,", "Ricardo Urbina", "hazard", "Christopher Savoie", "near Garacad, Somalia,", "peanuts, nuts, shellfish, peanuts, tree nuts, wheat and soy.", "if they can demonstrate they have been satisfactorily treated for at least 12 months.", "How I Met Your Mother", "preserved corpses having sex", "Utah teenager", "inmates", "as many as 50,000 members", "U.S. State Department and British Foreign Office", "the two remaining crew members", "an occupied building.", "up to $50,000", "Arthur E. Morgan III,", "Caster Semenya", "Ewan McGregor", "a baseball bat", "a one-of-a-kind navy dress with red lining", "$60 million", "test-launched a rocket capable of carrying a satellite,", "three", "state senators", "gun", "$1.5 million", "off the coast of Dubai", "Davidson college students", "Diego Milito", "upper respiratory infection", "officers at a Texas  airport appear to have properly followed procedures when they allegedly forced a woman to remove her", "McDonald's", "collapsed apartment building", "gang rape", "leftist Workers' Party.", "protest child trafficking and shout anti-French slogans", "full garden and pool, a tennis court,", "October 3,", "Chinese", "was part of a planned training exercise designed to help the prince learn to fly in combat situations.\"", "the head of the Imperial Family and the traditional head of state of Japan", "the end of the 2015 season", "November 2016", "sewing machines", "exploits on the Island", "Duncan I of Scotland", "half a million acres", "hiphop", "four", "Stanford", "a rough, rugged rock", "Lawrencium"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4891625370243791}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.42857142857142855, 0.0, 0.5, 0.18181818181818182, 0.8, 0.6666666666666666, 0.0, 0.9696969696969697, 0.5263157894736842, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-3307", "mrqa_newsqa-validation-1283", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-9119", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5002", "mrqa_searchqa-validation-679"], "SR": 0.359375, "CSR": 0.5474537037037037, "EFR": 1.0, "Overall": 0.7372251157407408}, {"timecode": 27, "before_eval_results": {"predictions": ["computational complexity theory", "economically", "Labor", "Children of Earth", "a broken pelvis,", "a photo album", "133", "one count of attempted murder in the second degree in the October 12 attack in Deerfield Beach, Florida.", "Sri Lanka", "Muslim", "a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "200", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "innovative, exciting skyscrapers", "Egyptian security forces in central Cairo,", "1831", "Glasgow, Scotland", "10 to 15 percent", "Kearny, New Jersey.", "19-12", "four university students and a safety officer", "additional information", "Mark Obama Ndesandjo", "New York City Mayor Michael Bloomberg", "The Delta Queen", "15", "opposition parties", "July 23.", "2004.", "citizenship", "\"I am sick of life -- what can I say to you?\"", "Gustav's top winds weakened to 110 mph,", "the 11th anniversary of the September 11, 2001, terror attacks.", "curfew", "Omar bin Laden", "Tim Masters", "surgical anesthetic propofol", "Peshawar", "Steve Jobs", "\"The Da Vinci Code,\"", "martial arts", "Bahrami's", "producing rock music with a country influence.", "\"The American president toured a mosque, laid a wreath at the grave of the founder of the Turkish republic,", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "$8.8 million", "near his home in Peshawar", "refusal or inability to \"turn it off\"", "fighting charges of Nazi war crimes for well over two decades.", "Minerals Management Service Director Elizabeth Birnbaum", "$420,000", "U.S.-flagged", "`` planted '' into the bracket in a manner that is typically intended so that the best don't meet until later in the competition", "1994", "disputes between two or more states", "a shorthand typist", "Carmen Miranda", "Lesley Garrett", "Denmark", "the tissues of the outer third of the vagina,", "Mark Neveldine and Brian Taylor.", "Ulysses S. Grant", "Kevin Bacon", "yellow"], "metric_results": {"EM": 0.46875, "QA-F1": 0.595493409776498}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.3636363636363636, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.4444444444444445, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.08, 0.0, 1.0, 1.0, 1.0, 0.7058823529411764, 0.5, 0.0, 0.0, 0.0392156862745098, 0.5, 0.8, 1.0, 1.0, 1.0, 0.5, 0.1818181818181818, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2215", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-1028", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-8092", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-4194", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420"], "SR": 0.46875, "CSR": 0.5446428571428572, "EFR": 1.0, "Overall": 0.7366629464285714}, {"timecode": 28, "before_eval_results": {"predictions": ["Ex post facto laws,", "temperature and light", "on comparing these particles with the bits of metal projected by his \"electric gun,\"", "Tens of thousands of new voters became the key to his Iowa win", "Hillary Clinton", "volatile", "an older generation", "Monica Majumdar", "800,000", "Monday and Tuesday", "a full garden and pool, a tennis court, or several heli-pads.", "the couple's surrogate", "The rebels have been fighting for an independent homeland for the country's ethnic Tamil minority since 1983.", "Tuesday night", "\"The biology professor is charged with capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "prisoners", "Ashley \"A.J. Jewell,", "The drama of an American ship captain held hostage by Somali pirates", "July in the Philippines", "Iran of trying to build nuclear bombs,", "The U.S. State Department and British Foreign Office", "former U.S. secretary of state.", "misdemeanor assault charges", "The Tupolev Tu-160 strategic bombers", "Argentine", "Facebook photo album full of pics of you looking smiley.", "Piers Morgan Tonight", "$50,000", "Leo Frank", "Omar bin Laden", "the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "U.S. President-elect Barack Obama", "the National September 11 Memorial Museum", "St. Louis, Missouri.", "hand-painted Swedish wooden clogs", "The comprehensive response has extended the lives of tens of thousands of Brazilians and saved the government billions,", "Pope Benedict XVI refused Wednesday to soften the Vatican's ban on condom use", "\"cliff effect.\"", "three searches", "Basel", "more than 700 guests each year in George and Martha's time.", "first grand Slam,", "suppress the memories and to live as normal a life as possible;", "$1.4 million", "the couple's surrogate lost the pregnancy.", "killed wife Christine", "role as a bride in the 2007 movie \"License to Wed\"", "The UNHCR recommended against granting asylum,", "2009", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "The six alleged victims", "Robert Barnett", "Javier Fern\u00e1ndez", "7.6 mm", "Scopes Trial in the United States", "Patrick Chukwuemeka Okogwu", "Petula Clark", "Lord Snooty", "Roman Sergeyevich Kostomarov", "August 1973", "ten", "The Stranger", "Los Angeles Times", "George Orwell"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5031652781185574}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 0.08333333333333334, 0.04878048780487805, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5263157894736842, 0.5, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.5, 0.9444444444444444, 0.4, 0.0, 0.3333333333333333, 0.0, 0.20689655172413796, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4096", "mrqa_squad-validation-1389", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2291", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1663", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2030", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-6832", "mrqa_hotpotqa-validation-2281", "mrqa_searchqa-validation-1869"], "SR": 0.390625, "CSR": 0.5393318965517242, "EFR": 1.0, "Overall": 0.7356007543103449}, {"timecode": 29, "before_eval_results": {"predictions": ["fully consistent with the conceptual definition of force offered by Newtonian mechanics.", "peer tuitions", "six", "Texas", "Tennyson", "1087", "Francis Drake", "woodwind", "Candice", "aluminum", "Laura Bancroft", "Soviet", "red deer", "South America", "Idi Amin", "American recording group The Black Eyed Peas.", "pink", "Tom", "anemia", "Sam Cooke", "African", "Robert", "Vienna", "Russian", "Woody Allen", "one-millionth of a second", "Citizen Kane", "\"The Stag\"", "Jolly Roger", "bears", "Shirley Jackson", "JetBlue", "a beret", "lillian", "Peru", "the Phantom of the Opera", "an eye", "Holy Roman Empire", "Oahu", "Bob Newhart", "shorthand", "Sean John", "(Apiaceae)", "Utah", "Dr. Alex' Head Cream", "Andrew Lloyd Webber", "Sam Shepard", "petroleum", "August Wilson", "Dr. Jack Shephard, Kate Austen, Sayid Jarrah, Hugo \" Hurley\"", "bay leaf", "Blue Nile", "China ( formerly the Republic of China ), France, the United Kingdom, and the United States", "Roman Reigns", "Mexican Seismic Alert System", "violin", "\"Big Spin flip", "Simeon Williamson", "11 November 1869", "Ten Walls", "Nia Kay", "new kidney", "surgical anesthetic propofol", "the deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan."], "metric_results": {"EM": 0.46875, "QA-F1": 0.529193376068376}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.846153846153846, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_squad-validation-10284", "mrqa_searchqa-validation-1839", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-14158", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-14060", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-12860", "mrqa_searchqa-validation-9533", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-514", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-16060", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-975", "mrqa_triviaqa-validation-2394", "mrqa_triviaqa-validation-6791", "mrqa_hotpotqa-validation-4819", "mrqa_newsqa-validation-1445"], "SR": 0.46875, "CSR": 0.5369791666666667, "EFR": 1.0, "Overall": 0.7351302083333333}, {"timecode": 30, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1484", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-1750", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1912", "mrqa_hotpotqa-validation-1914", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2281", "mrqa_hotpotqa-validation-2310", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2440", "mrqa_hotpotqa-validation-2456", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-3310", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-5002", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-748", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-918", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4876", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-5390", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-975", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3154", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-86", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-12242", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-1265", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12897", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14932", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15244", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1595", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1696", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1773", "mrqa_searchqa-validation-1926", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5478", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6770", "mrqa_searchqa-validation-6958", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8791", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-9391", "mrqa_searchqa-validation-9683", "mrqa_squad-validation-100", "mrqa_squad-validation-10001", "mrqa_squad-validation-1002", "mrqa_squad-validation-10063", "mrqa_squad-validation-10174", "mrqa_squad-validation-10186", "mrqa_squad-validation-10256", "mrqa_squad-validation-10316", "mrqa_squad-validation-10370", "mrqa_squad-validation-10386", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-1078", "mrqa_squad-validation-1177", "mrqa_squad-validation-1219", "mrqa_squad-validation-1263", "mrqa_squad-validation-133", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1612", "mrqa_squad-validation-1613", "mrqa_squad-validation-168", "mrqa_squad-validation-175", "mrqa_squad-validation-1759", "mrqa_squad-validation-1779", "mrqa_squad-validation-1813", "mrqa_squad-validation-1891", "mrqa_squad-validation-1980", "mrqa_squad-validation-2018", "mrqa_squad-validation-2085", "mrqa_squad-validation-2085", "mrqa_squad-validation-2109", "mrqa_squad-validation-2129", "mrqa_squad-validation-2167", "mrqa_squad-validation-2214", "mrqa_squad-validation-2302", "mrqa_squad-validation-246", "mrqa_squad-validation-2586", "mrqa_squad-validation-2666", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-2754", "mrqa_squad-validation-2913", "mrqa_squad-validation-2939", "mrqa_squad-validation-2975", "mrqa_squad-validation-3037", "mrqa_squad-validation-313", "mrqa_squad-validation-3213", "mrqa_squad-validation-3370", "mrqa_squad-validation-3479", "mrqa_squad-validation-350", "mrqa_squad-validation-3535", "mrqa_squad-validation-3581", "mrqa_squad-validation-3667", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3811", "mrqa_squad-validation-3842", "mrqa_squad-validation-385", "mrqa_squad-validation-3922", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-4096", "mrqa_squad-validation-4179", "mrqa_squad-validation-4246", "mrqa_squad-validation-4292", "mrqa_squad-validation-4469", "mrqa_squad-validation-4591", "mrqa_squad-validation-4669", "mrqa_squad-validation-468", "mrqa_squad-validation-4860", "mrqa_squad-validation-4877", "mrqa_squad-validation-4898", "mrqa_squad-validation-5007", "mrqa_squad-validation-5030", "mrqa_squad-validation-5042", "mrqa_squad-validation-5135", "mrqa_squad-validation-5325", "mrqa_squad-validation-5351", "mrqa_squad-validation-5613", "mrqa_squad-validation-5699", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5716", "mrqa_squad-validation-5721", "mrqa_squad-validation-5809", "mrqa_squad-validation-5875", "mrqa_squad-validation-5933", "mrqa_squad-validation-6025", "mrqa_squad-validation-6116", "mrqa_squad-validation-6157", "mrqa_squad-validation-6163", "mrqa_squad-validation-6274", "mrqa_squad-validation-6341", "mrqa_squad-validation-6383", "mrqa_squad-validation-6579", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6705", "mrqa_squad-validation-6803", "mrqa_squad-validation-7036", "mrqa_squad-validation-7168", "mrqa_squad-validation-7233", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7294", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7428", "mrqa_squad-validation-7491", "mrqa_squad-validation-7516", "mrqa_squad-validation-7531", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7707", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-780", "mrqa_squad-validation-7805", "mrqa_squad-validation-7838", "mrqa_squad-validation-7909", "mrqa_squad-validation-8386", "mrqa_squad-validation-850", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8523", "mrqa_squad-validation-8555", "mrqa_squad-validation-8601", "mrqa_squad-validation-8603", "mrqa_squad-validation-8680", "mrqa_squad-validation-8861", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8905", "mrqa_squad-validation-8927", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9302", "mrqa_squad-validation-9322", "mrqa_squad-validation-9328", "mrqa_squad-validation-9412", "mrqa_squad-validation-9436", "mrqa_squad-validation-95", "mrqa_squad-validation-9590", "mrqa_squad-validation-9695", "mrqa_squad-validation-9901", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-1052", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1840", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3045", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3577", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4430", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5226", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7613"], "OKR": 0.85546875, "KG": 0.4484375, "before_eval_results": {"predictions": ["climate change", "coronary thrombosis.", "a Qutb", "(ANNA)", "(William) Harris", "Medusa", "(Ivanhoe)", "kerosene", "England", "Shirley Temple", "flagellation", "(Sir) Lyndon B. Johnson", "Bart Simpson", "( Wesley) Clark", "Frasier", "Columbus Day", "Vegetarianism", "Dix", "amyotrophic lateral sclerosis", "Bill Clinton", "( Emily) Dickinson", "Mexico", "cosmology", "a trace of copper", "Welsh and English", "decoupage", "William Faulkner", "Arethusa", "John Newbery", "Bucharest", "Down syndrome", "manager", "Wheat", "(2)", "\"So Long", "insulin", "a Ninja", "The Winds of War", "a pear tart", "Mark Twain", "(ne) Powell", "Anacondas", "(Testudo nigra)", "a Knesset", "Administrative Professionals Week", "Life", "Dante's Inferio", "a calves", "Lulu Kennedy-Cairns", "Down by the Sally Gardens", "eulogy", "Roald Dahl", "Massachusetts", "third", "the leaves of the plant species", "Amy", "alpestrine", "Michael Sheen", "St. Patrick's Day in 1988", "Kona", "Santiago Herrera", "three", "Secretary of State", "Fernando Torres"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6578124999999999}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-3010", "mrqa_searchqa-validation-3606", "mrqa_searchqa-validation-12276", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-875", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-4055", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-2416", "mrqa_searchqa-validation-16391", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-349", "mrqa_searchqa-validation-3032", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-16220", "mrqa_searchqa-validation-3267", "mrqa_searchqa-validation-16274", "mrqa_searchqa-validation-4325", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-9172", "mrqa_triviaqa-validation-4167", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5627"], "SR": 0.53125, "CSR": 0.5367943548387097, "EFR": 0.9666666666666667, "Overall": 0.7083484543010752}, {"timecode": 31, "before_eval_results": {"predictions": ["corrosion", "transport appliances", "the word (Unabridged)", "(El Eli) Lilly", "Manhattan Project", "Maryland", "the Scotch egg", "\"carnaval\"", "Earnhardt", "\"Beer Drinker of the Year 2002\"", "Ezra Pound", "the Cuyahoga River", "\"Sacha Baron\"", "\" Apollo 13\"", "a coyote", "terminal", "\"The Boss\"", "the Chocolate Satin pie", "the Air Force Academy", "(Lime) Lime", "the monkey", "the nucleus", "Don Juan", "Texas", "the Computing-Tabulating-Recording Company", "(Chuck) Yeager", "the CIA", "\"blood\"", "lanciferous lancinate lancination", "the Wadi Hanifah valley", "Abnormal Psychology", "milk", "1990", "anaphylaxis", "copper", "the Ropers", "Bank of America", "the Lampoon", "Terry Bradshaw", "Florence", "farce", "tobacco", "Columbia University", "(Evita) Peron", "Don Quixote", "The Oprah Show", "Seattle", "insulin", "dilettante", "the cephalopod", "Ricky Martin", "(Burt) Rivers", "1989", "sea water and fresh water", "Toronto, Ontario, Canada", "the French", "power", "Frederick Forsyth", "Daniil Shafran", "67,575", "various names", "a Christian farmer", "At least 13", "The Everglades,"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5107278138528139}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-3276", "mrqa_searchqa-validation-757", "mrqa_searchqa-validation-9215", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-2914", "mrqa_searchqa-validation-9740", "mrqa_searchqa-validation-15434", "mrqa_searchqa-validation-10027", "mrqa_searchqa-validation-16718", "mrqa_searchqa-validation-12607", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-4589", "mrqa_searchqa-validation-4764", "mrqa_searchqa-validation-7211", "mrqa_searchqa-validation-14766", "mrqa_searchqa-validation-9617", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-3155", "mrqa_searchqa-validation-6987", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-11232", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-8628", "mrqa_triviaqa-validation-2879", "mrqa_triviaqa-validation-5038", "mrqa_newsqa-validation-3090", "mrqa_newsqa-validation-3911"], "SR": 0.40625, "CSR": 0.53271484375, "EFR": 0.9736842105263158, "Overall": 0.7089360608552632}, {"timecode": 32, "before_eval_results": {"predictions": ["Eric Roberts", "the Department for Culture, Media and Sport.", "wigs", "John Sevier", "Wayne Gretzky", "Boris Godunov", "Williamsburg", "Pitcairn", "air superiority", "Halloween", "a port-wine stain", "hurricane", "Broadway Cast", "one foot", "the Automobile Association", "Jutland", "a mutual fund", "the Two Sicilies", "1773", "Prometheus", "a thick cream soup", "four cups", "Minnesota", "Violetta Chamorro", "Alisa Hamilton", "Pillsbury", "oxygen", "The Last Mimzy", "jeans", "Jenna Bush", "Jonathan Demme", "silver", "the Pacific", "the ear", "Moulin Rouge", "Cape Cod", "the turtle shell", "febreze", "Dave Reichert", "ice age", "an award of a Medal of Honor", "Polish", "Walter Payton", "Orleans", "Mars", "the bomber", "copper", "Tom Ridge", "George Babbitt", "Meg Tilly", "Jeopardy", "imperative", "18", "George Halas", "Battle of Long Island", "the Dormouse", "Salema", "Kent", "Gareth Jones", "Prince Sung-won", "Newcastle upon Tyne, England", "Illness", "Dan Parris, 25, and Rob Lehr, 26,", "2,000 euros"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6036458333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_searchqa-validation-16822", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-13952", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-6114", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-13050", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-11313", "mrqa_searchqa-validation-2960", "mrqa_searchqa-validation-8969", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-12784", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-5282", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-1605", "mrqa_hotpotqa-validation-3971", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2730"], "SR": 0.53125, "CSR": 0.5326704545454546, "EFR": 1.0, "Overall": 0.7141903409090908}, {"timecode": 33, "before_eval_results": {"predictions": ["2100", "Apollo 5", "Benazir Bhutto,", "the Democratic VP candidate", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth\\'s closest neighbor.", "the i report form", "Muqtada al-Sadr", "Twitter", "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"", "the iPod Touch", "success as a recording artist", "Janet Napolitano", "the chemical at the Qarmat Ali water pumping plant in southern Iraq", "citizenship", "10,000", "4,000", "CNN/Opinion Research Corporation", "California", "Michael Arrington,", "Ralph Cifaretto", "Bobby Darin", "Two", "hacker groups anonymous and LulzSec.", "Transportation Security Administration", "Maude", "death", "Nineteen", "weight-loss", "1918-1919.", "Liza Murphy", "Turkey", "a snare and temptation and illusion", "\"I wanted to come here, and I wanted to see my kids graduate from this school district.\"", "Gary Player", "attempted murder", "Nicole", "guard in the jails", "a \"prostitute\"", "Mississippi", "40", "April 22.", "bartering -- trading goods and services without exchanging money", "37", "Nairobi, Kenya", "the war years", "Silicon Valley.", "between 1917 and 1924", "June 6, 1944", "making her comeback last year after giving birth to baby daughter Jada,", "Atlanta", "don't have to visit laundromats", "\"very compressed area.\"", "Charlene Holt", "the gastrocnemius", "Miami Heat", "4", "the Finch family", "architect", "the Secret Intelligence Service", "Ready Player One", "alcoholic drinks", "Mars", "crushable", "Tartarus"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6346197369286256}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.5454545454545454, 0.3333333333333333, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.15384615384615383, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2608695652173913, 1.0, 0.4, 1.0, 0.0, 0.2857142857142857, 1.0, 0.2857142857142857, 1.0, 0.2222222222222222, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.7368421052631579, 1.0, 0.625, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-3020", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1288", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-4032", "mrqa_triviaqa-validation-3514", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-201"], "SR": 0.53125, "CSR": 0.5326286764705883, "EFR": 1.0, "Overall": 0.7141819852941176}, {"timecode": 34, "before_eval_results": {"predictions": ["for complicity and to Odinga declaring himself the \"people's president\"", "KGPE", "Lerotholi Polytechnic", "John Kevin Delaney", "1979", "The Catholic Church in Ireland", "Trey Parker and Matt Stone", "Province of Canterbury", "310", "Bohemia", "Czech", "people working in film and the performing arts", "Washington, D.C.", "Sophie Lara Winkleman", "Portsmouth", "Sam the Sham", "coaxial", "Graham Hill", "Marika Nicolette Green", "Katherine Harris", "\"The Big Hurt,\"", "World War I", "John Richard Schlesinger, CBE ( ; 16 February 1926 \u2013 25 July 2003)", "Nikolai Trubetzkoy", "3,500,000", "Rabies", "Big Bad Wolf", "Shakespeare", "1974", "Orwell", "July 8, 2014", "Lehmber Hussainpuri", "Costa del Sol", "Gabriel Iglesias", "Kolkata", "two", "Roscoe Lee Browne", "23", "video game", "Peach", "U.S.", "Jimmy Ellis", "July 11, 2016", "1984 in Kolkata", "Drunken Master II", "San Francisco, California", "\"Jawbreaker\"", "Daniel Espinosa", "Blake Shelton, and Pharrell Williams", "Ben Stokes", "Duke University", "Royal Albert Hall and The Kennedy Center.", "Louis XV", "2,140 kilometres ( 1,330 mi )", "`` king ''", "Munich", "Admiral Johan van Galen", "Islamophobia", "New Delhi, India", "Cpl. Cesar Laurean", "Chevron", "\"Like a Rolling Stone\"", "Nick", "General McClellan"], "metric_results": {"EM": 0.484375, "QA-F1": 0.56289592760181}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false], "QA-F1": [0.11764705882352941, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8422", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-4780", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-3889", "mrqa_hotpotqa-validation-3576", "mrqa_naturalquestions-validation-10354", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4748", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3428", "mrqa_searchqa-validation-2753", "mrqa_searchqa-validation-15973"], "SR": 0.484375, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.71390625}, {"timecode": 35, "before_eval_results": {"predictions": ["downward pressure on wages", "ABC Television Center", "Cincinnati, Ohio", "North Sea", "Headless Body in Topless Bar", "\"Gliding Dance of the Maidens\"", "Patti Smith", "\"Darconville\u2019s Cat\"", "\"Kitty Hawk\"", "Augustus Germanicus", "Netherlands", "Eva Ibbotson", "the 70 m and 90 m events", "Charles Otto Puth Jr.", "Tak and the Power of Juju", "Don DeLillo", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "comedy", "1997", "University of Southern California", "New York City", "extreme nationalist, and nativist", "1st Earl Grosvenor", "Christopher McCulloch", "Marigold Newey", "an album", "The Wachowskis", "a fantasy role-playing game", "WikiLeaks", "top division", "Ramsey County", "fantasy role-playing game", "New Jersey", "east", "Giacomo Puccini", "Thomas Jefferson", "Winchester", "his virtuoso playing techniques and compositions in orchestral fusion", "1912", "Eisenhower Executive Office Building", "\"The Original Sound Track from Five Summer Stories\"", "\"American Chopper\"", "the first and second segment", "King of England and Ireland", "Hindi", "Netherlands", "soccer", "Kings Point, New York", "a Ballon d'Or", "Westminster system", "Bruce Grobbelaar", "was less dangerous", "seven years earlier on Christmas Eve", "`` Mirror Image ''", "79", "United States", "football", "the PDSA Dickin Medal", "in an effort to make the animals' lives as natural as possible.\"", "Kurt Cobain", "paid tribute to pop legend Michael Jackson,", "our sea", "a sedimentary rock", "Chekhov"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6480893493761141}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.6666666666666666, 0.23529411764705882, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7182", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-455", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-2257", "mrqa_naturalquestions-validation-4338", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-6520", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1352", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-2183"], "SR": 0.546875, "CSR": 0.5316840277777778, "EFR": 1.0, "Overall": 0.7139930555555555}, {"timecode": 36, "before_eval_results": {"predictions": ["is a phylum of animals that live in marine waters worldwide", "the Walther P38", "the Commanding General", "the Veneto region of Northern Italy", "various deities, beings, and heroes", "a campaign setting", "Wandsworth, London", "1967", "Smithsonian", "capital crimes", "1592", "Smoothie King Center", "Nanna Popham Britton", "2013", "the night of 9\u201310 March 1945", "Currer Bell", "Jim Davis", "Ted Bundy", "Premier League club Manchester United", "Parlophone", "McDowell County, West Virginia", "Life Is a Minestrone", "Louis Silvie \"Louie\" Zamperini", "an English professional footballer", "Shakespeare in the Park", "calcifuges", "South Australia", "a multi-control USB mouse", "George Gordon Byron, 6th Baron Byron, FRS (22 January 1788 \u2013 19 April 1824)", "Singapore", "1853", "the Big Bad Wolf", "seven nights a week", "Mickey Mouse cup", "Manchester\u2013Boston Regional Airport", "Frank Ocean", "Gangsta's Paradise", "plays for Turkish club Be\u015fikta\u015f.", "in most casinos", "331 episodes", "Kristina Ceyton and Kristian Moliere", "1835", "Helsinki, Finland", "Francisco P. Felix, a former mayor of Cainta,", "Carrefour", "2014", "\"Read It and Weep\"", "Wet 'n Wild Orlando", "Victoria", "the 5", "526 people per square mile", "the Caucasus region", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "the Charlotte Hornets", "a burrow", "Billie Holiday", "the Cumberland", "cardio", "the American Civil Liberties Union", "three of the bombers", "Three Little Pigs", "Spider-Man", "Cynthia Nixon", "a French parliamentary commission recommended a partial ban on any veils that cover the face"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5912016369047619}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.4, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.7142857142857143, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.28571428571428575, 0.0, 0.6666666666666666, 0.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.5, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4534", "mrqa_hotpotqa-validation-4576", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-3660", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-3590", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-1580", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-5055", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-1753", "mrqa_hotpotqa-validation-614", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-4653", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-423", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-891", "mrqa_searchqa-validation-15738", "mrqa_newsqa-validation-297"], "SR": 0.453125, "CSR": 0.5295608108108107, "EFR": 1.0, "Overall": 0.713568412162162}, {"timecode": 37, "before_eval_results": {"predictions": ["castles and vineyards", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "2000", "The lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "will American go bankrupt?", "The Spanish flight crew is innocent and should be released,", "On Wednesday evening he walked into the Central Methodist Church in downtown Johannesburg and joined a long queue of people waiting for shelter and food.", "President Paul Biya, left, walks with Pope Benedict XVI at the airport in Yaounde, Tuesday.", "Herman Cain", "Zulfikar Ali Bhutto,", "offered money or other discreet aid for the effort if it could be made available,", "News of the World tabloid.", "Argentina lays claim not just to the islands, but to any resources that could be found there.", "prostate cancer", "July 23.", "Roqaya al-Sadat,", "the single-engine Cessna 206 went down,", "from the capital, Dhaka, to their homes in Bhola", "software magnate", "150 passengers", "Sharon Bialek", "disqualified from a bout for pulling on the top-knot of an opponent, and has gained a reputation as the enfant-terrible of sumo.", "did not speak to those who had gathered but shadow-boxed to spectators and cameras before meeting his distant relatives in 2002.", "The Delta Queen will go out of service if Congress does not grant the ship another exemption from a 1960s federal law,", "the state's attorney", "photos", "\"Twilight\" book series", "The First Stop Resource Center assists veterans and their families through various periods of crises, including homelessness and addiction.", "cities throughout Canada", "to stop rocket fire on its southern cities and towns.", "opening of its new restaurant next to the home of Mona Lisa as something completely normal.", "writing and starring in 'The Prisoner' about a former spy locked away in an isolated village who tries to escape each episode.", "Friday", "Belfast", "Swat Valley", "Former Beatles", "Washington Redskins fan and loved to travel,", "Dolgorsuren Dagvadorj", "Sicily", "last summer.", "Kuranyi's", "revelry", "London Heathrow's Terminal 5", "18", "two months ago", "Casablanca, Morocco", "publicly criticized his father's parenting skills.", "$250,000", "at a construction site in the heart of Los Angeles.", "$3 billion", "Manchester United.", "7 July", "Bob Dylan", "The London terminus is St Pancras International", "a casement window", "Denver", "Brazil", "Phelan Beale", "sandstone", "2005", "Babe Ruth's", "Penn Station", "coffee", "David Lodge"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4945666362464131}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.5263157894736842, 0.05714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 0.0, 0.17391304347826084, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.08695652173913045, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.9333333333333333, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-998", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-925", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-241", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2938", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1516", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-3711", "mrqa_triviaqa-validation-2027", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-4180", "mrqa_searchqa-validation-11671", "mrqa_searchqa-validation-15555"], "SR": 0.40625, "CSR": 0.5263157894736843, "EFR": 1.0, "Overall": 0.7129194078947367}, {"timecode": 38, "before_eval_results": {"predictions": ["death of a heretic.\"", "Lars von Trier", "37", "cancerous tumor.", "her home", "is the U.N. nuclear watchdog agency's strongest warning yet that Iran could be aiming to build a nuclear bomb.", "evokes childhood memories in this four-line ode to Mom.", "84-year-old", "free laundry service", "celebrity-studded gala and a three-day party.", "opium", "Turkey,", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Galveston, Texas,", "Six", "The dogs' canine high jinks are closer to \"Beverly Hills Chihuahua\"", "581", "make life a little easier", "a remote part of northwestern Montana", "building bombs,", "forgery and flying without a valid license,", "2050,", "Eintracht Frankfurt", "Juan Martin Del Potro.", "50", "The sound of pounding hooves thunders in the high desert air.", "involved in forged credit cards and identity theft", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Akshay Kumar", "133 people", "more than 100", "The Ski Train", "a man's lifeless, naked body", "Flint, Michigan.", "O2 Arena.", "Philippines", "$500,000", "work rule issues.", "The judge in the federal trial of alleged \"underwear bomber\" Umar Farouk AbdulMutallab refused Tuesday to prevent the prosecution from calling the device he allegedly carried a \"bomb.\"", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "The children have been living in an orphanage in Abeche", "second time", "noose", "Congress", "Sen. Joe Lieberman", "Bastian Schweinsteiger", "Gov. Bobby Jindal", "one bomber.", "Scardia,", "\"17 Again\"", "United Arab Emirates", "( n )", "New Mexico", "The Soviet Union, too, had been heavily affected", "jewelled Easter eggs", "a northern supercontinent known as Laurasia.", "the A38", "ethereal", "Guinness World Records", "Citizens for a Sound Economy", "bees, honey, and the Black Madonna", "Zenda", "Jean Lafitte", "a foul"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5178689295887948}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8, 0.25, 0.06060606060606061, 1.0, 0.4, 0.0, 1.0, 0.4, 0.1212121212121212, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.4, 1.0, 1.0, 1.0, 0.0, 0.23255813953488372, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.17391304347826086, 0.35294117647058826, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.26666666666666666, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3052", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-3006", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-4860", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3713", "mrqa_hotpotqa-validation-4020", "mrqa_searchqa-validation-1177", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-14613"], "SR": 0.421875, "CSR": 0.5236378205128205, "EFR": 0.9459459459459459, "Overall": 0.7015730032917532}, {"timecode": 39, "before_eval_results": {"predictions": ["brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait.", "Western New York and Central New York", "in people and animals", "Andy Serkis", "Martin Lawrence", "Phillipa Soo", "sedimentary", "in all land - living organisms", "John Young", "semi-autonomous organisational units", "W. Edwards Deming", "Tom Tucker", "Cyndi Grecco", "any unfavourable and unintended sign ( including an abnormal laboratory finding ), symptom, or disease temporally associated with the use of a medicinal ( investigational ) product", "`` rigid ''", "Fleetwood Mac", "Jim Justice", "December 12, 2017", "All of the chicken sold is reared in South Africa", "1832", "Curtis Armstrong", "the pia mater", "St Pancras International", "Jerry Leiber and Mike Stoller", "DeWayne Warren", "Charles Woodson", "Jesse Wesley Williams", "Peter Cetera", "291", "Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Book of Exodus", "January 2002", "Kristy Swanson", "2001", "1994", "Washington", "Triple Alliance of Germany, Austria - Hungary, and Italy", "Australia", "Wisconsin", "The standing rib roast", "Lyndon B. Johnson", "David Tennant", "Ben Findon, Mike Myers and Bob Puzey", "Charles Path\u00e9", "Barbara Windsor", "1857", "1997", "eleven", "Mel Gibson", "boy", "works in a bridal shop", "Bolton", "The Wrestling Classic", "aircraft", "Mike Fiers", "Rockland", "Delilah Rene", "three", "\"The Hutus were considered inferior,", "Frank Ricci", "Zanzibar", "Mao Zedong", "telephone", "her boyfriend,"], "metric_results": {"EM": 0.5, "QA-F1": 0.601928887085137}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false], "QA-F1": [0.07142857142857142, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.4, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727273, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9513", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-2365", "mrqa_hotpotqa-validation-2210", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3661", "mrqa_searchqa-validation-12993", "mrqa_newsqa-validation-3331"], "SR": 0.5, "CSR": 0.523046875, "EFR": 0.9375, "Overall": 0.699765625}, {"timecode": 40, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1840", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3310", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5424", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-854", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3525", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-370", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4658", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2470", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-11232", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-1177", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14978", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15004", "mrqa_searchqa-validation-15504", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-16353", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4567", "mrqa_searchqa-validation-4764", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8176", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8366", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8791", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-9803", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-1540", "mrqa_squad-validation-1586", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2167", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2473", "mrqa_squad-validation-2640", "mrqa_squad-validation-2672", "mrqa_squad-validation-270", "mrqa_squad-validation-2757", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3407", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3786", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-3939", "mrqa_squad-validation-4010", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4484", "mrqa_squad-validation-4546", "mrqa_squad-validation-4671", "mrqa_squad-validation-4750", "mrqa_squad-validation-4799", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5019", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5634", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-6029", "mrqa_squad-validation-6318", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6594", "mrqa_squad-validation-6630", "mrqa_squad-validation-6981", "mrqa_squad-validation-7023", "mrqa_squad-validation-7168", "mrqa_squad-validation-7194", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7294", "mrqa_squad-validation-7466", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-7907", "mrqa_squad-validation-7932", "mrqa_squad-validation-8066", "mrqa_squad-validation-8066", "mrqa_squad-validation-8127", "mrqa_squad-validation-8209", "mrqa_squad-validation-8253", "mrqa_squad-validation-8386", "mrqa_squad-validation-8399", "mrqa_squad-validation-8488", "mrqa_squad-validation-8501", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8901", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9855", "mrqa_squad-validation-9868", "mrqa_squad-validation-9901", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1052", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4357", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6418", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6681", "mrqa_triviaqa-validation-6704", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-895"], "OKR": 0.845703125, "KG": 0.49375, "before_eval_results": {"predictions": ["Turkana", "Action Jackson", "David", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "Jacksonville, Florida, and Petersburg, Virginia", "The Third Five - year Plan", "Hermann Ebbinghaus", "Ronald Reagan", "Baltimore", "to collect menstrual flow", "April 1st", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Lesley Gore", "Andrew Lincoln", "Joe Spano", "a focal point", "a political ideology", "Matt Monro", "Wednesday, 5 September 1666", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "response to a perceived harmful event, attack, or threat to survival", "The succession follows the order of Vice President, Speaker of the House of Representatives, President pro tempore of the Senate, and then the heads of federal executive departments who form the Cabinet of the United States", "retail", "July 2, 1776", "a balance sheet", "within an English - language book", "1994", "Fred Ott", "Tevin Campbell", "September 24, 2017", "early Christians of Mesopotamia", "Anna Murphy", "peninsular mainland", "`` new version ''", "the revolutionaries named their newly independent country La Rep\u00fablica Dominicana", "1976", "Brad Johnson", "Egypt", "Montreal", "2003", "May 30, 2017", "a turlough", "Hathi Jr", "The 50 stars on the flag represent the 50 states of the United States of America", "The euro", "local authorities", "The episode `` Two Fathers ''", "2017 / 18 Divisional Round game against the New Orleans Saints", "Nick Kroll", "Holly Marshall", "1,350", "Brad Crenshaw", "The Jetsons", "Venezuela", "pale body", "Keelung", "England", "Sunday,", "Elizabeth Birnbaum", "Karen Floyd", "Michelob", "burrito", "Three\\'s Company", "Australian actor and film producer"], "metric_results": {"EM": 0.5, "QA-F1": 0.5676657137515403}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.4, 0.1111111111111111, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.29411764705882354, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-7464", "mrqa_triviaqa-validation-336", "mrqa_hotpotqa-validation-1328", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-15726"], "SR": 0.5, "CSR": 0.522484756097561, "EFR": 1.0, "Overall": 0.7247313262195122}, {"timecode": 41, "before_eval_results": {"predictions": ["many middle eastern scientists", "1889", "to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "ummat al - Islamiyah", "2018", "Saint Alphonsa", "Peggy Lipton", "in the basic curriculum", "air moisture", "the left of the dinner plate", "Yondu Udonta", "Terry Kath", "by 1824", "November 5, 2017", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "\u20b9 39.50 lakh", "one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "supervillains who pose catastrophic challenges to the world", "Atlanta", "2007", "Australia", "Bed and breakfast", "Masha Skorobogatov", "HTTP / 1.1", "Jack Gleeson", "clockwise rotation", "October 6, 2017", "the Sunni Muslim family", "Gatiman express", "Escherichia coli", "Roman Reigns", "Erica Carroll", "Audrey II", "Acid rain", "Kaley Christine Cuoco", "Quantitative psychological research", "Jules Shear", "Darren McGavin", "1952", "Randy VanWarmer", "April 3, 1973", "manta rays and Scorpion fish", "computer science and artificial intelligence", "St. Louis Cardinals", "Wilhelm Groener", "9.1 %", "Bruno Mars", "between the Mediterranean Sea to the north and the Red Sea in the south", "Lord's", "Kingsford, Michigan", "Honor\u00e9 Mirabeau", "Daniel Defoe", "1961", "birmingham", "First Street", "Selected Writings by Steve Biko", "Princes Park", "\"Bloody brilliant. We had a champagne breakfast to celebrate.\"", "Kyra and Violet,", "\"Empire of the Sun,\"", "Bulldog Drummond", "Petsmart", "Carrie Underwood", "1970"], "metric_results": {"EM": 0.5, "QA-F1": 0.5988764483065954}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 0.35294117647058826, 0.6666666666666666, 0.56, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.1818181818181818, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.9, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-1455", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-5038", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-3380", "mrqa_searchqa-validation-3332"], "SR": 0.5, "CSR": 0.5219494047619048, "EFR": 0.9375, "Overall": 0.7121242559523809}, {"timecode": 42, "before_eval_results": {"predictions": ["In his book Luther's Last Battles: Politics and Polemics 1531\u201346 (1983)", "1861\u20131865", "private Ivy League research university", "shooting guard", "703", "December 1974", "Sufism", "Capellini", "1614", "international association football competitions", "German and American", "age thirteen", "McComb, Mississippi", "Santa Fe", "Donald Duck", "Kew Gardens", "the northeastern part", "American Horror Story", "Chicago, Illinois", "Free Range Films", "The Deep Blue Sea", "Vernon L. Smith", "Australian actor", "Frank Fertitta, Jr.", "October 16, 2015", "the Battelle Energy Alliance", "Kaep", "I Write What I Like", "August 2005", "SKUM", "51,271", "Sun Valley, Idaho", "Guardians of the Galaxy Vol. 2", "burlesque", "Pulitzer Prize", "Scott Carson", "IFFHS World's Best Goalkeeper", "Suspiria", "singer, songwriter, actress, and radio and television presenting", "Belladonna", "Russell T Davies", "\"Creed\"", "spot-fixing scandal", "Erich Schmidt-Leichner", "the City of Peace", "Alfred in \"Die Fledermaus\" by Johann Strauss, Sellem in Igor Stravinsky's \"The Rake's Progress\"", "Movie Masters", "6,241", "Hennepin County", "Australian coast", "Adelaide Botanic Garden, Hutt Street, and Victoria Park", "the 1967 film Cool Hand Luke", "Michael Buffer", "Presley Smith", "AFC Wimbledon", "Erewhon", "Sushi", "a share in the royalties", "Asashoryu,", "\"The Real Housewives of Atlanta\"", "Antnio Guterres", "ratify the Constitution of the United States", "n", "Dairy Queen"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6500631313131313}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2666666666666667, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2523", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3352", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4296", "mrqa_hotpotqa-validation-1461", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-973", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5385", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-780", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-4201", "mrqa_hotpotqa-validation-2239", "mrqa_hotpotqa-validation-2395", "mrqa_naturalquestions-validation-5293", "mrqa_triviaqa-validation-3569", "mrqa_triviaqa-validation-748", "mrqa_newsqa-validation-1125", "mrqa_searchqa-validation-3012", "mrqa_searchqa-validation-732", "mrqa_searchqa-validation-11496"], "SR": 0.53125, "CSR": 0.5221656976744187, "EFR": 1.0, "Overall": 0.7246675145348838}, {"timecode": 43, "before_eval_results": {"predictions": ["25", "Evey's mother", "from 1848 to 1852", "Norwood", "American Revolutionary War military encampment", "1986", "Kansas City Wiz", "Jacking", "February 5, 2015", "The Worm", "MGM Resorts International", "Sam Raimi", "StubHub Center", "1912", "Balloon Street, Manchester", "Liverpool Bay", "St. Louis, Missouri", "Stephen King", "\"Seducing Mr. Perfect\"", "British Labour Party", "The Five", "Ang Lee", "Taylor Swift", "Objectivism", "\"Traumnovelle\"", "Mandarin", "London Tipton", "KlingStubbins", "the Goddess of Pop", "Chevron Corporation", "Black Panther Party", "Baldwin", "John \"John\" Alexander Florence", "YouTube", "Kohlberg K Travis Roberts", "Salzkammergut", "Rain Man", "feats of exploration", "video game", "Father Dougal McGuire", "just over 1 million", "Mulberry", "London", "Subway restaurants", "cancer", "Campbellsville", "Mark Helfrich", "A 10-time Tony Award nominee", "Larry Cordle", "Field Marshal Lord Gort", "Owsley Stanley", "Donna Mills", "16 seasons", "gas exchange", "the underground organization of the Irish Republican Brotherhood", "Sam Allardyce", "gin", "almost 9 million", "vitamin injections", "to lose bouts,", "to gain exposure to a stock", "Oxford University Dramatic Society", "croppin' John", "Haiti"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6729809253246752}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.4, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-1830", "mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-2770", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-3856", "mrqa_hotpotqa-validation-4501", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-8767", "mrqa_triviaqa-validation-2810", "mrqa_newsqa-validation-3325", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-14771"], "SR": 0.5625, "CSR": 0.5230823863636364, "EFR": 1.0, "Overall": 0.7248508522727273}, {"timecode": 44, "before_eval_results": {"predictions": ["broken arm", "Israel", "Wyoming", "Franklin Roosevelt", "zero", "Copenhagen", "Pulitzer Prize", "the 904 Olympics", "New Zealand", "three showgirls", "The Honeymooners", "sesame oil", "enamel", "J.L. Hudson's", "method acting", "Sam Kinison", "President pro tempore", "Roman Empire", "Alaska", "Matt Leinart", "sultan al-Athir", "Jeremy Bentham", "the United States of America", "the Mekong", "King Neptune", "the Olympic Games", "Dan Morrison", "s Francisco", "solid", "Tony\\'s Restaurant", "Ivory Coast", "Lord of the Rings: The Return of the King", "Birch-tree", "Alanis Morissette", "a tie", "soggy mess", "King Minos", "\"Man Appeal\"", "Schindler\\'s List", "Jonathan Rhys Meyers", "Stephen Crane", "Mississippi", "a Rainbow loom necklace", "Yellow Brick Road", "the Madding Crowd", "Steely Dan", "Linda Tripp", "the Great Unconformity", "sultan of Egypt", "adios", "the Gadsden Treaty", "New Zealand to New Guinea", "Roman Reigns", "the new dynasty", "the Marshall Plan", "Dick Turpin", "black", "15", "Cymbeline", "Mary-Kay Wilmers", "the shipping industry -- responsible for 5% of global greenhouse gas emissions,", "Hundreds", "2008", "three"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5255208333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2627", "mrqa_searchqa-validation-7525", "mrqa_searchqa-validation-3120", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-14186", "mrqa_searchqa-validation-6399", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-12098", "mrqa_searchqa-validation-15476", "mrqa_searchqa-validation-8837", "mrqa_searchqa-validation-15203", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-2669", "mrqa_searchqa-validation-10061", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-11577", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-16207", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-7176", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-11005", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-6157", "mrqa_hotpotqa-validation-5093", "mrqa_newsqa-validation-3979"], "SR": 0.484375, "CSR": 0.5222222222222221, "EFR": 0.9696969696969697, "Overall": 0.7186182133838384}, {"timecode": 45, "before_eval_results": {"predictions": ["the exploration area and allowing televised liftoff of the LM", "the leader of a drug cartel that set off two grenades during a public celebration in September, killing eight people and wounding more than 100.", "Robert Barnett,", "glass shards", "Sunday.", "\"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "a Christian farmer", "Islamabad", "reported in December that Jackson was battling a potentially fatal disease that required a life-saving lung transplant, his publicist responded that he was \"in fine health\" and that the story was \"a total fabrication.\"", "(Mokotedi Mpshe,", "\"project work\"", "a female cadaver", "12 hours", "the United States, NATO member states, Russia and India", "the Obama and McCain camps", "$273 million", "Wigan Athletic", "it -- you know -- black is beautiful,\"", "at least 25 dead", "former U.S. secretary of state", "vowed revenge for Israel's air and ground assault on Gaza and called Israel's actions against Hamas militants \"a gift\" from U.S. President-elect Barack Obama.", "the civil affairs division of the U.S. military,", "U.S.", "1.2 million", "North Korea", "a bookish intellectual who's cool in a crisis and quick on his feet, like Ken Jennings with a shot of adrenaline.", "Angela Merkel", "at least 12 months.", "Passers-by", "British troops in Iraq", "Lance Cpl. Maria Lauterbach", "Picasso's muse and mistress, Marie-Therese Walter.", "80", "Polo", "bartering", "the site of the clashes,", "the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "fluoroquinolone", "North Korea", "human rights abuses against ethnic Somalis by rebels and Ethiopian troops are rampant.\"", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "Antioquia, Colombia", "a student who admitted to hanging a noose in a campus library,", "to protect ocean ecology, address climate change and promote sustainable ocean economies.", "Asian qualifying Group 2", "The South African captain Graeme Smith led from the front with 75 as his team wrapped up victory.", "Six", "flooding and debris", "then-Sen. Obama", "The 23-year-old Rezai -- who had only claimed WTA Tour titles at Strasbourg and Bali prior to Madrid -- continued her remarkable week with a 6-2 7-5 victory, adding Williams' scalp", "the radical Islamist militia that controls the city", "201", "IBM", "Gorakhpur railway station, Uttar Pradesh", "chief Inspector of Prisons", "Charlie Cairoli", "Sweden", "2017", "Edmund Ironside", "Comme des Gar\u00e7ons", "President Roosevelt", "\"Wherefore art thou\"", "the Shang dynasty", "Germany"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4805324074074074}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true], "QA-F1": [0.4, 0.0, 1.0, 1.0, 1.0, 0.08, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.07407407407407408, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.13333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.6666666666666666, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4027", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1848", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-1257", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-19", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-1863", "mrqa_hotpotqa-validation-3844", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-11067", "mrqa_searchqa-validation-4478"], "SR": 0.390625, "CSR": 0.5193614130434783, "EFR": 1.0, "Overall": 0.7241066576086956}, {"timecode": 46, "before_eval_results": {"predictions": ["Thames River", "an independent homeland for the country's ethnic", "Turkish President Abdullah Gul,", "text messaging", "received no reports from pilots in the air of any sightings", "in Fayetteville, North Carolina,", "the surge,", "Noriko Savoie", "work together to stabilize Somalia and cooperate in security and military operations.", "not guilty", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "You can go from rags to riches there.", "the United States", "Manny Pacquiao", "Garth Brooks", "The son of Gabon's former president", "Wally", "a body", "Prince George's County prison", "The train in front had stopped", "Karen Floyd", "helped the United States frame the challenges.", "since 1983.", "$20 million to $30 million,", "Daryeel Bulasho Guud", "Zulfikar Ali Bhutto,", "the insurgency,", "Stoke City.", "Former detainees", "three", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.\"", "his past and his future", "a terrorist.", "travel in cars with tinted windows", "mild to moderate depression", "and brave beyond her years.", "job opportunities for nearly 200,000 Iraqi citizens in infrastructure, industrial projects, support services and other business activities.", "Mitt Romney", "\"The Sopranos,\"", "The Louvre", "Jason Polis", "a delegation of American Muslim and Christian leaders", "the legitimacy of that race.", "Australian officials", "misdemeanor", "$10 billion", "\"procedure on her heart,\"", "safety issues in the company's cars", "Newcastle", "54", "1 mile ( 1.6 km )", "Abanindranath Tagore CIE", "1996", "Kiri Te Kanawa", "Submarine Sunk", "1960's", "the onset and progression of Alzheimer's disease", "The Future", "1770", "Jacob and Wilhelm Grimm", "Seasons in the Sun", "Geraldine Farrar", "Michael Harney"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6805793039799972}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.34782608695652173, 1.0, 0.0, 0.5, 0.15384615384615385, 1.0, 0.2, 0.9333333333333333, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666665, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2804", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-993", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2547", "mrqa_newsqa-validation-247", "mrqa_triviaqa-validation-4212", "mrqa_hotpotqa-validation-5485", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-13873", "mrqa_naturalquestions-validation-3802"], "SR": 0.5625, "CSR": 0.5202792553191489, "EFR": 1.0, "Overall": 0.7242902260638298}, {"timecode": 47, "before_eval_results": {"predictions": ["waffles", "President Obama", "Harry Potter", "a stagecoach", "Serbian Cyrillic", "chocolate Pecan pie", "the Charleston", "shiatsu", "Siegfried", "taxonomy", "Anne Rice", "Anne Murray", "the lithosphere", "Monronella Wyatt", "Lady Jane Grey", "Santeria", "the Duggar family", "the Lincoln penny", "Rookwood", "optimistic", "Belarus", "Airplane", "French toast", "gingerbread", "Swiss Cheese", "Sex Pistols", "Samuel Johnson", "Agatha Christie", "Jack Dempsey", "conglomeratus", "brood", "Edison", "Mount Everest", "black-eyed pea dip", "the Star-Spangled Banner", "Battlestar Galactica", "Yugoslavia", "the Surgeon", "a Place Bet", "War and Peace", "Frank Lloyd Wright", "Falcon Crest", "Harold Godwinson", "Grant and Sherman", "Adam Smith", "yeast", "Pearl S. Buck", "(VICTOR MARIE HUGO)", "Atlanta", "Ayn Rand", "Sisyphus", "2017 season", "Nathan Hale ( June 6, 1755 -- September 22, 1776 )", "the United Kingdom", "Theodore Roosevelt", "bacteria", "Gettysburg", "Sam Raimi", "1940s and 1950s", "Sleepy Brown", "\"They know of our respect for the civil community,\" the statement said, adding that a government that \"never has looked out for these indigenous communities and has plunged them into war can't be their defenders.", "a judge to order the pop star's estate to pay him a monthly allowance,", "ALS6", "Stevie Wonder"], "metric_results": {"EM": 0.640625, "QA-F1": 0.737939133986928}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.8, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05555555555555555, 0.7058823529411764, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12474", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-16128", "mrqa_searchqa-validation-15255", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10067", "mrqa_searchqa-validation-1056", "mrqa_searchqa-validation-16434", "mrqa_searchqa-validation-11677", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14093", "mrqa_searchqa-validation-15591", "mrqa_searchqa-validation-2263", "mrqa_searchqa-validation-15327", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-4245", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1952"], "SR": 0.640625, "CSR": 0.5227864583333333, "EFR": 1.0, "Overall": 0.7247916666666666}, {"timecode": 48, "before_eval_results": {"predictions": ["Lalo Schifrin", "the mid-1980s", "an American veterinary and former child actor", "annually in late January or early February", "The Turbo Charged Prelude", "John Adams of Massachusetts, Benjamin Franklin of Pennsylvania, Thomas Jefferson of Virginia, Robert R. Livingston of New York, and Roger Sherman of Connecticut", "the Ute name", "the sixth series", "the summer of 2003", "in the face", "the 1970s", "from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Phosphorus pentoxide", "the Student League for Industrial Democracy ( SLID )", "Donald Gets Drafted", "Paul and Timothy", "the Great Plains and U.S. Interior Highlands region", "a burden to be carried as penance", "Michael Phelps", "British", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W", "Martin Lawrence", "1986", "1932", "Aeschylus", "on BBC One on Saturday evenings", "the American Revolutionary War", "The Abbott and Costello Show", "the 2009 model year", "Cyrus", "Saint Alphonsa", "Billie Jean King", "the Second Battle of Manassas", "V", "The Chainsmoker", "HTML", "the Jews", "Andy", "along Interstate 20", "pools campaign contributions from members and donates those funds to campaign for or against candidates, ballot initiatives, or legislation", "The bills taken up under legislative power of parliament are treated as passed provided majority of members present at that time approved the bill either by voting or voice vote", "In the eight episode series", "Julie Deborah Kavner", "September 19, 2017", "May 2002", "Chelsea", "San Antonio, Texas", "Luther Ingram", "Omar Khayyam", "1984", "Senator Joseph McCarthy's Senate Permanent subcommittee on Investigations, an investigation known as the Army -- McCarthy hearings", "\"F\u00fcr Elise\"", "a devil", "Harrods", "Adolfo Rodr\u00edguez Sa\u00e1", "Tool", "seven years", "a rocket", "Ali Bongo", "a motor motorcycle accident.", "The Sisters Rosensweig", "Cops", "Vatican City", "Timothy Dalton"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5316476902762699}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.4444444444444444, 0.0, 0.16, 0.3636363636363636, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.8421052631578948, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-74", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3122", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-9281", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-1449", "mrqa_naturalquestions-validation-9160", "mrqa_triviaqa-validation-4371", "mrqa_hotpotqa-validation-2441", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-2167", "mrqa_searchqa-validation-4594"], "SR": 0.40625, "CSR": 0.5204081632653061, "EFR": 0.9736842105263158, "Overall": 0.7190528497583244}, {"timecode": 49, "before_eval_results": {"predictions": ["Louisa May Alcott", "Davy Crockett", "Sugar Ray", "suspicious Minds", "Jupiter", "Newspaper", "the three Wise Men", "Ladies Professional Golf Association", "Henry VIII", "corpulent", "Willa Cather", "Copacabana", "Krakow", "Daredevil", "Algiers", "the emerald", "French cheese", "Mauna Loa", "Mark David Chapman", "Macy\\'s", "Sonny Corleone", "Fred Claus", "the Deathly Hallows", "the Kremlin", "the raven", "New York Cosmos", "Richard Feynman", "John Grisham", "Positron emission tomography", "Catherine the Great", "Eisenhower", "Ellen Wilson", "arthritis", "the National Gallery of Art", "Crispix", "Gabriela Sabatini", "St. Louis", "\"Yesterday Henry loved me\"", "the Caspian tern", "the Beagle", "Gene Autry", "the Wing-T formation", "Luzon", "Henry Hudson", "a diamond", "Roger Brooke Taney", "the United Nations Charter", "stalking", "The Unbearable Lightness of Being", "the Appian Way", "Joseph", "`` Nearer, My God, to Thee ''", "A marriage officiant, solemniser, or `` vow master ''", "1 October 2006", "a financial trader turned Internet entrepreneur", "Kenny Everett", "Matthew Bellamy", "Jack White", "Isabella II of Jerusalem", "villanelle", "Revolutionary Armed Forces of Colombia,", "Oaxaca", "$5.5 billion", "will be speaking to a small group of friends, colleagues and close associates,\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.6374007936507937}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.8, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6521", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-9168", "mrqa_searchqa-validation-14716", "mrqa_searchqa-validation-9072", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-1620", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-2632", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-6765", "mrqa_searchqa-validation-12896", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-7013", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-2231", "mrqa_naturalquestions-validation-8217", "mrqa_triviaqa-validation-2643", "mrqa_triviaqa-validation-1277", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3761"], "SR": 0.546875, "CSR": 0.5209375, "EFR": 1.0, "Overall": 0.724421875}, {"timecode": 50, "UKR": 0.693359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2302", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2458", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-870", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3062", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3757", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-9778", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2470", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10752", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-10981", "mrqa_searchqa-validation-11232", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-12474", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15203", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3295", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4567", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6765", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7287", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8176", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-9803", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1540", "mrqa_squad-validation-1586", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2523", "mrqa_squad-validation-2640", "mrqa_squad-validation-270", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3770", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4546", "mrqa_squad-validation-4671", "mrqa_squad-validation-4750", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-6029", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6630", "mrqa_squad-validation-7023", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8066", "mrqa_squad-validation-8209", "mrqa_squad-validation-8253", "mrqa_squad-validation-8386", "mrqa_squad-validation-8488", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9819", "mrqa_squad-validation-9868", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-4949", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-895"], "OKR": 0.8125, "KG": 0.4546875, "before_eval_results": {"predictions": ["Russia offered money or other discreet aid for the effort if it could be made available,", "recall", "American", "helicopters and unmanned aerial vehicles", "1-0", "President Obama", "Kris Allen", "Arabic, French and English", "Saturday", "snake-hunter", "a skilled hacker could disrupt the system and cause a computer attack on its control system.", "in a Starbucks this summer.", "is getting through to the average person, in Cairo, in Jeddah and Dubai.", "the former Massachusetts governor in an ad Sunday in Iowa's The Des Moines Register newspaper.", "Knox's parents,", "telling CNN his comments had been taken out of context. He said the murder of the boss could never be justified.", "her fianc\u00e9,", "\"You think of torture, you think of some horrible physical act done to an individual. This was not any one particular act; this was just a combination of things that had a medical impact on him, that hurt his health.", "a woman", "in Haiti.", "228", "Israel's vice prime minister compared Iran to Nazi Germany", "\"The oceans are kind of the last frontier for use and development,\"", "black is beautiful,\"", "India", "2009", "Nafees A. Syed", "At least 38", "for death squad killings carried out during his rule in the 1990s.", "Rod Blagojevich,", "10", "Dr. Maria Siemionow,", "Tom Hanks", "her boyfriend,", "forgery and flying without a valid license,", "14", "Britain. He holds a Saudi passport.", "Dogpatch Labs Europe", "two", "is pretty much resolved,\"", "16", "\"bystander effect\": The more people involved in a criminal incident, the less likely any one of them will intervene to do something about it.", "Tom Hanks", "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "boyhood experience in a World War II internment camp", "the United States", "June 2002.", "his daughter, Roqaya al-Sadat,", "At least 15", "three", "London's", "the Federated States of Micronesia and the Indonesia ( which consists of thousands of islands )", "the inner core", "Turducken", "Bonnie and Clyde", "'reactive to far ambush'", "a latte", "wineries", "Belgian", "Guthred", "soothsayer", "Coca-Cola (3) (1962)", "Waldorf Astoria New York", "Fringillidae"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5347965475282409}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false], "QA-F1": [0.9333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.2666666666666667, 0.4, 0.13333333333333333, 0.0, 1.0, 0.1904761904761905, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.1111111111111111, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.2105263157894737, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.25, 0.8, 1.0, 0.0, 1.0, 0.17391304347826084, 1.0, 0.9600000000000001, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-3228", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-3629", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-914", "mrqa_newsqa-validation-3651", "mrqa_naturalquestions-validation-10209", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-1389", "mrqa_hotpotqa-validation-471", "mrqa_searchqa-validation-7167", "mrqa_searchqa-validation-15063", "mrqa_triviaqa-validation-4377"], "SR": 0.40625, "CSR": 0.5186887254901961, "EFR": 1.0, "Overall": 0.6958471200980392}, {"timecode": 51, "before_eval_results": {"predictions": ["Tuesday night,", "a judge to order the pop star's estate", "\"Red Lines,\"", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "Herman Thomas", "\"bleaching\" in which algae living in the coral die and leave behind whitened skeletons.", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "2,000 euros ($2,963)", "Christopher Savoie", "where they can learn in safer surroundings.", "eight-week long", "\"Oprah is an angel, she is God-sent,\"", "D.J. Knight of Pearlman, Texas,", "Grayback Forestry in Medford, Oregon,", "January", "Anjuna beach in Goa", "A member of the group dubbed the \"Jena 6\"", "the IV cafe.", "Venus Williams", "two", "Monday", "Seoul", "\"Dr. No\"", "collaborating with the Colombian government,", "she also believed police were trying to cover up the truth behind her daughter's murder,", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "the breathing tube from her throat.", "Pakistan's", "Charles Lock", "\"This is not something that anybody can reasonably anticipate,\"", "Somali-based", "gun charges,", "\"Hairspray,\"", "more than 20 times", "bipartisan", "helping on the sandbags", "to sniff out cell phones.", "2,000 people,", "41,", "a \"happy ending\" to the case.", "Dharamsala, India.", "Liza Murphy", "Arsene Wenger", "four months ago,", "India", "that Birnbaum had resigned \"on her own terms and own volition.\"", "hundreds", "photos", "Arizona", "raping and killing a 14-year-old Iraqi girl.", "from Spain to the Caribbean", "`` E-mail surveillance ''", "Warren Hastings", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Bactrian Camels", "faggots", "green tint", "a creek", "2007", "the zona glomerulosa of the adrenal cortex in the adrenals gland", "travel to Dallas, Texas", "vice presidential running mate", "Animal Crackers", "the Ogaden"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5841637962731712}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.0909090909090909, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.7692307692307693, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.4, 0.0, 0.3333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3166", "mrqa_newsqa-validation-1188", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-2985", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-784", "mrqa_newsqa-validation-475", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-1488", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-397", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-7781"], "SR": 0.453125, "CSR": 0.5174278846153846, "EFR": 1.0, "Overall": 0.6955949519230769}, {"timecode": 52, "before_eval_results": {"predictions": ["Pope", "mild to moderate depression", "Summer", "\"I reject this course because it sets goals that are beyond what we can achieve at a reasonable cost,", "VBS.TV", "the children of street cleaners and firefighters.", "Switzerland", "2009", "Tim Clark, Matt Kuchar and Bubba Watson", "Jaime Andrade", "400 farmers", "Friday,", "Lisa Brown", "President Robert Mugabe", "five Lebanese", "planned attacks", "\" Teen Patti\"", "Seasons of My Heart", "a man had been stoned to death by an angry mob.", "improve the environment by taking on greenhouse gas emissions.", "Jared Polis", "dancing against a stripper's pole", "bragging about his sex life on television", "\"Three Little Beers,\"", "12.3 million", "Union Station", "English", "Little Rock", "the estate of Titanic survivor Barbara Dainton-West,", "\"The New Promised Land: Silicon Valley.\"", "American Bill Haas", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Tuesday in Los Angeles.", "after giving birth to baby daughter Jada,", "co-writing credits", "speed sailing", "Jeffrey", "2.5 million", "wife of Gov. Mark Sanford,", "Somali", "NATO fighters", "U.S. Vice President Dick Cheney", "5:20 p.m.", "Former Beatles", "\"The Rosie Show,\"", "a remote highway", "helicopters and unmanned aerial vehicles", "a \"prostitute\"", "Hanin Zoabi", "two", "April 2010.", "1985", "1603", "`` Entropy ''", "\"rice paper\"", "PPTH", "because it's exploration of worker motivation, enabling better managerial practices and higher job satisfaction", "Brigadier General Raden Panji Nugroho Notosusanto", "24 December 1692", "Cushman", "a bobtail squid", "past", "the Whitewater Development Corporation", "the New York frontier"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6410596804511277}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 0.0, 0.8421052631578948, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.8, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-1590", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-934", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-2536", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5395", "mrqa_searchqa-validation-14214", "mrqa_searchqa-validation-9278", "mrqa_searchqa-validation-4725", "mrqa_searchqa-validation-5273"], "SR": 0.515625, "CSR": 0.5173938679245282, "EFR": 1.0, "Overall": 0.6955881485849056}, {"timecode": 53, "before_eval_results": {"predictions": ["Anvil firing", "Abigail", "Encore Las Vegas", "David Jolly", "9 November 1955", "Japan", "Adrian Peter McLaren", "Anthony John Herrera", "2008\u201309", "Lev Ivanovich Yashin", "Lu\u00eds Carlos Almeida da Cunha,", "Mot\u00f6rhead", "Solace", "1943", "Interscope Records", "Wings of Desire", "May 27, 2016", "the National Football Conference (NFC)", "Bobby McGee", "Hudson Bay Mining and Smelting Company", "Bourbon County", "100 metres", "Acela Express", "5249", "Johnnie Ray", "Harry Booth", "John McClane", "James Packer", "black nationalism", "USC Marshall School of Business", "Saint Motel", "\"You're Next\"", "its eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "Christianity", "Fife", "Peter 'Drago' Sell", "Marktown", "Hidden America with Jonah Ray", "Bangkok", "Gregg Popovich", "Roy Spencer", "Chicago", "constant support from propaganda campaigns.", "2012", "torpedoes", "Minneapolis in Hennepin County in the Twin Cities metropolitan area", "1501", "the Man Booker Prize", "Manchester United", "Nic Cester", "Kim Carnes", "a convergent plate boundary", "the 2013 -- 14 television season", "1947, 1956, 1975, 2015 and 2017", "Prime Minister of the United Kingdom", "Asgard", "Manchester", "Sri Lanka's", "Felipe Massa.", "Security officer Stephen Johns reportedly opened the door", "Admiral Hyman Rickover", "snowy Evening", "Vestal Virgins", "Aaron Lewis"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6489556633059923}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.6666666666666666, 0.0, 1.0, 0.18181818181818182, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5, 1.0, 0.7368421052631579, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-5855", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-4945", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-5865", "mrqa_triviaqa-validation-4355", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-2941", "mrqa_searchqa-validation-127", "mrqa_searchqa-validation-7762"], "SR": 0.5625, "CSR": 0.5182291666666667, "EFR": 0.9642857142857143, "Overall": 0.6886123511904761}, {"timecode": 54, "before_eval_results": {"predictions": ["Betty Crocker", "Frank Sinatra", "khaki", "Frank Sinatra", "Antarctica", "robota", "Easter Island", "the troposphere", "Huntsville, AL", "Khrushchev", "Venus", "the pupil", "Jordan", "German", "toga", "George Washington", "Canadian River", "Hairspray", "The Prince and the Pauper", "Bangkok", "nitrogen and carbon dioxide", "Ponzi", "Jay Martin", "Islamic Republic", "Imagine", "the Spanish Republic", "Three Coins in the Fountain", "George Wallace", "tuna", "Heaven", "the Byzantine Empire", "Fall Out Boy", "freezing", "silver", "a cone shaped bill", "William Jennings Bryan", "The Blues Brothers", "Ford Madox Ford", "Voltigeur", "Copenhagen", "Blossom", "double bass", "Flight of the Bumblebee", "D", "Panama Canal", "Dan Marino", "Crustaceans", "Hestia", "Band of Brothers", "Russia", "self-evident", "Profit maximization happens when marginal cost is equal to marginal revenue", "Havana Harbor", "2017 - 12 - 10", "Thai", "Heather Stanning and Helen Glover", "redheaded", "April 1, 1949", "Lantern Waste", "Glam metal", "July", "Abhisit Vejjajiva", "prisoners at the South Dakota State Penitentiary", "1989"], "metric_results": {"EM": 0.671875, "QA-F1": 0.730849358974359}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6583", "mrqa_searchqa-validation-4968", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-7142", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-15081", "mrqa_searchqa-validation-16507", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-4922", "mrqa_searchqa-validation-8925", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-12780", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-10730", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-7321"], "SR": 0.671875, "CSR": 0.5210227272727272, "EFR": 1.0, "Overall": 0.6963139204545454}, {"timecode": 55, "before_eval_results": {"predictions": ["Transportation Security Administration", "an open window", "helping to plan the September 11, 2001, terror attacks,", "Iran", "1960s", "Janet Napolitano", "legislation Wednesday requiring federal oil industry regulators to wait at least two years after leaving government service before going to work for companies they helped regulate.", "Stanford University,", "How I Met Your Mother,", "Opry Mills,", "three", "February 5,", "Arabic, French and English,", "Fernando Gonzalez", "The minister later apologized,", "10 below", "KBR", "L'Aquila,", "crocodile eggs", "200", "Sam Raimi,", "The Palm,", "82", "1975", "Leo Frank,", "50", "the inspector-general of the House,\"", "Friday,", "two", "France's famous Louvre museum", "Ashley \"A.J.\" Jewell,", "be silent.", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "two weeks after Black History Month", "two people", "one of Africa's most stable nations.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "federal officers' bodies", "hundreds of contraband cell phones", "CEO of an engineering and construction company", "$50,000", "a national telephone survey", "These planning processes are urgently needed", "Michelle Rounds", "DBG, in Nairobi, Kenya,", "a strict interpretation of the law,", "Monday and Tuesday", "\"It's really nice to have people who eat anything really appreciate the vegan treats,\"", "speed sailing", "246", "Patrick McGoohan,", "provides the public with financial information about a nonprofit organization", "President Gerald Ford", "thicker consistency and a deeper flavour than sauce", "windmills", "France", "Mt Kenya", "R&B", "Donald Duck", "English", "the Edo era", "the Grand Canal", "ten-dollar", "bachata music"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7473278560710626}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4444444444444445, 0.9600000000000001, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.8, 1.0, 1.0, 0.9090909090909091, 1.0, 0.4, 0.29629629629629634, 1.0, 0.17391304347826086, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.25, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-2666", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-1446", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-2943", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-2866", "mrqa_searchqa-validation-2063", "mrqa_naturalquestions-validation-4925"], "SR": 0.578125, "CSR": 0.5220424107142857, "EFR": 0.9629629629629629, "Overall": 0.6891104497354497}, {"timecode": 56, "before_eval_results": {"predictions": ["and", "$150 billion over 10 years in clean energy.", "Kellogg Brown", "Sunday", "\"Empire of the Sun,\"", "Buenos Aires.", "a minor league baseball team in that stadium.", "Sonia Sotomayor", "Sharon Bialek", "There's no chance", "15-year-old", "137", "22-year-old", "\"bystander effect\":", "Bryant Purvis", "570 billion pesos ($42 billion)", "Ralph Lauren", "will not support the Stop Online Piracy Act,", "Golden Globe Awards", "Krishna Rajaram,", "helping to plan the September 11, 2001,", "The pilot, whose name has not yet been released,", "$10 billion", "viruses in question are sexually transmitted.", "plutonium production.", "\"It hurts my heart to see him in pain,", "Pakistan's", "in a canyon in the path of the blaze Thursday.", "left the medical engineering company where she worked.", "an antihistamine and an epinephrine auto-injector for emergencies,", "U.S. Holocaust Memorial Museum,", "racial intolerance.", "Jobs", "Al-Shabaab,", "in an appearance last week in Broward County Circuit Court.", "3-0", "Tuesday", "Hamas ministry spokesman Taher Nunu", "off the coast of Dubai", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Dr. Jennifer Arnold and husband Bill Klein,", "80 percent of a woman's face", "five", "British Prime Minister Gordon Brown's", "Pacific Ocean territory of Guam", "Angola,", "the same drama that pulls in the crowds", "May 4", "a \"happy ending\" to the case.", "Harry Nicolaides,", "ALS6,", "dispense summary justice or merely deal with local administrative applications in common law jurisdictions", "autompne ( automne in modern French ) or autumpne in Middle English", "a cascade of events through phosphorylation of intracellular proteins that ultimately transmit ( `` transduce ''", "hound", "fish", "thailand", "Cold Spring", "February 13, 1946", "Arrowhead Stadium", "Thomas Jefferson", "refrigerator", "X-Files", "1961"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6565047500967123}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true], "QA-F1": [0.18181818181818182, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.16, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.4736842105263158, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.782608695652174, 0.0, 0.07692307692307691, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-933", "mrqa_newsqa-validation-675", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-4113", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-9271", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-797", "mrqa_hotpotqa-validation-298"], "SR": 0.5625, "CSR": 0.5227521929824561, "EFR": 0.9642857142857143, "Overall": 0.6895169564536341}, {"timecode": 57, "before_eval_results": {"predictions": ["$55.7 million", "led the weekend box office, grossing $55.7 million during its first weekend.", "Kurdish militant group in Turkey", "Dr. Maria Siemionow,", "Alwin Landry's supply vessel Damon Bankston", "a city of romance, of incredible architecture and history.", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "free laundry service.", "the number of new cases is falling.", "Amitabh Bachchan", "\"Rent,\" \"Cabaret\" and \" Proof,\"", "Dancy-Power Automotive Group showroom", "closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "anyone wanting to harm them,", "Screen Actors Guild", "11 healthy eggs", "Atlanta's Hartsfield-Jackson International Airport", "Guinea, Myanmar, Sudan and Venezuela.", "four", "federal chamber of deputies,", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus -- and went on a 100-day killing rampage.", "$60 billion on America's infrastructure.", "bullet wounds in the back of a friend's head, seeing friends grabbing their arms, and blood just everywhere.", "Tim Clark, Matt Kuchar and Bubba Watson", "Mark Fields, executive vice president of Ford,", "three", "those Taliban who abandon violence and respect the human rights of their fellow citizens,\"", "Isabella", "A Colorado prosecutor Friday asked a judge to dismiss the first-degree murder charge against Tim Masters,", "near his home in Peshawar", "Dogpatch Labs", "Lavau's son, Sean, found his father after hearing \"faint yelling for help on the roadway from the canyon below,\"", "cross-country skiers who used the football matches in knee-deep mud to strengthen their leg muscles.", "no one is sure", "\"The Sopranos,\"", "a one-shot victory in the Bob Hope Classic", "Sadat's bullet-riddled body was rushed to the Maadi Military Hospital and the president was proclaimed dead at 2.40 p.m. due to \"intense nervous shock and internal bleeding in the chest cavity,\"", "Stratfor,", "Adam Yahiye Gadahn,", "an antihistamine and an epinephrine auto-injector", "EU naval force", "workers walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Deputy Treasury Secretary", "heavy turbulence", "success as a recording artist", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "two Emmys", "Henrik Stenson", "Three", "Dore Gold, former Israeli ambassador to the United Nations said, \"The IAEA has inspected the known nuclear sites of Iran. But it's believed they still have other clandestine nuclear sites where they may be enriching uranium", "for using recreational drugs", "Roger Federer", "Iran", "December 2, 2013", "Hayes", "overprotective clownfish", "Morgan Spurlock,", "football", "War & Peace", "Eugene Levy", "Floyd Patterson", "Jan & Dean", "Sweeney Todd: The Demon Barber of Fleet Street", "classical male singing voice"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5625987950896039}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.5, 0.5555555555555556, 0.0, 1.0, 0.0, 0.9411764705882353, 0.8918918918918919, 0.4, 0.0, 1.0, 0.0, 0.4, 0.24242424242424243, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.4444444444444445, 1.0, 0.14285714285714288, 1.0, 0.2666666666666667, 1.0, 1.0, 0.1111111111111111, 0.25, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.15384615384615383, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4444444444444445, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-1509", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2479", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-1141", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1121", "mrqa_naturalquestions-validation-2170", "mrqa_triviaqa-validation-6757", "mrqa_hotpotqa-validation-3321", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-10144"], "SR": 0.40625, "CSR": 0.5207435344827587, "EFR": 0.9736842105263158, "Overall": 0.6909949240018148}, {"timecode": 58, "before_eval_results": {"predictions": ["at a depth of about 1,300 meters in the Mediterranean Sea.", "machine guns and two silencers", "the 3rd District of Utah.", "Kenneth Cole", "\"If Russian long-range bombers should need to land in Venezuela, we would not object to that either.", "President Bush", "Stephen Tyrone Johns", "Transport Workers Union", "Two soldiers, a policeman and four militants", "23-year-old", "prisoners at the South Dakota State Penitentiary", "responsibility for the abductions", "food, music, culture and language of Latin America", "Workers'", "is fighting an unjust war for an America that went too far when it invaded Iraq", "Pope", "boyhood experience in a World War II internment camp", "Secretary of State", "Mother", "San Diego,", "opium poppies", "1979", "Alexandre Caizergues,", "sportswear,", "military trials for some Guantanamo Bay detainees.", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "iPhone 4S", "iTunes,", "Indonesian", "eight-week", "order", "Natalie Cole", "Somali-based", "folding table", "the Obama administration", "Ventures", "British", "Long Island", "former Procol Harum bandmate Gary Brooker", "Former Mobile County Circuit Judge Herman Thomas", "Sen. Barack Obama", "July 23.", "suicides", "the lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "Sunday", "motion for a preliminary injunction against a Mississippi school district and high school", "105-year", "helping consumers move beyond these hard times", "lifeless, naked body", "iTunes,", "eight", "the summer of 1990", "`` Cheitharol Kummaba ''", "Ricky Nelson", "Blind Faith", "Portugal,", "Sphinx", "John Wilkes Booth", "Charles Nungesser", "United States of America (USA),", "cherry almond", "tides", "bananas", "Leonard Bernstein"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6921121974430798}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.8571428571428571, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7000000000000001, 0.8571428571428571, 0.5, 1.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.888888888888889, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2135", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3952", "mrqa_newsqa-validation-2617", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4798", "mrqa_triviaqa-validation-3070", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-722", "mrqa_searchqa-validation-14467", "mrqa_searchqa-validation-7269"], "SR": 0.578125, "CSR": 0.5217161016949152, "EFR": 0.9629629629629629, "Overall": 0.6890451879315755}, {"timecode": 59, "before_eval_results": {"predictions": ["Supplemental oxygen", "John Smith", "Bill Irwin", "in Brahmagupta's Brahmasputha Siddhanta ( 7th century )", "March 6, 2018", "multiple", "Egypt", "Alan Tudyk", "the coast of Guant\u00e1namo Bay in Cuba", "the 1840s", "the 1940s", "1972", "helps scientists better understand the spread of pollution around the globe", "electors", "The resulting molecule, now mature insulin, is stored as a hexamer in secretory vesicles", "Holy See", "Carpenter", "Kirstjen Nielsen", "$2.18 billion", "the 9th century", "a bowl", "February 7, 2018", "White Sox", "The Star Spangled Banner", "a global superpower", "Tara / Ghost of Christmas Past", "Stephen Foster", "1975", "every year from 6 -- 14 July", "majority of members present at that time", "off the northeast coast of Australia", "`` Blood is the New Black ''", "on the continent of Antarctica", "the courts", "the primal rib", "in the pancreas", "Steve Valentine", "political ideology", "Anakin", "Lesley Gore", "Tom Hanks", "the Khoisan language of the \u01c0Xam people", "peninsular", "Terry Kath", "16 best - selling religious novels", "`` skin - changer ''", "Johnny Cash", "pre-Columbian times", "human induced greenhouse warming", "The Romantics", "Gorakhpur Junction", "Maarten Tromp", "sternum", "European horseracing", "Tainy Sledstviya", "travel diary", "\"Nebo Zovyot\"", "Caster Semenya", "Berga an der Elster", "North Korean Foreign Ministry spokesman described U.S. Vice President Dick Cheney as a \"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "Nellie Bly", "Sgt. Forrest Irvin", "godliness", "sour"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6124999999999999}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.7692307692307692, 1.0, 0.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-9881", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8227", "mrqa_triviaqa-validation-945", "mrqa_hotpotqa-validation-3429", "mrqa_newsqa-validation-2405", "mrqa_searchqa-validation-7286", "mrqa_searchqa-validation-8408"], "SR": 0.5625, "CSR": 0.5223958333333334, "EFR": 0.9642857142857143, "Overall": 0.6894456845238095}, {"timecode": 60, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2302", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-2850", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-3485", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-870", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3062", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-9281", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-9778", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1590", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1678", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1708", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3867", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10752", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12763", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14969", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16642", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2791", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3295", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4219", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4922", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7317", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-9176", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1540", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2523", "mrqa_squad-validation-270", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4546", "mrqa_squad-validation-4750", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-6029", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6630", "mrqa_squad-validation-7023", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7531", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-8066", "mrqa_squad-validation-8209", "mrqa_squad-validation-8386", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8931", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9868", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4371", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7683"], "OKR": 0.7734375, "KG": 0.46171875, "before_eval_results": {"predictions": ["nuclear", "Sax Rohmer", "Jackie Chiles,", "the Dominican Republic", "Crystal Gayle", "carry On Cleo", "St. Augustine", "a healthy naturally occurring rock, crystal or sea salt", "DIJON,", "Wisconsin", "plum", "having an affair with their maid", "Wonga", "Secretary of State William H. Seward", "\u201ccryogenics\u201d", "The World is Not Enough", "Barnaby Rudge", "James Chadwick", "Gary and Andrew", "visible path of a meteoroid", "Lesley Lawson", "California", "cesium", "Picasso", "\"Caffeinated coffee provides a natural increase in blood GCSF levels,\"", "trumpet", "King George I", "sugar", "black cat", "earache", "90%", "muezzin", "the moon", "watts", "Alan Ladd", "black bean", "Virginia", "Charles Darwin", "PJ Harvey", "Jim Jones", "the grueling decathlon,", "Runcorn", "\"the Gentile Times,\"", "Amnesty International", "Maxwell", "Ann Darrow", "Arthur Miller", "Carousel", "Paradise Lost", "potassium ion,", "Paris", "headdresses", "Janie Crawford", "Julia Roberts", "their unusual behavior, such as the number of men killed and the manner of the attacks.", "The Seduction of Hillary Rodham", "Apsley George Benet Cherry-Garrard", "\"A Bug's Life,\" \"Monster's Inc.\"", "86", "allegations that a dorm parent mistreated students at the school.", "daiquiri", "apples", "Morocco", "the nerves and ganglia outside the brain and spinal cord"], "metric_results": {"EM": 0.625, "QA-F1": 0.6704427083333333}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.375, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-6812", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3854", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-6285", "mrqa_hotpotqa-validation-3308", "mrqa_newsqa-validation-2618"], "SR": 0.625, "CSR": 0.524077868852459, "EFR": 1.0, "Overall": 0.6959874487704918}, {"timecode": 61, "before_eval_results": {"predictions": ["a paragraph about the king and crown prince", "Mugabe and Tsvangirai", "\"Security officials made some readjustments to inauguration security as a precaution and did not change the threat level,", "Samuel Herr,", "checkposts and military camps", "Cannes", "customers are lining up for vitamin injections that promise", "Caster Semenya", "Andrew Morris,", "Alberto Espinoza Barron,", "In-Saharan Africa", "41,280", "Casey Anthony", "18", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "Phoenix, Arizona,", "\"The Adventures of Superman\"", "\"Wicked.\"", "between 1917 and 1924", "Mississippi", "$3 billion,", "Steve Williams", "Oprah Winfrey's school in South Africa", "Dr. Jennifer Arnold and husband Bill Klein,", "\"Up,\"", "\"The Real Housewives of Atlanta\"", "action films", "Wigan Athletic", "40-year-old", "1,500 Marines", "Current TV", "civilians,", "Michelle Obama", "Adriano", "returning combat veterans", "hanmanska Garden.", "\"We essentially closed the wheelhouse doors.", "cambodia", "staff sergeant in the U.S. Air Force,", "nine", "Jason Voorhees", "george Washington", "chairman of the House Budget Committee", "al-Shabaab", "bremen", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "the RheinEnergieStadion.", "\"Holocaust shame,\"", "rare thriller writer", "December Monday", "the Louvre.", "Prem Lata Agarwal", "Ms. Stout", "Terry Kath", "geocentric", "William Shakespeare", "Sarkozy", "Roger", "Thor", "Piedmont", "george III", "Taj Mahal", "colon", "Carol Worthington"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5894570707070708}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.30303030303030304, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-626", "mrqa_newsqa-validation-1929", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-3139", "mrqa_newsqa-validation-1704", "mrqa_newsqa-validation-80", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-400", "mrqa_triviaqa-validation-4714", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-4692"], "SR": 0.546875, "CSR": 0.524445564516129, "EFR": 0.9655172413793104, "Overall": 0.689164436179088}, {"timecode": 62, "before_eval_results": {"predictions": ["in the Angeles National Forest", "\"outlaws\"", "1960s song \"A Whiter Shade of Pale\"", "Ciudad Juarez,", "Kenneth Cole", "little blue booties.", "David McKenzie", "$8.8 million", "\"I'm just getting started.\"", "Saturday,", "are not susceptible to attack,", "14", "planning processes are urgently needed", "Mandi Hamlin", "Malcolm X", "financial gain,", "a face-to-face interview with the president", "Six", "Steve Jobs", "head injury.", "Daniel Radcliffe", "Tillakaratne Dilshan", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "a construction site in the heart of Los Angeles.", "Jaipur", "Al-Aqsa mosque", "Ameneh Bahrami", "criminals", "whether to close some entrances, bring in additional officers, and make security more visible,\"", "\"theoretically\"", "Transportation Security Administration", "E. coli", "Six members of Zoe's Ark", "The United States", "100 percent", "Brian Mabry", "150", "\"Watchmen\"", "Tibet's independence from China,", "\"She was focused so much on learning that she didn't notice,\"", "the shipping industry -- responsible for 5% of global greenhouse gas emissions,", "two", "responsibility for the abductions", "Brian David Mitchell,", "One of Osama bin Laden's sons", "as part of its 18-month journey around the world.", "Italian Serie A title", "an independent homeland since 1983.", "Tutsis", "are co-chairs of the Genocide Prevention Task Force.", "are considering tighter restrictions on propofol,", "William Shakespeare's As You Like It", "De Waynene Warren", "1954", "jackstones", "apple core", "The Merchant of Venice", "designated hitter", "\"The Leader In Me \u2014 How Schools and Parents Around the World Are Inspiring Greatness, One Child at a Time\"", "11 June 1959", "Plutarch", "Buddhism", "cab", "week 1:00 a.m."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6083452138139638}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.12121212121212123, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.4, 0.5, 0.0, 0.0, 0.7692307692307692, 0.4, 0.4, 0.0, 0.0, 1.0, 0.8, 0.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3617", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-6383", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-6532", "mrqa_hotpotqa-validation-2543", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-5563"], "SR": 0.515625, "CSR": 0.5243055555555556, "EFR": 0.967741935483871, "Overall": 0.6895813732078853}, {"timecode": 63, "before_eval_results": {"predictions": ["Jaime Andrade,", "\"It was never our intention to offend anyone,\"", "President Obama and Britain's Prince Charles", "the insurgency,", "Reporters Without Borders", "Genocide Prevention Task Force.", "British", "five female pastors", "Bob Bogle,", "India", "82", "in Fullerton, California,", "$249", "Zulfikar Ali Bhutto, former president and prime minister of Pakistan,", "two African-Americans in her senior class", "Jonathan Breeze, the CEO of Jet Republic,", "Best Picture winner \"Slumdog Millionaire\" (No. 4)", "President Obama and Britain's Prince Charles", "public opinion in Turkey.", "high-ranking drug cartel member Arnoldo Rueda Medina.", "left his indelible fingerprints on the entertainment industry.", "murder in the beating death of a company boss who fired them.", "she struggled more than she showed during Tuesday night's short program that captivated the skating world.", "he was mad at the U.S. military because of what they had done to Muslims in the past,\"", "sRI International,", "Derek Mears", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Heshmatollah Attarzadeh", "\"Zed,\"", "because the Indians were gathering information about the rebels to give to the Colombian military.", "Carrousel du Louvre,", "Mubarak,", "Princess Diana,", "revelry", "Human Rights Watch", "45 minutes,", "on Thursday.", "1959,", "London's", "\"illegitimate.\"", "the BBC's central London offices", "Russia", "industrialized nations", "Zuma", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "to sniff out cell phones.", "shock, quickly followed by speculation about what was going to happen next,\"", "U.S. Food and Drug Administration", "$1.5 million.", "former Pakistani Prime Minister Nawaz Sharif", "raping and killing a 14-year-old Iraqi girl.", "in 1837", "raconteur ( Austin Winkler ) and his former lover ( Emmanuelle Chriqui )", "the following year", "Reel Life:", "bluebells", "FIFA fever", "General Allenby", "turns out to be a terrible date", "The Treaty of Gandamak", "England", "James Lipton", "beak", "Candide"], "metric_results": {"EM": 0.5, "QA-F1": 0.6186152487836307}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.26666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.4615384615384615, 0.2857142857142857, 0.0, 0.8333333333333333, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.12903225806451615, 0.19047619047619047, 0.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5581395348837209, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.3636363636363636, 0.0, 0.0, 1.0, 0.5, 0.25, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2894", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-889", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3401", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-1098", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-3482", "mrqa_naturalquestions-validation-1912", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-3824", "mrqa_hotpotqa-validation-5720", "mrqa_hotpotqa-validation-4086", "mrqa_searchqa-validation-1971"], "SR": 0.5, "CSR": 0.52392578125, "EFR": 1.0, "Overall": 0.6959570312500001}, {"timecode": 64, "before_eval_results": {"predictions": ["Wolfgang Amadeus Mozart", "Denmark\u2013Norway, Brandenburg and Sweden", "John Schlesinger", "business magnate", "Phineas and Ferb", "Two Is Better Than One", "Karl-Anthony Towns", "Omega SA", "9 November 1967", "the designated hitter rule", "Jay Park", "Wayne County, Michigan", "Japan", "Cleopatra VII Philopator", "8,211", "Allies of World War I,", "August Heckscher", "Orange County", "Gareth Barry", "death", "Ned Flanders", "Westley Sissel Unseld", "Ken Howard", "Fat Albert", "Thomas Joseph \"T. J. Lavin", "15", "Germany", "I-League club Salgaocar", "\"Fudge\"", "1887", "Tuesday", "January 2001", "Sony Studio Liverpool", "Britain", "Ry\u016bkyuan sailors", "Major Charles White Whittlesey", "fennec", "from 1993 to 1996", "Spring city", "Port Moresby, Papua New Guinea", "Macau Peninsula, Macau", "1993", "The Bangor Daily News is an American newspaper covering a large portion of rural Maine, published six days per week in Bangor, Maine.", "The Land of Enchantment", "Hawaii", "William Finn", "My Backyard", "homosexuality, gay sex, and the gay bear subculture", "The Tales of Hoffmann", "reputation", "October 3, 2017", "1984 Summer Olympics in Los Angeles", "Afghanistan", "Michael Crawford", "parallelogram", "Operation Frequent Wind", "Matthew 2:11", "toxic smoke from burn pits", "15,000", "breast cancer.", "yard", "Casablanca", "Vilna", "Asaph Hall"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6708066239316239}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 0.08333333333333334, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-4702", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2383", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-306", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-1394", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-5655", "mrqa_naturalquestions-validation-75", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-305", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-4147", "mrqa_searchqa-validation-7441", "mrqa_searchqa-validation-4036"], "SR": 0.578125, "CSR": 0.5247596153846155, "EFR": 1.0, "Overall": 0.6961237980769232}, {"timecode": 65, "before_eval_results": {"predictions": ["10 October 2010", "Vice President of the United States", "Claudio Javier L\u00f3pez", "the Las Vegas Strip in Paradise, Nevada", "The Swiss federal popular initiative \"against mass immigration\"", "I Should Have Known Better", "John McClane", "Dan Brandon Bilzerian", "Philadelphia Naval Shipyard", "The Pentagon", "1958", "our greatest comedienne - Australia's Lucille Ball", "Canadian", "Kim Yeon-soo", "spot-fixing", "The Monster", "the Easter Rising of 1916", "Spain, Mexico and France", "Juilliard School", "Mark Brandon \"Chopper\" Read", "small forward", "California State University, Long Beach", "Floridians", "Erreway", "Eric Liddell", "Sam Kinison", "Yunnan-Fu", "Bob Hurley", "the longest-serving head of any private bank in the country", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "9,984", "Sunset Publishing Corporation", "the Wabanaki Confederacy", "1966", "Flamingo Las Vegas", "The Soloist", "The British East India Company", "Labour Party", "Walt Disney and Ub Iwerks", "1983", "stolperstein", "musical artist", "BMW X6", "Cleveland Cavaliers", "Larnelle Steward Harris", "May 5, 2015", "Bank of China Tower", "general secretary of the Norwegian Anthroposophical Society", "Barnoldswick", "New Orleans, Louisiana", "eight", "IB Career - related Program", "Mark Jackson", "1991", "The North Atlantic Treaty Organization", "trout", "Montpelier", "peanut and milk", "African National Congress Deputy President Kgalema Motlanthe,", "13.", "Portugal", "Splice", "law firm of Crane, Poole and Schmidt", "The FlareSide bed"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6082844065656566}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.5714285714285715, 1.0, 0.9090909090909091, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.15999999999999998, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4444444444444444, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-3891", "mrqa_hotpotqa-validation-1135", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-776", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3536", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-1123", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-1138", "mrqa_newsqa-validation-3733", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-16601", "mrqa_naturalquestions-validation-1856"], "SR": 0.484375, "CSR": 0.5241477272727273, "EFR": 1.0, "Overall": 0.6960014204545455}, {"timecode": 66, "before_eval_results": {"predictions": ["October 25, 1881", "Harold Holt", "August 23, 1970 \u2013 October 31, 1993", "Despicable Me 3", "Hillary Rodham Clinton", "More than 1,800", "king John of Denmark", "Bring Me Sunshine (1994) was originally a three-part retrospective in tribute to Eric Morecambe", "Juve", "November 6, 2018", "four sections", "the fifth level", "five", "Macau, China", "American black bear", "1345 to 1377", "sandstone", "May 4, 2004", "VIMN Russia", "rickyard", "Harlem neighborhood", "season one episode \"Boating School\"", "Parapsychologist", "2014 New Year Honours", "Greg Gorman and Helmut Newton", "Prince Ioann Konstantinovich", "2015", "Winecoff Hotel fire", "B-17 Flying Fortress bomber", "Bit Instant", "Hawaii, United States", "Lt. Col. Masahiko Takehita", "the Ruul", "Dana Fox", "remixes", "Issaquah", "Ghanaian national team", "Scott Paul Carson", "John Snow", "Manchester, England", "\"The Tonight Show\"", "Tamaulipas", "boundary river", "\"Naked\"", "technical director", "Victoria, Duchess of Kent", "Las Vegas", "Washington, D.C.", "Brenton Thwaites", "AVN Adult Entertainment Expo", "Boyd Gaming", "Hirschman", "The Romantics", "the brain, muscles, and liver", "Chamberlain", "the French Open", "Austria", "at airports", "dual nationality", "Cpl. Richard Findley,", "rain", "38,310 tons", "neon", "customers are lining up for vitamin injections that promise"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6181671626984127}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.8, 0.25, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 0.2222222222222222, 1.0, 0.3333333333333333, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-592", "mrqa_hotpotqa-validation-2541", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-1676", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-5544", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-3126", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-843", "mrqa_hotpotqa-validation-4117", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-3419", "mrqa_hotpotqa-validation-1101", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-6684", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-2698"], "SR": 0.515625, "CSR": 0.5240205223880596, "EFR": 1.0, "Overall": 0.6959759794776119}, {"timecode": 67, "before_eval_results": {"predictions": ["Brian", "1956", "Portugal", "North Atlantic Ocean", "eleven", "Anthony Hopkins", "Andrew Gold", "$19.8 trillion", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "January 15, 2007", "1982", "Super Bowl XXXIX", "Brittany Paige Bouck", "31", "Louis XV's", "White Sox", "won", "John 6 : 67 -- 71", "Executive chef Danny Veltri", "Narendra Modi", "Ossie Schectman", "Britney Spears", "Marty Robbins", "Lightning thief", "state legislators of Assam", "using a baby as bait", "Medal of Honor recipient Jared", "Guy Berryman", "`` The person who has existence in two parallel worlds", "Spanish / Basque", "Johannes Gutenberg", "Jason Momoa", "Angela Bassett", "1868 war veterans", "Elena Anaya", "`` You are the body of Christ and individually members of it ''", "2014", "southern hemisphere", "from Fort Kent, Maine", "Robin", "between the two cherubim", "Vincent Price", "Nepal", "actions taken by employers or unions that violate the National Labor Relations Act of 1935", "December 12, 2017", "March 31 to April 8, 2018", "Manhattan", "1969", "location", "Neil Young", "provinces along the Yangtze River and in provinces in the south", "Thomas Jefferson", "Land of the Rising Sun", "clyneux-Jackson", "Adam Dawes", "Robbie Gould", "Musicology", "two years,", "Lashkar-e-Tayyiba", "HPV (human papillomavirus)", "dancer", "necropolis", "Spider-Man", "Fidel Castro"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6888192873303167}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16, 1.0, 0.0, 0.47058823529411764, 1.0, 0.0, 1.0, 1.0, 0.5384615384615384, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6153846153846153, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-5775", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-9107", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-1629", "mrqa_newsqa-validation-1170", "mrqa_searchqa-validation-13420"], "SR": 0.5625, "CSR": 0.5245863970588236, "EFR": 0.9285714285714286, "Overall": 0.6818034401260504}, {"timecode": 68, "before_eval_results": {"predictions": ["Jeff Barry", "Golde", "2018", "James Madison", "one", "September 29, 2017", "c. 1000 AD", "Samantha Jo `` Mandy '' Moore", "March 1995", "1957", "Alex Dice Clay", "an investor couple", "Erica Rivera", "the dermis", "throughout the body", "the Germanic elements `` hrod '' meaning renown and `` beraht '' meaning bright", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "11 : 15 p.m.", "Sylvester Stallone", "Frankel, who had worked with Patricia Field on his feature - film debut Miami Rhapsody as well as Sex and the City, knew that what the cast wore would be of utmost importance in a movie set in the fashion industry", "the President of the United States", "November 25, 2002", "S - shaped", "Marley & Me", "State Bar of Arizona", "19th - century", "summer months", "The Romantics", "W. Edwards Deming", "Jodie Sweetin", "the Comancheria, consisted of present - day eastern New Mexico, southeastern Colorado, southwestern Kansas, western Oklahoma, and most of northwest Texas and northern Chihuahua", "1992", "presidential representative democratic republic", "Eddie Murphy", "Vincent Price", "Andy Serkis", "Emperor d'Abo", "O'Meara", "Kimberlin Brown", "petition for a writ of certiorari", "international aid", "Tom Waits, The Neville Brothers, DoMaJe, and Steve Earle", "`` Reveille ''", "Part XI", "StubHub Center", "Johannes Gutenberg", "plate tectonics", "Himadri Station", "Rajendra Prasad", "1997", "March 11, 2016", "psychologist", "Irish Red Setter", "Hattie McDaniel", "Tian Tan Buddha", "Bonkyll Castle", "Danny Elfman", "the area was sealed off, so they did not know casualty figures.", "Australian Environment Minister Peter Garrett", "Turkey from inside northern Iraq.", "piracy", "lifejackets", "the server foot fault", "Josh"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6367400828338328}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.33333333333333337, 0.0, 0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.7499999999999999, 0.0, 0.4, 1.0, 0.5714285714285715, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-8950", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-1340", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-516", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1504", "mrqa_searchqa-validation-1397"], "SR": 0.53125, "CSR": 0.5246829710144927, "EFR": 0.9666666666666667, "Overall": 0.6894418025362319}, {"timecode": 69, "before_eval_results": {"predictions": ["Syracuse University", "The Fault in Our Stars", "The 2008\u201309 UEFA Champions League", "traditional music", "water", "May 4, 2004", "Northumbrian", "2001", "American Horror Story", "Salisbury", "University of Nevada, Las Vegas (UNLV)", "\"Scars to Your Beautiful\",", "November of that year", "Morse Field at Harold Alfond Sports Stadium", "compact car", "Jos\u00e9 Bispo Clementino dos Santos", "Laurie Metcalf", "the Austrian Empire", "Russell T Davies", "George Balanchine", "Oliver Parker", "he turned 51, he died of cancer", "Snowball II", "Oakland", "FCI Danbury", "Division of Barton", "Lucy Gichuhi", "The Blue Ridge Parkway", "1875", "London", "Shut Up", "James David Lofton", "Big Bad Wolf", "Dennis Hull,", "books, films and other media", "Iranian-American", "You're Next", "the Bay of Fundy", "2009", "Umberto II of Italy", "2016 United States elections", "War Is the Answer", "1891", "Supergirl", "DI Humphrey Goodman", "Cinderella", "1,382", "left-arm", "Syracuse", "Buckingham Palace", "Rhode Island", "Anglican", "the United States", "1972", "beard", "Rodgers and Hammerstein", "Ramadan", "Tibet's", "70,000", "1994", "Lord Peter Wimsey", "sulfuric acid", "neon", "Sean Maddox"], "metric_results": {"EM": 0.625, "QA-F1": 0.6813920454545455}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.0, 0.4, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5619", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-4315", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3703", "mrqa_hotpotqa-validation-4352", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-278", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-5942", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-4425", "mrqa_triviaqa-validation-7105"], "SR": 0.625, "CSR": 0.5261160714285714, "EFR": 1.0, "Overall": 0.6963950892857144}, {"timecode": 70, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-139", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5611", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-875", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11005", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13441", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16534", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7326", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8807", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-10284", "mrqa_squad-validation-10352", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1498", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2123", "mrqa_squad-validation-215", "mrqa_squad-validation-2197", "mrqa_squad-validation-2248", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3464", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3750", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-3904", "mrqa_squad-validation-4096", "mrqa_squad-validation-4469", "mrqa_squad-validation-457", "mrqa_squad-validation-4669", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-5869", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-6636", "mrqa_squad-validation-682", "mrqa_squad-validation-6838", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8028", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8500", "mrqa_squad-validation-8529", "mrqa_squad-validation-8719", "mrqa_squad-validation-8754", "mrqa_squad-validation-8966", "mrqa_squad-validation-9165", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4953", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6879", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-934"], "OKR": 0.814453125, "KG": 0.45859375, "before_eval_results": {"predictions": ["November 17, 2017", "Melissa Disney", "the Archies", "a writ of certiorari", "Edgar Lungu", "December 25", "Marcus Atilius Regulus", "Hot Wings", "You are a puzzle", "San Francisco", "a crust of mashed potato", "Gregor Mendel", "October 1, 2015", "New Zealand", "New Jersey", "`` drive - through '' or `` stop and go ''", "laurel wreath", "101.325 kPa", "1923", "Renhe Sports Management Ltd", "Michael Schumacher", "dispense summary justice", "John Quincy Adams", "Ray Charles", "Days of Our Lives", "2013", "the skin", "Upstate New York", "Henry Purcell", "5,534", "Lord Irwin", "three", "Roman Reigns", "Efren Manalang Reyes", "the second Persian invasion of Greece", "provide bridging funding for existing federal programs at current, reduced, or expanded levels", "2004", "Fix You", "Ed Roland", "Jos\u00e9 Mart\u00ed", "16 August 1975", "when the forward reaction proceeds at the same rate as the reverse reaction", "Sumitra", "The management team", "Ravi River", "Sir Rowland Hill", "Pangaea", "Sally Field", "a medium with a lower index of refraction, typically a cladding of a different glass, or plastic", "Dr. Sachchidananda Sinha", "Barbara Windsor", "limestone", "The Hague", "The World is Not Enough", "held as Chairperson of the Organisation of African Unity from 25 May 1963 to 17 July 1964", "Leona Louise Lewis", "Hong Kong Disneyland", "\"We want to reset our relationship and so we will do it together.'\"", "an average of 25 percent", "President Barack Obama,", "1st", "Galileo Galilei", "Aristophanes", "Amy Bishop Anderson,"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7056919642857143}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-8470", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-8326", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-1004", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2361", "mrqa_searchqa-validation-14422", "mrqa_searchqa-validation-10531", "mrqa_newsqa-validation-2288"], "SR": 0.640625, "CSR": 0.5277288732394366, "EFR": 0.9565217391304348, "Overall": 0.6975532474739743}, {"timecode": 71, "before_eval_results": {"predictions": ["Jackie", "Chester", "superhuman abilities", "76,416", "Rawhide", "Chris Weidman", "University of Missouri", "Orfeo ed Euridice", "Martin Scorcese", "Citgo", "Kagoshima Airport", "Southern Progress Corporation", "Laura Dern", "British Conservative Party", "Mickey's PhilharMagic", "45", "1241", "Hamburger Sport-Verein e.V.", "Kolkata", "Liga MX", "\"Queen City\"", "BC Dz\u016bkija", "Texas", "sixteen", "Rob Reiner", "The R-8 Human Rhythm Composer", "WAMC", "126,202", "Hong Kong Mak\u00e9l\u00e9l\u00e9", "polypeptide chain", "Sacramento Kings", "1945", "Fairfax", "Syracuse University", "Sir Thomas Daniel Courtenay", "Martin \"Marty\" McCann", "Brown Mountain Overlook", "Republic of Maldives", "Kegeyli rayon\u0131", "Ella", "1986", "quarterly", "William Bradley DuVall (born September 6, 1967)", "Coronation Street", "Duke", "Richard Masur", "anabolic steroids", "diving duck", "House of Hohenstaufen", "local South Australian and Australian produced content", "EBSCO", "Karen Taylor", "as a pH indicator, a color marker, and a dye", "Thomas Edison", "The Town Council", "Runic", "cycling", "Brett", "58 people", "Security officer Stephen Johns", "osprey", "Fyodor Dostoevsky", "Turtle Wax", "Dangjin"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6361483134920635}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.25, 0.5, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1395", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-1220", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2938", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-5277", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-82", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5763", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-4244", "mrqa_newsqa-validation-3956", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2941", "mrqa_searchqa-validation-14348"], "SR": 0.515625, "CSR": 0.5275607638888888, "EFR": 1.0, "Overall": 0.7062152777777777}, {"timecode": 72, "before_eval_results": {"predictions": ["inverse scattering", "International Society for the Study of the Origin of Life", "Australian Supercars Championship", "The Jacksonville Jaguars", "\"Crossed: Family Values\"", "Philadelphia, Pennsylvania", "\"gemeinn\u00fctzige\"", "926 East", "Laurie Metcalf", "Nine-card Brag", "\"God Spell\" (1973)", "first freshman", "Free Range Films", "Mondays", "\"Beauty and the Beast\"", "Jehovah", "fourth President of Pakistan", "Tamara Ecclestone Rutland", "Newport", "Tomorrowland", "1985", "David Michael Bautista Jr.", "1002", "1", "Norman Macdonnell", "late 19th and early 20th centuries", "Salim Stoudamire", "1967", "Casey Bond", "actress", "Christian Kern", "Naomi Elaine Campbell", "The dyers of Lincoln", "The 2013\u201314 Premier League", "Ghostbusters Spooktacular", "an American financier", "Maldives", "Them", "its air-cushioned sole", "4,972", "Hailee Steinfeld", "Perth, Western Australia", "Harry F. Sinclair", "Interstate 22", "William McKinley", "Denmark", "Tunisian", "lieutenant general", "Indian state of Gujarat", "Nicole Kidman", "Eric Bana", "In 2015, 13.5 %", "1980", "Isaiah Amir Mustafa", "Michigan", "David Letterman", "Australia", "seven", "Congress", "July", "Michelangelo", "airplanes", "Stone Mountain", "buying"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6050792523448774}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false], "QA-F1": [0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.1818181818181818, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.375, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-294", "mrqa_hotpotqa-validation-1612", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-5582", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-1007", "mrqa_hotpotqa-validation-5857", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-982", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-270", "mrqa_searchqa-validation-5340"], "SR": 0.53125, "CSR": 0.527611301369863, "EFR": 0.9666666666666667, "Overall": 0.6995587186073059}, {"timecode": 73, "before_eval_results": {"predictions": ["SAS Technical Services", "1972", "Indianapolis, Indiana", "a controversial public figure", "1970", "the 1745 rebellion of Charles Edward Stuart", "96", "a Christian evangelist", "West African descendants", "Rigoletto", "James Stenbeck", "December 19, 1998", "Philip Quast", "November 20, 1942", "Argentine cuisine", "22 September 2015", "David Edward Williams, OBE (born 20 August 1971), known professionally as David Walliams", "Waylon Smithers", "2006", "Scotland", "Lakshmibai", "Umberto II", "England", "beer and soft drinks", "five", "C. H. Greenblatt", "mermaid", "1535", "In 1969, Hillary Rodham wrote a 92-page senior thesis for Wellesley College about community organizer Saul Alinsky entitled \"There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "England", "Field Marshal Lord Gort", "Laurel, Mississippi", "The pronghorn", "Kel Mitchell", "coffee cake", "15", "Key West", "Pablo Escobar", "Dave Bautista", "Iftikhar Ali Khan", "October 25, 1881", "Bob Dylan", "\"The New York Times\" magazine", "Harrison Ford", "In October 2004, the Dover Area School District of York County, Pennsylvania, changed its biology teaching curriculum to require that intelligent design be presented as an alternative to evolution theory", "Law Adam", "1943", "wine", "Animorphs", "Jeff Meldrum", "quantum mechanics", "early - to - mid fourth century", "the gated community of Pebble Beach", "$1 billion worldwide", "Mull", "Jeffrey Archer", "Oliver Harmon Jones", "in Galveston, Texas,", "celebrities", "are concerned that the legislation will foster racial profiling, arguing that most police officers don't have enough training to look past race while investigating a person's legal status.", "the Ohio", "beaver", "William Somerset Maugham", "bullfighting"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6385179924242425}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, true], "QA-F1": [0.4, 1.0, 1.0, 0.6, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.48484848484848486, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-1423", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-5747", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-4360", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-4547", "mrqa_triviaqa-validation-7264", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-3584", "mrqa_searchqa-validation-3745"], "SR": 0.53125, "CSR": 0.527660472972973, "EFR": 0.9666666666666667, "Overall": 0.6995685529279279}, {"timecode": 74, "before_eval_results": {"predictions": ["1947", "Bury Football Club", "Nye County", "1930s and 1940s", "two", "a governor", "40 Acres and a Mule Filmworks", "The Panther", "al-Qaeda", "China", "Tamil Nadu", "Skyscraper", "17 October 2006", "April 1, 1949", "Big Machine Records", "Brady John Haran", "Jesus", "Heywood \"Woody\" Allen (born Allan Stewart Konigsberg; December 1, 1935) is an American filmmaker, writer, actor, comedian, and musician", "moth", "Elliot Fletcher", "husband and wife Kansas Joe McCoy and Memphis Minnie", "Martha Wainwright", "Christopher McCulloch (also known as \"Jackson Publick\")", "Strange Interlude", "1989 until 1994", "Tallahassee City Commission", "the Royal Air Force", "Mika Pauli H\u00e4kkinen", "World Music Awards", "1998", "Peter 'Drago' Sell, (born August 5, 1982) is an American mixed martial artist specializing in Brazilian Jiu Jitsu", "\"Barney Miller\"", "\"Coyote Ugly\"", "\"Odorama\", whereby viewers could smell what they saw on screen through scratch and sniff cards", "Cersei Lannister", "Andy Roddick", "Bigfoot (also known as Sasquatch) is a cryptid which supposedly is a simian-like creature", "Kim Jong-hyun", "College Football Scoreboard", "A Boltzmann machine", "January 1930", "12", "\"Pour le M\u00e9rite\"", "Franklin, Indiana", "The virus is zoonotic,", "October 6, 1931", "New Boston Air Force Station", "Herb Brooks", "Elizabeth River", "Metrolink", "\"Complex\" magazine", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "Covington, Kentucky", "Elizabeth Dean Lail", "the women's doubles event", "Newcastle Falcons", "Rihanna", "Sarah", "order after demonstrators rose up across Greece in a third day of rioting over Saturday's killing of a 15-year-old boy that has left dozens injured and scores of properties destroyed.", "a nuclear weapon", "the early 18th century", "1,626,000", "Gag", "the 2009 H1N1 strain"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6585614651751506}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.28571428571428575, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.2857142857142857, 1.0, 0.0, 0.7142857142857143, 0.0, 0.4444444444444445, 1.0, 0.8571428571428571, 1.0, 0.4, 0.8, 0.0, 1.0, 0.3, 0.6666666666666666, 1.0, 0.125, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0689655172413793, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-3340", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-4499", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-958", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3336", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-5475", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2579", "mrqa_triviaqa-validation-2322", "mrqa_newsqa-validation-122", "mrqa_searchqa-validation-5810", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-12635"], "SR": 0.515625, "CSR": 0.5275000000000001, "EFR": 0.967741935483871, "Overall": 0.6997515120967741}, {"timecode": 75, "before_eval_results": {"predictions": ["France", "Shanghai", "Paul Bunyan", "Vislor Turlough", "Pandora", "Turton Tower", "alaska", "Cowboy Builders", "the DeLorean Motor Company", "VXX", "Lundy Island", "Alamo", "David Hockney", "cylinder", "Janis Joplin", "tubular corkscrew", "Humphrey Bogart", "Christopher Robin", "Antoine Lavoisier", "1960", "Nicaragua", "King County Executive", "Steely Dan", "U.S. ambassador to the United Nations", "Jane Austen", "the Cuban missile crisis", "lime", "The Rocketeer", "sea spray", "Venus", "Declaration of Independence", "decorate", "Hot Chocolate", "Jim Peters", "Armageddon", "Kansas", "carry On Cleo", "Berlin", "Gondwana", "salmon", "the Wild Bunch", "the Island", "Project Gutenberg", "Bloodaxe", "California", "Nissan", "Isar", "A View to a Kill", "Allende", "The Green Mile", "Poland", "around 1600 BC", "Sean O' Neal", "May 18, 2018", "Dissection", "Baden-W\u00fcrttemberg", "March", "A McCain spokesman attacked Obama's plan, saying the Democrat's \"agenda to raise taxes and isolate America from foreign markets will not get our economy back on track or create new jobs.\"", "Smithline, of Wyckoff, New Jersey,", "$500,000", "vitamin A", "Azkaban", "conga drums", "Vibe"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6152243589743589}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.30769230769230765, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-444", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-2875", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-191", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-225", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-6727", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-65", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3850"], "SR": 0.578125, "CSR": 0.5281661184210527, "EFR": 1.0, "Overall": 0.7063363486842105}, {"timecode": 76, "before_eval_results": {"predictions": ["the murders of his father and brother.", "a \" happy ending\" to the case.", "head injury.", "Bryant Purvis", "1983", "Jason Chaffetz", "Bryant Purvis", "jazz", "$30 million,", "1994,", "dental", "peanuts, nuts, shellfish and fish", "fusion teams", "April 2010.", "police chased him and a gunfight ensued.", "October 19,", "Addis Ababa,", "a president who understands the world today, the future we seek and the change we need.", "five", "St. Louis, Missouri.", "if Gadhafi suffered the wound in crossfire or at close-range", "off Somalia's coast.", "the Indian army and separatist militants in Indian-administered", "The Charlie Daniels Band", "France", "planning processes are urgently needed", "Saturday.", "Obama", "\"a whole new treasure trove of fossils\"", "and renewable energy at home everyday,\"", "insect stings,", "Kenneth Cole", "a one-shot victory in the Bob Hope Classic on the final hole", "Natalie Cole's", "\"The people kill him with the blocks, because the people are angry. They are not hungry, they are angry,\"", "Hundreds", "California-based Current TV", "Colorado prosecutor", "100% of its byproducts", "shark River Park in Monmouth County", "from Galveston, Texas, to Veracruz, Mexico,", "nine-wicket", "70,000", "Nazi war crimes suspect", "his former caddy,", "three", "fatally shooting a limo driver", "named his company Polo because \"it was the sport of kings. It was glamorous, sexy and international.\"", "overhaul domestic policies", "President Barack Obama,", "Patrick McGoohan,", "Hathi Jr", "minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Bacon", "raw hides", "Daniel Fran\u00e7ois Esprit", "fort boyard", "1,521", "Australian", "Edward R. Murrow", "fort boyard", "Duncan", "lt. colonel", "neo-Nazi"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6629549600637357}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9565217391304348, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 0.8, 1.0, 0.47619047619047616, 1.0, 0.8, 1.0, 1.0, 0.8, 0.5, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2382", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2385", "mrqa_newsqa-validation-2610", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2061", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-6091", "mrqa_hotpotqa-validation-4863", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-7657"], "SR": 0.5625, "CSR": 0.528612012987013, "EFR": 1.0, "Overall": 0.7064255275974026}, {"timecode": 77, "before_eval_results": {"predictions": ["Terry the Tomboy", "Gatwick Airport", "Abdul Razzak Yaqoob", "Robert Matthew Hurley", "William Shakespeare", "Syracuse University", "American", "\"Peshwa\" (Prime Minister)", "Tennessee", "Lowe's Companies, Inc.", "Arthur Schnitzler's 1926 novella \"Traumnovelle\" (\"Dream Story\")", "Hindi", "\"Big Mamie\"", "first", "Kate Millett", "2000 Summer Olympics", "Leucippus", "Centre of Excellence", "Columbine", "381.6 days", "Richard Strauss", "South West Peninsula League", "1994", "Yorgos Lanthimos", "Roscoe Lee Browne", "the \"Black Abbots\"", "chocolate-colored Labrador Retriever", "more than 265 million", "\"The Brothers Karamazov\"", "(Jumh\u016briyyat as-S\u016bd\u0101n)", "Telugu and Tamil", "Australian-American", "Albert Park", "Sevens", "Brenton Thwaites", "Charles and Thomas Guard", "1919", "Elise Stefanik", "Manhattan, New York City", "King of the Polish-Lithuanian Commonwealth", "pop music and popular culture", "Almeda Mall", "Ronald Joseph Ryan", "robot Overlords", "Doctor of Philosophy", "L.M. Montgomery", "Madeleine L' Engle", "February 5, 2015", "Nathan Bedford Forrest", "the Battle of Iwo Jima", "capital crimes or capital offences", "the Tigris and Euphrates rivers", "between 1765 and 1783", "during his first year in Spain", "gilda", "Heisenberg", "giuseppe smith", "Tsvangirai", "change course", "South Africa", "(Little Flower)", "pint", "October", "Mandi Hamlin"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7062375992063492}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 0.8, 1.0, 0.5, 0.4, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-5879", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-266", "mrqa_hotpotqa-validation-2550", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-5420", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-3130", "mrqa_hotpotqa-validation-5569", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-2009", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-2751", "mrqa_newsqa-validation-391"], "SR": 0.59375, "CSR": 0.5294471153846154, "EFR": 1.0, "Overall": 0.7065925480769231}, {"timecode": 78, "before_eval_results": {"predictions": ["a Robin's nest", "Gianlorenzo Bernini", "the Land of the Hummingbird", "Denzel Washington", "a prologue", "Ben- Hur", "(Al) Capone", "a prism schism", "Bucharest", "Tennessee", "Dick Wolf", "a mokorotlo", "Helena Bonham Carter", "Cincinnati", "a slave", "Miss Havisham", "the Thames", "the top", "high", "New Jersey", "Tarsus", "gold rush", "a structure for storing grain", "Eli Whitney", "Breckenridge", "the \"Dongfanghong-I\"", "bishops", "Esperanto", "the Hundred Years' War", "Mending Wall", "Twelfth Night", "(Christian Slater)", "the Odyssey", "Special Boat Teams", "bananas or avocados", "Today", "Sally Field", "earmark", "Turin", "a collective noun", "a deviated septum", "the Golden Girls", "polo", "a quick search", "the Maritimes", "1773", "Richard Daley", "silicon", "Bee", "the best man", "the Missouri Waltz", "The Romantics", "Danny Veltri", "a popular and influential campaign song of the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "a low voice or whisper", "werner", "Wildcats", "Knowlton Hall", "Ronald Reagan", "The interview", "U.S. relief effort in Haiti.", "183", "Jacob Zuma,", "1987"], "metric_results": {"EM": 0.609375, "QA-F1": 0.690286330049261}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true], "QA-F1": [0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.20689655172413793, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3938", "mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-14670", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-10493", "mrqa_searchqa-validation-12301", "mrqa_searchqa-validation-7541", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-14621", "mrqa_searchqa-validation-8526", "mrqa_searchqa-validation-7331", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-12526", "mrqa_searchqa-validation-2056", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-15622", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-3743", "mrqa_searchqa-validation-5831", "mrqa_naturalquestions-validation-4552", "mrqa_triviaqa-validation-3185", "mrqa_triviaqa-validation-5498", "mrqa_hotpotqa-validation-5573", "mrqa_newsqa-validation-93"], "SR": 0.609375, "CSR": 0.5304588607594937, "EFR": 1.0, "Overall": 0.7067948971518987}, {"timecode": 79, "before_eval_results": {"predictions": ["Marrix", "reddish tint", "Auscaloosa", "fish", "six", "Unicorn", "David Seville", "Whiskas", "Aintree", "50p", "salt", "salt tower", "Mujib", "a scarlet tanager", "werner lincoln", "long jump", "Kipps: The Story of a Simple Soul", "rial", "Venezuela", "maritime", "George H.W. Bush", "werner gooch", "golf", "1", "Flamboyant publisher Hugh Hefner", "Florentia", "Kofi Annan", "I Faraglioni", "right", "George Eliot", "Richard II", "maritime", "mountain of light", "a form of contactless communication between devices like smartphones or an unpowered NFC", "salt tower", "philistine", "South Africa", "Topeka", "Brazil", "George Osborne", "The Good Life", "Florence", "Sicily", "Peter Gabriel", "Space Oddity", "the Smiths", "Osiris", "Jeffery Deaver", "\u00c9dith Piaf", "maritime", "the International Red Cross and Red Crescent Movement", "Asuka", "Canada south of the Arctic", "Beldam / Other Mother", "Ballarat Bitter", "Mani", "\"the backside.\"", "110 mph,", "workers at EMS are afraid to speak out because they might get fired,", "to stabilize Somalia and cooperate in security and military operations.", "The Discovery Channel", "an Enigma", "Edinburgh", "obscenity"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5014768217893218}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.8571428571428571, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-2346", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-1090", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-1601", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-7331", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-7729", "mrqa_triviaqa-validation-5529", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6818", "mrqa_naturalquestions-validation-1872", "mrqa_naturalquestions-validation-2851", "mrqa_hotpotqa-validation-507", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2234", "mrqa_searchqa-validation-15162"], "SR": 0.453125, "CSR": 0.5294921875, "EFR": 0.9428571428571428, "Overall": 0.6951729910714286}, {"timecode": 80, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3967", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10493", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16534", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16891", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8807", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2248", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3750", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4669", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-5869", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8500", "mrqa_squad-validation-8529", "mrqa_squad-validation-8719", "mrqa_squad-validation-8754", "mrqa_squad-validation-8966", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7717", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-934"], "OKR": 0.82421875, "KG": 0.4890625, "before_eval_results": {"predictions": ["eight", "r.A.V. of Amsterdam", "Amram and Jochebed, of the tribe of Levi ( Ex. 6:16\u201320 )", "Lucille Ball", "hart", "liqueur", "Rod Stewart", "Hungary", "Greek", "groucho Marx", "new ironsides", "collage", "To Kill a Mockingbird", "fish", "cincy", "1966", "st. Benedict", "Billy Fury", "Tuesday", "Giacomo Meyerbeer", "Whist", "hugh Laurie", "Fan Graphs and Baseball Prospectus", "A4", "Mussolini", "George Osborne", "Margaret Thatcher", "jon pertwee", "sheep", "Jacob", "strictly come dancing", "Chicago", "Hague", "sub-Saharan", "Inigo Montoya", "President Nixon", "ex-members of Mother Love Bone", "Aleister Crowley", "Elton John", "\"Great and Most Fortunate Navy\"", "setts", "batsman", "seth", "word", "Saddam Hussein", "Tombstone", "liriope", "Indonesia", "simeon Williamson", "Swiss", "daedalus", "in the books of Exodus and Deuteronomy", "starch", "David Ben - Gurion", "DTM", "Tetrahydrogestrinone", "7pm", "Ma Khin Khin Leh,", "prisoners at the South Dakota State Penitentiary", "Shanghai", "the jugular vein in the neck", "Peter and Jane", "Think big", "on the inner edge of the Nebula Arm"], "metric_results": {"EM": 0.5, "QA-F1": 0.5452380952380953}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 0.47619047619047616]}}, "before_error_ids": ["mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-6040", "mrqa_triviaqa-validation-5921", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-6871", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-363", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-4563", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-2903", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-6791", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9726", "mrqa_searchqa-validation-4839", "mrqa_searchqa-validation-4136", "mrqa_naturalquestions-validation-808"], "SR": 0.5, "CSR": 0.5291280864197531, "EFR": 1.0, "Overall": 0.7110599922839507}, {"timecode": 81, "before_eval_results": {"predictions": ["Cleckheaton", "the Haitian Revolution", "bear", "three", "dwain chambers", "Arthur Schopenhauer", "people", "steel", "Columba", "drake", "a power outage", "snapdragons", "weather", "for gallantry", "Zachary Taylor", "1951", "bachus-Polka", "teatro San Cassiano in Venice", "Cambridge", "meerkat", "Saturn", "aintree", "a carb", "Clark Gable", "turkish", "Laos", "turkish", "bacardi", "vanilla", "Patrick Henry", "Apocalypse Now", "double nelson", "bear", "elkie Brooks", "aston strong", "edwina currie", "meyer tchaikovsky", "bears", "Edinburgh", "Charlotte Corday", "algiers", "baltimore", "golf", "yellow", "Norwich", "Trenton", "the hose", "sea otter", "cardamom", "czechoslovakia", "benedaniel ostroff", "243 days", "Sunni Muslim family", "Terry Reid", "due to a leg injury", "8 May 1989", "David Yates", "Mohamed Mohamud Qeyre", "15-year-old's", "The paper said the trip had caused fury among some in the military who saw", "a petition", "a bear", "pemmican", "Reform"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5410125968992248}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5581395348837209, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-7522", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-6689", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-7747", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-2755", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-7053", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-5145", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-5820", "mrqa_triviaqa-validation-704", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-522", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-3778", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-8180", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-14428"], "SR": 0.46875, "CSR": 0.5283917682926829, "EFR": 0.9411764705882353, "Overall": 0.6991480227761837}, {"timecode": 82, "before_eval_results": {"predictions": ["ilfracombe", "the secant", "thailand", "fibrocartilage", "Yardbirds", "Brazil", "1123", "julia i", "Bruce Alexander", "niece", "manhunt 2", "Carson City", "a spirit-lifting jingle", "jewellers", "prawns", "smith davina hingis", "house sparrow", "Northwestern University", "love never dies", "lunar new year holiday called Tet", "insula", "Frank Saul", "126 mph", "David Davis", "Eric Coates", "\u00c9dith Piaf", "Arthur", "anteros", "football", "alaska", "white-tailed gnu", "the Breakfast Club", "chestnut", "i second that emotion", "John Donne", "lomond", "Salt Lake City", "red", "a chestnut", "the Battle of Austerlitz", "Venus", "the Compact Pussycat", "tintin", "Mercury", "guitar", "Celsius", "Phil Spector", "florida", "Galileo Galilei", "i second that emotion", "w. R. Greer", "foreign investors", "the Gilbert building", "meyer heyerfwagen VIII Maus ( `` Mouse '' ) was a German World War II super-heavy tank", "two", "The Sleaford and North Hykeham by-election", "Four Weddings and a Funeral", "his mother, Katherine Jackson, his three children and undisclosed charities.", "The last survivor of the Titanic, 97-year-old Millvina Dean, is auctioning off her remaining mementos of the doomed ship", "1979", "Israel", "Phil Mickelson", "the Crimean War", "Larry Ellison,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5345487845487845}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.26666666666666666, 0.0, 1.0, 1.0, 0.18181818181818182, 0.07692307692307693, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-1361", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-2425", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-4127", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-5389", "mrqa_triviaqa-validation-5799", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-7246", "mrqa_hotpotqa-validation-3037", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-1594", "mrqa_searchqa-validation-3536"], "SR": 0.515625, "CSR": 0.5282379518072289, "EFR": 1.0, "Overall": 0.7108819653614458}, {"timecode": 83, "before_eval_results": {"predictions": ["lymph node", "Swedish", "Toyota", "March 19", "Julie Andrews Edwards", "mad", "fungi", "verona", "phil Spector", "fidelio", "norway", "minder", "Pisces", "peppers", "yMCA", "willy", "macbeth", "origami", "Diana Ross", "my fellow Americans", "nettle", "cartoons", "an astronaut", "riyal", "blank oats", "boris becker", "one Thousand", "john of gaunt", "sewing machine", "Bristol Aeroplane Company", "tenor saxophonist", "tutankhamun", "colony", "fedora", "indus", "Helen Gurley Brown", "edward hay", "krakatoa", "pinocchio", "norway", "abraham smith", "venice noratorio", "road map", "biluim", "rue", "Ken Platt", "hmsden", "julia hargreaves", "Sarah Vaughan", "Mr Loophole", "15", "his friends, Humpty Dumpty and Kitty Softpaws", "Watson and Crick", "the appendicular skeleton", "25 November 2015", "Jean Erdman", "Netherlands", "9 a.m.", "to the U.S. Holocaust Memorial Museum", "The nation's new \"first dog\" has heightened interest in its breed -- Portuguese water dog", "Robert Langdon", "Parris Island", "watts", "sunflower"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6458333333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.4444444444444445, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4062", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-6348", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-2247", "mrqa_triviaqa-validation-4471", "mrqa_triviaqa-validation-4917", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-6998", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-6094", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-971", "mrqa_triviaqa-validation-5470", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3468", "mrqa_hotpotqa-validation-413", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-144", "mrqa_searchqa-validation-9567"], "SR": 0.5625, "CSR": 0.5286458333333333, "EFR": 1.0, "Overall": 0.7109635416666666}, {"timecode": 84, "before_eval_results": {"predictions": ["David Ben - Gurion", "is said to be unattainable", "Karina Smirnoff", "the public", "the eighth episode of Arrow's second season", "Clarence L. Tinker", "2002", "1992", "the words spoken to Adam and Eve after their sin, reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time", "John Roberts", "Kelly Reno", "butane", "Parker's pregnancy at the time of filming", "2004", "a state or other organizational body that controls the factors of production", "Ben Willis", "The Enchantress", "epic fantasy", "Leon Huff", "the seventh cranial nerve", "jonifer elizabeth", "the March of Dimes", "gold", "The management team", "Paris", "The Parlement de Bretagne", "Matt Flinders", "during prenatal development", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "a premedication", "Ferm\u00edn Francisco", "three", "Coriolis effect", "94 by 50 feet ( 28.7 by 15.2 m )", "The enthalpy of fusion of a substance, also known as ( latent ) heat of fusion", "Washington metropolitan area", "Vanessa Ferlito", "Leslie", "ice giants", "2010", "1920s", "in different parts of the globe", "Empire of Japan", "cylinder of glass or plastic that runs along the fiber's length", "The Intolerable Acts", "Hanna Alstr\u00f6m", "Internal epithelia", "Bob Dylan", "Latitude", "Buddhism", "a statistical advantage for the casino that is built into the game", "Sikhism", "ronald reagan", "John Mortimer", "Everglades", "niece", "the second", "Kitty Kelley,", "Strategic Arms Reduction Treaty", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "george calvert", "governor", "Ontario", "medellin"], "metric_results": {"EM": 0.4375, "QA-F1": 0.597676282051282}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.33333333333333337, 1.0, 1.0, 0.7692307692307692, 0.4615384615384615, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666667, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.8, 0.9523809523809523, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615385, 1.0, 0.8571428571428572, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-4145", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5555", "mrqa_hotpotqa-validation-4375", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-2355", "mrqa_searchqa-validation-13409", "mrqa_searchqa-validation-1600"], "SR": 0.4375, "CSR": 0.5275735294117647, "EFR": 0.9166666666666666, "Overall": 0.6940824142156863}, {"timecode": 85, "before_eval_results": {"predictions": ["July 14, 2017", "7 August 2021", "manufacturing", "North Dakota ( 21.5 % )", "Tom Selleck", "Nicolas Anelka", "18 - season", "flour and water", "Mahatma Gandhi", "John von Neumann", "Missouri River", "Fa Ze YouTubers", "Orangeville, Ontario, Canada", "May 2016", "up to 100,000", "Kiss", "one season", "30 months", "October 2012", "winter festivals", "gesture in which the head is tilted in alternating up and down arcs along the sagittal plane", "July 2, 1928", "push the food down the esophagus", "Jackie Robinson", "The Lightning thief", "TLC - All That Theme Song 1 : 04", "230 million kilometres ( 143,000,000 mi )", "September 8, 2017", "William the Conqueror", "Abid Ali Neemuchwala", "Sri Lanka Podujana Peramuna, led by former president Mahinda Rajapaksa", "in the city of Chicago", "The User State Migration Tool", "1807", "the season - five premiere episode `` Second Opinion ''", "the Ming dynasty", "Saint Peter ( the keeper of the `` keys to the kingdom '' )", "1773", "September 25, 1987", "20 years from the filing date", "a homodimer of 37 - kDa subunits", "Billie Jean King", "Internal epithelia", "February 16, 2016", "detritus", "Ravi River", "Michael Biehn ( uncredited )", "12 to 36 months old", "Alice Cooper", "in Connecticut", "mounted inside the pedestal's lower level", "boxelder bug", "acetone", "William Claude Dukenfield", "21 August 1986", "Taylor Swift", "2006", "8 to 10 inches of snow", "Mugabe", "Johannesburg", "Jeremy Corbyn", "Charles Boyer", "The Last of the Mohicans", "The Electromagnetic Bomb"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6120685253267973}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 0.5, 0.125, 0.5, 1.0, 1.0, 1.0, 0.25, 0.9090909090909091, 1.0, 1.0, 1.0, 0.5714285714285715, 0.4, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.6, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-1359", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-1368", "mrqa_naturalquestions-validation-1789", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-91", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-3641", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-2651", "mrqa_searchqa-validation-5288", "mrqa_searchqa-validation-13973"], "SR": 0.484375, "CSR": 0.5270712209302326, "EFR": 0.9696969696969697, "Overall": 0.7045880131254405}, {"timecode": 86, "before_eval_results": {"predictions": ["Georgia Groome", "the Redenbacher family", "Louis Hynes", "Database", "Kenneth Cook", "Russia", "1556", "July 4, 1776", "Egypt", "a blighted ovum or anembryonic gestation", "left hand ring finger", "through the Torres Strait", "in the pouring rain at a rest stop", "3D modeling", "statute or the Constitution", "2012", "Connecticut", "Lewis Hamilton", "1974", "1998", "Brazil and Paraguay", "to form a higher alkane", "1947", "the Ramones", "Southern Arizona", "Jurchen Aisin Gioro clan", "China in American colonies", "Jacques Cousteau", "New Mexico", "Nepal", "538", "Ismailism", "the President", "Road / Track ( no `` and '' )", "March 8, 2018", "Gary Player", "1997", "Holden Nowell", "pathology", "332", "if the occurrence of one does not affect the probability of occurrence of the other", "the central sulcus", "at the turn of the traditional lunisolar Chinese calendar", "mitosis", "The Maidstone Studios in Maidstone, Kent", "the Outfield", "International Border", "Claire Rhiannon Holt", "photodiode", "1858", "2010", "George III", "Barbara Good", "Donington Park", "Kolkata", "The Handmaid's Tale", "U.S. Representative", "Asashoryu,", "Herman Cain,", "World Trade Center", "a hormone", "a Conehead", "the British Parliament", "Sean Maguire"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5791914682539683}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true], "QA-F1": [0.5714285714285715, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 1.0, 0.0, 0.0, 0.5, 0.5, 0.5, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.2, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-4850", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-5546", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-3150", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-15707"], "SR": 0.46875, "CSR": 0.5264008620689655, "EFR": 0.9705882352941176, "Overall": 0.7046321944726166}, {"timecode": 87, "before_eval_results": {"predictions": ["Norway", "fish", "Romeo and Juliet", "The Golden Girls", "Niger", "Artemis", "using a vertical stroke with surrounding arcs", "Spanish", "driving Miss Daisy", "malta", "Robert Galbraith", "Dubai", "Bristol", "Double Trouble", "Switzerland", "Barack Obama", "The Observer", "Yakutat, Alaska", "timothy laurence", "Brian Clough", "Peter Paul Rubens", "pembrokeshire Coast National Park", "blood", "willy", "javelin throw", "Port Moresby", "Majestic Theatre", "Maxwell Farrago", "non-Orthodox synagogues", "Cambridge", "winnie Mae", "Richard Curtis,", "Keswick", "Louis Le Vau", "horseshoes", "David Copperfield", "the Union Gap", "'Hansel and Gretel' cottage", "India", "Charlotte", "Michael Phelps", "Vietnam", "Mumbai", "mumps", "stop motion effects", "Sebastian Flyte", "the Apollo 11 lunar module", "kelp", "Hindi", "Madrid", "Paul Bunyan", "Reverse - Flash", "the nose", "summer of 1990 and continued until 1992", "1986", "Central African Republic", "1967", "the first of 1,500 Marines will be part of the initial wave of President Obama's", "1,500", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "cholesterol", "volcanoes", "eight", "Baldwin, Nassau County, New York"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6442708333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_triviaqa-validation-3998", "mrqa_triviaqa-validation-994", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7209", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-6967", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-4680", "mrqa_triviaqa-validation-7042", "mrqa_triviaqa-validation-3602", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-8386", "mrqa_hotpotqa-validation-3558", "mrqa_newsqa-validation-1861", "mrqa_newsqa-validation-2586", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-1657", "mrqa_hotpotqa-validation-1557"], "SR": 0.609375, "CSR": 0.52734375, "EFR": 0.96, "Overall": 0.702703125}, {"timecode": 88, "before_eval_results": {"predictions": ["Cool Runnings", "five times", "an explosion and a fire", "Lego", "elderships", "Andrew Davis", "Drowning Pool", "Lombardy", "7", "\"Realty Bites\"", "2010", "Florida Panthers", "capital crimes or capital offences", "the 10-metre platform event", "Saturday Night Live", "3,000", "beer", "Shohola Falls", "The Kennedy Center", "Saoirse Ronan", "Rose Mary Woods", "Netherlands", "Miss Universe 2010", "John R. Leonetti", "Fat Albert", "Oklahoma State", "stoneware", "Prudence Jane Goward", "Revolver", "Walcha", "The Double Life of V\u00e9ronique", "Manhattan Project", "Apatosaurus", "Currer Bell", "Anne Perry", "Alpine, New Jersey", "Argentina", "Province of Syracuse", "David Irving", "Northside", "219", "Charice", "Harlem", "Francis Schaeffer", "Eric Edward Whitacre", "Darci Kistler", "Laura Dern", "Liberty", "Axl Rose", "the second", "youngest publicly documented people to be identified as transgender", "Bob Parr", "1928", "October 2, 2017", "Cuba", "bird", "oregon", "a book.", "South Africa", "Russia", "14th", "canine", "Jane Grey", "Yemen"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6869357638888889}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5, 1.0, 1.0, 0.375, 0.22222222222222224, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-1405", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-568", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-2211", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-1870", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-5133", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-5824", "mrqa_newsqa-validation-1389", "mrqa_searchqa-validation-15444", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-7264"], "SR": 0.59375, "CSR": 0.5280898876404494, "EFR": 0.9615384615384616, "Overall": 0.7031600448357822}, {"timecode": 89, "before_eval_results": {"predictions": ["2012 NBA draft", "848", "Skipton Castle", "Consigliere", "Love the Way You Lie", "Beatles", "shortstop", "Francis Egerton, 3rd Duke of Bridgewater", "Tamworth", "\"Teenage Dream\"", "Eric Whitacre", "online role-playing video game", "Nicole Kidman", "Karl Kraus", "water", "Orchard County", "he turned 51, he died of cancer", "1947", "2004", "September 1903", "over 170", "from 1977 to 2013", "Boulder High School in Boulder, Colorado", "Tian Tan Buddha", "Rocky Mountain Institute", "Macau, China", "La Nouba", "Stephanie Plum", "Memphis", "National Collegiate Athletic Association", "Figaro", "Eve Hewson", "Leon Uris", "U2 360\u00b0 Tour", "Eielson Air Force Base", "1,462", "University of New South Wales", "the God of Israel", "three times", "Budget Rent a Car", "Dallas", "Terrence Jones", "Democratic", "2015", "\u00c6thelstan", "historically black", "half a million acres", "Jyothika", "\"The Bob Edwards Show\"", "Bill Miner", "50JJB Sports Fitness Clubs", "the Tigris and Euphrates rivers", "$2.187 billion", "chromosomes in each pair", "Goldie Hawn", "nepal", "spain", "North Korea", "$40 and a bread.", "enjoyed a cold shower in his home in New Zealand.", "Myron Leon \" Mike\" Wallace", "a volcano", "spain", "Capitol Hill,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6796875}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-4715", "mrqa_hotpotqa-validation-3763", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-1699", "mrqa_hotpotqa-validation-2833", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-5603", "mrqa_naturalquestions-validation-6931", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-5948", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-339", "mrqa_searchqa-validation-4758"], "SR": 0.546875, "CSR": 0.5282986111111111, "EFR": 1.0, "Overall": 0.7108940972222222}, {"timecode": 90, "UKR": 0.68359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1547", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3812", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8546", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13830", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8529", "mrqa_squad-validation-8754", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-2575", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4585", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-7717", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-920"], "OKR": 0.8125, "KG": 0.4828125, "before_eval_results": {"predictions": ["Delaware River", "The Shirehorses", "Detroit", "\"The Expendables 2\"", "Bob Hill", "Pennsylvania State University", "Princess Aisha bint Hussein", "1951", "Apalachee", "River Clyde", "aging issues", "Indooroopilly Shoppingtown", "Chow Tai Fook Enterprises", "Rhode Island School of Design", "Bill Walton", "Vanessa Hudgens", "1983", "Andrew Preston", "Eddie Gottlieb Trophy", "Rhodesia", "Ed O'Neill", "My Backyard", "Lommel", "Bharat Ratna", "Aberdeenshire", "three-part", "1969", "Stalybridge Celtic Football Club", "Property management", "Vincent Anthony Guaraldi", "Pulitzer Prize for drama", "one live album", "2002", "Anno 2053", "Province of New Jersey", "1932 and 1934", "Hopi", "American", "pop music and popular culture", "South Australia", "SKUM", "Rain Man", "Sunirik Atoll", "Carlos Santana", "1942", "2027 Fairmount Avenue", "historic buildings, arts, and published works", "magnas", "The Washington Post", "143,007", "James I", "committed suicide", "1970", "the nucleus", "lung", "discus thrower", "spain", "three", "police dogs", "aggression, from aboard the USNS Impeccable,\"", "Sierra Leone", "I.W. Murnau", "Airborne", "11th year in a row"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7188616071428571}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.75, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-4514", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-1048", "mrqa_hotpotqa-validation-5498", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-2599", "mrqa_hotpotqa-validation-1925", "mrqa_hotpotqa-validation-777", "mrqa_hotpotqa-validation-5537", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4864", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5991", "mrqa_newsqa-validation-2935", "mrqa_newsqa-validation-414", "mrqa_newsqa-validation-3310", "mrqa_searchqa-validation-6970", "mrqa_searchqa-validation-10274"], "SR": 0.609375, "CSR": 0.5291895604395604, "EFR": 1.0, "Overall": 0.7016191620879121}, {"timecode": 91, "before_eval_results": {"predictions": ["Milwaukee", "Nathan Lane", "E.M. Forster", "James Joyce", "Jamestown", "Helen Hayes", "a cupronickel coin", "Philip II", "Dobermann", "Vermont", "Moses", "the Moors", "Margaret Mitchell", "Henrik Ibsen", "cruller", "Witch", "Hungary", "Sugar Smacks", "February 2", "New Balance", "The Siberian Husky", "Animal Crackers", "nitrogen and oxygen", "a platypus", "Nixon", "a bicycle", "The Magic Mountain", "Fiji", "Manassas", "Jacqueline Kennedy Onassis", "Forbes", "Death Row Records", "Jimmy Hoffa", "AI", "a rabbit", "a car buying scam", "The Wiener Journal", "apartheid", "Browning", "compound interest", "molly ringwald", "The Home Depot", "the Marine Corps", "Ho Chi Minh", "a supernatural weeper", "Arkansas", "The Stone of Destiny", "remoulade", "Florida", "Night Fever", "Latin", "Jane Addams", "Thomas Chisholm", "Norman Pritchard", "Ron Howard", "spider", "Istanbul", "The Battle of the Rosebud", "40 Acres and a Mule Filmworks", "1851", "26", "$22 million", "Sixteen", "Ricky Nelson"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7832859848484848}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12465", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-8297", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-524", "mrqa_searchqa-validation-13732", "mrqa_searchqa-validation-8137", "mrqa_searchqa-validation-10072", "mrqa_searchqa-validation-6520", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-15184", "mrqa_searchqa-validation-13793", "mrqa_searchqa-validation-11111", "mrqa_searchqa-validation-3990", "mrqa_searchqa-validation-13167", "mrqa_naturalquestions-validation-2176"], "SR": 0.671875, "CSR": 0.5307404891304348, "EFR": 1.0, "Overall": 0.701929347826087}, {"timecode": 92, "before_eval_results": {"predictions": ["a bust", "A Chorus Line", "Pamplona", "French toast", "Pop-Tarts", "Aesop", "the Shawnee", "postscript", "Babe Ruth", "the children of prostitutes", "the Orinoco", "John Bunyan", "Sicilian pizza", "Punjabi", "insulin", "Jimmy Hoffa", "Zion", "Newton", "Cincinnati", "George Washington", "the Timberland boot", "Bronchoconstriction", "the Taurid", "Phil Cavilleri", "the wall", "porter", "Davy Crockett", "Michelangelo", "Penny Lane", "grease", "Kathleen Kennedy Townsend", "Henry Cavendish", "israel", "Euphoria", "Don Quixote", "Charlie and the Chocolate Factory", "Baboon", "Last Summer", "Kenneth Blackwell", "genes", "the Wild Thornberrys", "the Bionic Woman", "Blufire", "Wynona Judd", "pardis", "King Edward III", "the HIV/AIDS", "the Smothers Brothers", "Henry Hudson", "the sidecar", "Ford", "The management team", "United States, the United Kingdom, and their respective allies", "2017", "table salt", "ThunderCats", "new coaches", "AMC Theatres", "The Carol Burnett Show", "Montreal, Quebec", "Matthew Fisher,", "$150 billion", "Tennessee.", "Johannes Gutenberg"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7130208333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-9251", "mrqa_searchqa-validation-6725", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-16819", "mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-8294", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-9414", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-8996", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-3896", "mrqa_searchqa-validation-9483", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-8750", "mrqa_triviaqa-validation-1759", "mrqa_hotpotqa-validation-3758", "mrqa_hotpotqa-validation-582"], "SR": 0.65625, "CSR": 0.5320900537634409, "EFR": 1.0, "Overall": 0.7021992607526882}, {"timecode": 93, "before_eval_results": {"predictions": ["the three-Cornered Hat", "7-11", "Oakland", "Australia", "Parsifal", "the Hippopotamus", "Grant & Sherman", "carbon", "Glory", "the hand", "a carriage", "the catacombs", "Bahrain", "the Four Cohans", "Jean Harlow", "the yardarm", "Iesous", "February 29nd", "mistletoe", "the council", "Shia", "Tijuana", "Cleopatra", "the Irish", "the Capitol", "The Charnel-House", "Homeland Security", "Jumper", "a batrachus", "paradise", "the Baltic Sea", "red red Saddles", "Dom u Dobreho", "While You Were Out", "2.25 Liters", "c", "Love Story", "Ramen", "Truman", "St. Louis", "red", "the Leonadolid", "Cancer's Return Shouldn't limit Elizabeth Edwards, Doctors Say... sara1981, Mar 22, 2007", "Stephen I", "Sgt. Pepper", "Cheddar", "the Amistad", "Prince", "the Milky Way", "the adjective", "Tycho Brahe", "displacement", "Office of Inspector General", "February 6, 2005", "congregation", "Massachusetts", "Charles Darwin", "the Peninsular War", "The Sun on Sunday", "24", "flash flood warnings", "jobs", "removal of his diamond-studded braces.", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6837611607142857}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.5, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1189", "mrqa_searchqa-validation-11754", "mrqa_searchqa-validation-5239", "mrqa_searchqa-validation-14109", "mrqa_searchqa-validation-9419", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-13462", "mrqa_searchqa-validation-11296", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-5782", "mrqa_searchqa-validation-3622", "mrqa_searchqa-validation-13212", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-9980", "mrqa_searchqa-validation-10682", "mrqa_searchqa-validation-12543", "mrqa_searchqa-validation-7097", "mrqa_searchqa-validation-15694", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-5137", "mrqa_naturalquestions-validation-6993", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-2759", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-4042"], "SR": 0.59375, "CSR": 0.5327460106382979, "EFR": 1.0, "Overall": 0.7023304521276595}, {"timecode": 94, "before_eval_results": {"predictions": ["Southend Pier", "9 February 2018", "before the beginning of the seventh century, a time when the Anglo - Saxons were either newly arrived or were still in close contact with their Germanic kinsmen in Northern Germany", "France's Legislative Assembly", "Peter Gardner Ostrum", "Jesse McCartney", "1902", "Splodgenessabounds", "February 27, 2007", "Toot - Toot -- A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Chuck Noland", "his teenage role as the title character on the Disney Channel television series The Famous Jett Jackson ( 1998 -- 2001 )", "1911", "60 by West All - Stars", "around 1872", "the plane crash", "The Witch and the Hundred Knight 2", "aorta", "Pittsburgh metropolitan area", "the beginning of the American colonies", "James Ray", "Tokyo", "The Outback", "regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Abigail Hawk", "January 2018", "The Archers is the world's longest - running radio soap opera", "Cody Fern", "homicidal thoughts of a troubled youth", "August 5, 1937", "San Francisco", "the most devastating stock market crash in the history of the United States", "The Magician", "Teddy Randazzo", ". java", "`` Killer Within ''", "Juliet", "1963", "Roman Reigns", "1901", "1972", "Yondu Udonta", "1977", "Richard Stallman", "Detective Inspector Lindsey Denton in the second and third series of Line of Duty ( 2014 -- 2016 )", "Abid Ali Neemuchwala", "Jay Baruchel", "March 26, 1973", "July 25, 2017", "1922", "Van Halen", "Cyclades", "Some Like It Hot", "bird", "Bhushan Patel", "Dar es Salaam", "96,867", "Black Entertainment Television founder Bob Johnson that appeared to criticize Obama's admitted past drug use were played on Martin's show.", "Iran's parliament speaker", "Sri Lanka", "love", "Mathew Brady", "the Lord of the Rings", "Greenham Common"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7070276824333619}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.13793103448275862, 0.5714285714285715, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.19999999999999998, 0.0, 1.0, 0.6, 0.4, 1.0, 1.0, 1.0, 0.972972972972973, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-5264", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2831", "mrqa_searchqa-validation-9281"], "SR": 0.609375, "CSR": 0.5335526315789474, "EFR": 0.92, "Overall": 0.6864917763157895}, {"timecode": 95, "before_eval_results": {"predictions": ["Jet Republic", "Frank Ricci,", "fining the computer chip giant a record  $1.45 billion for abusing its dominant position in the computer processing unit (CPU) market.", "drug cartels", "the Little Rock Nine", "five", "Ben Roethlisberger", "Rihanna", "Muslim", "Asashoryu", "\"Slumdog Millionaire\"", "Ma Khin Khin Leh,", "$17,000", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the pirates", "10 municipal police officers", "Bangladesh", "Rima Fakih", "Mikkel Kessler", "Friday,", "if the Democrat's \"agenda to raise taxes and isolate America from foreign markets will not get our economy back on track or create new jobs.\"", "fifth and final Olympics in Sydney", "suppress the memories and to live as normal a life as possible;", "the Obama administration.", "Muslim revolutionary named Malcolm X", "Dangjin", "\"mentally deranged person", "central business district of Bangkok", "The patient, who prefers to be anonymous,", "stole", "only one", "Black History Month", "opposition parties", "via YouTube", "Cambodian territory", "2.5 million", "against using injectable vitamin supplements because the quantities are not regulated.", "I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "A. Syed, a junior at Harvard University majoring in government, is an editorial editor at The Harvard Crimson as well as a senior editor and columnist for the Harvard-MIT journal on Islam and society, Ascent.", "Abhisit Vejjajiva", "Tuesday's iPhone 4S news,", "in the Yemeni port city of Aden", "228", "More than 15,000", "the Rwandan genocide", "Operation Pipeline Express.", "$5.5 billion to build.", "nearly $106.5 million", "Long troop deployments in Iraq, above, and Afghanistan", "prostate cancer,", "bartering", "William Wyler", "1608", "The Epistle of Paul to the Philippians", "kachhi", "equal", "fractal geometry", "Paper", "SBS", "The Number Twelve Looking Like You", "the Chamber of Secrets", "diphthong", "water lily", "nuclear weapons"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6738609799912809}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.372093023255814, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3913", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-2891", "mrqa_naturalquestions-validation-7728", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-2184", "mrqa_hotpotqa-validation-3247", "mrqa_searchqa-validation-9089"], "SR": 0.5625, "CSR": 0.5338541666666667, "EFR": 0.9642857142857143, "Overall": 0.6954092261904762}, {"timecode": 96, "before_eval_results": {"predictions": ["\"one of the most magnificent expressions of freedom and free enterprise in history\"", "off Somalia's coast.", "2009", "club managers,", "5,600", "a construction site in the heart of Los Angeles.", "central Cairo,", "free services.", "delivers a big speech", "Basilan", "\"People of Palestine\"", "Fareed Zakaria", "Gov. Mark Sanford", "56,", "$17,000", "ClimateCare,", "Australian officials", "fake his own death by crashing his private plane into a Florida swamp.", "longest domestic relay in Olympic history,", "Police", "one", "The FARC", "knowingly exposed the soldiers to a cancer-causing toxic chemical.", "in a campus library,", "July", "the two remaining crew members from the helicopter,", "a sixth member of a Missouri family", "\"GoldenEye\"", "$1.5 million", "\"People have lost their homes, their jobs, their hope,\"", "30", "Larry King", "HPV (human papillomavirus)", "\"The strike means all buses, subways and trolleys in Philadelphia and on the Frontier line in Bucks and Montgomery counties stopped running at 3 a.m.", "Body Tap,", "The Southern Baptist Convention,", "Stephen Tyrone Johns", "Math teacher Mawise Gumba", "\"The first line of law and order", "Facebook", "helps consumers move beyond these hard times and has reignited a whole industry.", "South African police", "Susan Atkins,", "off the coast of Dubai", "\"She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "\"falling space debris,\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "Silicon Valley.", "63", "\"Slumdog Millionaire\"", "Lillo Brancato Jr.", "works in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "Joe the Lion ( Sylvester Stallone )", "A tree - topper or treetopper", "butterflies", "vanilla", "France", "1241 until his death in 1250", "February 1940", "Modbury", "Jacob Marley", "Peter Jonas", "Pin the Tail on the Donkey", "Mount Hood"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7801921583850933}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.9333333333333333, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.07142857142857142, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 0.6666666666666666, 0.1, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-1163", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-2261", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-5305", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-5864", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-6959"], "SR": 0.671875, "CSR": 0.5352770618556701, "EFR": 0.9047619047619048, "Overall": 0.683789043323515}, {"timecode": 97, "before_eval_results": {"predictions": ["Madeline Kahn", "Winnie Mae", "Leicester City", "Pocahontas", "Geoffrey Chaucer", "Travis", "Jordan", "John Donne", "West Virginia", "dogs", "watchmaking", "Central African Republic", "ballet", "the South Bank", "Cornwall", "the Severn", "green", "Fred Trueman", "The Mayor of Casterbridge", "Athens", "Yemen", "Loch Morar", "leprosy", "Manhunt 2", "your phone", "piano", "a gun", "collies", "Karachi", "the month of Ramadan", "2 Samuel", "George Fox", "bat", "secretary", "France", "Melissa", "haddock", "few", "Ross MacManus", "roundlaypiano", "dry rot", "cuckoo", "Northumberland", "6", "Midnight Cowboy", "36", "1912", "a wedge", "Northern Ireland", "Jorge Lorenzo", "Pat Houston", "Bill Irwin", "1273.6 cm", "J. Presper Eckert and John William Mauchly's ENIAC", "The Florida Panthers", "Giuseppe Fortunino Francesco Verdi", "67,575", "in July", "sailing", "Mary Procidano,", "Hedy Lamarr", "Quinn the Eskimo", "the Komodo Dragon", "goalkeeper"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7035353535353535}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 0.18181818181818182, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-5438", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-3406", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-7539", "mrqa_triviaqa-validation-7413", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-1585", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-3750", "mrqa_hotpotqa-validation-3953", "mrqa_newsqa-validation-271", "mrqa_searchqa-validation-1862"], "SR": 0.65625, "CSR": 0.5365114795918368, "EFR": 0.9545454545454546, "Overall": 0.6939926368274583}, {"timecode": 98, "before_eval_results": {"predictions": ["the Kite Runner", "yves Saint Laurent", "Emily Post", "shepherd", "( Frederic) Chopin", "mulata", "glass", "Bolshoi Ballet", "Mending Wall", "Nathan Lane", "Cheaper by the Dozen", "Ferdinand", "Marlon Brando", "Sagamore Hill", "florida", "Copenhagen", "Arctic", "Hudson", "Blofeld", "sand", "Richard Cory", "Franois Truffaut", "dollface", "one", "Chlorine", "Fidel Castro", "Hanoi", "the Byzantine Empire", "el Ghazal", "Flav", "delmonico's", "nepal", "Hawaii", "Hoffa", "roma", "Cincinnati", "bulldog", "gin", "John Paul Jones", "walk the plank", "Three Amigos", "Halloween", "George II", "stonehenge", "Grease", "Nevada", "a tick", "acid or neutral soil", "halsey", "Bangkok", "arteries", "DeWayne Warren", "Doug Diemoz", "New England Patriots", "Philippines", "micky dolenz", "brash", "Iynx", "400 MW", "high court", "1-1", "\"The U.S. subcontraction out an assassination program against al Qaeda... in early 2006.\"", "the conversion", "uncle"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2008", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-15451", "mrqa_searchqa-validation-9719", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-13423", "mrqa_searchqa-validation-14990", "mrqa_searchqa-validation-9525", "mrqa_searchqa-validation-6036", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-7874", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-7997", "mrqa_searchqa-validation-9594", "mrqa_searchqa-validation-2402", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-7141", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-9807", "mrqa_naturalquestions-validation-8903", "mrqa_hotpotqa-validation-3975", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1424"], "SR": 0.578125, "CSR": 0.5369318181818181, "EFR": 1.0, "Overall": 0.7031676136363637}, {"timecode": 99, "UKR": 0.66796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1547", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3812", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-1021", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11412", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13167", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-13374", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13830", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14826", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15694", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-2620", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-5423", "mrqa_searchqa-validation-5635", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7267", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8297", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9807", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8529", "mrqa_squad-validation-8754", "mrqa_squad-validation-9306", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-218", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2391", "mrqa_triviaqa-validation-2575", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3173", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4585", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-920"], "OKR": 0.8046875, "KG": 0.47109375, "before_eval_results": {"predictions": ["Gunpei Yokoi", "2022", "Thomas Mundy Peterson", "$2 million", "company Mirabilis", "first Sunday after Easter", "acid rain", "The kid then blabs it out when Miley is around, leading Oliver to show her the picture of Jake with another girl", "1943", "Coriolis effect", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "Owen Wilson and Jennifer Aniston as Marley's owners", "1966", "in the transmission", "arm", "England and Wales", "Nicole Gale Anderson", "Massachusetts", "an explosion", "John Goodman", "the episode `` Kobol's Last `` ''", "March 1602", "Liberia", "third", "the Hongwu Emperor", "lead vocalist Bart Millard", "to reduce pressure on the public food supply", "The management team", "Nick Kroll", "Longline", "Valens", "the chest, back, shoulders, torso and / or legs", "a legal case in certain legal systems", "Massachusetts", "proof", "king and parliament", "2017 / 18", "2001", "cadmium", "Jason Momoa", "Phillip Paley", "The Tin Woodman", "July 21, 1861", "Bonnie Aarons", "Aaron Harrison", "in the cell nucleus", "drizzle, rain, sleet, snow, graupel and hail", "Internal epithelia", "Cleveland Indians", "controlled synthesis of materials as thin films ( a process referred to as deposition )", "in Graub\u00fcnden", "Lady Diana", "George Washington", "Lily Allen", "Tatton Park", "Selina D'Arcy", "October 30, 1964", "Ashley \"A.J.\" Jewell,", "42 years old", "step up.\"", "Uruguay", "(Charlie) McCarthy", "William Shakespeare", "HBO World Championship Boxing"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6614869505494505}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.23076923076923075, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.28571428571428575, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.5, 0.15384615384615383, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-7963", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-402", "mrqa_triviaqa-validation-1348", "mrqa_hotpotqa-validation-2069", "mrqa_searchqa-validation-14211", "mrqa_searchqa-validation-872"], "SR": 0.546875, "CSR": 0.5370312500000001, "EFR": 0.9655172413793104, "Overall": 0.6892596982758621}]}