{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_simplecl_lr=5e-5_ep=5_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=5.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_simplecl_lr=5e-5_ep=5_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2150, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.75, "Overall": 0.75}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding", "applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "Calvin cycle", "ships", "Archbishop of Westminster", "coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever", "Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Weinenberger", "only \"essentials", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "driving them in front of the army", "business", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "still be standing", "T cells", "1080i HD", "the state", "30 July 1891", "more wealth than half of all Americans combined", "journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "mutualistic relationship", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7696511243386244}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-7230", "mrqa_squad-validation-8662", "mrqa_squad-validation-4240", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.703125, "CSR": 0.7265625, "EFR": 0.9473684210526315, "Overall": 0.8369654605263157}, {"timecode": 2, "before_eval_results": {"predictions": ["a magnetic field", "photosynthetic function", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Combined Statistical Area", "European Union law", "monophyletic", "General Teaching Council for Scotland", "biochemical oxygen demand", "ditch digger", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "prevented it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs", "St. Johns River", "The increasing use of technology", "10 years", "Batu", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "against governmental entities", "Anglo-Saxon", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding", "Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "south of and including Italy", "1913", "patient compliance issues", "20th century", "become utterly debased", "\"Bells\"", "replace the... white one upon becoming one of these | a geisha", "Abraham Lincoln", "The Sky This Week for September 2 to September 11", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The Basset hound is second only to this dog breed in having the keenest sense of smell", "The Dardanelles", "the Alleged 9-11 Hijackers - Emerald  Within 24h of the attacks, CNN had this first FBI list of 19.", "half the northbound cars wait 90 minutes", "the prehistoric and medieval period", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7053571428571429}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3497", "mrqa_squad-validation-2717", "mrqa_squad-validation-2040", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-7013", "mrqa_squad-validation-6753", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-9895", "mrqa_squad-validation-6361", "mrqa_squad-validation-6658", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.640625, "CSR": 0.6979166666666667, "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor", "Muqali", "member state size", "they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3driie\u015bcie", "oxyacetylene", "9.6%", "Commander", "macrophages and lymphocytes", "kill Luther", "Duncan", "Sharia", "Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "a light-driven method of synthesizing ATP to power the Calvin cycle without generating oxygen", "The Book of Roger", "", "Africa", "Pierre Bayle", "a strain that caused the Black Death is ancestral to most modern strains of the disease", "32.9%", "30\u201360%", "1368\u20131644", "reciprocating", "Pedro Men\u00e9ndez de Avil\u00e9s", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "Parlophone", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25 laps", "to be identified as transgender", "672 km2", "Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "to regain the trust of customers", "Himalayan", "murder"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8112980769230769}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-1780", "mrqa_squad-validation-1009", "mrqa_squad-validation-4019", "mrqa_squad-validation-4631", "mrqa_squad-validation-8872", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5251", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.765625, "CSR": 0.71484375, "EFR": 0.8, "Overall": 0.757421875}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "25-minute", "their captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output (of a total function)", "The Central Region", "March Battle of Fort Bull, French forces destroyed the fort and large quantities of supplies, including 45,000 pounds of gunpowder", "Murray Gold and Ben Foster", "debased", "Super Bowl XLIV", "Urarina", "a domestic scale", "force model that is independent of any macroscale position vector", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Resurgence", "the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "adenosine triphosphate", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm", "Terry and June Medford", "Pritzker Architecture Prize", "arrows", "Common moles", "a complex number raised to the zero power", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a valid passport, Air NEXUS card, or an Alien Registration Card, Form I-551", "a caliper", "\"nucleons\"", "James Hoban", "Amelia Earhart", "1963", "a cricket bat making process", "WrestleMania 34", "The United States of America", "Tuesday's iPhone 4S news", "Charles M. Schulz Museum"], "metric_results": {"EM": 0.625, "QA-F1": 0.709561011904762}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4166666666666667, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-3752", "mrqa_squad-validation-6197", "mrqa_squad-validation-1601", "mrqa_squad-validation-10251", "mrqa_squad-validation-7537", "mrqa_squad-validation-10466", "mrqa_squad-validation-9484", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-2248", "mrqa_searchqa-validation-4355"], "SR": 0.625, "CSR": 0.696875, "EFR": 1.0, "Overall": 0.8484375}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m. weekdays", "dammed", "vaccination", "62", "Maciot de Bethencourt", "Spain", "Carolina's defense", "Cam Newton", "eastwards", "accessory pigments that override the chlorophylls' green colors", "his last statement", "Pleistocene epoch", "he published his findings first", "Nurses", "time and space", "1951", "Marches", "black earth", "Nederrijn", "opposite end from the mouth", "british", "1991", "Kuz nets curve hypothesis", "lost chloroplast's existence", "Schr\u00f6dinger", "90\u00b0 out of phase", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand content", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "near the center of the chloroplast", "cotton spinning", "2010", "psilocybin", "\"Krabby Road\"", "English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Dirkki Farr", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Mim", "Fred Gwynne", "Jenn Brown", "1999 Odisha cyclone", "Fat Albert", "Frontline", "valkyries, norns, and v\u00e6ttir", "Shinola", "modern genetics", "british", "suspend all aid operations", "british", "India"], "metric_results": {"EM": 0.5, "QA-F1": 0.5556885822510823}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-791", "mrqa_squad-validation-1064", "mrqa_squad-validation-9176", "mrqa_squad-validation-5450", "mrqa_squad-validation-7831", "mrqa_squad-validation-7463", "mrqa_squad-validation-10386", "mrqa_squad-validation-3257", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.5, "CSR": 0.6640625, "EFR": 0.96875, "Overall": 0.81640625}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "an immunological memory", "Calvin cycle", "Zhenjin", "specialised education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "voters were supposed to line up behind their favoured candidates", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "The European Court of Justice", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "velocity", "Conservative Party", "international data communications network", "the environment in which they lived", "Darian Stewart", "Great Fire of London", "acular", "Moscone Center", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "successfully preventing it from being cut down", "baptism", "England", "one hundred pennies", "a coffee house", "Parkinson's disease", "Tintin", "piu forte", "inch", "West Germany", "McKinney", "Sarek", "Solomon", "Blackstar", "geomorphology", "Earth", "krokos", "Richmond in North Yorkshire", "Passenger Pigeon", "Richard Wagner", "false teeth", "Debbie Rowe", "Ethiopia", "1973", "The Return of the Pink Panther", "London", "Stephen Graham", "Metro Memphis", "JAKARTA, Indonesia", "\"Late Registration\"", "Pope Benedict XVI", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.578125, "QA-F1": 0.61171875}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-4297", "mrqa_squad-validation-3939", "mrqa_squad-validation-10287", "mrqa_squad-validation-89", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_hotpotqa-validation-426", "mrqa_newsqa-validation-3541", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.578125, "CSR": 0.6517857142857143, "EFR": 0.8888888888888888, "Overall": 0.7703373015873016}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time magazine", "Rhine-kilometers", "14", "150", "North American Aviation", "to register as a professional on the General Pharmaceutical Council (GPhC) register", "the Sovereign", "weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "at elevated partial pressures", "torn down in 1904", "interacting and working directly with students", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "conservative force", "Battle of Fort Bull", "swimming-plates", "eleven", "it would undermine the law", "1332", "separately from physicians", "the south", "Geordie", "fuel", "US$10 a week", "the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds (64 kg)", "1806-07", "spock", "a shed", "Bill Clinton", "police car", "Dead Man's curve", "paris", "geptune", "paris", "paris", "Rookwood", "paris", "Edward R. Murrow", "geldspathic rock", "jedoublen/jeopardy", "jonathan dollar", "paris", "paris", "paris", "Christopher Marlowe", "iPhone", "congruent", "Genoa", "paris", "jonathan kiedis", "paris", "The Federal Reserve", "eight", "Jane Eyre", "World War II", "Hussein's Revolutionary Command Council", "The meter reader", "paris", "March 22"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6471354166666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4166666666666667, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6324", "mrqa_squad-validation-2434", "mrqa_squad-validation-6773", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_newsqa-validation-858"], "SR": 0.609375, "CSR": 0.646484375, "EFR": 0.92, "Overall": 0.7832421875}, {"timecode": 8, "before_eval_results": {"predictions": ["a flour mill Boulton & Watt were building", "every four years", "Alan Turing", "2\u20133", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "17th", "the usual counterflow cycle", "pattern recognition receptors", "climate change in addition to deforestation", "Glucocorticoids", "The Late Late Show", "entertainers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "Torchwood: Miracle Day", "November 1979", "linear", "breaches of law in protest against international organizations and foreign governments", "Cobham", "Alfred Stevens", "Behind the Sofa", "the Simien Mountains", "Florida State University", "MC Hammer", "Mao Zedong", "the Supreme Court", "Hawaii", "the Kiwanis Club", "the log cabin", "Symphony No. 9 in E minor", "a tornado", "George Sand", "\"Z\" and \"J.\"", "the Clinica Regina Margherita", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "lox", "neurotransmitters", "the gas molecules are not connected", "The Princess Diaries", "cotto", "Massachusetts", "larynx", "Dagny Taggart", "Arbor Day", "cinnamom", "the right angle", "Kentucky", "Madison", "the Chinese Exclusion Act", "Supreme Court", "1995", "Harry Nicolaides", "Mineola", "Blender", "2018\u201319 UEFA Europa League"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6549479166666666}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-10067", "mrqa_squad-validation-3310", "mrqa_squad-validation-434", "mrqa_squad-validation-5374", "mrqa_squad-validation-8747", "mrqa_squad-validation-5422", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5814", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-5174"], "SR": 0.578125, "CSR": 0.6388888888888888, "EFR": 1.0, "Overall": 0.8194444444444444}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law said only people established in the Netherlands could give legal advice", "terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Presque Isle (near present-day Erie, Pennsylvania) on Lake Erie's south shore", "William S. Paley", "anaerobic bacteria", "more greenish", "eicosanoids and cytokines", "live", "50-yard line", "captured the mermaid", "1/6", "DC traction motor", "richest 1 percent", "divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops", "1882", "Mel Jones", "Greenland", "Alastair Cook", "Swadlincote", "inversely proportional", "flytrap", "last Ice Age", "Allison Janney", "2026", "Georgia", "disregard for the life and safety of others as to amount to a crime and deserve punishment", "1984", "4 September 1936", "Andrew Moray and William Wallace", "Andrea Brooks", "Pangaea", "Have I Told You Lately", "sinoatrial node", "fourth quarter of the preceding year", "2013 non-fiction book of the same name by David Finkel", "prevent further offense by convincing the offenders that their conduct was wrong", "Bob Dylan", "1977", "judges", "Lynda Carter", "virtually no limit", "substitute", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Courteney Cox", "Tintin", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "People Against Switching Sides", "global greenhouse emissions", "Billy Budd, Billy Budd", "En banc"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7334275793650793}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.888888888888889, 0.6, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-9764", "mrqa_squad-validation-10204", "mrqa_squad-validation-8596", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2507", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-12968"], "SR": 0.609375, "CSR": 0.6359375, "EFR": 0.84, "Overall": 0.7379687500000001}, {"timecode": 10, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.873046875, "KG": 0.45078125, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "Governor Vaudreuil", "pastors and teachers", "Justin Tucker", "1543", "None", "Sierra Freeway", "Switzerland", "unit-dose", "War of Currents", "guardians of the tradition", "continental European countries", "Roger Goodell", "festivals", "9 venues", "Adelaide", "14", "Around 200,000", "Kitty Hawk", "Nidal Hasan", "the University of Maryland", "priest", "the role of Zander in Michael Damian's film, High Strung: Free Dance.", "Consigliere", "DuPont, Washington", "Harry F. Sinclair", "the brewing of beer on a small scale for personal, non-commercial purposes", "December 1974", "2012", "1999", "2004", "Best Sound", "Nelson Rockefeller", "Fort Snelling", "Asif Kapadia", "the State House in Augusta", "1970", "1978", "Barack Obama", "My Cat from Hell", "Richard B. Riddick", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Kal Ho Naa Ho", "Key West, Florida", "the gastrocnemius muscle", "John Roberts", "repechage", "Jean Bernadotte", "two", "Madonna", "Freddie Mercury", "the U.S. Marine Band"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7607772435897435}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-10259", "mrqa_squad-validation-2337", "mrqa_squad-validation-4562", "mrqa_squad-validation-9578", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-674", "mrqa_searchqa-validation-4509"], "SR": 0.6875, "CSR": 0.640625, "EFR": 1.0, "Overall": 0.74484375}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000", "Northumberland", "Beroe", "from tomb and memorial", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308", "Queen Victoria", "large compensation pools", "the main opposition party", "Charlesfort", "Adaptive (or acquired) immunity", "Battle of the Restigouche", "Boston", "force of gravity", "executive producer", "David Lynch", "psychologist", "every year", "Sir Arthur", "blood", "Hong Kong", "ambilevous", "Robin", "a horse", "Burma", "Jim McDivitt", "River Hull", "Tet", "James", "Copenhagen", "Troy", "a non-governmental organisation focused on human rights with over 3 million members and supporters around the world.", "John Gorman", "buffalo", "Edinburgh", "Viking", "Paul Gauguin", "Action Comics", "The Enigma code", "change in the energy of the system", "Novak Djokovic", "New Zealand", "Oasis", "The Golden Girls", "red", "Rajasthan", "The Monkees", "floating ribs", "The G8", "golf", "John F. Kelly", "Bee Gees", "Adelaide", "Edward John", "Sabina Guzzanti", "$13 million global crime ring", "a quark", "gas", "major celebrity with praise"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5476029995331466}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3529411764705882]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5388", "mrqa_squad-validation-4325", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-8421", "mrqa_squad-validation-6449", "mrqa_squad-validation-7887", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.484375, "CSR": 0.6276041666666667, "EFR": 0.7878787878787878, "Overall": 0.6998153409090909}, {"timecode": 12, "before_eval_results": {"predictions": ["convulsions", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Jadaran", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australia", "September 1901", "The United States of America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "James G. Kiernan", "Omega SA", "December 6, 1933", "alternate", "Yasir Hussain", "Malayalam movies", "Kennedy Road", "2002", "31", "Grant Field", "Bill Boyd", "Dr. John Patrick \"Jack\" Ryan Sr. KCVO (hon.), Ph.D.", "Emilia-Romagna Region in Northern Italy", "Buckingham Palace", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Jane", "William Bradford", "four number-one singles in the U.S. within a 12-month period.", "The Lion King", "Gary Ross", "International Boxing Hall of Fame", "The 2000 PGA Championship", "Revolver", "Jack Nicklaus", "repudiation, change of mind, repentance, and atonement", "Mussolini", "Ryan O' Neal", "off the coast of Dubai", "1918", "butyric acid", "Boston", "plover"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6903183621933622}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_squad-validation-6108", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-269", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-5690", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790", "mrqa_searchqa-validation-694", "mrqa_triviaqa-validation-2147"], "SR": 0.640625, "CSR": 0.6286057692307692, "EFR": 0.9565217391304348, "Overall": 0.7337442516722408}, {"timecode": 13, "before_eval_results": {"predictions": ["My Katie is in all things so obliging and pleasing to me that I would not exchange my poverty for the riches of Croesus.", "Fred Silverman", "occupational burnout", "Saudi", "I committed the act of which you accuse me. I don't deny it; in fact, I am proud of it. I feel I did the right thing by violating this particular law; I am guilty as charged", "tentilla (\"little tentacles\") that are covered with colloblasts, sticky cells that capture prey", "$20.4 billion, or $109 billion in 2010 dollars", "twelve residential Houses", "Anglo-Saxons", "Christopher Eccleston recorded special video introductions for each episode", "stricter discipline based on their power of expulsion", "Dane was killed in a horse-riding accident", "1522", "eight", "Of course [the price of oil] is going to rise... Certainly! And how!", "Roman law meaning 'empty land'", "Henry Hudson", "chipmunk", "The Red King", "Melbourne", "Albania", "trout", "Mayflower", "Johnny Weissmuller", "lacrimal", "George Best", "alla capella", "The Great British Bake Off", "Red Lion", "Fenn Street School", "Smiths", "Peter Crouch", "The Nobel Prize in Literature", "Pakistan", "The Observer", "United States", "Big Fat Gypsy Wedding", "beards", "Andes", "Thor", "The Comitium", "Moon River", "Tina Turner", "SW19", "Lancashire", "The Pacific Ocean", "horseback", "Rustle My Davies", "climatic types", "Charlie Brown", "the Roshi", "avocado", "Black Sea", "glucose", "In their history, the Eagles have appeared in the Super Bowl three times, losing in their first two appearances but winning the third, in 2018. They won three NFL Championships,", "The episode typically ends as a cliffhanger showing the first few moments of Sam's next leap", "Abu Dhabi", "Craig William Macneill", "terminal brain cancer", "800,000", "Mount Mazama", "giant slalom", "Serie B ConTe.it", "Saoirse Ronan"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6302825893600819}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true], "QA-F1": [0.41379310344827586, 1.0, 1.0, 0.6666666666666666, 0.4782608695652174, 0.33333333333333337, 0.4, 1.0, 1.0, 0.0, 0.2222222222222222, 0.8, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.07142857142857142, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2516", "mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-3953", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-1215", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-2335", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-9026", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315", "mrqa_hotpotqa-validation-1687"], "SR": 0.515625, "CSR": 0.6205357142857143, "EFR": 0.9032258064516129, "Overall": 0.7214710541474655}, {"timecode": 14, "before_eval_results": {"predictions": ["Ferncliff Cemetery", "The Ruhr", "Hulu", "time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW", "Magnetophon tape recorder", "he explored the mountains in hunter's garb", "Rotterdam", "human inequality can be addressed/corrected, while still not resulting in an increase of environmental damage.", "Charles Dickens", "force", "best teachers", "imperfection", "albatross", "a whirl wind of energy", "save the best for last", "The National Gallery of Art", "netherlands", "water", "Geneva", "russellas", "6", "turkeys", "skyle russell", "goldfish", "William", "a fire", "a light-year", "moznick", "Don Juan", "moqtada al-Sadr.", "Prince of Wales", "cocoa butter", "Violent Femmes", "cereals", "angels", "laser", "James Fenimore Cooper", "Veep", "sparkles", "a pastry-cook", "a boxer", "panda", "Copenhagen", "Madonna", "David Doig", "Madrid", "jackson", "rufino", "climbing", "abuse in asylums", "plasma membrane", "17.5 % of the world's population", "netherlands", "a menorah", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "netherlands", "netherlands"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4880208333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10428", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-9250", "mrqa_searchqa-validation-14628", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-4416", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-7170"], "SR": 0.453125, "CSR": 0.609375, "EFR": 0.9714285714285714, "Overall": 0.7328794642857143}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "infrastructure", "Osama bin Laden", "September 1944", "paramagnetic", "criminal investigations and arrests", "complexity classes", "1963", "Jamukha", "consultant", "711,988", "Mumbai Rajdhan Express", "Speaker of the House of Representatives", "Hugo Weaving", "the passing of the year", "The Dursley family", "( amanda) amanda (us) Feilder)", "the somatic nervous system and the autonomic nervous system", "Shruti Sharma", "Jethalal Gada", "Kevin Sumlin", "tree species", "The United States is the only Western country currently applying the death penalty", "Canada", "two - stroke engines and chain drive", "the English", "a writ of certiorari", "Bill Condon", "Guant\u00e1namo Bay", "exclusive rights granted by a sovereign state or intergovernmental organization to an inventor or assignee for a limited period of time", "the Colony of Virginia", "January 2017", "2013", "Tatsumi", "December 15, 2017", "the Nusaybah family", "Magnavox Odyssey", "The Buckwheat Boyz", "Christianity", "India", "the nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome, which contains enzymes used for penetrating the female egg", "The Occupation of the Ruhr", "Moscazzano", "the stems and roots of certain vascular plants", "Lager", "the most recent Super Bowl champions", "the reverse direction", "San Francisco", "Hal Derwin", "presbyters", "2007", "55 - 75", "Robert Boyle", "the solar system", "Ascona", "Ludwig van Beethoven", "former Boca Juniors", "Akshay Kumar", "Harriet M. Welsch", "(Bob) Zemeckis", "(Bon) nitride", "a centerpiece"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5798448946886446}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 1.0, 0.7499999999999999, 0.0, 0.30769230769230765, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.2222222222222222, 0.5, 0.8, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.5, 0.8, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-6716", "mrqa_squad-validation-6921", "mrqa_squad-validation-6025", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-11316", "mrqa_searchqa-validation-397"], "SR": 0.4375, "CSR": 0.5986328125, "EFR": 0.8888888888888888, "Overall": 0.7142230902777778}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit primes", "the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n", "RCA", "the Venetian merchant Marco Polo", "November 2006 and May 2008", "complex", "temperature that are too cold in northern Europe for the survival of fleas", "Chloroplasts are highly dynamic\u2014they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the leader of the political party or coalition with the most seats", "April 1887", "two hamburger patties American cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun", "Rock Follies", "Montmorency", "\"Brings out the tiger in you, in you!\"", "Elton John", "beer", "Laura Sandys", "a time the problem  is due to credit issues with government debt and not just credit problems", "Corfu", "midrib", "Leopold II", "8 minutes", "the National Industrial Conference Board", "four red stars", "cyclops", "temperature", "Silent Spring", "the value of unknown electrical resistance", "white spirit", "holly", "Harold Wilson", "Denmark", "William (Grant) and -- after a few mini-heartbreaks -- falls in love", "James Mason", "a meteor", "West Point", "the ostrich", "Moby Dick", "William Golding", "the 5th Fret Method", "The Runaways", "Virginia Zvonareva", "Max Bygraves", "the time 4-digit A-road in GB at 53 miles from Long Sutton to Bury St Edmunds", "Nicola Walker", "Virgin", "1949", "Port Talbot", "a time of variable length, decades to thousands of years", "\"The best is yet to come.\"", "Nicola Adams", "Sax Rohmer", "Individuals have legal rights to control information about themselves", "May 2010", "Bruce R. Cook", "James II", "the Obama administration on June 12 announced a task force devoted to federal ocean planning", "the man ran out of bullets", "temperature", "Aerosmith", "dog trainer Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5328361742424242}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false], "QA-F1": [0.6666666666666666, 0.3, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.9166666666666666, 0.2727272727272727, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.1818181818181818, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2666666666666667, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8976", "mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-8105", "mrqa_squad-validation-5001", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-4622", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-2147", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-1537", "mrqa_searchqa-validation-15652", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-4298"], "SR": 0.421875, "CSR": 0.5882352941176471, "EFR": 0.972972972972973, "Overall": 0.728960403418124}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "fort Beaus\u00e9jour in June 1755, cutting the French fortress at Louisbourg off from land-based reinforcements", "journalist", "Seventy percent", "modern hatred of the Jews, cloaking it with the authority of the Reformer", "Germany and Austria", "inclusions (or clasts) are found in a formation", "Sweynforkbeard", "the King", "eight", "State Route 41", "Mickey Mouse", "manly characters", "Spain and Portugal", "may be they separate themselves, sensual, having not the", "Google", "dance", "brothels", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "sauteed Zucchini", "Virginia Woolf", "Vasco da Gama", "canter", "Musculus gluteus maximus", "1972", "Arbor Day", "Countrywide Financial Corp.", "a stop sign", "Conan O Brien", "arthur davis", "gwalior", "( Nikita) Khrushchev", "Other Rooms", "The Night Digger", "wooded mountain range", "russell macgma", "joan davis", "a sepoy", "last", "2013", "submarines", "Joan", "may include rice, noodle and soup dishes in warungs (local)", "Trinidad and Tobago", "Vladimir Nabokov", "may", "Peter Pan", "hama", "a laser beam", "Phi Beta Phi", "Elizabeth Weber", "Numbers 22 : 28", "vaud, Switzerland", "Prince Philip", "Cleopatra VII Philopator", "5.3 million", "pilot", "spc. Megan Lynn Touma", "neptune", "The Clash", "dunder Mifflin Paper Company"], "metric_results": {"EM": 0.359375, "QA-F1": 0.46625087535014004}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.23529411764705882, 1.0, 0.0, 0.5714285714285715, 1.0, 0.2, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-10273", "mrqa_squad-validation-4260", "mrqa_squad-validation-2609", "mrqa_squad-validation-5121", "mrqa_squad-validation-1092", "mrqa_squad-validation-4561", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-2347", "mrqa_naturalquestions-validation-3284", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-5872", "mrqa_newsqa-validation-349", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-6435"], "SR": 0.359375, "CSR": 0.5755208333333333, "EFR": 1.0, "Overall": 0.7318229166666665}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "the surface", "Alfred Stevens", "state intervention through taxation could boost broader consumption, create wealth, and encourage a peaceful, tolerant, multipolar world order", "the twin prime conjecture", "third", "1886/1887", "clerical", "The Apollo", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "schoolteacher and publisher", "the Ruia Group", "David Anthony O'Leary", "a family member", "Tamil Nadu", "Attorney General and as Lord Chancellor of England", "the upper Missouri River", "the fennec fox", "Norwood, Massachusetts", "1993", "the 10-metre platform", "liquidambar styraciflua", "the Battle of Chester", "Flashback", "Tennessee", "Marco Fu", "Francis the Talking Mule", "davis (Derek) Jacobi", "The King of Hollywood", "evangelical Christian", "paternalistic policies enacted upon Native American tribes", "The Hindu Group", "the Kona coast", "1919", "Shakespeare", "2013", "Guthred", "CHIP", "South Australia", "1941", "1912", "the Teatro Carlo Felice", "The Maze Runner", "the pronghorn", "ambassador to Ghana", "Life Is a Minestrone", "the coffee shop Monk's", "Sir Ernest Rutherford", "an arm", "(D Dot)", "France", "20 million to $30 million", "(Ulysses S. Grant)", "(Sidney) Tibbs", "the eventual closure of Guant Bay prison and CIA \"black site\" prisons", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.484375, "QA-F1": 0.585406454248366}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.1111111111111111, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.5, 0.0, 0.5882352941176471, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_squad-validation-9888", "mrqa_squad-validation-9085", "mrqa_squad-validation-8026", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.484375, "CSR": 0.5707236842105263, "EFR": 0.9393939393939394, "Overall": 0.7187422747208931}, {"timecode": 19, "before_eval_results": {"predictions": ["40,000 plant species, 2,200 fishes, 1,294 birds, 427 mammals, 428 amphibians, and 378 reptiles", "swimming-plates", "MHC I", "Executive Vice President of Football Operations and General Manager", "two", "France", "Time", "Nafzger", "Warszawa", "Troggs", "Sch schizophrenia", "Cressida", "Tom Osborne", "Moses", "Inekingese", "In 1956, she became Foreign Minister", "Fiddler on the Roof", "Monopoly", "In 1963 she said, \"I feel as though I'm suddenly on stage for a part I never rehearsed\"", "Poland", "masks", "Alien", "the Tower of London", "reptiles", "Cher", "onion", "Walter Alston", "Bhutto", "Coca-Cola", "Red Bull", "Chaillot", "Ibrahim Hannibal", "butter", "grow a Beard", "The Soup Nazi", "Pyrrhus", "Guatemala", "Treasury", "the Rue Morgue", "huevos rancheros", "August Strindberg", "Sacher Torte", "South Africa's", "Parachuting", "Lovebird", "a diamond", "flavor", "Daisy Miller", "calculators", "death", "Frank Sinatra", "the Sonnets", "South Africa", "In the British Royal Navy", "Pearl Harbor", "M\u00e1laga-Costa del Sol", "Stour", "In 1815, it became the \"Annales de chimie et de physique\"", "gull-wing", "In denying the show's producers the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state", "sexual assault", "1994", "state senators", "38"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5359375}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4264", "mrqa_squad-validation-4730", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-835", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6633", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-542", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.453125, "CSR": 0.56484375, "EFR": 0.9714285714285714, "Overall": 0.7239732142857143}, {"timecode": 20, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.78125, "KG": 0.45234375, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "The Muslims in the semu class", "John W. Weeks", "9th", "inside hospitals and clinics", "US$3 per barrel", "Trajan's Column", "the Mascarene Plateau", "Barbara Anderson Lee", "Golda Meir", "xerophyte", "anions", "Uranus", "George III", "Mickey Spillane", "Iolani Palace", "Gandalf", "Mungo Park", "squash", "Bill Pertwee", "hematite", "Sam Mendes", "the R\u00edo Grande del Norte", "Emeril Lagasse", "\u201cShine\u201d", "Karl Marx", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "four hours", "norway", "Jamaica", "Skylab", "Sydney", "Peter Purves", "Boreas", "Frobisher Bay", "Dumbo", "William Makepeace Thackeray", "Botany Bay", "Peterborough United", "Porto", "albedo", "11", "Washington, D.C.", "red", "a neutron star", "Groucho Marx", "(Virginia Elliott", "duke Francis of Teck", "Algeria", "Juan Carlos I de Borb\u00f3n", "Barry White", "gin", "Peter Chisum", "1966", "guitar feedback", "The LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "Peary", "Simon & Garfunkel", "Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6120907738095238}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-7119", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-922", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.546875, "CSR": 0.5639880952380952, "EFR": 0.8620689655172413, "Overall": 0.6741176621510673}, {"timecode": 21, "before_eval_results": {"predictions": ["spring of 1349", "center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight languages", "foreclosure", "work applications to be developed with Flash", "February 6, 2005", "computers", "159", "an Easter egg", "between the Eastern Ghats and the Bay of Bengal", "1975", "John Vincent Calipari", "winter solstice", "Billie Jean King", "Rudolf Virchow", "rocks and minerals", "October 30, 2017", "to avoid the inconvenienceiences of a pure barter system", "four", "Sardis", "Lykan", "in the pachytene stage of prophase I of meiosis", "West Virginia", "bhuj", "Hank J. Deutschendorf II", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "Emma Thompson", "moral", "May 19, 2008", "Albert Einstein", "May 26, 2017", "1992", "restored to life", "Master Christopher Jones", "transform agricultural productivity", "pamela", "Portuguese", "in the bloodstream or surrounding tissue", "Thomas Bernard", "Fox Ranch in Malibu Creek State Park", "Gibraltar", "Dmitri Mendeleev", "attack by its U-boats", "31", "attack on many characteristics of parties in their own right", "12 times", "local organization of businesses whose goal is to further the interests of businesses", "control purposes", "twelve", "Paige O'Hara", "ghee", "The Crow", "Corinna and seven-time Formula One World Champion Michael Schumacher", "micronutrient-rich", "neptune and husband Bill Klein", "top designers", "hong Kong", "gold", "blackfield Cathedral", "hugh", "liver"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5610279306982577}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.7058823529411764, 0.0, 1.0, 0.6666666666666666, 0.7368421052631579, 0.0, 0.0, 1.0, 0.4615384615384615, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.125, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1723", "mrqa_squad-validation-1449", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-14780", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.453125, "CSR": 0.5589488636363636, "EFR": 0.8857142857142857, "Overall": 0.6778388798701298}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor in wealthier nations", "an intuitive understanding", "evenly round the body", "2,869 young people between the ages of 18 and 24 in a computer-assisted study", "president of NBC's entertainment division", "the Wesel-Datteln Canal", "Melanie Griffith", "a symbol of Roman legions", "lexicographer", "the Islamic Republic of Iran", "One Flew Over the Cuckoo's Nest", "barbecue sauce", "Royal Wives", "the Harpers Ferry", "a Confeitaria Colombo", "the Nun's Priest's Tale", "the House of Bourbon", "Target", "the meadow katydid", "Russia", "Tom Terrific", "magnesium", "The Swamp Fox", "the Civil War", "a German Shepherd", "peanuts", "the Himalayan territorial dispute", "Parker House", "Damascus", "the Jennies", "a laser scanner", "Greg Montgomery", "the 1096 quake", "the Buonapartes", "the Scuppernong grape", "Virginia Woolf", "apogee", "Cherry Garcia", "The wonderful lamp", "Diamond Jim Brady", "axiom", "Princeton", "Eric Knight", "Apple Music", "The Sound of Music", "Pygmalion", "T.S. Eliot", "the Andes Mountains", "Les Sylphides", "Asteroid impact avoidance", "the Nutcracker", "an earthquake", "Labour Party", "the 1996 World Cup of Hockey", "minced meat", "Falstaff", "the reddish pigment pheomelanin", "the Republican Party", "Wojil", "Bangor Air National Guard Base", "1995", "cancer awareness", "12-hour-plus shifts of backbreaking labor", "a meeting with the president to discuss her son. Bush refused to see her, and she abandoned her protests in Texas last year."], "metric_results": {"EM": 0.4375, "QA-F1": 0.5347699175824175}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [0.5714285714285715, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-7541", "mrqa_squad-validation-2194", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-1553", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-9255", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-4236", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-7685", "mrqa_triviaqa-validation-3110", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.4375, "CSR": 0.5536684782608696, "EFR": 1.0, "Overall": 0.6996399456521739}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "the Solim\u00f5es Basin", "seven", "the 21st century", "Mombasa, Kenya", "republicans", "18", "Adidas", "her mom would assess a school test score of 98 with a \"What about those other two points?\"", "body of the aircraft", "museum-worthy pieces", "United States", "Michigan", "on the 24th.", "Two", "Russia", "The Tinkler", "$106,482,500", "Tuesday.", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "a misdemeanor failure to appear in court warrant", "three out of four", "tennis", "Toy Story", "Christmas", "90", "involved in an Internet broadband deal with a Chinese firm.", "$75", "a free laundry service", "Doral", "Jeffrey Jamaleldine", "an attack targeting a mid-level Taliban commander responsible for attacks against Afghan civilians and coalition forces.", "the chaos and horrified reactions after the July 7, 2005, London transit bombings", "more than 1.2 million", "republicans", "citizens", "love", "Multnomah Falls, about 90 miles east", "Seasons of My Heart", "raping and murdering", "150", "Anil", "misdemeanor assault charges", "made it clear he intends to uphold the traditional Catholic teaching on artificial contraception", "allergies to peanuts, nuts, shellfish, peanuts, tree nuts, wheat and soy", "Frank", "a model of sustainability", "Kenyan and Somali governments", "a long-term goal for reducing\" greenhouse emissions.", "motor motorcycle", "the Isthmus of Corinth", "Gavin DeGraw", "Old Trafford", "nirvana", "Bobbi Kristina Brown", "Shayne Ward", "Christina Ricci", "Floyd Nathaniel \"Nate\" Hills", "March", "honey", "John Wadsworth", "emeralds"], "metric_results": {"EM": 0.390625, "QA-F1": 0.48602509836171315}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0625, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.9411764705882353, 0.4, 0.4, 0.0, 0.6666666666666666, 0.0, 0.08, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.10526315789473684, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.390625, "CSR": 0.546875, "EFR": 0.9743589743589743, "Overall": 0.6931530448717949}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "AKS primality test", "Deficiencies", "Gold footballs", "1967", "Dunlop", "XVideos", "South Africa", "Sports Illustrated", "Robert A. Iger", "Regional League North", "November 5, 2002.", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Taipei City", "Minneapolis, Minnesota", "Idisi", "Ambroise Thomas", "Hans Rosenfeldt", "May 4, 2004.", "Everything Is wrong", "Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "imp My Ride", "Columbia Records.", "Q\u0307adar A\u1e8bmat-khant Ramzan", "Derry City F.C.", "Fort Hood, Texas", "Port Macquarie-Hastings", "London", "1999", "2006", "swingman", "Samuel Joel \"Zero\" Mostel (February 28, 1915 \u2013 September 8, 1977)", "October 13, 1980", "the Chechen Republic", "Orpington", "1 May 1926", "Nikolai Alexandrovich Morozov", "1968", "Berthold Heinrich K\u00e4mpfert,", "Girl Meets World", "January 15, 1975", "Pansexuality", "a leopard", "2,463,431", "the churches of Galatia and Phrygia ''", "Narnia", "( phj\u028c\u014b. t\u0255ha\u014b )", "Albion", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Joe Biden", "\"Michael Phelps, partying your face off in public is not the way to reclaim your good guy image.", "Jay Gillespie", "ermine", "Persian Gulf"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6447630494505494}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.4, 0.0, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2699", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-855", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-1797", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.515625, "CSR": 0.545625, "EFR": 0.967741935483871, "Overall": 0.6915796370967742}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "If the water level drops, such that the temperature of the firebox crown increases significantly", "Northern Rail", "South Korea", "paralysis", "discerning", "Romania", "Pocahontas", "Matlock", "Washington", "Chile and Argentina", "The Blue Boy", "Three Worlds", "Liriope", "the Egyptian Goddess of Creation", "Delaware", "Pyrenees", "The Mayor of Casterbridge", "Dutch", "Salem witch trials", "Gryffendor", "Sam Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie", "Richard Walter Jenkins", "(Placename) an inland republic in W Africa", "Jimi Hendrix", "Javier Bardem", "Independence Day", "hydrogen", "Jordan", "So Solid Crew", "Roger Black", "Matthew 2:11", "penult or penultima", "the Bachelor of Science degree", "the Common Ash", "Ian Botham", "squash", "Leander Club", "(born 17 September 1929)", "The Real Miracle of Charlotte's Web", "Poland", "full-contact and similar to other indoor football leagues", "Quintana Roo", "Paradise", "Authority ( derived from the Latin word auctoritas )", "1 mile ( 1.6 km ) in width in several places", "Steve Valentine", "1984 to 1985", "Virgin", "UFC Fight Pass", "Airbus A330-200", "fill a million sandbags and place 700,000 around our city.", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "percipient", "Amelia Earhart", "No Country for Old Men"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6357961558369166}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2, 0.0, 0.0, 0.2857142857142857, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-2266", "mrqa_triviaqa-validation-2393", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5472", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-8995", "mrqa_newsqa-validation-3605", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-5324"], "SR": 0.546875, "CSR": 0.5456730769230769, "EFR": 0.7586206896551724, "Overall": 0.6497650033156498}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "catechism questions", "warszawianka", "the disk", "2016", "Abu Talib", "Mel Tillis", "Pangaea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "eight years after an amendment increased the tenure length by two years", "edd Kimber", "The Jewel of the Nile", "Orange Juice", "a photodiode", "September 9, 2010", "Jesse Frederick James Conaway", "dromedary", "Dan Stevens", "jen chandler", "the Grey Wardens", "1979", "October 27, 2016", "Authority", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Luther Ingram", "Jodie Foster", "John Hamilton", "Sanchez Navarro", "high rates of inflation and hyperinflation", "1936", "British Columbia, Canada", "New York University", "Game 1", "2001", "Washington Redskins", "books of Exodus and Deuteronomy", "September 14, 2008", "Consular Report of Birth Abroad", "Pasek & Paul", "Chicago metropolitan area", "Francisco Pizarro", "1940", "Norman", "Mary Rose Foster", "John Smith", "The eighth and final season", "1623", "neutrality", "he cheated on Miley", "a jenkins", "rarities", "The Rocky and Bullwinkle", "Taylor Swift", "jenkins", "Michael Crawford", "$22 million", "14-day mission", "flooding", "Angostura", "david", "budget"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4993011277937749}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.8, 0.0, 0.125, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.35294117647058826, 0.33333333333333337, 1.0, 0.375, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.20000000000000004, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-1258", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-2872", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-5461"], "SR": 0.40625, "CSR": 0.5405092592592593, "EFR": 0.9210526315789473, "Overall": 0.6812186281676412}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions, demonstrating Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "mathematical models of computation", "Best Supporting Actress", "around 300", "an anvil firing", "2007", "Robert G. Durant", "she is often regarded as the first to recognise the full potential of a \"computing machine\"", "London's West End", "she is generally considered to have liberal political views", "The Hanford Site", "Native American tradition", "Mindy Kaling (the series' star)", "Alonso L\u00f3pez", "private equity", "Ginger Rogers", "nellen hemery", "churros", "City of Onkaparinga", "eastern", "Arsenal Football Club", "Michael B. Barkin", "torpedoes", "1969 until 1974", "skiing and mountaineering", "June 11, 1973", "John Kavanagh", "Protestant Christian movement", "defensive player", "Henry J. Kaiser", "Saoirse Ronan and Billy Howle", "122,067", "Wandsworth, London", "Premier League club Arsenal", "Daniel Andre Sturridge", "USS Essex (CV-9) and \"Air Group 4\"", "Ron Cowen and Daniel Lipman", "Fordyce, also Isabella Hedgeland", "Captain while retaining the substantive rank of Commodore", "david maffei", "Andrzej Go\u0142ota and Tomasz Adamek", "Russell T Davies", "Geraldine Page", "Manchester", "3,000", "Albert II, Prince of Monaco, Umberto II", "Minnesota", "Jeux", "saloon-keeper and Justice of the Peace", "John Lennon", "The International Imitation Hemingway Competition", "The Emperor of Japan", "Mary Rose Foster", "11 February 2012", "changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "nellen", "Australian", "nellenicke's aphasia", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Amanda Knox's aunt", "100,000", "brandy", "nellen and Paul McCartney", "North Carolina"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5348970257173382}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true], "QA-F1": [0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.8, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.15384615384615385, 0.0, 0.33333333333333337, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6875000000000001, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.40625, "CSR": 0.5357142857142857, "EFR": 1.0, "Overall": 0.6960491071428571}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "the USSR", "common flagellated", "Mildred", "Adidas", "Secretary of State Hillary Clinton", "billions of dollars", "one", "the Beatles", "Communist Party of Nepal (Unified Marxist-Leninist)", "Pope Benedict XVI", "around 8 p.m. local time", "Sri Lanka's Tamil rebels", "64", "CNN's Moscow-based Senior International Correspondent Matthew Chance", "at least 12 months", "Michael Griffin, Disney's vice president for public affairs.", "Adriano", "70 percent of his father-in-law's farm", "183", "American Civil Liberties Union", "deployment of unmanned drones", "40", "Liverpool Street Station", "137", "54-year-old", "GAO report", "Jacob", "South Africa", "Markland Locks and Dam", "more than 4,000", "Oaxacan countryside of southern Mexico", "provided Syria and Iraq 500 cubic meters of water", "the Catholic League", "August 19, 2007", "10 years", "all three pleaded not guilty", "Japanese officials", "her children \"have no problems about the school, they are happy about everything.\"", "consumer confidence", "he fired several rounds at the soldiers with the intent of killing them.", "six", "nearly 28 years", "July 18, 1994", "Dan Brown", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "along the Chao Phraya River", "two", "Colombia", "more than 4,000", "bragging about his sex life", "Jean F Kernel", "10 : 30am", "Johannes Gutenberg", "tide-wise", "Christian Wulff", "Ambroz Bajec-Lapajne", "teacher, teachers teacher, speaker and writer", "the George Washington Bridge", "Johns Creek", "Colombia", "Aristotle's lantern", "Colombia Safe White & Light Tuna"], "metric_results": {"EM": 0.421875, "QA-F1": 0.6015073194349511}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5454545454545454, 1.0, 1.0, 0.2857142857142857, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.9090909090909091, 0.9473684210526316, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.5, 0.5, 0.9473684210526316, 0.0, 1.0, 0.0, 0.5, 0.1111111111111111, 0.4, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-4120", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-9007", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-12506", "mrqa_searchqa-validation-4780"], "SR": 0.421875, "CSR": 0.5317887931034483, "EFR": 1.0, "Overall": 0.6952640086206896}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "anchovy", "lovebirds", "Chicago", "monk seal", "Wilhelm II", "an Inquired", "Take me out to the ball game", "an expression used in drinking a person's health.", "\"What hath God wrought\"", "South Island, or Te Waipounamu", "Saint Elmo's Fire", "bach", "H. G. Wells", "a Holstein cow", "illegible", "Scrabble", "Mussolini", "Valkyries", "rain", "bach", "Jodie Foster", "Elysian Fields", "\"Vietnam.\"", "Thomas Edison", "Manhattan Project", "Charles I", "divorce", "Enchanted", "the Liberty Bell", "bach", "bike", "Destiny's Child", "bach", "a robin", "Prednisone", "Margot Fonteyn", "eels", "\"Mac\" McMillan and wife\"", "(Whizzer) White", "77 Sunset Strip", "Galileo Galilei", "Existentialism", "John Donne", "Beijing", "Annie's Song", "murder", "Charles Lindbergh", "a queen", "neurons", "the Holy See", "James W. Marshall at Sutter's Mill in Coloma, California", "a single, implicitly structured data item", "South Korea", "\"Stranger\" or \"foreigner\"", "M*A*S*H TV series", "The Boeing EA-18G Growler", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "e-mails", "HPV (human papillomavirus) vaccine"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6425595238095239}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.8333333333333333, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-935", "mrqa_newsqa-validation-1372"], "SR": 0.53125, "CSR": 0.5317708333333333, "EFR": 0.9666666666666667, "Overall": 0.68859375}, {"timecode": 30, "UKR": 0.669921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.779296875, "KG": 0.4609375, "before_eval_results": {"predictions": ["ten minutes", "1892", "motivated students, ignoring attention-seeking and disruptive students.", "Nikolai Sergeyevich Trubetzkoy", "June 1925", "bushwhackers", "British", "Santiago", "the Baudot code", "Jacksonville", "DTM and its successor \u2014 the International Touring Car Championship", "Switzerland", "Accokeek, Maryland", "Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres from Adelaide station", "1 December 1948", "omnisexuality", "Westfield Plaza", "southwest Denver, Colorado near Bear Creek", "Atlanta, Georgia", "Boston Red Sox, with whom he won the 1996 World Series against the Atlanta Braves, and the Tampa Bay Devil Rays,", "Scunthorpe", "2004", "Donald McNichol Sutherland", "Towards the Sun", "the heart of the southern (Dolomitic) Alps", "Ian Brayshaw", "The impresario", "Islamic Studies", "January 30, 1930", "Lucius Cornelius Sulla Felix", "the Australian folk song \"Waltzing Matilda\"", "the design, development, manufacture and sale of vehicles bearing the Jaguar and Land Rover (including Range Rover) marques", "tempo", "Milk Barn Animation", "Jenson Alexander Lyons", "Timothy Dowling", "London", "Ella Jane Fitzgerald", "Patricia Arquette", "Otto Robert Frisch", "AMC", "31", "Robbie Gould", "Edward Trowbridge Collins Sr.", "Ben Campbell", "twenty-three", "ethnic, cultural or racial group", "A. R. Rahman", "Whoopi Goldberg", "September 8, 2017", "volcanic activity", "on a sound stage in front of a live audience in Burbank, California", "a horse stories of all time", "Werner Heisenberg", "the Kiel Canal", "the strike early Tuesday in Philadelphia, Pennsylvania, shutting down buses, subways and trolleys that carry almost a million people daily.", "Eintracht Frankfurt", "Republican", "a pach", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6262581839562674}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.4444444444444445, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.20000000000000004, 0.4, 1.0, 0.0, 1.0, 0.4, 0.20000000000000004, 0.3157894736842105, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.08695652173913045, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1959", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-2407", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-4173", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_triviaqa-validation-1106", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-6730"], "SR": 0.484375, "CSR": 0.530241935483871, "EFR": 0.9696969696969697, "Overall": 0.6820190310361681}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "The 7.63\u00d725mm Mauser (.30 Mauser Automatic) round", "the Harpe brothers", "French", "1944", "Clara Petacci", "2007", "Marco Hietala", "Shankar", "Cody Miller", "\"Grimjack\" (from First Comics) and \"Firestorm\", \"The Spectre\", and \"Martian Manhunter\"", "video game", "Carson City", "MTV", "Mickey's Christmas Carol", "ten", "New Jersey", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Baron Cherwell", "Rawhide", "member of the Military Band of Hanover", "Don DeLillo", "The Seduction of Hillary Rodham", "balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Whitesnake", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Roseann O'Donnell", "Saturday in May", "Taylor Swift", "Miller Brewing", "The Arizona Health Care Cost Containment System", "Indianapolis Motor Speedway", "Nevada", "Tampa Bay Storm", "Jango Fett", "High Court of Admiralty", "An All-Colored Vaudeville Show", "German", "Lucy Muringo Gichuhi (n\u00e9e Munyiri) ( ) (born 23 September 1962)", "Valley Falls", "dice", "Andrew \" Nick\" Offerman", "Dutch", "JackScanlon", "Leonard Bernstein", "three", "France", "carbonic acid", "secretary or scribe", "eight", "a nuclear weapon", "2005", "Beastie Boys", "Madison", "a Mustang"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6807291666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3276", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-3737", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1394", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.5625, "CSR": 0.53125, "EFR": 0.9642857142857143, "Overall": 0.6811383928571428}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "Dries", "LSD", "Henry I", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "Watson, Jr.", "football", "a multi-user real-time virtual world", "fondu", "South Korea", "1934", "Steve Coogan", "Nicky Marceau", "Boston Marathon", "Carl Smith", "Humble Pie", "Jorge Lorenzo", "the Rabbit", "checkers", "Paul O\u2019Grady", "Arthur, Prince of Wales", "Grail", "Ronald Reagan", "Bergerac", "climate", "Coney Island Old Island Pier", "the Sutton Hoo burial ship", "the esophagus", "Guildford Dudley", "Amoco Cadiz", "John Howard", "Uriah the Hittite", "His Holiness", "12", "Cornell", "Flybe", "The Altamont Speedway Free Festival", "duck", "The Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomato", "the senior-most judge of the supreme court", "Christians", "Representatives", "Machine Gun Kelly", "Central Avenue", "middleweight", "Mokotedi Mpshe", "wood", "comfort those in mourning,", "Canterbury", "Harold Macmillan", "a hole"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5895833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-7632", "mrqa_triviaqa-validation-3007", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4060", "mrqa_searchqa-validation-6833"], "SR": 0.546875, "CSR": 0.5317234848484849, "EFR": 0.9310344827586207, "Overall": 0.6745828435214211}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "early 1938", "Adidas, the brand loved for its sports gear", "Ennis, County Clare", "Stratfor's website", "in the last few months", "Jaime Andrade", "1 percent of children ages 3 to 17", "girls", "the 1,900-acre YFZ ranch, occupied by followers of a polygamist sect", "the island's dining scene", "gasoline", "jenkins", "a Airbus A320-214", "two Emmys for work on the 'Columbo' series starring Peter Falk.", "a dike", "mikey", "abduction of minors.", "lus ticos", "a one-of-a-kind navy dress with red lining", "mosteller,", "Florida", "Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "T.I.", "Pew Research Center", "nirvana", "a nurse who tried to treat Jackson's insomnia in his home", "jerry polis", "race or its understanding of what the law required it to do.", "He won it with a clear strategy that was stuck to with remarkably little internal drama.", "between the ages of 14 to 17.", "lain clarkson", "misdemeanor", "1.2 million", "100,000", "peshawar, Lynne Tracy", "crossfire by insurgent small arms fire,", "2002 for British broadcaster Channel 4", "Noriko Savoie was asked repeatedly in court if she would try to take the children and flee to Japan.", "a \"new chapter\" of improved governance", "luka Modric", "when people gathered outside as the conference in the building ended.", "shelling of the compound", "in the mouth.", "Atlantic Ocean", "movahedi", "Nepal", "Jiverly Wong,", "sexual assault", "the Louvre", "September 21.", "gyback", "oxygen saturations", "The Yongzheng Emperor", "Narendra Modi", "Steve davis", "74", "james Bay", "musical research", "Steve Vincent Buscemi", "Mick Jackson", "West Virginia", "Gary Oldman", "paris"], "metric_results": {"EM": 0.375, "QA-F1": 0.4787372133075258}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 0.125, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.16666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.31250000000000006, 0.8333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-3936", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9569", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7079", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4643", "mrqa_searchqa-validation-44"], "SR": 0.375, "CSR": 0.5271139705882353, "EFR": 0.9, "Overall": 0.6674540441176471}, {"timecode": 34, "before_eval_results": {"predictions": ["most common form of school discipline", "boudins", "Robert A. Heinlein", "Mumbai", "Indiana", "animals", "\"Billie Jean\"", "Laos", "Tristan Farnon", "Westminster Abbey", "Battle of Agincourt", "alicyclic", "King George III", "Kent", "Miss Prism", "Diptera", "a turkey", "transuranic", "Harold Shipman", "River Wyre", "Reno (Wash\u0103 County)", "All Things Must Pass", "Hong Kong", "Mercury", "Doctor Who", "North Yorkshire", "George Blake,", "Nirvana", "Janis Joplin", "Kenya", "St. Mark's", "Moscow", "Caracas", "skin care", "hair", "lacquer", "Adonijah", "John Carpenter\u2019s", "DitaVon Teese", "collapsible support assembly", "republicans", "Argentina", "French", "the 26th,", "glomerulonephritis", "a rabbit", "Rocky Marciano", "The Benedictine Order", "luentry", "June Brae", "Jack Klugman", "four", "1965", "2018", "Qutab Ud - Din - Aibak", "Danny Lebern Glover", "Trey Parker and Matt Stone", "140 to 219 passengers", "Hundreds of militants, believed to be foreign fighters, launched attacks on various military check posts in Pakistan's border with Afghanistan Saturday night and early Sunday morning,", "Democrats", "31 meters (102 feet)", "Monty Python", "Sacramento, California", "The Ryukyu Islands"], "metric_results": {"EM": 0.546875, "QA-F1": 0.619386574074074}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.07407407407407407, 1.0, 0.5333333333333333, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1924", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-4463", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-4166", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3995", "mrqa_naturalquestions-validation-8444", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-4416", "mrqa_searchqa-validation-3920"], "SR": 0.546875, "CSR": 0.5276785714285714, "EFR": 0.7931034482758621, "Overall": 0.6461876539408867}, {"timecode": 35, "before_eval_results": {"predictions": ["1970s", "Aristotle", "daiquiri", "calvary", "armadillos", "The Globe theatre", "Danielle Steel", "Absalom", "molly", "The Goonies", "peter roosevelt", "quito", "river seine", "wine", "Alyssa milano", "bites a dog", "\"The Star-Spangled Banner\"", "The Rolling Stones", "london", "king", "Benjamin Franklin", "peter janda", "a urinal", "moon landing site", "Spain", "Cadillac", "Matt Damon", "peter davie roosevelt", "shalom", "white", "balfour", "dictum", "Easton", "scrabble", "arthur", "peter sak", "a baby", "paul paul caron paul prince", "Stephen Vincent Bent", "Brooke johnson", "pukka", "Nancy Sinatra", "david", "wine noir", "Robert Lowell", "\"Bob ate the pie\"", "Richmond", "duke arthur peteriah davie vowels", "Amy Tan", "piazza San Felice", "pithos", "Grenada", "the Mahalangur Himal sub-range of the Himalayas", "Lava and Kusha", "Heroes and Villains", "cami de Repos", "bauxite", "1", "2015", "October 20, 2017,", "Columbus", "Gustav's top winds weakened to 110 mph,", "piedad cordoba,", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5668402777777777}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4706", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-5293", "mrqa_searchqa-validation-9559", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-245", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-8076", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-16671", "mrqa_searchqa-validation-5487", "mrqa_naturalquestions-validation-7039", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212", "mrqa_newsqa-validation-2307"], "SR": 0.484375, "CSR": 0.5264756944444444, "EFR": 0.9696969696969697, "Overall": 0.6812657828282829}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "Bowe Bergdahl,", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "a Columbian mammoth", "Symbionese Liberation Army", "a floating National Historic Landmark,", "recall", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile", "75", "Iraqi Kids", "veterans struggling with homelessness and addiction.", "CNN", "Kingdom City", "a vast personal fortune", "Ku Klux Klan", "Felipe Calderon", "137", "2-1", "Dancing With the Stars", "a simple song themes about love and loss.", "Michael Jackson", "military commissions", "Venezuela", "John and Elizabeth Calvert", "the Nazi war crimes suspect", "a number of calls,", "Mandi Hamlin", "Iraq", "veterans could be recruited by right-wing extremist groups.", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole,", "pilots have a regulatory duty and professional responsibility to not fly if they know they have a physical or mental condition that makes them unsafe to fly.", "Oklahoma", "ABC's \"Dancing With The Stars\"", "Malawi", "246", "a small child", "six", "people of Palestine", "Nearly eight in 10", "a one-shot victory in the Bob Hope Classic", "the Muslim north of Sudan", "37", "Clifford Harris,", "her sons Matthew and Daniel", "Susan Boyle", "Colorado", "UNICEF", "United States, NATO member states, Russia and India", "27-year-old", "45", "Justin Fletcher", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "from 1993 to 1996", "Ecuador", "Halloween", "Gregor Mendel", "Peter & Jane"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5311371100164204}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.1, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.06896551724137932, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4202", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.453125, "CSR": 0.5244932432432432, "EFR": 0.9714285714285714, "Overall": 0.681215612934363}, {"timecode": 37, "before_eval_results": {"predictions": ["$20 billion", "the Veneto region of Northern Italy", "Preston, Lancashire, UK", "Daniel Auteuil", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie", "1905", "\"All the Way\"", "Yoruba people", "Archbishop of Canterbury", "brother-in-law", "Chrysler", "Portal", "a chronological collection of critical quotations about William Shakespeare", "Terrence Alexander Jones", "fifth", "one", "Evey's mother", "O", "The Grandmaster", "Scotland", "early 1960s", "\"for his contributions to the theory of the atomic nucleus and the elementary particles\"", "Russian Empire", "Cold Spring", "Hilary Duff", "The Ogallala Aquifer", "October 21, 2016, by Streamline and Interscope Records", "fifth", "Everything Is wrong", "Massapequa", "1988", "Dan Bilzerian", "Spitsbergen", "1967", "residential", "Giuseppe Verdi", "band director", "1875", "$10\u201320 million", "Mandarin", "Uncle Fester", "March", "The Frog Prince", "Portuguese holding company", "Los Angeles", "The New Yorker", "Walter Egan", "frustration with the atmosphere in the group at that time", "It was a Confederate victory", "Alison Krauss", "Graham Henry", "earwax", "mental health and recovery.", "the Bronx.", "billions of dollars", "birthstone", "Simon Legree", "Sideways", "pindar"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6331645070615659}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 0.7692307692307693, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-3402", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-767", "mrqa_searchqa-validation-8753"], "SR": 0.53125, "CSR": 0.524671052631579, "EFR": 1.0, "Overall": 0.6869654605263158}, {"timecode": 38, "before_eval_results": {"predictions": ["Turing machines", "Nepal", "Wang Chung", "Panama", "an an ooskeletons", "Thailand", "Mary Kies", "an eXtension", "Georgie Porgie", "Mork & Mindy", "Catherine de Marylandicis", "an equine behavior", "Benito Jurez", "Southern California", "Fort Mann", "INXS", "Longitudes and Attitudes", "an an oxlike head and a long tufted tail", "an Extra-Terrestrial Intelligence", "Arthur", "Jackson Pollock", "Clara Barton", "Nine to Five", "an osmake", "an elk", "Winnipeg", "Anastasio Somoza Debayle", "All My Sons", "Princess Margaret, Countess of Snowdon, and Antony Armstrong-Jones", "1937", "an Alaria", "feminism", "San Diego's House of Blues", "the gallbladder", "The Good Earth", "midway", "Liechtenstein", "Custer", "Mount Gilead", "salt", "Ruth and Leo Steinem", "Catherine de Medici", "Tonga", "Minos", "Gulliver", "alcohol", "Belmont Park Amusement Park", "Coup de grce", "Elle Macpherson", "Richard Gephardt", "Bucharest", "Fawcett", "to function like an endocrine organ", "attack on Pearl Harbor", "negative", "an inch", "Greek, Indian and Muslim savants", "Bishop's House, Ely", "Lowndes", "Northern Rhodesia", "his son-in-law Cleve Landsberg,", "Anjuna beach", "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to his advantage.", "4"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4953125}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-10873", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-5266", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-11995", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-12026", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2836", "mrqa_searchqa-validation-2041", "mrqa_searchqa-validation-13649", "mrqa_searchqa-validation-4651", "mrqa_searchqa-validation-11425", "mrqa_naturalquestions-validation-3692", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-3579"], "SR": 0.390625, "CSR": 0.5212339743589743, "EFR": 1.0, "Overall": 0.6862780448717949}, {"timecode": 39, "before_eval_results": {"predictions": ["over the age of 18", "Nalini Negi", "Blue laws", "1980 Summer Olympics", "IB", "the medulla oblongata", "Andreas Vesalius", "`` The Crossing ''", "Nicole DuPort", "Angus Young", "Palmer Williams Jr.", "late as the 1890s", "prospective studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "Michigan State Spartans", "Franklin and Wake counties", "60", "( / ta\u026a\u02c8t\u00e6n\u026ak / )", "Sally Field", "Elizabeth Dean Lail", "Shastri", "Texas - style chili con carne", "6 March 1983", "David Kaye", "James Arthur", "James Watson and Francis Crick", "Arctic Ocean in the north to the Southern Ocean ( or, depending on definition, to Antarctica ) in the east", "during the American Civil War", "Thomas M Loudon", "slavery", "Sir Ernest Rutherford", "Buddhist", "1889", "parthenogenesis", "on the two tablets", "Buffalo Bill", "$19.8 trillion", "Sleeping with the Past", "boy or girl", "1820s", "Chernobyl Nuclear Power Plant", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "at standard temperature and pressure", "John Ernest Crawford", "during the 2013", "Cathy Dennis and Rob Davis", "1924", "Americans", "`` Chinese '' ( \u4e2d )", "Sedimentary rock", "Carmen", "a waterfowl", "glass", "Rikki Farr's", "Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "in the Gaslight Theater", "Dragnet", "Depeche Mode", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6696969956460845}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 0.0, 0.17391304347826084, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.09999999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.8, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-269", "mrqa_searchqa-validation-1911"], "SR": 0.578125, "CSR": 0.52265625, "EFR": 0.8888888888888888, "Overall": 0.6643402777777778}, {"timecode": 40, "UKR": 0.62890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.75390625, "KG": 0.43671875, "before_eval_results": {"predictions": ["architect or engineer", "Naples", "malaria, dengue fever, yellow fever", "President Jefferson", "Rubik Cube", "kettledrum", "Meringue", "hostage-takers", "a small one-hand axe", "Department of Justice", "Jimmy Doolittle", "John Brown", "# Quiz # Question", "One Hundred Years of Solitude", "Frida Kahlo", "Aziraphale", "Jeeves & Wooster", "Corsica", "the aetherial sphere", "William Pitt the Younger", "Gourmet", "Madonna", "Welterweight", "the yo-yo", "Winston-Salem", "A Streetcar Named Desire", "Edinburgh, Scotland", "Lyme disease", "defensive", "Colorado State Flower", "Italy", "Kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "The Spooderwick Chronicles", "a petition signed by a certain... in 1891, permitting a certain number of citizens to make a request to amend a", "Chicago", "the Great Pyramid", "Herod", "Alaska", "\"more likely to be killed by a terrorist\" than to ever", "South America", "anaphylaxis", "Peter Pan", "Kuwait", "the gmatical number", "the Day of the Locust", "CONTINENTAL DRIFTING", "Charlie Sheen", "Call of the Wild", "Gibraltar", "the National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1923", "Bahrain", "island is El Hiero", "Hans Lippershey", "Sippin' on Some Syrup", "Larry Eustachy", "Isabella II", "Stanford", "Vicente Carrillo Leyva, a leader of the Carrillo Fuentes drug cartel,", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5625032249742001}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.33333333333333337, 0.5, 0.5, 1.0, 1.0, 0.0, 0.5, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.631578947368421, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.8, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-8987", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-8025", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-13067", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-437", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-5758", "mrqa_searchqa-validation-15319", "mrqa_naturalquestions-validation-6197", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-3554"], "SR": 0.4375, "CSR": 0.5205792682926829, "EFR": 0.9444444444444444, "Overall": 0.6569109925474255}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "aurochs", "Israel", "Prince Rainier", "Harper", "Jeff Bridges", "Humphrey Bogart", "honda", "Alan Bartlett Shepard Jr.", "(Antoine) Lavoisier", "(John) le Carr\u00e9", "\"flipping\"", "the Prestonfield Hotel", "island of Hispaniola", "the Zulu warriors", "blood", "Ironside", "Aristotle", "Basil Fawlty", "South Sudan", "Tuesday", "polish", "Lincoln", "the east coast", "(Antoine) Lavoisier", "the NOW Magazine", "the East and North, among them the Alpi Apuane, rich of the renowned white marble, and wide hilly areas throughout the central part, covered with olive plantations, vineyards and sunflowers.", "Battle of the Alamo", "Beaujolais", "Edmund Cartwright", "Der Stern", "(Peter) Rubens", "the popes", "mhoiz", "Barry McGuigan", "Wisconsin", "John Barbirolli", "Eton College", "Harrods", "Charles Dickens", "(Ted)) Hankey", "General Joseph W. Stilwell", "a leaf", "sternum", "Portuguese", "Guerrero", "the islands of Greece", "Ed Miliband", "commitment", "an iron lung", "The Mandate of Heaven", "in the fascia surrounding skeletal muscle", "the man with a face described as looking like the devil - two protrusions emanating from his forehead ( like horns ), eyes burning like'fire in a cave '", "the Distinguished Service Cross", "Indian classical", "1998", "11", "\"an eye for an eye,\"", "Arabic, French and English", "the German Arado Company", "the owl", "Seinfeld", "Cress"], "metric_results": {"EM": 0.5, "QA-F1": 0.6076636904761905}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-2983", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-3792", "mrqa_triviaqa-validation-4630", "mrqa_naturalquestions-validation-7945", "mrqa_hotpotqa-validation-1596", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.5, "CSR": 0.5200892857142857, "EFR": 0.9375, "Overall": 0.655424107142857}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Norman", "George Strait", "Andrew Gold", "1983", "virtual reality simulator", "Banquo", "Pakistan", "during initial entry training", "MFSK and Olivia", "Isaiah Amir Mustafa", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "Paracelsus", "John C. Reilly", "Strabo", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "epidermis", "in serial format in Collier's Weekly magazine ( 27 January -- 16 April 1898 )", "1770 BC", "360", "a single, implicitly structured data item in a table", "after in most cases", "Gunpei Yokoi", "216", "Justin Bieber", "Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "ideology", "160km / hour", "Chinese cooking for over 400 years, most often as bird's nest soup", "Andrew Garfield", "due to not being profitable", "Gibraltar", "electrons", "cut off close by the hip, and under the left shoulder", "Lulu", "ranking used in combat sports", "Tokyo for the 2020 Summer Olympics", "1972", "Virgil Tibbs", "Ray Henderson", "in 1961 during the Cold War", "usernames, passwords, commands and data", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Kate Flannery as Anne", "Lake Wales, Florida", "1923", "Gutenberg", "Wichita", "Tina Turner", "joseph Galliano", "Henry John Kaiser", "Marilyn Martin", "SARS", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "Siemionow", "23 million square meters (248 million square feet)", "neon", "Love Love Love", "the ark of acacia", "island of Basilan"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5582872805299276}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.5, 1.0, 0.45454545454545453, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.15384615384615385, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.0909090909090909, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.3846153846153846, 0.33333333333333337, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.1111111111111111, 0.0, 0.15384615384615385, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-6901", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.4375, "CSR": 0.5181686046511628, "EFR": 0.9722222222222222, "Overall": 0.661984415374677}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "a latte", "Sheffield United", "Apple", "Richard II", "john Wayne", "Scotland", "the Sun", "James Hogg", "Texas", "john Tony Colquitt", "soap", "bulgaria", "Louis XVI", "john john john rossetta Stone", "two", "Jupiter", "Plato", "a chord", "john priest", "Love Panther", "Wilson", "black coal, hard coal, stone coal, dark coal", "Henry II", "darthur", "eukharisti\u0101", "Cubs", "Bear Grylls", "thursdays", "Tanzania", "Val Doonican", "a tittle", "Pyotr Tchaikovsky", "thom Thomas Sankara", "Edward Knoblock", "an elephant", "the CPI or the Creel Committee", "New Zealand", "Mendip", "graffiti", "Jane Austen", "God bless America", "under the UK\u2019s Trade Mark Registration Act 1875", "boxing", "Benjamin Disraeli", "I Wanna Be Like You", "(The Great Leap)", "Jan van Eyck", "prime minister yitzhak Rabin", "Shania Twain", "John Nash", "redox ( both reduction and oxidation occurring simultaneously ) reactions", "meaning", "used as a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "1996 through 2013", "Elvis' Christmas Album", "to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "Robert Park", "eight in 10", "Cairo", "Jackson Pollock", "moose", "tax"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5087301587301587}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.19047619047619047, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-5629", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-722", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1305", "mrqa_searchqa-validation-15709", "mrqa_newsqa-validation-1551"], "SR": 0.4375, "CSR": 0.5163352272727273, "EFR": 1.0, "Overall": 0.6671732954545455}, {"timecode": 44, "before_eval_results": {"predictions": ["1994", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "end of the 18th century", "1935", "Hubert Alyea", "Kauffman Stadium", "concentration camp", "Premier League", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union.", "1995 to 2012", "Ian Curtis", "Rothschild", "republic of China", "lexy", "model", "alternate uniform", "1874", "Citric acid", "North Dakota and Minnesota", "Matt Lucas", "Zambia", "hot shots calendar", "Soren Johnson", "Saint Louis", "Chesley Burnett \"Sully\" Sullenberger III", "Francis", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "joe Floyd Hasselbaink", "Battle of Prome", "35,000", "eastern shore of the Firth of Clyde, Scotland", "first and only U.S. born world grand prix champion", "2015", "20th", "faby Apache", "Lev Ivanovich yashin", "Carrefour", "(Arthur) Currie", "Benjam\u00edn", "UOB Plaza and Republic Plaza", "the first Spanish conquistadors in the region of North America", "the Chickamauga Wars", "12", "Antiochia", "gatwick airport", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County", "honey bees", "squash", "british", "soy", "Nineteen political prisoners", "How I Met Your Mother", "a collapsed ConAgra Foods plant lies atop parked cars Tuesday in Garner, North Carolina.", "Everest", "I.M. Pei", "Florence Nightingale", "the Gibraltar of America"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5577566964285714}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.33333333333333337, 1.0, 1.0, 0.0, 0.125, 0.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2715", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-3252", "mrqa_triviaqa-validation-3878", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16341"], "SR": 0.453125, "CSR": 0.5149305555555556, "EFR": 0.9714285714285714, "Overall": 0.6611780753968254}, {"timecode": 45, "before_eval_results": {"predictions": ["several critical pamphlets on Islam", "Spain", "Samuel", "downtown Oklahoma City", "insulin", "Miene \"Minnie\" Schoenberg", "John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "Sunset Boulevard", "The Duke of Westminster", "Frozen", "perfume empire", "Wyoming", "\u201cblessed\u201d", "The La's", "Javier Bardem", "1", "Lee Harvey Oswald", "microsoft", "Sherlock Holmes", "Bayern Munchen", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine", "Confucius", "Japan", "stewardi(i)", "Beijing", "Christian Dior", "Phoenicia", "Bobby Moore", "The Frighteners", "Jerez de la Frontera", "plac\u0113b\u014d", "MIRACLE AT ST. ANNA", "FC Porto", "'V Vinyl'", "an argument", "Rochdale", "Portuguese", "Madagascar", "Tallinn (   or, ; names in other languages) is the capital and largest city of Estonia.", "The Landlord's Game", "myxomatosis", "Sri Lanka", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "a mid-size four - wheel drive luxury SUV", "Switzerland", "eastern India", "World Famous Gold & Silver Pawn Shop", "high school.", "A Colorado prosecutor", "South Africa", "aves", "ABBA", "Phoenicia", "New York Giants"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5506048387096774}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true], "QA-F1": [0.33333333333333337, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8387096774193548, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-4230", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-4034", "mrqa_triviaqa-validation-2485", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528"], "SR": 0.484375, "CSR": 0.5142663043478262, "EFR": 0.9090909090909091, "Overall": 0.648577692687747}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "Rugby", "a modem", "In 1996 was the 53rd quadrennial presidential election.", "june 12, 1924", "Penn State", "k Karnak", "Berlusconi", "Leviathan", "Mending Wall", "wombat", "an apple", "thunder", "joseph", "Louise de La Vallire (The D'Artagnan Romances, #3.2)", "iTunes", "jesse", "jedoublen/jeopardy", "Romeo", "KLM", "julius cohen", "Wolverine", "an eye", "a goat", "Planet of the Apes", "a knish", "India", "the Reading Railroad", "Leon Trotsky", "cheese", "the Justice Department", "love", "Ignace Jan Paderewski", "jesse cohen", "Charles M. Schulz", "the Chesapeake Bay", "Frida Kahlo", "Jane Austen", "julius", "mutual fund", "polygons", "Tennessee", "lm", "a jena", "China's Tiananmen Square", "The Oresteia", "cinnamon buns", "jesse Erwin Rommel", "Florida Gulf Coast Eagles", "Thomas Mundy Peterson", "USS Chesapeake", "From 1900", "george terrier", "crocodiles, gharials, caimans and alligators", "Hindi", "London", "John Snow", "national team", "soldiers had not gone anywhere they were not permitted to be.", "Afghanistan", "Tuesday", "1955"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5479166666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-7406", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-2185", "mrqa_searchqa-validation-1294", "mrqa_searchqa-validation-4703", "mrqa_searchqa-validation-8988", "mrqa_searchqa-validation-3091", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7238", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-15513", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-5800", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-14657", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-201", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-1216"], "SR": 0.484375, "CSR": 0.5136303191489362, "EFR": 0.9696969696969697, "Overall": 0.6605717077691812}, {"timecode": 47, "before_eval_results": {"predictions": ["Kauai", "The Lord Mayor", "Shel Silverstein", "beers", "a trolley", "Liverpool", "Mount Rushmore", "the Xenophon", "greece", "Jim Bunning", "George Harrison", "The Starfighter", "a woofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "celebrity rehabs", "Darfur", "a bicentennial", "midway", "George Gershwin", "alpaca", "the Atlantic", "Heredity", "The Bicentennial Man", "a rod", "a heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fulgencio Batista", "The Indianapolis 500", "the Twist", "Beryl", "a cuckoo", "Tower St", "aves", "Joan of Arc", "a palindrome", "a quid", "Vanilla Ice", "A Night at the Roxbury", "Steinbeck", "Eric Knight", "Heroes", "the Ganges", "Thomas Mann", "The book of First Chronicles", "Sing Sing", "Rajendra Prasad", "1945", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Bedfordshire", "Charles V", "The Witch", "the Lord's Resistance Army", "In the Pacific Islands", "Netflix", "The Justice Department released a statement late Tuesday afternoon protesting Urbina's order.", "Casa de Campo International Airport", "July in the Philippines", "the greater risk-adjusted return of value stocks over growth stocks"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6834365287490287}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.22222222222222218, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.7692307692307693, 0.5, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-3452", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2504", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3405", "mrqa_hotpotqa-validation-741"], "SR": 0.59375, "CSR": 0.5152994791666667, "EFR": 0.9230769230769231, "Overall": 0.6515815304487179}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "Eva Mendes", "The Hump between India and China", "The Simpsons", "The Incredibles", "a cheetah", "Charlie Brown", "the god Odin", "Japan", "Sea-Monkeys", "daffodils", "\"24\"", "Neil Simon", "Voyager 2", "a gulls", "The Nez Perce", "Eva Peron", "incense", "the Hawkeyes", "The NBA Draft", "Swiffer", "The Huckleberry Hound", "Austria", "The Bourne", "Peru", "The Trojan War", "Atolls", "the Colosseum", "Cambodia", "The New Yorker", "Songs of Innocence", "Uvula", "a catechism of the Council of Trent", "Jacob", "Scrubs", "Cheyenne", "the Black Sea", "The Madness of King George", "Frank Sinatra", "the Zambezi", "a translator", "(2 Samuel)", "The Police", "Jamestown", "American funk rock band", "Robert Ford", "St. Francis of Assisi", "Lemon Meringue", "Herman Melville", "Tarzan & Jane", "(Scott) Zolak", "1919", "eight years", "Taron Egerton", "Batman", "Stieg Larsson", "marriage", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "F-150 pick-up truck.", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6875}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-10269", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-5532", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.671875, "CSR": 0.5184948979591837, "EFR": 0.9047619047619048, "Overall": 0.6485576105442177}, {"timecode": 49, "before_eval_results": {"predictions": ["1942", "The Stonemason's Yard", "Carmen Zapata", "Isles of Scilly", "Israel\u2019s international Ben Gurion Airport", "sexual imagination", "fourteen", "kidney", "apple", "Thierry Roussel", "Novak Djokovic", "Apollo 11", "five", "(Jeanette) Sterke", "John Ford", "tin", "Longchamp", "Nippon or Nihon", "Henry Ford", "a joey", "(Wyoming)", "USS Missouri", "Pyrenees Mountains", "basketball", "Janis Joplin", "Mr. Stringer", "Basketball Association (ABA)", "South Africa", "Rubber Soul", "Ed Miliband", "Scotland", "an aeoline", "Mary Wright Sewell", "Republic of Upper Volta", "Fred Perry", "40", "75 or over", "Queen Victoria", "John Masefield", "Rio de Janeiro", "Hezbollah", "Bengali", "Claire", "Guatemala", "Carousel", "Leicester", "Frank Saul", "radishes", "Holly", "Downton Abbey", "achlais", "Garfield Sobers", "Herman Hollerith", "The BETA game", "Golden Gate National Recreation Area", "Forbes", "The English Electric Canberra", "Ford, Lincoln or Mercury", "a pure meritocracy, and one becomes successful because he or she has a \"big brain.\"", "he was one of 10 gunmen who attacked several targets in Mumbai", "a bubonic plague", "Salsa", "Maria Callas", "Hern\u00e1n Jorge Crespo"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6262152777777779}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4, 1.0, 0.0, 0.2666666666666667, 1.0, 0.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-3654", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1784", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-1091", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-7720", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-3302", "mrqa_searchqa-validation-4559", "mrqa_searchqa-validation-12808", "mrqa_hotpotqa-validation-3207"], "SR": 0.5625, "CSR": 0.5193749999999999, "EFR": 0.8928571428571429, "Overall": 0.6463526785714285}, {"timecode": 50, "UKR": 0.625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.8046875, "KG": 0.44609375, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "the 1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "2018", "Charlie Adlard", "in Koine Greek : apokalypsis, meaning `` unveiling '' or `` revelation '' ( before title pages and titles, books were commonly known by their first words )", "1962", "non-ferrous", "the state sector", "sacroiliac joint or SI joint ( SIJ )", "Joudeh Al - Goudia family", "after World War II", "a child's favourite colour", "The Massachusetts Compromise was a solution reached in a controversy between Federalists and Anti-Federalists over ratification of the United States Constitution", "L.K. Advani", "30 months and for women 18 months", "Jason Marsden", "Charles Lebrun", "Ashrita Furman", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "By the early 1960s", "480 - Maricopa County and parts of Pinal County, east of the Phoenix city limits and the Phoenix neighborhood of Ahwatukee", "the beginning of the American colonies", "2013", "Diego Tinoco", "The Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables", "2004", "Glenn Close", "Cefal\u00f9, Caen, Durham", "Johannes Gutenberg", "Dan Stevens", "Alex Ryan", "Dr. Addison Montgomery", "Carolyn Sue Jones", "Leon Battista Alberti", "a column - like or oval ( egg - shaped ) symbol of Shiva", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Article 1, Section 2, Clause 3", "birch", "a reflex response to food that is in the mouth, and also as a response to the sensation of food within the esophagus itself", "Dolly Parton", "wiliver", "wilash", "Jack Murphy Stadium", "Black Abbots", "Prince Amedeo, 5th Duke of Aosta", "mental health", "Suba Kampong township", "2004.", "larynx", "Moby- Dick", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6254005717630218}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.08333333333333333, 1.0, 1.0, 0.5, 0.5, 0.28571428571428575, 0.0, 0.0, 0.07999999999999999, 0.14814814814814814, 0.6250000000000001, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.10526315789473684, 0.4, 1.0, 1.0, 0.24242424242424243, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.15384615384615383, 1.0, 1.0, 1.0, 0.17391304347826086, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-6810", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-1577", "mrqa_newsqa-validation-3406", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-14676", "mrqa_newsqa-validation-2294"], "SR": 0.53125, "CSR": 0.5196078431372548, "EFR": 0.9333333333333333, "Overall": 0.6657444852941177}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "lagaan ( English : Taxation ; also called Lagaan : Once Upon a Time in India )", "Alicia Vikander", "the person compelled to pay for reformist programs", "Scottish post-punk band Orange Juice", "1837", "Zoe Badwi, Jade Thirlwall's cousin, was supporting the gigs in Australia", "22 November 1914", "Shareef Abdur - Rahim", "2018", "the breast or lower chest of beef or veal", "in the mid - to late 1920s", "in the intersection of Mud Mountain Road and Highway 410, looking southeasterly", "margaret mitchell", "2007", "God's first recitation and inscription", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "15 February 1998", "brothers Henry, Jojo and Ringo Garza", "Thomas Alva Edison", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew", "`` Mirror Image ''", "Article One of the United States Constitution", "E-2s and E-3s", "1603", "Eduardo", "follows a child with Treacher Collins syndrome trying to fit in", "Kansas", "Efren Manalang Reyes, OLD, PLH ( born August 26, 1954 ), nicknamed the Magician and Bata, is a Filipino professional pool player", "the Mayor's son", "the Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee, which now consists of four subcommittees, each of which considers and votes for candidates from a separate era of baseball", "Herman Hollerith", "ulnar nerve", "December 18, 2017", "the Brewster family, descended from the Mayflower", "2015", "Buddhism", "Rodney Crowell", "in Atlanta", "peninsular mainland jutting out into the Mediterranean Sea at the southernmost tip of the Balkans", "21 June 2007", "the chairman ( more usually now called the `` chair '' or `` chairperson '' ), who holds whatever title is specified in the bylaws or articles of association", "Germany", "Gamora", "Darlene Cates", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "Bennett Cerf", "Matt Monro", "Joe Willie Kirk", "margaret nelson", "Vito Corleone", "blood, platelets, and plasma", "Baugur Group", "Venice", "Hyundai Steel", "westlord Opryland", "100 percent", "westminster", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic Tamil minority since 1983."], "metric_results": {"EM": 0.4375, "QA-F1": 0.5732567655942269}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 0.5714285714285715, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.11764705882352942, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9473684210526316, 1.0, 0.2857142857142857, 0.22222222222222224, 0.4102564102564102, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615385, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.7894736842105263, 0.9047619047619048, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7142857142857143]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-407", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208", "mrqa_newsqa-validation-1718"], "SR": 0.4375, "CSR": 0.5180288461538461, "EFR": 0.8055555555555556, "Overall": 0.6398731303418803}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilnius", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Serial (Bad) Weddings", "U.S. Senator, Justin Smith Morrill who authored the Morrill Land-Grant Acts of 1862 and 1890", "the Pacific War", "1949", "\"gunslinger\"", "John Samuel Waters Jr.", "1945", "Sacramento Kings", "S6", "the Magic Band", "Supergirl", "April 1, 1949", "Scottish Premiership club Hearts and the Northern Ireland national team", "Standard Oil", "Donald Bradman", "Anatoly Lunacharsky", "Bob Hurley", "\"Macbeth\"", "Brad Silberling", "1976", "Italy", "Vaisakhi List (Punjabi)", "\"Twice in a Lifetime\"", "7 Series (2012\u2013present)", "Len Wiseman", "1975", "Texas Tech Red Raiders", "Walldorf", "Elvis' Christmas Album", "the sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "\"Thocmentony\", meaning \"Shell Flower\"", "a \"coordinator\"", "Godiva Chocolatier", "Manchester United and the England national team", "Futurama", "Los Alamos National Laboratory", "Russia", "Lush Ltd.", "Telugu", "1952", "a land grant college", "Restoration Hardware", "1942", "Kauffman Stadium", "Matti Smith", "C. H. Greenblatt", "Dave Kelly", "The former confers executive power upon the President alone, and the latter grants judicial power solely to the federal government", "introverted feeling ( Fi ) and Extroverted Intuition ( Te )", "Belgium", "the Big Bopper", "Jackson Pollock", "Alwin Landry's supply vessel Damon Bankston", "about 3,000 kilometers (1,900 miles)", "The Casalesi clan", "Wyatt Earp", "Scrabble", "8 E 3rd St. Wendell", "a leap year"], "metric_results": {"EM": 0.5, "QA-F1": 0.6509272807506503}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.5, 0.5217391304347826, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.8, 0.4, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-2084", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-1835", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-6706", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784"], "SR": 0.5, "CSR": 0.5176886792452831, "EFR": 0.96875, "Overall": 0.6724439858490566}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "The cervical cancer vaccine,", "eight-day", "9-week-old", "American Muslim and Christian leaders", "18", "Darrel Mohler", "Lance Cpl. Maria Lauterbach and her fetus", "Operation Pipeline Express", "admitting they learned of the death from TV news coverage,", "a house party", "full Senate Sotomayor,", "The catamaran and its message has been warmly received. It's hoped the shipping industry -- responsible for 5% of global greenhouse gas emissions,", "Grand Ronde, Oregon.", "a bag", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "14-day", "the fact that the teens were charged as adults.", "Kris Allen", "The United States has an interest in promoting a system of international norms and institutions that averts potential.", "rwanda", "Arsene Wenger", "scored a hat-trick as AC Milan went second in Serie A with a 5-1 win over Torino in the San Siro", "the Genocide Prevention Task Force", "Mohammed Mohsen Zayed", "The U.S. Food and Drug Administration Tuesday ordered the makers of certain antibiotics to add a \"black box\" label warning", "Jacob Zuma", "the return of a fallen U.S. service member", "Sporting Lisbon", "The opposition group, also known as the \"red shirts,\"", "Saturday", "social networking sites", "40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Democratic VP candidate", "Gainsbourg", "Italian club from Barcelona", "three men with suicide vests who were plotting to carry out the attacks,", "between June 20 and July 20,\"", "The Nixon-Medici meeting in Washington", "Jordanian Prince Ghazi bin Muhammad", "Buddhism", "Bollywood superstar Ben Kingsley", "Pakistani territory", "a fight outside of an Atlanta strip club", "\"Britain's Got Talent.\"", "Sen. Barack Obama", "Swamp Soccer", "the man facing up, with his arms out to the side.", "stand down.", "Belfast's Odyssey Arena", "The ACLU", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Bruno Mars", "2018", "surfer", "invertebrates", "white", "November 6, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Beta Monocerotis", "fish", "a crust of mashed potato"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6579960518282886}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6153846153846153, 0.0, 0.4, 0.07692307692307691, 1.0, 1.0, 0.9523809523809523, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 0.0, 0.21052631578947367, 1.0, 0.5454545454545454, 0.30769230769230765, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.14285714285714285, 1.0, 0.6666666666666666, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.36363636363636365, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-3864", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-3097", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-2038", "mrqa_searchqa-validation-12411", "mrqa_naturalquestions-validation-10616"], "SR": 0.546875, "CSR": 0.5182291666666667, "EFR": 0.896551724137931, "Overall": 0.6581124281609195}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou", "Squamish, British Columbia, Canada", "2018", "2004", "in Armenia", "the illegitimate son of Ned Stark, the honorable lord of Winterfell, an ancient fortress in the North of the fictional continent of Westeros", "Michael Bird as Tony Rydinger, a popular boy at Violet's school, a government agent who is responsible for helping the Parrs stay mundane and undercover", "In the 2010 draft, Latavious Williams", "Ren\u00e9 Verdon", "24 judges, against a maximum possible strength of 31", "Jesse Frederick James Conaway", "an an 80 - year - old female", "declared neutrality", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in positions 14 - 15, 146 - 147 and 148 - 149", "In November 2016", "YouTube", "desublimation", "eight", "Anglo - Norman French waleis", "The three wise monkeys", "in lymph", "In the mitochondrial inner membrane", "Kansas", "the eventual Super Bowl champion New England Patriots", "Chesapeake Bay", "Fred Ott", "to refer to a former sexual or romantic partner, especially a former spouse", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "in a Norwegian town circa 1879", "16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins", "geographical location near the Equator", "Development of Substitute Materials", "a pagan custom", "in various submucosal membrane sites of the body", "2013", "John Garfield as Al Schmid", "Ummah ( Arabic : \u0627\u0644\u0623\u0645\u0629 \u0627\u0644\u0645\u0624\u0645\u0646\u064a\u0646 ummat al - mu'min\u012bn )", "C. Sankaran Nair", "the absolute temperature", "no longer a fundamental right", "Robert Gillespie Adamson IV", "18th century", "1998", "The pulmonary arteries", "Norman Whitfield and Barrett Strong", "Henry Moseley", "Hendersonville, North Carolina", "the temporal lobes of the brain and the pituitary gland", "1803", "UPS", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller's Standard Oil Company", "misdemeanor assault charges", "$106,482,500", "introducing legislation Thursday,", "Stone Temple Pilots", "real estate investment trusts (REITs)", "Hubert Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5206583185739541}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true], "QA-F1": [0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.4347826086956522, 0.06896551724137931, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.6666666666666666, 1.0, 0.9411764705882353, 0.0, 0.0, 1.0, 1.0, 0.0, 0.21428571428571425, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6, 0.0, 1.0, 0.625, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.4666666666666667, 1.0, 0.5714285714285715, 0.4, 0.0, 0.0, 0.3333333333333333, 1.0, 0.16, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.36363636363636365, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5330", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-9759", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-3624", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-1887", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.359375, "CSR": 0.5153409090909091, "EFR": 0.8536585365853658, "Overall": 0.6489561391352551}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer,", "John Updike", "Smokey the Bear", "clouds of the most delicate pink", "Jericho", "clean", "asteroids", "\"plankton\"", "In 1876, Rutherford B. Hayes won the election (by a margin of one electoral vote), but he lost the popular vote by more than 250,000 ballots to", "Eleanor Roosevelt", "the War of 1812", "Bangladesh", "success", "medals", "Judd Apatow", "Pulsed Laser", "Jamaica", "Walt Disney World", "Mexico", "Artemis", "pH", "the Aladdin", "9 to 5", "the Beach Boys", "walk the plank", "ice cream", "Mike Huckabee", "catherine the great", "Texas", "constellations", "AILD", "Charlotte", "Ross Perot", "the Black Sea", "C. S. Lewis", "Thomas Paine", "back to the Future", "an antelope", "Anne Boleyn", "Guatemala", "Dizzy", "to push something with your belly", "reasoning", "Fermi", "Icarus", "a suspension bridge", "Tigger", "songs", "the marathon", "QWERTY", "do's and don'ts", "collect menstrual flow", "1787", "cartilage", "spen King", "Victoria", "the recorder", "UFC 50: The War of '04", "newspapers, television, radio, cable television, and other businesses", "March 17, 2015", "4.6 million", "the Dalai Lama", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5811784569597069}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-717", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-755", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-1425", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1245", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.484375, "CSR": 0.5147879464285714, "EFR": 1.0, "Overall": 0.6781138392857142}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "Lorraine", "british", "The Potteries", "Lorraine Chamberlain", "iron", "Little arrows", "Lorraine", "cats", "Lorraine O'Sullivan", "the Republic of the Congo", "Battle of Camlann", "German mathematician David Hilbert", "1905", "british", "british", "british learsen", "Sissy Jupe", "Muhammad Ali", "carbon", "Bass", "M65", "Boxing Day", "cheers", "the Taliban", "alpestrine", "a toad", "to make something better", "bokm\u00e5l", "skirts", "Australia", "Blucher", "Atlas", "Sachin Tendulkar", "a black Ferrari", "River Hull", "Tenerife and Gran Canaria", "South Africa", "bone", "Nutbush", "Lorraine Kray", "Shinto", "Batley", "The Greater Antilles", "malts", "Pluto", "pensioner Jim Branning (John Bardon)", "\u201cfrozen\u201d and then revived in the future", "Fleet Street", "the 3,209ft Lake District peak", "baseball", "President pro tempore", "Athens", "iOS, watchOS, and tvOS", "Leslie James \"Les\" Clark", "the fourth season of \"American Idol\"", "\"Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "the News of the World tabloid.", "propofol,", "Emmett Kelly", "Lorraine", "Lorraine Lorraine", "Can't Change Me"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5453869047619048}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.05714285714285715, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2448", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-1086", "mrqa_searchqa-validation-378", "mrqa_naturalquestions-validation-7270"], "SR": 0.484375, "CSR": 0.5142543859649122, "EFR": 0.9393939393939394, "Overall": 0.6658859150717704}, {"timecode": 57, "before_eval_results": {"predictions": ["against outside influences in next month's run-off election,", "Monday,", "eight-week long", "foxes", "The directive comes as the coalition seeks to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "to fritter his cash away on fast cars, drink and celebrity parties.", "Stratfor,", "we feel very empowered.", "Aravane Rezai", "murder in the beating death of a company boss", "David Beckham", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "kite surfers and wind surfers", "\"Brazil will be playing a bigger role in hemispheric affairs and seeking to fill whatever vacuum the U.S. leaves behind.", "anti-government protesters", "Madhav Kumar Nepal", "Saturday", "both Russian residents and worldwide viewers, in English or in Russian, what they think about Russia's role in the international community.", "Dube, 43, was killed in Johannesburg around 8 p.m. local time Thursday", "11", "slapped out his eye and slashed open his left eyebrows.", "Bush said that both countries should be able to take part in NATO's Membership Action Plan, or MAP, which is designed to help aspiring countries meet the requirements of joining the alliance.", "jubilation at their chance to vote in a post-Hosni Mubarak era during a second day of parliamentary elections,", "the refusal or inability to \"turn it off\"", "Janet Napolitano", "nine newly-purchased bicycles at the scene, and think they were used to carry the explosives.", "The alleged surviving attacker from last month's Mumbai terror attacks", "Pakistan from Afghanistan, and used rocket launchers and machine guns in their attacks.", "the fact that the teens were charged as adults.", "Siri", "dogs who walk on ice in Alaska.", "10 to 15 percent", "Israel", "british", "The incident Sunday evening", "Landry", "Bush of a failure of leadership at a critical moment in the nation's history.", "Alexandre Caizergues,", "stuart gerrard", "three", "the Golden Gate Yacht Club of San Francisco", "a sailboat, named Cynthia Woods, was one of about two dozen boats heading from Galveston, Texas, to Veracruz, Mexico,", "foxes", "Camp Lejeune, North Carolina", "2002", "a cloud of dust marks the trail of a herd of wild horses as they race across the arid plain.", "the job bill's controversial millionaire's surtax, which would increases taxes on those with incomes of more than $1 million.", "seven", "One of Osama bin Laden's sons", "a monocot related to lilies and grasses", "Spain", "Dylan Walters", "2004", "foxes", "Ambassador Bridge", "The University of Liverpool", "Count Schlieffen", "Chillingham Castle", "the 400th anniversary", "River liffey", "scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.375, "QA-F1": 0.4875453099637882}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true], "QA-F1": [0.9333333333333333, 1.0, 0.6666666666666666, 0.0, 0.08695652173913045, 0.0, 1.0, 0.0, 0.0, 0.8333333333333333, 1.0, 0.0, 1.0, 0.25, 0.0, 0.1212121212121212, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.09523809523809522, 0.0, 1.0, 1.0, 0.14285714285714288, 0.07407407407407407, 0.14285714285714288, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.5, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3166", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-2674", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-2911", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212", "mrqa_searchqa-validation-7178"], "SR": 0.375, "CSR": 0.5118534482758621, "EFR": 0.925, "Overall": 0.6625269396551724}, {"timecode": 58, "before_eval_results": {"predictions": ["\"Ted\"", "1,467", "1911", "Nicole Kidman", "14", "the National Basketball Development League (NBDL)", "Gust Avrakotos", "involuntary euthanasia", "Moon shot", "a common pochard", "The Summer Olympic Games", "Miami Gardens", "St. Louis Cardinals", "2007", "the 1993", "University of Vienna", "Jack Ridley", "The Pennsylvania State University", "Willis (Sears) Tower", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australian and New Zealand", "major intersections", "Flex-fuel", "The Savannah River Site", "Minnesota Timberwolves", "Haunted", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "the Mach number (M or Ma)", "James Gay-Rees", "1872", "poetry", "Who's That Girl: Original Motion Picture Soundtrack", "musicologist", "Lauren Alaina", "Prince Amedeo", "Ben Ainslie", "\"Forbidden Quest\"", "non-alcoholic", "paper-based card for competitions and plastic to conceal PINs, where one or more areas contain concealed information which can be revealed by scratching off an opaque covering.", "White Horse", "Andrew Lloyd Webber", "Malayalam movies", "Peter Nowalk", "Annette", "an exultation of spirit", "Major William Lennox and Master Sergeant Robert Epps", "riyadh", "Lady Gaga", "African violet", "three", "The museum was scheduled to open on the 11th anniversary of the September 11, 2001, terror attacks.", "the Carrousel du Louvre,", "A Tale of Two Cities", "The Angel Gabriel", "Braveheart", "( Boss) Tweed"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5603587962962963}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.07407407407407407, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-3935", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-2051", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_hotpotqa-validation-4874", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955"], "SR": 0.484375, "CSR": 0.5113877118644068, "EFR": 0.9696969696969697, "Overall": 0.6713731863122753}, {"timecode": 59, "before_eval_results": {"predictions": ["two reservoirs in the eastern Catskill Mountains", "connotations of the passing of the year", "John Barry", "Aristotle", "largest Greek island in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "2010", "Coroebus", "Anakin", "1952", "iron", "Jesse Frederick James Conaway", "autopistas, or tolled ( quota ) highways", "supported modern programming practices and enabled business applications to be developed with Flash", "Anne Murray", "1957", "actions taken by employers or unions that violate the National Labor Relations Act of 1935", "a four - page pamphlet", "Have I Told You Lately", "2.4 % of the world's land surface area", "second Persian invasion of Greece", "Lana Del Rey", "1979", "season seven", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "Byzantine Greek culture and Eastern Christianity became founding influences in the Arab / Muslim world and among the Eastern and Southern Slavic peoples", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "two Frenchmen", "Felix Baumgartner", "1995", "2026", "Gupta Empire", "Abigail Hawk", "Gary Mitchell", "Pyeongchang", "late 1989 and 1990", "1919", "23 September 1889", "carbon, chlorine, and fluorine, produced as volatile derivative of methane, ethane, and propane", "October 27, 2017", "three levels", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson", "Jack Barry", "headdresses", "Wet Wet", "Andaman & Nicobar Islands", "One Direction", "Delacorte Press", "Drifting", "1927", "Indian film", "Iran", "\"wipe out\" the United States if provoked.", "Celsius", "Washington D.C", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6534726687319772}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.9361702127659575, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.5384615384615384, 0.0, 1.0, 0.0, 0.7142857142857143, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-6837", "mrqa_triviaqa-validation-4980", "mrqa_newsqa-validation-1877", "mrqa_searchqa-validation-14090", "mrqa_hotpotqa-validation-4642"], "SR": 0.546875, "CSR": 0.5119791666666667, "EFR": 0.9310344827586207, "Overall": 0.6637589798850575}, {"timecode": 60, "UKR": 0.63671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.783203125, "KG": 0.453125, "before_eval_results": {"predictions": ["a military coup", "Victor Vector", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "(1963\u201393)", "Mike Holmgren", "2,627", "theScanian War", "Sparky", "Fort Albany", "American", "Virgin", "October 21, 2016", "Kiss", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "High Knob", "Miss Universe 2011", "Maryland", "(2008)", "free expression, freedom of choice, other social freedoms, and \"laissez-faire\" capitalism", "Sami Brady", "French Canadians", "1964 to 1974", "Vanarama National League", "City Mazda Stadium", "the Kriegsmarine of Nazi Germany", "Wes Archer", "June 2007", "Helsinki", "multiple topics targeting fathers who categorize themselves as a \"geek.\"", "Thomas Mawson", "Tony Aloupis", "various", "North Dakota", "Sir Francis Nethersole", "The Panther", "British", "Fainaru Fantaj\u012b Tuerubu", "California State University system", "city of Onkaparinga", "31 October 1999", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "\"Europop\".", "1698", "a progressive radial orientation to a common point", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "Raza Jaffrey", "David Letterman", "for Gallantry", "ArcelorMittal Orbit", "Government Accountability Office", "Brian Smith.", "$50 less,", "high and dry", "An American Tail", "Cats", "Peru"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6466877880184332}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.26666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6451612903225806, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2635", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_newsqa-validation-3315"], "SR": 0.578125, "CSR": 0.5130635245901639, "EFR": 0.9259259259259259, "Overall": 0.6624072651032179}, {"timecode": 61, "before_eval_results": {"predictions": ["Blades", "George Blake", "rita Hayworth", "crayfish", "The Aidensfield Arms", "jeremy", "France", "Manchester", "Creation", "Britten", "Ian Woosnam", "November", "Wonga", "Alan Ladd", "Genghis Khan", "Kofi Annan", "Emanuel Levy", "left side", "Istanbul", "lamb", "Space Oddity", "collie", "35", "copepods", "florida", "Mike Hammer", "Gareth Gates", "Dame Evelyn Glennie", "a heart", "Zaragoza", "David Bowie", "Billy Wilder", "Mr Loophole", "brooches", "4.4 million", "The Post", "Westminster Abbey", "Ralph Lauren", "Pentecost", "Morgan Spurlock", "Oliver", "Debbie Fisher", "Barry Humphries", "cations", "George Santayana", "Rudolf Nureyev", "St. Helens", "cat", "apple", "Argos", "Rodgers & Hammerstein", "at the 1964 Republican National Convention in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater ; Richard Nixon gave that nomination speech", "By 1770 BC", "The United States Secretary of State", "5", "Brad Pitt", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "Roanoke colony", "the Louvre", "Minneapolis", "YIVO"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6135416666666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666665, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6779", "mrqa_triviaqa-validation-600", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-4780", "mrqa_triviaqa-validation-366", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4021", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842"], "SR": 0.515625, "CSR": 0.5131048387096775, "EFR": 0.8387096774193549, "Overall": 0.6449722782258065}, {"timecode": 62, "before_eval_results": {"predictions": ["Curtis James Martin Jr.", "Gabriel Jesus Iglesias", "The Snowman", "bushan paul paul", "Helsinki, Finland", "D' Angelo", "Tommy Cannon", "Scottish national team", "203 people per square kilometer / 526 people", "Wardell Edwin Bond", "Illinois's 15 congressional district", "Rochester", "7,500 and 40,000", "5,112 feet (1,559 m)", "Viacom International Media Networks", "the lead roles of Timmy Sanders and Jack in the series \"Granite Flats\" and film \"King Jack\", respectively.", "perjury and obstruction of justice", "Ralph Richardson", "Sturt", "Big Machine Records", "the title character", "Europe", "Trilochanapala", "deadpan sketch group", "larger than a subcompact car but smaller than a mid-size car", "Spanish", "Algernod Lanier Washington", "14,000 people", "Highlands of Scotland", "37", "Taoiseach of Ireland", "the 137th edition", "Mr. Nice Guy", "Japan Airlines Flight 123", "wrestling", "in Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "barcode", "survival horror", "The United States of America (USA), commonly known as the United States (U.S.) or America ( USA),", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis, Tennessee", "Swiss Confederation has adopted various provisions of European Union law in order to participate in the Union's single market", "New York to the north, and New Jersey to the east", "Sophia Charlene Akland Monk", "Reinhard Heydrich", "\"lo Stivale\" (the Boot)", "Mesopotamia", "14 December", "Woodrow Wilson", "our mutual friend", "lion", "Volkswagen", "Roland S. Martin", "Pope Benedict XVI", "in St. Louis, Missouri.", "pearl", "root beer", "overbite", "Iran of trying to build nuclear bombs."], "metric_results": {"EM": 0.390625, "QA-F1": 0.5574024283008658}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.4, 0.6666666666666665, 0.0, 0.8571428571428571, 0.6666666666666666, 0.3333333333333333, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0625, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.16666666666666666, 0.3636363636363636, 0.75, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4494", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5442", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3356", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-353", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-15884"], "SR": 0.390625, "CSR": 0.5111607142857143, "EFR": 0.9487179487179487, "Overall": 0.6665851076007325}, {"timecode": 63, "before_eval_results": {"predictions": ["India", "Claude Monet", "Brazil", "Jacob Zuma,", "apartment building in Cologne, Germany,", "July", "2005 & 2006 Acura MDX", "Ryan Adams", "80 percent of the woman's face", "BC Place Stadium.", "27-year-old's", "next week", "April 26, 1913.", "7-1", "jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "Swamp Soccer", "Noriko Savoie", "The Falklands, known as Las Malvinas in Argentina, lie in the South Atlantic Ocean off the Argentinean coast", "everyone can use solar and renewable energy at home everyday,\"", "Roger Federer", "tennis", "Three", "in the 1950s", "Gary Player", "1 out of every 17 children under 3 years old in America", "\"Rin Tin: The Life and the Legend\"", "litter reduction and recycling.", "President Bill Clinton", "an average of 25 percent", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Ahmed", "2005", "his son is fighting an unjust war for an America that went too far when it invaded Iraq five years ago", "Tomas Olsson, the journalists' Swedish attorney.", "Israel will release in exchange for two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "Sunday's", "between government soldiers and Taliban militants in the Swat Valley.", "Jamaleldine, a 31-year-old U.S. Army scout who proudly wears a Stetson hat and spurs on his boots,", "The Rev. Alberto Cutie", "all day", "The TNT series", "to try to make life a little easier for these families by organizing the distribution of wheelchair for Iraqi Kids.", "In a court filing for a protective order, Wimunc said that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "recite her poetry", "in the head", "350 U.S. soldiers were beaten, starved, and forced to work in tunnels for the German government.", "neck", "dining scene", "Andrew Garfield", "Philadelphia Eagles", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "gold", "The Mystery of Edwin Drood", "Bligh", "Melbourne", "1998", "(born 23 July 1989)", "Tuesday", "Evian", "Ashbury", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6374093728698415}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.125, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.23529411764705882, 0.9090909090909091, 0.0, 0.8, 0.4, 0.21428571428571427, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.5714285714285715, 0.125, 0.0, 0.3636363636363636, 0.11764705882352941, 1.0, 0.5, 0.0, 0.3636363636363636, 0.7659574468085107, 0.13333333333333333, 0.6666666666666666, 0.11764705882352941, 1.0, 0.8, 1.0, 1.0, 0.4347826086956522, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2995", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-3434", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-2862", "mrqa_hotpotqa-validation-5662", "mrqa_searchqa-validation-5963"], "SR": 0.453125, "CSR": 0.51025390625, "EFR": 0.8857142857142857, "Overall": 0.6538030133928572}, {"timecode": 64, "before_eval_results": {"predictions": ["\"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "George III", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Konstant\u012bns Raudive", "The West Cheshire Association Football League", "Transporter", "1983", "December 13, 1920", "Irish", "265 million", "January 2004", "The Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "The Hawaii House of Representatives", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "Woodsy owl", "Daniel Louis Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian Rush", "John Alexander", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "El Nacimiento in M\u00fazquiz Municipality", "2269", "Holston River Valley", "July 10, 2017", "London", "jazz homeland section of New Orleans", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "George Balanchine", "The Terminator", "Samoa", "\"Bad Blood\"", "Timo Hildebrand", "Univision", "16th-century", "the Upper East Side", "The Statue of Freedom", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE )", "Mexico", "Julie Andrews Edwards", "Captain Mark Phillips", "Democratic VP candidate", "$995,", "\"Nu au Plateau de Sculpteur,\"", "12 bottles of organic vodka", "The Bridges of Madison County", "Thomas Jefferson", "foreign exchange option ( commonly shortened to just Foreign option or currency option )"], "metric_results": {"EM": 0.609375, "QA-F1": 0.672953869047619}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.9166666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-343", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2012", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-4073", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.609375, "CSR": 0.5117788461538462, "EFR": 1.0, "Overall": 0.6769651442307693}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "U.S. Bank Stadium in Minneapolis, Minnesota", "cells", "Indo - Pacific", "leaves of the plant species Stevia rebaudiana", "Field Marshal Paul von Hindenburg", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions forms the joint venture Bullwinkle Studios", "July 31, 2010", "T - Bone Walker", "as the entrance to the 1889 World's Fair", "Bobby Darin", "Alex Skuby", "four volumes were illustrated by E.H. Shepard ( 1927 )", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "denigrating incumbent Democrat Martin Van Buren", "Robert Irsay", "the rate at which soil is able to absorb rainfall or irrigation", "1940", "the pulmonary arteries", "Puente Hills Mall", "1977", "HTTP / 1.1 200 OK", "June 1992", "UK permanent residents that is free at the point of use, being paid for from general taxation", "28 July 1914", "Richard Stallman", "the start", "1894", "by the early - to - mid fourth century", "small marsupials including the endangered sandhill dunnart ( Sminthopsis psammophila ) and the crest - tailed mulgara ( Dasycercus cristicauda )", "Ian `` Dicko '' Dickson", "the final scene of the fourth season", "Auburn Tigers football team", "first attached to the nuclear envelope", "a contemporary drama in a rural setting", "Yuzuru Hanyu", "The Italian Agostino Bassi", "Rachel Sarah Bilson", "plant food, mainly grass and sedges, which were supplemented with herbaceous plants, flowering plants, shrubs, mosses, and tree matter", "Jonathan Cheban", "2005", "computers or in an organised paper filing system", "bicameral Congress", "Missouri River", "sport utility vehicles and off - road vehicles", "March 2, 2016", "Fred Perry", "Reel Life", "Hansel and the girl Gretel", "Get Him to the Greek", "Netflix", "Union Hill section of Kansas City, Missouri", "three", "Rolling Stone", "fifth", "Tina Turner", "Bingo SOLO", "Amsterdam", "salve"], "metric_results": {"EM": 0.46875, "QA-F1": 0.587710237242755}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.13793103448275862, 0.4, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 1.0, 0.2222222222222222, 0.6, 1.0, 0.0, 0.0, 0.9090909090909091, 0.35294117647058826, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-9972", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1515", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-2282", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-5315", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.46875, "CSR": 0.5111268939393939, "EFR": 0.9411764705882353, "Overall": 0.6650700479055258}, {"timecode": 66, "before_eval_results": {"predictions": ["Braille", "bret huggins", "60", "Steely Dan", "Strictly Come Dancing", "hugh h. h. Asquith", "about a mile north of the village of Dunvegan", "hladetina", "call me Ishmael\u2019 from Moby- Dick\u201d (Penzler, Crown, 26)", "the Iron Age", "ciao", "Tallinn", "1925 novel", "The Gunpowder Plot of 1605", "Moldova", "niger", "Edwina Currie", "spain", "IKEA", "p Pablo Picasso", "Some Like It Hot", "Ralph Vaughan Williams", "Tony Blair", "Pickwick", "four", "Caracas", "Ireland", "Ayrton Senna", "Jim Peters", "horse racing", "onion", "Pat Houston", "1948", "an overgrown upper canine tooth, generally the one on the left", "Sikhism", "an anteater", "kabuki", "email", "Zachary Taylor", "indigo", "Thursday", "St. George fighting the dragon", "Swindon Town", "cricket", "Jordan", "Burma", "Northern line between Tottenham Court Road and Warren Street", "hongi", "basketball", "Grumpy", "Italy", "Far Away", "Buddhism", "eukaryotic cell", "Hechingen", "1986", "Charles Lytton", "Eleven people died and 36 were wounded in the Monday terror attack,", "Joe Pantoliano", "Robert Barnett", "Get Smart", "The Bridges of Madison County", "Paraguay", "HackThis Site"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6092261904761904}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.05714285714285715, 0.5, 1.0, 0.4, 0.16666666666666669, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-5239", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-5362", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-5405", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_hotpotqa-validation-1714"], "SR": 0.5625, "CSR": 0.5118936567164178, "EFR": 0.75, "Overall": 0.6269881063432836}, {"timecode": 67, "before_eval_results": {"predictions": ["Yann Martel", "The Archers", "A. Cartier", "bantu", "Cambridge", "b Brunei", "1825", "lorraine", "jeremy parisitosis", "Geoffrey Chaucer", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "james May", "grey squirrels", "Richard Lester", "Buick", "polish", "gooseberry", "no George", "The Butler", "Elizabeth Montgomery", "Il Divo", "Barack Obama", "1984", "U 47 to 4. September the British steamer Bosnia", "China", "nairobi", "Charles I", "Roy Plomley", "Leon Baptiste", "360", "Robert Schumann", "1123", "Mitford", "Sparta", "Hyundai", "thirtieth", "Julian Fellowes", "haddock", "Yemen", "bartertown", "mainland China", "Nowhere Boy", "lincoln", "head and neck", "quant", "Edward laure", "35", "back", "i ymdrechion drwy ei enwebu ar gyfer Gwobr #Gofalu", "Kody and his first wife Meri", "South Asia", "Uralic languages", "New York City", "1942", "a card (or cards) during a card game", "bobby darin", "Noida,", "russell lauredon", "Oakland Raiders", "Mediterranean", "Isabella", "Turing"], "metric_results": {"EM": 0.5, "QA-F1": 0.5992559523809524}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-186", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-1662", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-1501", "mrqa_naturalquestions-validation-5787", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508", "mrqa_searchqa-validation-475"], "SR": 0.5, "CSR": 0.51171875, "EFR": 0.78125, "Overall": 0.633203125}, {"timecode": 68, "before_eval_results": {"predictions": ["bark commands or ask questions to the phone,", "Philippines", "heavy turbulence", "Brian Smith", "Tim Clark, Matt Kuchar and Bubba Watson", "first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.\"", "people with ties to the U.S. Consulate in Ciudad Juarez, Mexico,", "Elin Nordegren", "\"We Found Love\" in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "immediate release into the United States of 17", "millionaire's surtax,", "on the set at \"E! News\" on Tuesday.\"", "about 50", "two-state solution", "Yusuf Saad Kamel", "the foyer of the BBC building in Glasgow, Scotland", "his father,", "Silvan Shalom", "South africa", "the insurgency,", "Section 60", "\"The Rosie Show,\"", "Ricardo Valles de la Rosa,", "March 24,", "completely changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\" in 1995,", "mouth.", "about 100", "Frank,", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio", "The father of Haleigh Cummings,", "off the coast of Dubai", "Gulf of Aden,", "10 municipal police officers", "businesses hiring veterans as well as job training for all service members leaving the military.", "general astonishment", "in a remote part of northwestern Montana before being gunned down itself,", "launch", "without bail", "February 12", "general astonishment", "The move frees up a place for another non-European Union player in Frank Rijkaard's squad.", "Chile", "separated in June 2004 when the boy's Brazilian mother, Bruna Bianchi Carneiro Ribeiro, told Goldman -- to whom she was then married", "Democratic VP candidate", "martial arts,", "Monica Lawson", "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "June 6, 1944,", "devastating impact on the city's population causing enormous suffering and massive displacement,\"", "nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "the Eurasian Plate", "horses", "k Kathryn C. Taylor", "Brooklyn", "anabolic-androgenic", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "zanzibar", "dualism", "Wordsworth"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5679853142353142}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.23999999999999996, 0.0, 1.0, 0.2727272727272727, 0.4444444444444445, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.625, 0.0, 0.0, 1.0, 0.0, 0.375, 1.0, 0.23076923076923075, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.9189189189189189, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-885", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-494", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1789", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-3117", "mrqa_searchqa-validation-5208"], "SR": 0.4375, "CSR": 0.510643115942029, "EFR": 0.9166666666666666, "Overall": 0.6600713315217391}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "Americana Manhasset", "Mayfair", "Minister for Social Protection", "28 January 1864, Halifax, Yorkshire, England", "Arab", "Doggerland", "Larry Richard Drake", "The Bad Hemingway Contest", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "\"Eternal Flame\".", "\"Back to December\"", "Heather Elizabeth Langenkamp", "two Nobel Peace Prizes", "Derry", "Daniel Craig", "Hamburger SV", "\"Four Weddings and a Funeral\"", "Eisstadion Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "late 12th Century", "Christopher McCulloch", "novel", "\"The Krypto Report\"", "Fort Saint Anthony", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "Japan", "1919", "Tak and the Power of Juju", "the western end of the National Mall in Washington, D.C., across from the Washington Monument", "Len Wiseman", "Stephen Crawford Young", "Lynyrd Skynyrd", "Gerry Adams", "\"Kill Your Darlings\"", "Girls' Generation", "Robert Matthew Hurley", "September 1901", "Tuesday", "anabolic\u2013androgenic steroids", "Cheshire, North West England", "Division I", "\"Kismet\" (1953)", "Kentucky", "1961", "1896", "2000", "Donald Sterling", "20 - year period", "Saint Peter", "mining", "Earth", "the best value diamond", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu", "Russian bombers", "Juilliard School", "\"l Wizard-hipped\" dinosaurs", "Boy Scouts of America", "Inuit"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7537763680033416}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.4444444444444444, 0.5, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-375", "mrqa_triviaqa-validation-984", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.59375, "CSR": 0.5118303571428571, "EFR": 0.9615384615384616, "Overall": 0.6692831387362637}, {"timecode": 70, "UKR": 0.66796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.7734375, "KG": 0.453125, "before_eval_results": {"predictions": ["Arkansas", "early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "\"From Here to Eternity\"", "12", "port city of Aden", "Scott Eastwood", "United States", "Patricia Valeria Bannister", "David Bautista", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Queensland", "\"master builder\" of mid-20th century New York City", "Honolulu", "Eureka", "The Iveys", "performances of \"khyal\", \"thumri\", and \"bhajans\"", "XVideos", "the Salzburg Festival", "political correctness", "devotional literature", "Martin O'Malley", "1891", "the Secret Intelligence Service", "Currer Bell", "University of Nevada", "The first stories appeared in ancient Assyria", "850 m", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Bohemian", "season nine", "Hanna", "Manchester Victoria station", "Pete Wareham", "My Love from the Star", "Captain James Cook", "George I", "Kye Bumzu", "37", "bass", "conservative PAC Citizens for a Sound Economy", "March 12, 1933", "H CO", "prophets", "Bill Russell", "Andre Agassi", "georgia", "Phillies", "completed flood preparations", "Caster Semenya", "to ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "Cuyahoga River", "uranium", "Peter Sellers", "river Elbe"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6844866071428571}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-3675", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-4446", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.5625, "CSR": 0.512544014084507, "EFR": 1.0, "Overall": 0.6814150528169014}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter", "stabbed Tate, who was 8\u00bd months pregnant, and wrote the word \"pig\" in blood on the door of the home", "Russian air force,", "female soldier", "Nearly eight in 10", "Goa", "Iran to Nazi Germany", "increased Dubai's shoreline by 100 percent", "Kenyan and Somali governments", "Susan Atkins", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "Helmand province,", "National September 11 Memorial Museum", "Harlem, New York.", "\"I don't think I'll be particularly extravagant.\"", "in last year's Gaza", "woman", "1959.", "his car,", "269,000", "issued his first military orders as leader of North Korea", "Apple co-founder", "a group of teenagers.", "Six", "Michael Jackson", "27-year-old's", "outside influences in next month's run-off", "sunstroke", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "combat veterans", "$1.5 million", "resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia and India", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews,", "Fiona MacKeown", "Sen. Barack Obama", "A.J. Jewell", "a motor scooter", "Israeli forces were responding to militant fire near the complex.", "Guinea, Myanmar, Sudan and Venezuela.", "beetles", "Sudanese orphans", "Aspirin", "February 28 or March 1", "Indo - Pacific", "mining and in the pits: initially metalworkers and, later, electricians", "Montezuma", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "oxys", "the Bird of Prey", "Harry Truman"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5965082833832834}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.24, 1.0, 0.0, 0.0, 1.0, 0.15384615384615383, 0.5, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.19999999999999998, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4163", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-933", "mrqa_naturalquestions-validation-4809", "mrqa_triviaqa-validation-2418", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.484375, "CSR": 0.5121527777777778, "EFR": 0.7878787878787878, "Overall": 0.6389125631313132}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997 ( Act No. 33 of 1997 )", "Sharyans Resources", "drives on all four wheels", "U.N. Owen '' ( i.e., `` Unknown '' )", "new coach Kevin Sumlin from 2012 to 2013", "Stromal connective tissue to maintain tissue / organ function", "the Old Testament", "Anatomy", "a maritime signal", "President Lyndon Johnson", "Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Olivia Olson", "Eukarya -- called eukaryotes", "Mara Jade", "Katherine Allentuck and Christopher Norris as a pair of girls whom Hermie and Oscy attempt to seduce", "determine the tenderness of meat", "Edward IV of England", "Ashrita Furman", "A 30 - something man ( XXXX )", "Jean Fernel", "2007 and 2008", "May 1980", "erosion", "an English occupational name for one who obtained his living by fishing or living by a fishing weir", "for nine weeks on the Billboard Hot 100 chart in 1960", "Ronald Reagan", "Ireland", "revenge and karma", "the misuse or `` taking in vain '' of the name of the God of Israel", "English and Wales Cricket Board ( ECB ) in 2003 for the inter-county competition in England and Wales", "in 1996", "15,000 BC", "Idaho", "early Christians of Mesopotamia", "eight hours ( UTC \u2212 08 : 00 )", "Dr. Rajendra Prasad", "Carlos Alan Autry Jr.", "Jay Baruchel", "Anthony Caruso as Johnny Rivers", "merengue", "Butter Island off North Haven, Maine in the Penobscot Bay", "1800 to 1850", "in 1903 and set in Yukon, Canada during the 1890s Klondike Gold Rush, when strong sled dogs were in high demand", "HTTP Secure ( HTTPS ) is an extension of the Hypertext Transfer Protocol ( HTTP ) for secure communication over a computer network", "3", "1939", "BBC", "the fifth studio album by English rock band the Beatles", "in all land - living organisms, both alive and dead", "Felicity Huffman", "jeremy glendower", "75", "J27", "Montana State University", "Sun Valley, Idaho", "president of Guggenheim Partners", "bikini atoll, the island in the Pacific where the U.S. had tested the atom bomb.", "doctors", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "the Amazon", "an online education management platform", "The Crow", "asylum in Britain."], "metric_results": {"EM": 0.4375, "QA-F1": 0.6026972838453817}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 0.2857142857142857, 0.5, 0.0, 0.0, 0.0, 0.2, 0.0, 0.6666666666666666, 0.3076923076923077, 1.0, 0.13333333333333333, 1.0, 0.5, 1.0, 0.2222222222222222, 0.8571428571428571, 1.0, 1.0, 0.4, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 0.18181818181818182, 1.0, 0.4, 1.0, 0.0, 0.7499999999999999, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.23076923076923078, 0.4, 0.6086956521739131, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-4656", "mrqa_triviaqa-validation-5787", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-3504", "mrqa_searchqa-validation-3477", "mrqa_searchqa-validation-10906"], "SR": 0.4375, "CSR": 0.5111301369863014, "EFR": 0.9722222222222222, "Overall": 0.6755767218417047}, {"timecode": 73, "before_eval_results": {"predictions": ["Venezuela", "Fall Guy", "crown", "Dr. Maria Montessori", "the Kinsey Millhone", "Arthur George Walker", "a science fiction novel", "March of the Penguin", "Patrick Ewing", "Fletcher", "an ambulance", "Condoleezza", "Pakistan", "China", "liquor", "Texas", "a Conductor", "John James Audubon", "Pontius Pilate", "germany", "synapses", "the half-pipe", "the Bitch", "\"carnaval\"", "Freakonomics", "George Washington", "the Carboniferous Period", "Champagne", "Red Heat", "New Orleans", "the Dominican Republic", "a carrel", "Love potions", "duke of Cambridge", "Sir Arthur Conan", "leaf", "Orion", "India", "carbon monoxide", "king john", "plug in", "the Abominable", "south Japan", "manslaughter", "computer programming", "Tennessee River", "Hipparchus", "Billy Idol", "their own", "a Rat", "Tom Hanks", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, thyroid, breast, lung, salivary glands, eye, and skin", "$689.4 million in North America and $1.528 billion in other countries", "on the left hand ring finger", "Conrad Murray", "Ravenclaw", "Czech Republic", "in Sochi, Russia", "two years", "Manchester\u2013Boston Regional Airport", "President Obama", "two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "the government.", "January 2000"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6281974969474969}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9047619047619047, 0.15384615384615385, 0.888888888888889, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-9337", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-16907", "mrqa_searchqa-validation-10889", "mrqa_searchqa-validation-13255", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-12290", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-2145", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-14970", "mrqa_searchqa-validation-10515", "mrqa_searchqa-validation-15757", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10093", "mrqa_hotpotqa-validation-4076", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.484375, "CSR": 0.5107685810810811, "EFR": 0.9696969696969697, "Overall": 0.6749993601556101}, {"timecode": 74, "before_eval_results": {"predictions": ["sugar", "( Angela) Rippon", "Anna Eleanor Roosevelt", "liver", "Private Eye", "the Rock", "Jack Ruby", "the 1500 meter event", "British Airways", "economics", "(A417)", "Pete Best", "Bonnie and Clyde", "avatar", "Santiago", "St Moritz", "Edmund Cartwright", "a Par-5", "(John) Hesiod", "Japanese silvergrass", "April", "(later Sir Arthur) Conan Doyle", "Wolfgang Amadeus Mozart", "bees", "Sun Hill", "Nutcracker", "Flyweight", "Adare", "Sesame Street", "photography", "Leslie Perowne", "Samuel Johnson", "Arts & Nature", "(Dennis) Weaver", "Ganga", "tabloid", "a control arm", "Melbourne", "the Odeon", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "the Crusade against the Albigenses", "Dame Kiri Te Kanawa", "Churchill Downs", "Upstairs Down stairs", "One Direction", "ulnar nerve", "Gibraltar", "111", "Merck Sharp & Dohme", "Pool F", "Vietnam War", "\"It feels great to be back at work,\"", "prisoners' rights and better conditions for inmates,", "after Wood went missing off Catalina Island,", "When Harry Met Sally", "Breckenridge", "The Fray", "President Clinton"], "metric_results": {"EM": 0.5625, "QA-F1": 0.619169061302682}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.13793103448275862, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-7620", "mrqa_triviaqa-validation-4938", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-7365", "mrqa_hotpotqa-validation-4763", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621"], "SR": 0.5625, "CSR": 0.5114583333333333, "EFR": 0.9285714285714286, "Overall": 0.6669122023809524}, {"timecode": 75, "before_eval_results": {"predictions": ["Robert FitzRoy", "Martin O'Neill", "2012", "3730 km", "Kind Hearts and Coronets", "Massachusetts", "Nippon Professional Baseball", "hiphop", "film", "Chione and Poseidon", "Brendan O'Brien", "John Churchill", "Sir William McMahon", "Hopi", "south kesteven", "Nashville", "Anne No\u00eb", "Tim Cluess", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Eunice Kennedy Shriver", "NXT Tag Team Championship", "Chinese coffee", "Love and Theft", "Adelaide", "4145 ft above mean sea level", "University of Georgia", "1 million", "Indian", "Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "William Randolph Hearst", "1 April 1985", "Arnold", "J. Cole", "Idisi", "The Books", "port of Mazatl\u00e1n", "Danish", "London, England", "1985 till 1988", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services", "Reese Witherspoon", "Koch Industries", "Billy J. Kramer", "Mindy Kaling", "1990", "September 21, 2016", "12 '' x 12 '' attached giant - sized booklet with state - of - the - art photography of the band's performance and outdoor session pictures", "earache", "Diego Garcia", "a peacock", "$2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site,", "alcove.", "injected with drugs by ICE agents against his will.", "Patrick", "( Russ) Brown", "Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6141264235957541}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.5, 0.4, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.23529411764705882, 0.0, 0.9473684210526316, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4239", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-5070", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-1393", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-4030"], "SR": 0.515625, "CSR": 0.5115131578947368, "EFR": 0.9032258064516129, "Overall": 0.66185404286927}, {"timecode": 76, "before_eval_results": {"predictions": ["Pet Sounds", "Inverness", "\u201cArs Gratia Artis\u201d", "Johann Strauss II", "James Callaghan", "cork", "Japan offshore market rates", "Dublin", "Pyrenees", "leprosy", "left", "Bill Kerr", "avocado", "Catherine of Aragon,", "The Double", "Relpromax Antitrust Inc.", "Supertramp", "Saturn's Hula-Hoops", "Octavian", "\"One Night / I Got Stung\"", "Heston Blumenthal", "Arkansas", "IT Crowd", "Some Like It Hot", "Mr Loophole", "Ernest Hemingway", "Wolf Hall", "No. 18 seed Ernests Gulbis", "( Alberto) Juantorena", "graffiti art", "Friedrich Nietzsche", "caffari", "cheese", "Annie and Clarabel", "kirstiania", "xylophone and piano player", "Moby Dick", "moss", "A History of Scotland, Vikings and Coast", "Friends Like You", "pea,", "Dr Tamseel", "the Sea of Galilee", "200", "Helen of Troy", "Alzheimer's disease", "The Firm", "1966", "an even break", "31536000", "Jordan", "arthropods, molluscs, roundworms", "desperation", "1933", "Boston Red Sox", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "Argentine coach Diego Maradona has urged Carlos Tevez to quit Manchester United at the end of the season and head for Italy.", "Rev. Alberto Cutie", "Michelle Obama", "alto trombone", "270", "place", "the American Red Cross"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5150229978354979}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.4, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-1759", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-6991", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-14139"], "SR": 0.4375, "CSR": 0.510551948051948, "EFR": 0.8055555555555556, "Overall": 0.6421277507215007}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player, whose course design company has been working on projects in the region.", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "The Kirchners", "iPods", "45 minutes, five days a week.", "killed wife Christine while they were on vacation in 2008, be dropped due to a \"unique set of circumstances.\"", "Conway", "2nd District of Colorado", "it's believed they still have other clandestine nuclear sites", "Zimbabwe", "Harry Nicolaides, 41, was arrested last August over his 2005 book titled \"Verisimilitude.\"", "a woman and her colleague were shot until she placed the 911 call,", "April 2010.", "skull,", "\"to be one of his four children and know that is there for the world to see,", "environmental", "father's parenting skills.", "Iran", "head injury.", "\"Antichrist.\"", "African National Congress Deputy President Kgalema Motlanthe,", "Hugo Chavez", "seven", "Frank's diary.", "The Lost Symbol", "Matthew Fisher,", "Rawalpindi", "Tim Masters,", "Helmand province, Afghanistan.", "climatecare,", "removal of his diamond-studded braces.", "Ennis, County Clare", "United States, NATO member states, Russia", "in Arabic, Russian and Mandarin that led police to 86 suspects in a series of raids that started Tuesday,", "Hamas,", "House Page Board, along with Republican Rep. Shelley Moore Capito of West Virginia.", "40", "four", "Courtney Love,", "84-year-old", "does not involve MDC head Morgan Tsvangirai.", "three", "undergoing renovation.", "Naples home.", "Hanford nuclear site,", "November 26,", "sportswear,", "Beijing", "protest child trafficking and shout anti-French slogans", "get better skin, burn fat and boost her energy.", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy", "meditation and acceptance practices on a regular basis as well as before and during competition", "harishchandra", "India and Pakistan", "allergic", "a lie detector", "chords from the second half of this song", "1963", "the \"Black Abbots\"", "a nurse bag", "south africa", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.5, "QA-F1": 0.6081656497097674}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true], "QA-F1": [0.2666666666666667, 0.15384615384615383, 1.0, 1.0, 1.0, 0.11538461538461539, 0.0, 0.0, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.18181818181818182, 1.0, 1.0, 0.5, 0.0909090909090909, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.14285714285714285, 0.15384615384615383, 0.8823529411764706, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-489", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877", "mrqa_searchqa-validation-8963"], "SR": 0.5, "CSR": 0.5104166666666667, "EFR": 0.96875, "Overall": 0.6747395833333334}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O\"", "Silk Road", "Denmark", "George Rogers Clark", "NaCl", "a coach dog", "Sweden", "Volleyball", "John Alden", "Ghost World", "Deuteronomy", "Compass Rose", "Japan", "Madison Avenue", "Job", "standard pitch", "Art Deco-style", "Spider-Man", "Lumbini, Nepal", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "lieutenant colonel", "National Archives Building", "Nostradamus", "Madrid", "3:10 to Yuma", "Antarctica", "Ian Fleming", "the Southern Christian Leadership Conference", "Moscow", "Boss 429", "art of Portraiture", "the Mormon Tabernacle Choir", "The Scarlet Letter", "Mason", "Bangkok", "Dixon's Ferry", "positron", "President Lyndon B. Johnson", "Jefferson", "Jerusalem", "Pushing Daisies", "cranberry", "tzatziki sauce", "Ch'iu", "Service Employees International Union", "charlotte russe", "canali", "Abraham", "a self-appointed or mob-operated tribunal", "Iran", "Rachel Kelly Tucker", "makes Maria a dress to wear to the neighborhood dance", "London", "Kermadec Islands", "Julius Caesar's", "\"Phrygian Ida\"", "\"The Danny Kaye Show\"", "2012", "CNN's best ten golf movies ever made", "the conversion", "Victor Mejia Munera,", "The oceans"], "metric_results": {"EM": 0.5, "QA-F1": 0.5598958333333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-2552", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-579", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.5, "CSR": 0.5102848101265822, "EFR": 1.0, "Overall": 0.6809632120253164}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "DeWayne Warren", "is actually wise", "Doug Pruzan", "a simple majority vote", "byte - level operations", "Rich Mullins", "September 19, 2017", "a marriage officiant", "After 1661", "Hermann Ebbinghaus", "Agostino Bassi", "a `` hit", "magneticically soft ( low coercivity ) iron", "Incumbent Democratic mayor Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "up to 40.5 metres ( 133 ft ) in Miyako in T\u014dhoku's Iwate Prefecture, and which, in the Sendai area, traveled up to 10 km ( 6 mi ) inland", "Chicago Cubs", "Evan Spiliotopoulos", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze Banks", "10 June 1940", "citizens", "the influence and significance of the artists'contributions to the development and perpetuation of rock and roll", "Amanda Fuller", "The Forever People", "1997", "mitochondrial membrane", "late 1980s", "Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Aidan Gallagher", "2002", "Evermoist", "Pangaea or Pangea", "Instagram's own account", "Leslie and Ben", "the dress shop", "6,259 km ( 3,889 mi )", "September 2009 to May 2011", "1963", "March 2, 2016", "the Mishnah", "the internal reproductive anatomy", "Brundisium", "France", "the Ukrainian Speleological Association", "England", "April 1, 1949", "CBS", "U.S. Army scout", "Mumbai", "Brian David Mitchell,", "Netherlands", "Florence", "Tiger Woods", "try and reduce the cost of auto repairs and insurance Premium"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6455366630918101}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.4, 1.0, 0.8, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.3529411764705882, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.23076923076923078, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8181818181818182]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-454"], "SR": 0.53125, "CSR": 0.510546875, "EFR": 0.9, "Overall": 0.661015625}, {"timecode": 80, "UKR": 0.6328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.763671875, "KG": 0.4328125, "before_eval_results": {"predictions": ["jollie ralston", "Miranda v. Arizona", "Oscar Wilde", "Vancouver Island", "violin", "georgia", "Vietnam", "Pride and Prejudice", "georgia fox", "senior training Manager", "Leadbetter", "Mikhail Gorbachev", "CBS", "jazz", "Earthquake", "jungle book", "julie caesar", "a dollhouse", "a gallon", "a Great Dane", "priests or the priesthood", "Cambodia", "jitsu", "Hunger Games", "head", "11 years and 302 days", "New Zealand", "Prussian 2nd Army", "georgia caesar", "Whisky Galore", "Tunisia", "13", "(Ted) Kennedy", "cumbria", "head, chin, and throat", "Google", "the shoulder", "Iran", "Downton Abbey", "bird", "Rudyard Kipling", "backgammon", "catherine", "(Albert) Einstein", "georgonzola", "Daniel Barenboim", "exploits on the Island", "ear", "a tree", "Imola Circuit", "trout", "Martin Lawrence", "Emmett Lathrop `` Doc '' Brown", "North Atlantic Ocean", "1961", "Boston Herald", "Lord Chancellor of England", "\"She's just had a pretty rough week because I think the full enormity of what has happened to her is beginning to hit home,\"", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "Tweedledee", "largest library", "Aung San Suu Kyi"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6347470238095237}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.05714285714285714, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-6781", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-4771", "mrqa_newsqa-validation-4128", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-3618"], "SR": 0.578125, "CSR": 0.5113811728395061, "EFR": 0.8518518518518519, "Overall": 0.6385059799382715}, {"timecode": 81, "before_eval_results": {"predictions": ["kentellen", "Worcester Cathedral", "domingo elipe Jacinto Dal\u00ef\u00bf\u00bd", "van rijn", "Illinois", "belize", "(Paul) Maskey", "(Stanislas) Wawrinka", "tartar sauce", "The Three Graces", "satyrs", "geoffrey scribe", "non-Orthodox synagogues", "martin van buren", "leeds", "geoffrey", "Operation", "white", "Jay-Z", "geoffrey clough", "honda", "runcorn", "Vietnam", "monaco", "van gogh", "sakhalin", "Croatia", "NBA", "steel", "dolittle", "Henri Paul", "The Hustle", "penguins", "Samuel Johnson", "coxico", "belgian", "Victor Hugo", "endosperm", "Adriatic", "heartburn", "music Stories", "HMS Conqueror", "geoffrey", "braille", "Standard Oil Company", "cynthia nixon", "Hamlet", "Wat Tyler", "Patrick Henry", "steam engine", "george", "Randy Watson", "Pakistan", "geoffrey", "Thorgan Ganael Francis Hazard", "Lithuanian", "(University of) Kentucky's department of geology", "almost 100", "accusations of improper or criminal conduct.", "is being treated there after being admitted on Wednesday.", "Superman", "Eriksson", "Towering Inferno", "member states on a voluntary basis"], "metric_results": {"EM": 0.515625, "QA-F1": 0.60625}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8333333333333333]}}, "before_error_ids": ["mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-4207", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.515625, "CSR": 0.5114329268292683, "EFR": 0.9354838709677419, "Overall": 0.655242734559402}, {"timecode": 82, "before_eval_results": {"predictions": ["maggie batista della Porta", "Netherlands", "tarn", "car", "Sheffield", "the Strait of Messina", "piano", "charleston", "prat cash", "chile", "Wild Atlantic Way", "Kyoto", "scuba", "repechage", "Steve Biko", "John Leguizamo", "peacock", "rita", "Miss Honey", "imola", "Albania", "antelope", "animals no matter how non dangerous or harmless they are", "boreas", "cadel Evans", "bullfight", "two", "Playboy", "ukraine", "Peter Ackroyd", "bromley-by- Bow on the District and Hammersmith & City lines", "Didier Drogba", "Athina", "mungo park", "death penalty", "Danny Alexander", "up to 14 clubs", "Bangladesh", "adonis", "toea", "Lady Gaga", "Sunset Boulevard", "raging bull", "ars gratia artis", "bologna", "All Things Must Pass", "maggie", "Tet", "Arabah", "as-tu", "Lady Penelope", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "August 18, 1945", "British rock group Coldplay", "Greg Gorman and Helmut Newton", "American jewelry designer", "Queen Isabella II", "Mexico", "the family", "Arizona", "Frdric Chopin", "Indiana Jones", "jawa", "just south of the Bellagio on the west side of Las Vegas Boulevard"], "metric_results": {"EM": 0.5, "QA-F1": 0.6002367424242424}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.8, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.39999999999999997]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-434", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-2433", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-5614", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-2084", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-1664", "mrqa_hotpotqa-validation-4838", "mrqa_hotpotqa-validation-4815", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-16678", "mrqa_hotpotqa-validation-668"], "SR": 0.5, "CSR": 0.5112951807228916, "EFR": 0.9375, "Overall": 0.6556184111445782}, {"timecode": 83, "before_eval_results": {"predictions": ["Bloom", "the Green Arrow", "parable", "julius juliet", "Spinal Tap", "Tennessee", "Detroit", "Ferris B Mueller's Day Off", "United States", "giza", "Ruth Bader Ginsburg", "the Boer War", "touch", "the Old Fashioned", "the Lawrence Welk Show", "Bonnie and Clyde", "penaeus monodon", "the College of William and Mary", "a chimp", "Indian reservations", "John Updike", "ganges", "Hindsight", "light", "his wife's charity hosts them", "coelacanth", "Northanger Abbey", "Cheers", "david smithy", "david adams adams", "Matt Leinart", "Group O", "Charles Edward Stuart", "the albatross", "Falklands", "taro", "a quip", "a lighthouse", "black", "Dan Rather", "Georgia-Pacific Corporation", "Buffalo Bill", "creation", "pig", "Harvard University", "neurons", "Hawaii", "Pierian Spring", "a dog", "dragonflies", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "prentecost", "humble pie", "charleston adams", "City and County of Honolulu", "Armidale, New South Wales", "1992", "\"It was quite surprising to learn of the request,\"", "Steven Chu", "top designers, such as Stella McCartney, to create a distinctive genre of sportswear and lifestyle fashion products.", "Rwanda declared a cease-fire in a genocide that left more than 800,000 dead."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6466179653679653}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-12028", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-15868", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-8018", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-13563", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_searchqa-validation-2946", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-5245", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3660"], "SR": 0.53125, "CSR": 0.5115327380952381, "EFR": 0.9333333333333333, "Overall": 0.6548325892857143}, {"timecode": 84, "before_eval_results": {"predictions": ["1970s", "Steveston Outdoor pool in Richmond, BC", "1930s", "Lenny Jacobson", "the status line", "each team", "overturned the Plessy v. Ferguson decision of 1896, which allowed state - sponsored segregation, insofar as it applied to public education", "weeks before the release of Xscape", "small packs, and in larger and smaller sizes", "approximately 230 million kilometres ( 143,000,000 mi )", "lack the slow hymns and dirges played at funerals ( although this is not a hard rule ; some organizations may have the band play something solemn towards the start of the parade in memory of members deceased since their last parade", "the placing of repentance ashes on the foreheads of participants to either the words `` Repent, and believe in the Gospel '' or the dictum `` Remember that you are dust, and to dust you shall return ''", "Castleford", "fourth C key from left on a standard 88 - key piano keyboard )", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "winter", "Samuel Taylor Coleridge", "Robber Barons", "2001", "Spencer Treat Clark", "marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "2004", "Xiu Li Dai and Yongge Dai", "Americans who served in the armed forces and as civilians during World War II", "Michael Crawford", "200 to 500 mg", "gastrocnemius", "metals, especially heavy metals, that occurs even in low concentrations", "Terry Kath", "Austin, Texas", "1945", "Pebble Beach Corporation", "Andaman and Nicobar Islands", "The midpiece", "Burj Khalifa", "Pangaea or Pangea", "mitochondrial membrane in bacteria", "U2", "Andrew Lincoln", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing ( NLP )", "10 years", "2026", "eleven", "Singing the Blues '' by Guy Mitchell in 1957", "After World War I", "Fred E. Ahlert", "Joanna Moskawa", "1962", "Loch Ness", "LFL", "a griffin", "Mick Jackson", "Queenston", "15", "Michelle Obama", "Consumer Product Safety Commission", "\"That's ridiculous!\"", "2 trailgator bars and attachment on 2 adult and 2 kids bikes", "A Thousand Darknesses", "Dwight D. Eisenhower", "Joel \"Taz\" DiGregorio,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6125330292000521}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.17647058823529413, 0.0, 0.0, 0.8333333333333334, 0.04651162790697674, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.75, 0.0, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-2065", "mrqa_triviaqa-validation-3036", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-4132", "mrqa_newsqa-validation-3992"], "SR": 0.46875, "CSR": 0.5110294117647058, "EFR": 0.8823529411764706, "Overall": 0.6445358455882353}, {"timecode": 85, "before_eval_results": {"predictions": ["Replica Rolex", "Vincent Motorcycle Company", "sprint", "ganges", "gerry adams", "aryl group", "Tom Mix", "Steve Jobs", "Val Kilmer", "Nirvana", "Donna Summer", "heel", "geese", "an authorization of the individual to fulfill a particular function or task", "Sheryl Crow", "Colonel Lacey", "the largest", "Franklin Delano Roosevelt", "neurons", "Prisoner and Escort", "Ossan (the Japanese term for middle-aged man)", "the Swordfish", "eardrum", "George Best", "faggots", "21", "Parson Brown", "Australia", "pascal", "British Airways", "five", "Challenger", "The World is Not Enough", "Italy", "Vienna", "Guys And Dolls", "david hockney", "iron", "Japan", "Bayern Munich", "American actress", "Italy", "Mexico", "New Years Day", "chili peppers", "Madagascar", "Beaujolais", "John Bercow", "kolkata", "the only way Is Essex", "David Bowie", "Candace", "Chung", "2007", "Dra\u017een Petrovi\u0107", "the Costa del Sol", "early Romantic period", "first grand Slam,", "propofol,", "bankruptcy", "vittorio Orlando", "Zinedine Zidane", "a killer whale", "newt"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6055803571428571}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5232", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5759", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-5256", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261", "mrqa_searchqa-validation-350"], "SR": 0.546875, "CSR": 0.5114462209302326, "EFR": 0.9655172413793104, "Overall": 0.6612520674619086}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "General Sir John Monash", "Tempo", "photographs, film and television", "Gretel Scarlett", "alt-right", "Karolina Dean", "Atlanta Athletic Club", "La Liga", "Best Prom Ever", "June 13, 1960", "Iran", "short-interspersed nuclear elements", "death", "London", "SBS", "quantum mechanics", "King Duncan", "January 16, 2013", "Forbes", "Anne and Georges", "David Villa", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "The Social Network", "Martha Wainwright", "Leafcutter John", "moth", "Bothtec", "Jim Thorpe", "De La Soul", "The Monster", "The Shropshire Union Canal", "1621", "A skerry can also be called a low sea stack", "Oliver Parker", "The Strain", "kamehameha I", "Colorado Buffaloes", "Roots: The Saga of an American Family", "five", "William Scott Elam", "\"The Jeffersons\"", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "8 August 1907", "Maidstone, Kent", "strings of eight bits ( known as bytes )", "The Witch and the Hundred Knight 2", "nathan leopold Jr.", "Bill may have felt like it was finally banned by the American government", "The Archbishop of Canterbury, the Most Rev and Rt Hon George Carey", "Amanda Knox's aunt Janet Huff", "numbers", "Jeddah, Saudi Arabia,", "Charlotte's Web", "Andrew Jackson", "Thomas Jefferson", "Willa Cather"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6050730519480518}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-5302", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-894", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-1656", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5047", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2051", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-1530"], "SR": 0.5625, "CSR": 0.5120330459770115, "EFR": 0.8928571428571429, "Overall": 0.6468374127668308}, {"timecode": 87, "before_eval_results": {"predictions": ["one of WSU's most famous alumni", "\"The Hand of Thrawn\"", "in 1754", "May 10, 1976", "Hamlet", "Erick Avari", "Milwaukee Bucks", "McLaren-Honda", "\"Buffy the Vampire Slayer\"", "The Spiderwick Chronicles", "American reality documentary television series that premiered on August 18, 2015", "\"Alberta\", a small-town girl who assumes the false identity of her former babysitter and current dominatrix", "Qualcomm", "water", "10-metre platform event", "Cincinnati Browns", "on the shore, associated with \"the Waters of Death\" that Gilgamesh had to cross to reach Utnapishtim, the far-away", "Guardians of the Galaxy Vol.  2", "November 15, 1903", "Bury St Edmunds", "Rothschild banking dynasty", "Mr. Church", "Bigger Than Both of Us", "Thomas Christopher Ince", "\"Drago\" Sell", "public house", "Los Angeles", "\"The Future\"", "Vyd\u016bnas", "al-Qaeda", "Darling", "Hempstead", "2 April 1977", "House of Commons", "William Finn", "\"Love Letter\".", "Indian", "German", "Barnoldswick", "7 November 1435", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Lindemann,", "Robert Jenrick", "Somerset County, Pennsylvania", "Salford, Lancashire", "British Conservative", "The Division of Cook", "Bajirao Ballal", "Prafulla Chandra Ghosh", "the retina", "Confederate forces", "Western Samoa", "comets", "adrienne Bolland", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Vivek Wadhwa,", "breeding tuataras for the past 35 years,", "snowmobiles", "a comets", "porcelain", "vasoconstriction of most blood vessels"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5290413533834586}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false], "QA-F1": [0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.3157894736842105, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.5, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6, 0.0, 0.14285714285714285, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-4820", "mrqa_hotpotqa-validation-1722", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-958", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-2094", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-5168", "mrqa_hotpotqa-validation-354", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-6935", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.390625, "CSR": 0.5106534090909092, "EFR": 1.0, "Overall": 0.6679900568181818}, {"timecode": 88, "before_eval_results": {"predictions": ["Sarah Palin", "\"La Mome\"", "Ulysses S. Grant", "Apollo", "Richard Wagner", "Atticus Finch", "Dr. StrangeJob", "copper and zinc", "weight plates", "Dunfermline", "bison", "Edmund Cartwright", "Mary Poppins", "leicestershire", "Black Wednesday", "Kiribati", "John Gorman", "The Daily Mirror", "copper", "Olympus Mons on Mars", "prussia", "Dee Caffari", "phrasmide or Iphinoe", "be\u2022li zi", "Tim Brooke Taylor", "clarence", "prawns", "James Hogg", "mORPG", "Fermanagh", "Colombia", "Kevin Painter", "llynberis", "Anne Boleyn", "Muhammad Ali", "Carmen Miranda", "clotte Green", "Pete Sampras", "August 10, 1960", "Estonia", "Sarajevo", "gluten", "an area surrounded by the territorial waters of another nation", "Arthur Ransome", "muthia murali", "Ridley Scott", "1968", "Futurama", "Christopher Ryan", "63 to 144 inches", "1925", "September 29, 2017", "Walter Brennan", "from 13 to 22 June 2012", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba (LeT)", "the \"surge\" strategy he implemented last year.", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "devil's food cake", "Michelangelo", "Missouri", "george joneson"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6150726010101011}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-5053", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_naturalquestions-validation-3589", "mrqa_newsqa-validation-161", "mrqa_searchqa-validation-6490"], "SR": 0.5625, "CSR": 0.5112359550561798, "EFR": 0.8928571428571429, "Overall": 0.6466779945826645}, {"timecode": 89, "before_eval_results": {"predictions": ["1", "a visual operating display", "Scottie Pippen", "Vaseline", "savings rate", "silver", "Gone with the Wind", "small", "nelly", "gladiators", "Finding Nemo", "barbed tongue", "The Kite Runner", "shark", "nairobi", "Oprah", "Dixie Chicks", "apple pie", "California", "luigi", "the Ionian Sea", "Pope John Paul II", "lobster mornay sauce", "Arab Emirates", "david geffen", "chariots", "Pablo Neruda", "The Fifth Amendment", "a mite", "Saturn", "nanny diaries", "liquid crystals", "Robert Frost", "judicial assertion", "almonds", "Crete", "Father Brown", "reuben", "The Outsiders", "waltz", "Belch", "Jane Austen", "Wisconsin", "help his old butler", "Q", "When Harry Met Sally", "Mexico", "pumice", "John Molson", "Jan & Dean", "a American physician and novelist", "Janis Joplin", "transmissions", "brothers Norris and Ross McWhirter", "Andorra", "Michael Faraday", "Gerald R. Ford", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "an upper respiratory infection,", "Fernando Gonzalez", "At least 14", "eight years in prison."], "metric_results": {"EM": 0.640625, "QA-F1": 0.7361979166666667}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.25]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-1293", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-11007", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-4067", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-4720", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-3145"], "SR": 0.640625, "CSR": 0.5126736111111111, "EFR": 0.9565217391304348, "Overall": 0.6596984450483092}, {"timecode": 90, "UKR": 0.6328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.796875, "KG": 0.4671875, "before_eval_results": {"predictions": ["Harpe brothers", "McComb, Mississippi", "\"Loch Lomond\"", "American reality television series", "Gweilo", "\"The Royal Family\"", "\"The Ninth Gate\"", "James G. Kiernan", "His mother Woizero Aselefech Wolde Hanna was the niece of Empress Taitu Bitul, consort of Emperor Menelik II of Ethiopia", "Erreway", "Protestant Christian", "Eardwulf", "Bellagio and The Mirage", "The Los Angeles Dance Theater", "Johnnie Ray", "Hampton University", "The Clash of Triton", "Jenji Kohan", "1", "the second line", "Scottish Premiership club Hibernian", "Ponca City", "Vincent Landay", "Randall Boggs", "June 12, 2014", "Hard rock", "Prince Louis of Battenberg", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University", "Bigfoot", "Cyclic Defrost", "England, Scotland, and Ireland", "\"Coal Miner's daughter\"", "Worcester", "1972", "Ang Lee", "Brad Silberling", "Blue (Da Ba Dee)", "Ealdorman of Devon", "La Scala, Milan", "Orson Welles", "1987", "Scott Mechlowicz", "Ryan Babel", "Melbourne Tullamarine Airport", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "the largest Mission Revival Style building in the United States", "Muhammad", "18", "East River", "Turkey", "gold certates", "sulfuric and nitric acids", "1913.", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "Lord of the Rings", "Jaguar", "wheat smut", "semi-autonomous organisational units"], "metric_results": {"EM": 0.625, "QA-F1": 0.7233630952380952}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-2109", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2708", "mrqa_naturalquestions-validation-7127", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_searchqa-validation-5567", "mrqa_naturalquestions-validation-373"], "SR": 0.625, "CSR": 0.513907967032967, "EFR": 0.9166666666666666, "Overall": 0.6654899267399268}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Dean, Torvill and Karen Barber", "to describe the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "Bart Howard", "Gearing is employed in the transmission, which contains a number of different sets of gears that can be changed to allow a wide range of vehicle speeds, and also in the differential", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "brain, muscles, and liver", "USS Chesapeake", "1977", "official residence of the President of the Russian Federation", "Darwin's On the Origin of Species", "inverted - drop - shaped icon that marks locations in Google Maps", "Richard Stallman", "January 2004", "1940", "50 U.S.C. 1541 -- 1548", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "oxidized, usually gaseous products, in a mixture termed as smoke", "Spanish sovereignty", "peptide bonds", "New England Patriots", "used their knowledge of Native American languages as a basis to transmit coded messages", "Zhu Yuanzhang", "1980 Summer Olympics", "Heather Stebbins", "The soma ( cell body )", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "2017", "Julie Adams", "1881", "The Demon Barber of Fleet Street", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "roughly 2,500 quadrillion liters", "Toot - Toot -- A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Theodore Roosevelt, Robert M. La Follette, Sr., and Charles Evans Hughes", "August 5, 1937", "citizens", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings", "Payson, Lauren, and Kaylie heading to the American Olympic Training Center as they prepare for the 2012 London Olympics", "2016", "Dr. Lexie Grey", "February 27, 2007", "Claims adjusters", "Taron Egerton", "1990", "smen", "British pop band T'Pau", "London", "card game", "Sparta", "World Famous Gold & Silver Pawn Shop in Las Vegas", "Darkroom", "Louis \"Louie\" Zamperini", "Former Alabama judge", "death of cardiac arrest on June 25.", "the first place.", "Jefferson", "Babel", "10 septembre", "Ponce de Len"], "metric_results": {"EM": 0.375, "QA-F1": 0.5355381564103605}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.5714285714285715, 0.6086956521739131, 0.2857142857142857, 1.0, 0.0, 0.3076923076923077, 0.4, 1.0, 0.0, 0.18181818181818182, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.16666666666666669, 0.15384615384615383, 0.0, 0.5, 1.0, 0.06666666666666667, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.25, 0.9411764705882353, 0.5945945945945945, 1.0, 0.0, 0.375, 0.1111111111111111, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.3333333333333333, 0.7272727272727273, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-7309", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_hotpotqa-validation-5243", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-5579"], "SR": 0.375, "CSR": 0.5123980978260869, "EFR": 0.725, "Overall": 0.6268546195652174}, {"timecode": 92, "before_eval_results": {"predictions": ["Miller Lite", "beetle", "the MacKenzie", "the northwest of England", "electronic junk mail or junk newsgroup", "Tahrir Square", "David Frost", "Newbury Racecourse", "detention", "Knutsford", "Portugal", "Spongebob", "Make Me an Offer", "China", "Maine", "Cranmer", "President of the United States", "the federal district of Washington, D.C.", "Jack Sprat", "Ronnie", "conclave", "Dublin", "The Mayor of Casterbridge", "feet", "Amsterdam", "John Lennon", "Lusitania", "Anne of Cleves", "Australia", "antelope", "Portugal", "Swaziland", "Philippines", "blood", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "Isambard Kingdom Brunel", "Canada", "Vickers Vimy", "Jinnah International Airport", "India", "\u00c6thelred", "Peter Paul Rubens", "John Ford", "six", "Mendip Hills", "Burma", "Charles Taylor", "Pancho Villa", "combine keys which are usually kept separate", "Jerry Leiber and Mike Stoller for The Coasters", "Total Drama : Revenge of the Island", "Taylor Swift", "Worcester County", "Brown Mountain", "Lucky Dube,", "Middle East and North Africa,", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "Hammurabi", "leucotomy"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7079861111111111}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false], "QA-F1": [0.8, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7264", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122", "mrqa_searchqa-validation-16895"], "SR": 0.640625, "CSR": 0.5137768817204301, "EFR": 0.782608695652174, "Overall": 0.6386521154745208}, {"timecode": 93, "before_eval_results": {"predictions": ["American heavy metal supergroup", "Supreme John Maitland", "1776", "Markle", "U.S. Bancorp", "Justin Adler", "American Le Mans Series", "Coahuila, Mexico", "Atomic Kitten", "methylenedioxy meth", "Colin Vaines", "California", "striker", "Jim Kelly", "Hawaii", "D\u00e2mbovi\u021ba River", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Miracle on Ice", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1977", "Franky G", "1958", "Easter Rising", "November 23, 2011", "John Monash", "\u00c6thelstan", "Middlesbrough F.C.", "left striker", "5,112 feet", "Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15 mi west", "February 18, 1965", "future AC/DC founders Angus Young and Malcolm Young", "the Goddess of Pop", "Flyweight division", "chocolate-colored Labrador Retriever", "1966", "March 14, 2000", "1927", "Gregg Popovich", "Princess Anne", "Neighbours", "Hall & Oates", "February 12, 2014", "northwest Washington", "1830", "Lake Powell", "an eagle", "chariots", "Louisiana", "\"stressed and tired force\" made vulnerable by multiple deployments,", "Citizens", "Tuesday", "The African Queen", "ferrets", "Gibraltar", "a numeric scale used to specify the acidity or basicity of an aqueous solution"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5735863095238095}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false], "QA-F1": [0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-5879", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-1527", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.484375, "CSR": 0.5134640957446808, "EFR": 0.9696969696969697, "Overall": 0.6760072130883301}, {"timecode": 94, "before_eval_results": {"predictions": ["Villa Park", "guinea", "mayflower", "four", "Guardian U.S.", "tartan", "Toy Story", "Daewoo", "lungs", "Periodic Table", "Left Book Club", "netherlands", "Saint Columba", "Donald Sutherland", "carl", "Ethiopia", "Cardiff", "sternum", "pressure", "James Murdoch", "Bridgeport", "a fluid", "Ambroz Bajec-Lapajne", "Squeeze", "Altamont", "Robert Plant", "George Costanza", "a stern tube", "korea", "lemurs", "Sir Robert Walpole", "eight", "Principality of Andorra", "a horse collar", "Edward Heath", "Kunsky", "St Paul's Cathedral", "27", "Formula One World Championship", "squash", "Mary Decker", "Godwin Austen", "netherlands", "Birdman of Alcatraz", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "Lady Godiva", "festival of Britain on London's South Bank", "steel toe caps", "long linen garment", "1940s", "0.30 in ( 7.6 mm ) thick and weighs 112 grams ( 4.0 oz )", "a hydrolysis reaction", "Neymar", "supporters of King Charles II and supporters of the Rump Parliament", "5.3 million", "6-4 loss,", "al Qaeda,", "UNICEF", "The B", "Florence Nightingale", "Saturn", "a global village"], "metric_results": {"EM": 0.578125, "QA-F1": 0.644417735042735}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-1929", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-6804", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-185", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_hotpotqa-validation-2959", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-9012", "mrqa_searchqa-validation-11091"], "SR": 0.578125, "CSR": 0.5141447368421053, "EFR": 0.9259259259259259, "Overall": 0.6673891325536062}, {"timecode": 95, "before_eval_results": {"predictions": ["pilot and aviation", "air-cushioned sole", "local South Australian and Australian produced content", "tribe Oryzomyini", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Van Diemen's Land", "Jim Kelly", "potsdam", "Edward James Olmos", "Girls' Generation", "2017", "two or three acts", "Dra\u017een Petrovi\u0107", "Prussia", "David Wells", "The castle lies a mile to the east of Roslin at grid reference [ NT287637], and is just downstream from Roslin Castle", "two-time Olympic gold medalist", "Argentine cuisine", "13th century", "Pru Goward", "The \"Nike Hypervenom\"", "Matt Groening", "Hazel Keech", "Minami-Tori-shima", "1993", "Jesus", "Sulla", "Riot Act", "a trio with his younger brothers Steve and Rudy", "right-hand batsman", "black nationalism", "\"Futurama\"", "FC Bayern Munich", "Deftones", "\"Pastime Paradise\"", "clitheroe football Club", "155 ft tall", "\"Cleopatra\"", "The Fault in Our Stars", "Liesl", "How the Grinch Stole Christmas", "twin-faced sheepskin with fleece on the inside", "White Horse", "banjo player", "Flavivirus", "Elise Marie Stefanik", "Francis Schaeffer", "Australia", "between 3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "President pro tempore of the Senate", "jordan maynard", "Quebec", "cold Comfort Farm", "red", "lightning strikes", "killings of his father and brother.", "Guernsey", "a civil rights conference", "bremen", "brain and spinal cord"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6639773261278196}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.2666666666666667, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 0.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.8571428571428572, 0.28571428571428575, 0.5, 1.0, 1.0, 1.0, 1.0, 0.625, 0.0, 0.28571428571428575, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-3848", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-648", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-1745", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-14797", "mrqa_naturalquestions-validation-7342"], "SR": 0.546875, "CSR": 0.5144856770833333, "EFR": 0.9655172413793104, "Overall": 0.6753755836925287}, {"timecode": 96, "before_eval_results": {"predictions": ["12951 / 52 Mumbai Rajdhani Express", "the year of the conception or birth of Jesus of Nazareth", "1987", "360", "Pradyumna", "Carol Ann Susi", "pyloric valve", "Ben Fransham", "John of Patmos", "Mark Lowry", "Phillip Paley", "Germany", "Einstein", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100", "James Madison", "Woodrow Strode", "Nadiadwala Grandson Entertainment", "Taylor Michel Momsen", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "The epidermis", "Julianne Hough", "United Nations", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "pigs", "take - it - or - leave - it contract, or a boilerplate contract", "31 December 1600", "The show was staged as an immersive production, with action happening around and among the audience", "American country music duo Brooks & Dunn", "May 31, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75 micrometers", "Miller Lite", "Oona Castilla Chaplin", "William Shakespeare's As You Like It", "Lulu", "the NFL", "Steve Russell", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S. Nicknames for the flag", "Profit maximization", "Melbourne", "April 1, 2016", "San Antonio", "1,281,900", "Michael Phelps", "royal oak", "The Krankies", "France", "Province of Syracuse", "June 11, 1986", "1-0", "200", "Bobby Jindal", "\"reshit\"", "John Deere", "gusts", "curfew"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7812464050533632}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.48275862068965514, 1.0, 0.326530612244898, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 0.9189189189189189, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6517", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-8179", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252"], "SR": 0.71875, "CSR": 0.5165914948453608, "EFR": 0.8333333333333334, "Overall": 0.6493599656357388}, {"timecode": 97, "before_eval_results": {"predictions": ["the Boston Symphony", "a cave", "a 5-cent Nut Goodie Bar", "a boll weevil", "a touchpad", "Wikipedia", "Sundance Kid", "Buddhism", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Edgar Allan Poe", "Sergey Brin", "Sanders", "Billy Corgan", "bread", "Yale", "\"Attila of the age dethroned... shut up within the circle of a little island of the Mediterranean\"", "Paris", "the Black Forest", "the Stanza della Segnatura", "an ant", "a birkenstock", "The Firebird", "Hafnium", "flax", "the Iliad", "the Wachowski brothers", "Rumpole of the Bailey", "(Al) Gore", "Steve Austin", "Kurt Warner", "2", "a small retail store", "Beauty and the Beast", "Ratatouille", "pro bono", "a Brown bear", "The Office", "Rosanna Arquette", "a Sasquatch", "Jackson Pollock", "glisten", "happy", "French", "Crayola", "a Ford Roadster", "a missionary", "orange", "Isaiah Amir Mustafa", "2000", "Americans acting under orders", "Mike Danger", "\"The Crow.\"", "L. P. Hartley", "Temacine Tamazight", "European Champion Clubs' Cup", "second largest", "North Korea,", "alcohol", "may indeed help people with irritable bowel syndrome,", "Prada"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5876358695652173}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.608695652173913, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1748", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-9850", "mrqa_searchqa-validation-4924", "mrqa_searchqa-validation-10403", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-96"], "SR": 0.53125, "CSR": 0.5167410714285714, "EFR": 1.0, "Overall": 0.6827232142857143}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "Grapefruit", "a part from (one's salary or wages)", "Millard fillmore", "cornea", "Crystal Light", "Rumpole", "the guillotine", "the Light Bulb", "Spider-Man", "Atlanta", "chile", "Dick Tracy", "Queen Latifah", "Van Allen", "beer", "Zen Buddhism", "El", "Zenith", "baboon", "wine", "The Sopranos", "Baby Gays", "natural selection", "Massachusetts", "Battle of the Bulge", "a crossword clue", "W. Somerset Maugham", "Sicilies", "Trafalgar", "republican systems", "Sir Francis Drake", "most Honorable Son", "Einstein", "Crazy 8", "Pituitary Gland", "Alfred Hitchcock", "Hank Aaron", "Special Boat Teams", "Florida", "Ectoplasm", "Thomas Jefferson", "messenger", "Dante", "Christopher Columbus", "(Joseph) Haydn", "meringue", "Serena", "Thought Police", "calcium", "four National Football League ( NFL ) franchises", "James Hutton", "961", "wlem", "his food", "Mary Seacole", "Orchard Central", "Fort Hood", "OutKast", "iPods", "suspend all", "Tuesday", "Nick Sager"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6617559523809524}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-15786", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3054", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6625", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-6821", "mrqa_naturalquestions-validation-7913", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040"], "SR": 0.578125, "CSR": 0.5173611111111112, "EFR": 0.8888888888888888, "Overall": 0.660625}, {"timecode": 99, "UKR": 0.6171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.775390625, "KG": 0.46171875, "before_eval_results": {"predictions": ["Niles", "Ben Rosenbaum", "July 14, 2017", "2020", "classical neurology", "Sir Edward Henry", "potential of hydrogen", "Peking", "Bart Howard", "2013 ( XLVIII )", "Ozzie Smith", "Saronic Gulf", "George Harrison", "Persian", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "4 ( IIII ) and 9 ( VIIII )", "2014 -- 15,", "Natural - language processing ( NLP )", "six - hoop game", "HTTP / 1.1", "$315,600", "three", "Sohrai", "the Intertropical Convergence Zone ( ITCZ )", "Cecil Lockhart", "James Long", "257,083", "March 23, 2018", "quarterback", "public sector ( also called the state sector )", "his Phone - A- friend", "2018", "1992", "Dan Stevens", "The fates of Laurie and the character of T - Dog in the episode `` Killer Within '' garnered favorable reviews from television commentators", "Disha Vakani", "Nickelback", "1999 to 2001", "Queen M\u00e1xima of the Netherlands", "The horn line at the end is performed by the Phenix Horns from Earth, Wind & Fire", "Deuteronomy 5 : 4 -- 25", "a revolution or orbital revolution", "Ren\u00e9 Georges Hermann - Paul", "Horace Lawson Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "just before World War II to 1973", "2009", "Manley", "Chaplin", "Francis Matthews", "(Hymenaeus)", "1907", "1776", "Field Marshal Stapleton Cotton", "three of the bombers -- Siddique Khan, Shehzad Tanweer and Jermaine Lindsay -- practicing their moves in various locations on June 28, 2005.", "eight-day journey that has also taken him to Japan and Singapore,", "101 new jobs to British workers,", "Spain", "( Clayton) Barbeau", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6414832410236821}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.16666666666666669, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.4, 0.5, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.25, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.058823529411764705, 0.16666666666666669, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-9962", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9987", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-2741", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2803", "mrqa_searchqa-validation-3524"], "SR": 0.5625, "CSR": 0.5178125, "EFR": 0.9285714285714286, "Overall": 0.6601361607142857}]}