{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=1000.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]_result.json', stream_id=4, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4080, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["fall of 1937", "The Skirmish of the Brick Church", "beliefs of Sunni Islamic thinkers", "\"The Lodger\"", "Londonistan", "a high-level marketing manager", "Houston, Texas", "cone-shaped", "San Francisco Bay Area's Levi's Stadium", "Ren\u00e9 Lalique", "absolution", "$105 billion", "ABC Cable News", "trial division", "their belief in the validity of the social contract", "Hyde Park", "four years", "Grey Street", "most of the items in the collection, unless those were newly accessioned into the collection", "literacy and numeracy", "Luther", "prime elements", "the Aveo", "one week", "Steymann v Staatssecretaris van Justitie", "1937", "The governments of the United States, Britain, Germany and France", "mother-of-pearl", "cholera", "Tower District", "ring theory", "Euclid's fundamental theorem of arithmetic", "Tony Hawk", "Beyonc\u00e9", "The Book of Discipline", "USSR", "Schmalkaldic League", "2006", "70%", "Einstein", "Genghis Khan", "four half-courses per term", "2011", "Brownlee", "Tracy Wolfson", "the wisdom and prudence of certain decisions of procurement", "1971", "the Uighurs surrendered to the Mongols first", "cnidarians", "CBS", "842 pounds", "two of Tesla's uncles", "up to \u00a332,583", "the City council", "three", "shopping", "4 weeks", "propulsion, electrical power and life support", "William Smilie", "George Westinghouse", "1279", "complexity classes", "\"everything that smacks of sacrifice\"", "a system to function"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7404040404040404}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9630", "mrqa_squad-validation-7687", "mrqa_squad-validation-4836", "mrqa_squad-validation-131", "mrqa_squad-validation-2297", "mrqa_squad-validation-5505", "mrqa_squad-validation-1802", "mrqa_squad-validation-9136", "mrqa_squad-validation-9061", "mrqa_squad-validation-116", "mrqa_squad-validation-5877", "mrqa_squad-validation-6294", "mrqa_squad-validation-7214", "mrqa_squad-validation-3699", "mrqa_squad-validation-8247", "mrqa_squad-validation-4419", "mrqa_squad-validation-553", "mrqa_squad-validation-3811", "mrqa_squad-validation-2092"], "SR": 0.703125, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 1, "before_eval_results": {"predictions": ["the Pulfrich effect", "semi-legal", "Westinghouse Electric", "Pax Mongolica", "The date of 2035", "internal strife", "the Marburg Colloquy", "Northumbria University", "non-cryogenic", "the defense and justification of empire-building", "the Carm Michael numbers", "1999", "Scorpion", "October 16, 2012", "a commune", "the metal locking screw on the camera lens", "Eldon Square Shopping Centre", "type III secretion system", "$680 billion", "296", "New Collegiate Division", "four", "18 million volumes", "15,100", "Warner Bros. Presents", "V\u03b39/V\u03b42 T cells", "1985", "the Augustinian friars", "third", "2012", "gold", "force model that is independent of any macroscale position vector", "378", "tourism", "the Jews", "all", "many celebrated seasons", "Charles-Fer Ferdinand University", "The Nationals", "a computational problem where a single output (of a total function) is expected for every input", "Katharina von Bora", "1888", "the middle of the continent", "Schmalkaldic League", "4:51", "Knaurs Lexikon", "constant pressure", "detective shows", "the southern and central parts of France", "Maria Fold and thrust Belt", "making it seem like climate change is more serious by overstating the impact", "1945", "1876", "Elway", "spring of 1349", "Extreme Makeover: Home Edition", "the Wesleyan Holiness Consortium", "it has settled as one of the pillars of history", "The four Railroads are fairly lucrative properties", "The Sphinx would devour anyone who could not answer her riddle", "The Smashing Pumpkins are an American alternative rock band from Chicago, Illinois, formed", "the 107th justice to serve on the United States Supreme Court", "32-year-long investigation into the enigmatic hijacker", "If the citizen's heart was heavier than a feather they would face torment in a lake of fire"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8365891167494787}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.2666666666666667, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 0.18181818181818182, 0.3076923076923077, 0.0, 0.0, 0.2105263157894737]}}, "before_error_ids": ["mrqa_squad-validation-8546", "mrqa_squad-validation-9024", "mrqa_squad-validation-10466", "mrqa_squad-validation-1030", "mrqa_squad-validation-1189", "mrqa_squad-validation-1600", "mrqa_squad-validation-4287", "mrqa_squad-validation-2166", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-1274", "mrqa_hotpotqa-validation-3713"], "SR": 0.78125, "CSR": 0.7421875, "EFR": 0.9285714285714286, "Overall": 0.8353794642857143}, {"timecode": 2, "before_eval_results": {"predictions": ["Solim\u00f5es Basin", "the seal of the Federal Communications Commission", "Islamism", "the E. W. Scripps Company", "Grand Canal d'Alsace", "food security", "exothermic", "Newcastle Diamonds", "the wisdom and prudence of certain decisions of procurement", "D\u00fcrer", "Grover Cleveland", "Erg\u00e4nzungsschulen", "concrete", "to promote advanced research and education networking in the United States", "The Newlywed Game", "the German-Swiss border", "the \"blurring of theological and confessional differences in the interests of unity.\"", "microbes", "300", "the electrostatic force", "Jim Nantz and Phil Simms", "1530", "from 12:00 to 6:00 p.m. Eastern Time", "employ consultant pharmacists and/or provide consulting services", "the murder of Christ", "1708", "the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)", "the Religious Coalition for Reproductive Choice", "The Arrow", "intractable problems", "yellow fever", "silver and inlaid with gold", "Richard Wilkinson and Kate Pickett", "elsewhere in the Northern United Kingdom", "president and CEO", "Von Miller", "the weak force", "a diverse phylum of bacteria capable of carrying out photosynthesis", "vaccination", "the plague theory", "the loss of soil fertility and weed invasion", "the fact (Fermat's little theorem)", "11th", "the most popular show", "Robert of Jumi\u00e8ges", "student populations", "German", "Persia", "superheaters", "two", "Johann von Staupitz", "lectures", "Short Short", "What a wonderful World", "the White House", "Dugout canoe", "Ganges", "the Heritage 1981 brand", "Britney Spears", "the title My Fair Lady", "Don Bradman", "Ford Motor Co.", "Fidenza", "Charles Scribner's"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7465411324786324}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5521", "mrqa_squad-validation-4847", "mrqa_squad-validation-597", "mrqa_squad-validation-5828", "mrqa_squad-validation-10460", "mrqa_squad-validation-8777", "mrqa_squad-validation-4974", "mrqa_squad-validation-9023", "mrqa_squad-validation-1188", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-9859", "mrqa_searchqa-validation-6857", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-3869"], "SR": 0.6875, "CSR": 0.7239583333333333, "EFR": 1.0, "Overall": 0.8619791666666666}, {"timecode": 3, "before_eval_results": {"predictions": ["Winter Film Capital of the World", "clear boundaries", "collenchyma tissue", "24 March 1879", "the Convention", "constant factors and smaller terms", "1996", "CBS", "genetic branches", "on a religious basis", "14,000", "Killer T cells", "about 11 million", "third", "The Earth's mantle", "expansions", "Necessity-based", "glaucophyte chloroplasts", "artisans and farmers", "inverted repeat", "pharmacists", "September 2007", "a declining state of mind", "G", "civil disobedience", "Sociologist", "World News Tonight", "the carriage of their respective basic channels", "cytotoxic", "Liao, Jin, and Song", "Tyneside Classical", "nine", "18 million", "four", "Christian Whiton", "his mother", "Johann Gerhard", "Korean", "The Time of the Doctor", "7 January 1900", "90\u00b0", "the Natives of these localities are very badly disposed towards the French, and are entirely devoted to the English", "stolen", "Centrum", "$200,000", "4,686", "economic", "1950s", "Karl Marx", "President of the United States of America", "Disneyland", "the king", "South Africa", "fibre optics", "Lawrence Brooks", "a cone-shaped utensil", "Yasser Arafat", "Pigeon", "voodoo", "a dowry", "Macduff", "Jean Halliwell", "Prince James, Duke of York and of Albany ( later King James II & VII )", "Charles Perrault"], "metric_results": {"EM": 0.640625, "QA-F1": 0.702951388888889}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1947", "mrqa_squad-validation-1714", "mrqa_squad-validation-9597", "mrqa_squad-validation-10107", "mrqa_squad-validation-8703", "mrqa_squad-validation-6409", "mrqa_squad-validation-3958", "mrqa_squad-validation-6670", "mrqa_squad-validation-6884", "mrqa_squad-validation-7083", "mrqa_squad-validation-2406", "mrqa_squad-validation-10186", "mrqa_squad-validation-1509", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-15033", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-495"], "SR": 0.640625, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 4, "before_eval_results": {"predictions": ["Mars", "education", "Kingdom of Prussia", "May 18, 1756", "odd prime", "economic growth by collecting resources from colonies, in combination with assuming political control by military and political means", "quantum electrodynamics", "topographic", "Hassan al Banna", "regional burden sharing", "Indianapolis Colts", "Pole Mokotowskie", "a school or other place of formal education", "Francis Blackburne", "black earth", "photolysis of ozone by light of short wavelength", "smart ticketing", "State Route 99", "F and \u2212F are equal in magnitude and opposite in direction", "free", "Air", "1,548", "whether the bill is within the legislative competence of the Parliament", "two", "Galileo Galilei", "Catholic", "patient care rounds drug product selection", "vicious and destructive", "greater scarcity", "The Eleventh Doctor", "86", "Arizona Cardinals", "Not designed to fly through the Earth's atmosphere or return to Earth", "2015", "one hunting excursion", "Deacons", "to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church", "socialist realism", "July 23, 1963", "1162", "a method which pre- allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes", "Barbara Walters", "as soon as 2050", "Red River", "the hundreds", "London and Buenos Aires", "Sub-Saharan Africa", "D, E or F", "the Carrousel du Louvre", "$1.5 million", "3 to 17", "Isabella", "Obama", "World Wide Village", "Preah Vihear temple", "Ralph Lauren", "Noriko Savoie", "T.I.", "(Zed)", "the Kenyan and Somali governments issued a joint communique declaring Al-Shabaab \"a common enemy to both countries.\"", "battles, political intrigue, and the characters", "oldpatricktoe-end", "Mulberry", "Shaft"], "metric_results": {"EM": 0.6875, "QA-F1": 0.751541832010582}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29629629629629634, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.25, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9926", "mrqa_squad-validation-8634", "mrqa_squad-validation-1891", "mrqa_squad-validation-10333", "mrqa_squad-validation-3706", "mrqa_squad-validation-2564", "mrqa_squad-validation-4746", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-2234", "mrqa_naturalquestions-validation-7134", "mrqa_triviaqa-validation-712", "mrqa_searchqa-validation-8929"], "SR": 0.6875, "CSR": 0.7, "EFR": 0.9, "Overall": 0.8}, {"timecode": 5, "before_eval_results": {"predictions": ["visitation of the Electorate of Saxony", "United States", "11", "May", "to employ limited coercion in order to get their issue onto the table", "1798", "3D printing technology", "Waterlogged", "Tim Allen", "wealth and income", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "40", "LeGrande", "filaments", "Joanna Lumley", "Energiprojekt AB in Sweden", "DuMont Television Network", "1913", "Egyptian Islamic Jihad organization", "consumer prices", "petroleum", "1870", "27.7 million tons", "the remainder of the British Isles", "Jean Auguste Dominique Ingres,", "Eliot Ness", "to obey the temporal authorities", "Edgar", "experience and extra responsibilities", "chameleon circuit", "All-Channel Receiver Act", "secular powers", "at the opposite end from the mouth", "areas controlled by Russia in 1914", "kinescope", "University Athletic Association (UAA)", "three", "Chebyshev", "reality television", "Disco", "Christopher Lloyd Smalling", "Mary Harron", "Polk", "Minette Walters", "1983", "79 AD", "1993 to 2001", "Major League Soccer", "1669", "University of Vienna", "\"lo Stivale\" (the Boot)", "Richa Sharma", "Centennial Olympic Stadium", "Violet", "Vernier, Switzerland", "October 21, 2016", "an Indian cricketer and former captain of the Indian cricket team", "international association football competitions", "The conversation", "9 February 2018", "Canada", "teenage", "an increase in dew point", "Donna Mills"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8171875000000001}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6846", "mrqa_squad-validation-3345", "mrqa_squad-validation-5519", "mrqa_squad-validation-2322", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4614", "mrqa_triviaqa-validation-4029", "mrqa_newsqa-validation-2982", "mrqa_searchqa-validation-4118"], "SR": 0.765625, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 6, "before_eval_results": {"predictions": ["woodblocks", "General Hospital", "the dot", "Denver Broncos", "129 MSPs", "Lenin", "completed (or local) fields", "\"push\" motivations", "alone", "stem cells", "John Pell, Lord of Pelham Manor", "quickly", "induction motor", "Holy War", "pressure terms", "ten million", "Tommy Lee Jones", "nine", "13.34% (116.7 sq mi or 302 km2)", "kilopond", "water level", "1981", "rules that conflict with morality", "early as the sixteenth century", "R\u00fcdesheim", "an epidemiological account of the plague", "time or space", "Jacksonville Consolidation", "Aristotle", "1724", "mid-Cambrian period", "Canada", "Stanford University", "Reuben Townroe", "small forward", "Alamo Bowl", "The King of Chutzpah", "Charles Russell", "German", "Minette Walters (born 26 September 1949)", "St. Patrick's Day in 1988", "Dulwich", "Michael Sheen", "Hungary", "ITV", "Ella Fitzgerald", "\"Confessions of a Teenage Drama Queen\"", "EBSCO Information Services", "Dutch", "Marc Bolan", "\"The Nightmares Before Christmas\" (1992)", "John Mills", "1992", "Saint-Domingue", "Airline Deregulation Act", "\"Kill Your Darlings\"", "University of Kansas", "The Land of Enchantment", "2001", "AnCIENT Seven Wonders of The World", "illegal", "Billy Bob Thornton (born August 4, 1955)", "Charles Martel", "The Blues Brothers"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7227306547619047}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5774", "mrqa_squad-validation-9430", "mrqa_squad-validation-7614", "mrqa_squad-validation-2567", "mrqa_squad-validation-5303", "mrqa_squad-validation-7476", "mrqa_squad-validation-9718", "mrqa_squad-validation-9098", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-4387", "mrqa_triviaqa-validation-2856", "mrqa_newsqa-validation-696", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-12796"], "SR": 0.65625, "CSR": 0.703125, "EFR": 0.9545454545454546, "Overall": 0.8288352272727273}, {"timecode": 7, "before_eval_results": {"predictions": ["Germany", "1985", "William Iron Arm", "Muhammad Abd al-Salaam Farag", "comedies", "lack of understanding", "British East Africa", "the Atlantic", "Jingshi Dadian", "economic instability", "Jean- Marc Bosman", "Ismailiyah", "Wiesner", "polynomial time", "quarterback", "ten times their own weight", "primes", "light", "success", "TGIF", "often married outside their immediate French communities", "George Westinghouse", "color confinement", "certification", "Alberto Calder\u00f3n", "Mercury", "Marconi successfully transmitted the letter S from England to Newfoundland", "private", "Cadeby", "Ten", "vice president", "Steven Gerrard", "the international community", "Hine's school", "Obama", "Silvio Berlusconi", "London Heathrow's Terminal 5", "at the House of Blues", "football", "sharia law", "Chandler Keys", "Sonia Sotomayor", "Kurdish militant group", "pilot", "Wednesday", "$50", "back at work", "flooding", "1971", "composer", "50,000", "to best your own fuel economy achievements", "Bhola district", "SSM Cardinal Glennon Children's Medical Center", "military", "he has no plans to fritter his cash away", "Secretary of State Hillary Clinton", "7th century", "Wigan", "2004", "the Mormon Tabernacle Choir", "Hagai Amir", "The Little Foxes", "Lord Tennyson"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5463834972394754}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.08695652173913045, 0.05714285714285715, 0.33333333333333337, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6034", "mrqa_squad-validation-6925", "mrqa_squad-validation-8339", "mrqa_squad-validation-4289", "mrqa_squad-validation-4692", "mrqa_squad-validation-9614", "mrqa_squad-validation-2160", "mrqa_squad-validation-8771", "mrqa_squad-validation-3069", "mrqa_squad-validation-10445", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_naturalquestions-validation-8664", "mrqa_hotpotqa-validation-2330", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-4905", "mrqa_searchqa-validation-1341"], "SR": 0.484375, "CSR": 0.67578125, "EFR": 0.9393939393939394, "Overall": 0.8075875946969697}, {"timecode": 8, "before_eval_results": {"predictions": ["2 July 1505,", "beginning in early September and ending in mid-May", "high risk preparations and some other compounding functions", "Non Governmental and Intergovernmental Organizations", "chief electrician position", "1671", "United States", "12 January 1943", "his own men", "Sonia Shankman Orthogenic School", "Innate immune systems", "the manufacturing sector", "Wardenclyffe Tower project", "horizontal compression", "Edgar Scherick", "Imperial", "Moscone Center", "Great Yuan", "Since the 1980s", "oxygen-16", "Lessing", "one advanced lay servant course", "the Tyne Tunnel", "Threatening government officials", "ca. 22,000\u201314,000 yr BP,", "two", "Lek", "New England Patriots", "work rule issues", "28 of those now on hardcourt surfaces.", "July 4.", "the Catholic League", "more than two years,", "Newark's Liberty International Airport,", "We Found Love", "\"a little girl presented a bouquet to Bill Clinton.\"", "a kidney transplant", "foster national reconciliation between religious and ethnic groups", "Addis Ababa,", "Christian bookstores across the country that carry the publication.", "military trials", "to launch a group that will serve as an alternative to the Organization of American States.", "to pay him a monthly allowance", "Tutsi and Hutu rivalry", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Draquila -- Italy Trembles.", "married", "his former Boca Juniors teammate and national coach Diego Maradona,", "17 Again", "five", "75", "Kgalema Motlanthe,", "preventing our public-owned seas from turning into sprawling, watery versions of Houston, Texas, or Atlanta, Georgia.", "about 3,000 kilometers (1,900 miles)", "two courses", "NATO fighters", "Austin, Texas,", "two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "Royals", "Anah\u00ed", "Romanian Communist leader, Nicolae Ceausescu,", "Nikkei 225 Stock Average", "Surrey", "David Bowie"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5883448858363263}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1818181818181818, 0.8, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333336, 1.0, 1.0, 0.0, 0.7142857142857143, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615383, 0.5, 0.0, 1.0, 0.0, 0.058823529411764705, 0.2608695652173913, 0.1818181818181818, 0.28571428571428575, 0.06451612903225806, 0.5, 0.0, 0.9523809523809523, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8526", "mrqa_squad-validation-1279", "mrqa_squad-validation-3113", "mrqa_squad-validation-589", "mrqa_squad-validation-1570", "mrqa_squad-validation-6128", "mrqa_squad-validation-7377", "mrqa_squad-validation-8084", "mrqa_squad-validation-2405", "mrqa_squad-validation-10083", "mrqa_squad-validation-5357", "mrqa_squad-validation-6671", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-490", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-93", "mrqa_searchqa-validation-8602", "mrqa_triviaqa-validation-1"], "SR": 0.453125, "CSR": 0.6510416666666667, "EFR": 1.0, "Overall": 0.8255208333333334}, {"timecode": 9, "before_eval_results": {"predictions": ["Private Education Student Financial Assistance", "philanthropy", "in an unmarked grave somewhere in Mongolia", "Daily Mail", "heard her songs; he followed the fishermen and captured the mermaid.", "action-reaction", "Metropolitan Statistical Areas", "orogenic wedges", "eleven", "Grissom, White, and Chaffee", "Amtrak San Joaquins", "two", "1", "Barbara Walters", "temperate", "girls", "by citizens", "by up to 3 pence in the pound", "Lessing", "the courts of member states and the Court of Justice of the European Union", "1887", "new laws or amendments to existing laws as a bill; a committee of the Parliament can present a bill in one of the areas under its remit; a member of the Scottish Parliament can introduce a bill", "487", "Presiding Officer", "expansion", "between 3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "in the axial skeleton ( 28 in the skull and 52 in the torso )", "Miletus", "moral", "The Maidstone Studios in Maidstone, Kent", "Russian name for several breeds of dogs similar to the husky", "National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1986", "Charles Darwin", "the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "in Poems : Series 1", "1927", "merengue", "lamina dura", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "Proposition 103", "1997 and the Middle East in 2000", "at the intersection of Mud Mountain Road and Highway 410", "San Francisco Bay", "Tom Brady", "2014", "Duck", "three levels", "Sylvester Stallone", "once every 23 hours, 56 minutes, and 4 seconds with respect to the stars", "The Divergent Series : Ascendant was never made, due to Allegiant's poor showing at the box office.", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year reported on the accompanying income statement", "1960", "costume party", "altitude", "1 point", "sow", "feces", "John Joseph Travolta", "Allies of World War I, or Entente Powers,", "U.S. 93", "have a smile on her face when her kids were around.", "Mike Tyson", "Utah"], "metric_results": {"EM": 0.5, "QA-F1": 0.6595724930223286}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428572, 0.16666666666666669, 0.0, 0.6666666666666666, 1.0, 0.0, 0.631578947368421, 1.0, 1.0, 0.33333333333333337, 0.3076923076923077, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.2857142857142857, 0.13333333333333333, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.125, 0.25, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 0.7692307692307693, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5937", "mrqa_squad-validation-805", "mrqa_squad-validation-3922", "mrqa_squad-validation-9641", "mrqa_squad-validation-2404", "mrqa_squad-validation-9452", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-893", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-1173", "mrqa_triviaqa-validation-2329", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2729", "mrqa_newsqa-validation-4179"], "SR": 0.5, "CSR": 0.6359375, "EFR": 0.96875, "Overall": 0.80234375}, {"timecode": 10, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1735", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1877", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3503", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3994", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6984", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-4000", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-69", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16779", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2470", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-350", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-9187", "mrqa_squad-validation-10015", "mrqa_squad-validation-10052", "mrqa_squad-validation-10068", "mrqa_squad-validation-1008", "mrqa_squad-validation-10083", "mrqa_squad-validation-10103", "mrqa_squad-validation-10107", "mrqa_squad-validation-10116", "mrqa_squad-validation-10125", "mrqa_squad-validation-10186", "mrqa_squad-validation-10210", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-1030", "mrqa_squad-validation-10308", "mrqa_squad-validation-10333", "mrqa_squad-validation-10333", "mrqa_squad-validation-10344", "mrqa_squad-validation-10367", "mrqa_squad-validation-10374", "mrqa_squad-validation-104", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10466", "mrqa_squad-validation-10493", "mrqa_squad-validation-1051", "mrqa_squad-validation-1052", "mrqa_squad-validation-1068", "mrqa_squad-validation-1113", "mrqa_squad-validation-116", "mrqa_squad-validation-1165", "mrqa_squad-validation-1178", "mrqa_squad-validation-1188", "mrqa_squad-validation-1193", "mrqa_squad-validation-1200", "mrqa_squad-validation-1207", "mrqa_squad-validation-1211", "mrqa_squad-validation-1257", "mrqa_squad-validation-1269", "mrqa_squad-validation-1279", "mrqa_squad-validation-131", "mrqa_squad-validation-1330", "mrqa_squad-validation-1348", "mrqa_squad-validation-1368", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1509", "mrqa_squad-validation-1527", "mrqa_squad-validation-1536", "mrqa_squad-validation-1541", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1714", "mrqa_squad-validation-1769", "mrqa_squad-validation-1802", "mrqa_squad-validation-1891", "mrqa_squad-validation-1947", "mrqa_squad-validation-1967", "mrqa_squad-validation-2030", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2166", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-2297", "mrqa_squad-validation-2331", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2405", "mrqa_squad-validation-2409", "mrqa_squad-validation-2438", "mrqa_squad-validation-25", "mrqa_squad-validation-2554", "mrqa_squad-validation-2559", "mrqa_squad-validation-2564", "mrqa_squad-validation-2567", "mrqa_squad-validation-2576", "mrqa_squad-validation-2579", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2717", "mrqa_squad-validation-2778", "mrqa_squad-validation-2822", "mrqa_squad-validation-2827", "mrqa_squad-validation-2870", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-3050", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-313", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3261", "mrqa_squad-validation-3269", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3388", "mrqa_squad-validation-3445", "mrqa_squad-validation-3492", "mrqa_squad-validation-3603", "mrqa_squad-validation-3617", "mrqa_squad-validation-365", "mrqa_squad-validation-3699", "mrqa_squad-validation-3759", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3815", "mrqa_squad-validation-3833", "mrqa_squad-validation-3837", "mrqa_squad-validation-3844", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3922", "mrqa_squad-validation-3938", "mrqa_squad-validation-3958", "mrqa_squad-validation-3976", "mrqa_squad-validation-4030", "mrqa_squad-validation-4086", "mrqa_squad-validation-4191", "mrqa_squad-validation-4231", "mrqa_squad-validation-4232", "mrqa_squad-validation-4248", "mrqa_squad-validation-4269", "mrqa_squad-validation-43", "mrqa_squad-validation-4419", "mrqa_squad-validation-4480", "mrqa_squad-validation-4491", "mrqa_squad-validation-4560", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4746", "mrqa_squad-validation-475", "mrqa_squad-validation-4765", "mrqa_squad-validation-4836", "mrqa_squad-validation-4847", "mrqa_squad-validation-4896", "mrqa_squad-validation-4935", "mrqa_squad-validation-5009", "mrqa_squad-validation-5075", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5164", "mrqa_squad-validation-5180", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5221", "mrqa_squad-validation-5272", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5357", "mrqa_squad-validation-5363", "mrqa_squad-validation-5424", "mrqa_squad-validation-5451", "mrqa_squad-validation-5455", "mrqa_squad-validation-5471", "mrqa_squad-validation-5505", "mrqa_squad-validation-5519", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5541", "mrqa_squad-validation-5616", "mrqa_squad-validation-5651", "mrqa_squad-validation-5670", "mrqa_squad-validation-5774", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-583", "mrqa_squad-validation-5840", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-5877", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5908", "mrqa_squad-validation-5937", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-5971", "mrqa_squad-validation-5976", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6048", "mrqa_squad-validation-6083", "mrqa_squad-validation-6098", "mrqa_squad-validation-6098", "mrqa_squad-validation-6128", "mrqa_squad-validation-6158", "mrqa_squad-validation-618", "mrqa_squad-validation-6238", "mrqa_squad-validation-6294", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6381", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6506", "mrqa_squad-validation-6527", "mrqa_squad-validation-6530", "mrqa_squad-validation-6569", "mrqa_squad-validation-6580", "mrqa_squad-validation-6605", "mrqa_squad-validation-6670", "mrqa_squad-validation-6681", "mrqa_squad-validation-6707", "mrqa_squad-validation-6754", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-69", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-6996", "mrqa_squad-validation-7002", "mrqa_squad-validation-7020", "mrqa_squad-validation-7022", "mrqa_squad-validation-7034", "mrqa_squad-validation-7080", "mrqa_squad-validation-7083", "mrqa_squad-validation-7092", "mrqa_squad-validation-7094", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7303", "mrqa_squad-validation-7304", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7372", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7420", "mrqa_squad-validation-7476", "mrqa_squad-validation-7502", "mrqa_squad-validation-7614", "mrqa_squad-validation-7687", "mrqa_squad-validation-7690", "mrqa_squad-validation-7704", "mrqa_squad-validation-775", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-7886", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7981", "mrqa_squad-validation-805", "mrqa_squad-validation-8052", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8197", "mrqa_squad-validation-8247", "mrqa_squad-validation-829", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8364", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8416", "mrqa_squad-validation-8479", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8526", "mrqa_squad-validation-8546", "mrqa_squad-validation-8580", "mrqa_squad-validation-8600", "mrqa_squad-validation-863", "mrqa_squad-validation-8680", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8777", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8953", "mrqa_squad-validation-8957", "mrqa_squad-validation-8965", "mrqa_squad-validation-9002", "mrqa_squad-validation-9012", "mrqa_squad-validation-902", "mrqa_squad-validation-9023", "mrqa_squad-validation-9024", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9136", "mrqa_squad-validation-9141", "mrqa_squad-validation-9208", "mrqa_squad-validation-9254", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9337", "mrqa_squad-validation-9411", "mrqa_squad-validation-9430", "mrqa_squad-validation-9452", "mrqa_squad-validation-9457", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9527", "mrqa_squad-validation-9569", "mrqa_squad-validation-9597", "mrqa_squad-validation-9614", "mrqa_squad-validation-9615", "mrqa_squad-validation-9624", "mrqa_squad-validation-9635", "mrqa_squad-validation-9641", "mrqa_squad-validation-9665", "mrqa_squad-validation-9718", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_squad-validation-9845", "mrqa_squad-validation-985", "mrqa_squad-validation-9926", "mrqa_squad-validation-9940", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.908203125, "KG": 0.4375, "before_eval_results": {"predictions": ["the Miller\u2013Urey experiment", "five", "Mughal emperors", "a series of strikes by coal miners and railroad workers over the winter of 1973\u201374 became a major factor in the change of government.", "\"formal\"", "HO", "increased settlement and deforestation", "Afranji", "Abercynon in south Wales", "drinking water", "permafrost", "10 to 15 million", "Cabot Science Library, Lamont Library, and Widener Library", "the Henry Cole wing", "the Warsaw City Council (Rada Miasta),", "The time and space hierarchy theorems", "The innate immune system", "Cow Counties", "all", "Royal Institute of British Architects", "the 6th century", "The owner", "United Parcel Service", "Ernie", "Sapporo", "The Rosetta Stone", "m\u0101-jan", "The Tonight Ensemble", "the right hand side of the second line of letters", "Jap\u00b7a\u00b7nese", "William Boyd", "Wolf Hall", "\"tennis - What is a Golden set? and what is the difference with Bagel set?", "the gums", "the White House", "Richmond, Va.", "Wawrinka", "Humphrey Bogart", "Nigel Hawthorne", "the French aristocracy", "Auric Goldfinger", "\"Hell Upside Down: The Making of The Poseidon Adventure", "5", "Mary Poppins", "Neil Armstrong", "the centre bull", "the Metropolitan Borough of Oldham, in Greater Manchester, England", "British Defence Secretary", "Old Ironsides", "Bullnose", "brown", "the skull", "\"Maljanne\"", "gold", "Brainy", "New Zealand", "Paige O'Hara", "Chris Martin", "the Ishtar Gate", "neo-Nazi", "several weeks", "an animal tranquilizer, can put users in a dazed stupor for about two hours, doctors said. Long-term use of ketamine can impair cognitive function and damage internal organs.", "Robin Lee", "the mouth"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6480148204286136}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6206896551724138, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3722", "mrqa_squad-validation-6223", "mrqa_squad-validation-962", "mrqa_squad-validation-944", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5918", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-865", "mrqa_triviaqa-validation-6822", "mrqa_triviaqa-validation-1579", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-515", "mrqa_newsqa-validation-265", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-2154"], "SR": 0.578125, "CSR": 0.6306818181818181, "EFR": 1.0, "Overall": 0.7507457386363636}, {"timecode": 11, "before_eval_results": {"predictions": ["\"Old Briton\"", "Tiffany & Co.", "1985", "ancient Y. pestis strains", "4k + 3", "Samuel Reshevsky", "mujahideen Muslim Afghanistan", "July 1977", "coal", "programmes", "Stanford Stadium", "friction", "his work", "monatomic", "MetroCentre", "SAP Center", "composite", "the courts of member states", "German-language publications", "27", "six", "summer", "groin vault", "Konakuppakatil Gopinathan Balakrishnan", "Burj Khalifa", "June 1945", "art of the book and architecture", "Bart Howard", "on the microscope's stage", "nihonium", "Max Martin", "2005", "31 December 1600", "February 29", "in the transmission", "216", "Dr. Addison Montgomery", "old English pyrige", "food", "the Kansas City Chiefs", "the Indians", "Claudia Grace Wells", "fascia surrounding skeletal muscle", "the amount of surface", "October 2004", "Alex Rodriguez", "Timothy B. Schmit", "Jaydev Shah", "Jodie Foster", "13,000 astronomical units", "1978", "wisdom", "Thomas Chisholm", "Rocinante", "before the first letter of an interrogative sentence", "Wikia", "The Daily Mirror", "an invoice, bill or tab", "the Cumberland Plain", "1,500", "Lee Myung-Bak", "Alexander Pushkin", "Emiliano Zapata", "The Time Machine"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7070064484126984}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 0.5333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10142", "mrqa_squad-validation-4963", "mrqa_squad-validation-3450", "mrqa_squad-validation-9024", "mrqa_squad-validation-3947", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-3602", "mrqa_triviaqa-validation-412", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-2913", "mrqa_newsqa-validation-3686", "mrqa_searchqa-validation-14088", "mrqa_searchqa-validation-2383"], "SR": 0.59375, "CSR": 0.6276041666666667, "EFR": 0.9615384615384616, "Overall": 0.7424379006410257}, {"timecode": 12, "before_eval_results": {"predictions": ["2014", "Muslim state", "Frederick William", "herbal remedies", "(various allied groups from Central Asia and the western end of the empire)", "96.26%", "19th", "Raoul Pierre Pictet", "$45,000", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "n < p < 2n \u2212 2", "illegal acts", "A", "The European Court of Justice", "chlorophyll a and phycobilins", "German-language publications", "1992", "25 percent of all money it raises for philanthropic causes in the Bay Area", "in the stems and roots of certain vascular plants", "Valens and Richardson", "1976", "British Army soldiers shot and killed people while under attack by a mob", "Judiththia Aline Keppel", "Buffalo Bill", "a fictional team of Naval Criminal Investigative Service ( NCIS ) agents stationed out of New Orleans, Louisiana and led by Special Agent Dwayne Cassius Pride ( Scott Bakula )", "noble gas", "A rotation", "Jaydev Shah", "Polly Walker", "Raya Yarbrough", "Darlene Cates", "2014 Winter Olympics in Sochi, Russia", "the evolution of light hair", "Las Vegas, Nevada", "his brother", "28 July 1914", "Jean F Kernel", "Haiti", "1948", "Sauron", "Chandan Shetty", "Bill Henderson", "Liam Cunningham", "Nancy Jean Cartwright", "Max", "Walter", "a premalignant flat", "the medulla oblongata", "Humpty Dumpty and Kitty Softpaws", "Ren\u00e9 Descartes", "Clarence Anglin", "Jason Lee", "October 1941", "2013", "on the continent of Antarctica", "Bath and Wells", "John Nash", "Alistair Grant", "1989 until 1994", "$2 billion", "one of Africa's most stable nations", "gregoria", "Brownsville", "Ralph Lauren"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7163277116402116}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.2, 0.0, 1.0, 0.1111111111111111, 0.5714285714285715, 1.0, 0.14814814814814814, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.6, 0.4, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8206", "mrqa_squad-validation-6655", "mrqa_squad-validation-394", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-5259", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-7346", "mrqa_triviaqa-validation-6797", "mrqa_hotpotqa-validation-2319", "mrqa_newsqa-validation-2444", "mrqa_searchqa-validation-3744"], "SR": 0.609375, "CSR": 0.6262019230769231, "EFR": 1.0, "Overall": 0.7498497596153847}, {"timecode": 13, "before_eval_results": {"predictions": ["early 1990s", "the 17th century", "Four thousand", "the class ofNP complete problems", "the seal of the Federal Communications Commission", "landlords found new residents willing to pay higher market rate for housing", "a historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\"", "1206", "2011", "in his lab", "their bright colors sometimes override the chlorophyll green", "Radio Corporation of America (RCA)", "up to three-fourths of the population of the Iranian Plateau", "partial funding", "Open Door Policy", "1538", "Cricket Izz Izzie dolls Tinkerbell Stevens Patient X", "a minimum adequate diet", "Tony Almeida", "Herbert Hoover", "Saint Wenceslaus", "coal", "Union Pacific & the Central Pacific", "kangaroo pal", "Homo erectus", "Latin", "Dakota Fanning", "Luxor", "the grindstone", "the Osmonds", "cold air", "butterflies", "an oval shape, either almost touching the hips, or at navel level, or raised above the dancer's head", "the Real Apprentices", "Calypso", "Richard I", "A Million Little pieces", "Give Me Liberty or Give Me Death", "John Lennon", "the Billy Goats Gruff", "jedoublen", "bacon strips", "Saturn", "the Urals", "Amsterdam", "Etna", "the Valley Isle", "Richard Nixon", "\"Under The Sea\"", "an running back playing halfback in a typical I formation.", "George Carlin", "Che Guevara", "a raven", "Kurdish", "Nicholas Sparks", "Lizzy Greene", "B\u00e9la Bart\u00f3k", "Spike McPike", "The English Electric Canberra", "the murder of 40-50 Karankawa people in Mexican Texas near present-day Matagorda by a party of White colonists in 1826", "Krishna Rajaram", "the country's third-largest oil refinery", "a month of training", "processing data, requiring that all flight-plan information be processed"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6110916248783896}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0909090909090909, 1.0, 0.5714285714285715, 0.0, 0.3636363636363636]}}, "before_error_ids": ["mrqa_squad-validation-1804", "mrqa_squad-validation-7578", "mrqa_squad-validation-9575", "mrqa_squad-validation-8229", "mrqa_squad-validation-6250", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-16887", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-16654", "mrqa_searchqa-validation-5858", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-6353", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-9558", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5059", "mrqa_hotpotqa-validation-1876", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-909"], "SR": 0.53125, "CSR": 0.6194196428571428, "EFR": 0.9666666666666667, "Overall": 0.7418266369047619}, {"timecode": 14, "before_eval_results": {"predictions": ["Pittsburgh Steelers", "J. S. Bach", "Denver", "from January 1964, until it achieved the first manned landing in July 1969,", "simple unicellular organisms", "The Quasiturbine", "Dave Logan", "Roger NFL", "by compressing and cooling", "December 2014", "Ladner", "the Ming dynasty", "Von Miller", "15,100", "Ronnie Hillman", "Masha Skorobogatov", "Daniel A. Dailey", "on the urinary floor", "31 December 1600", "in the Blue Ridge Mountains of Virginia", "1916", "Milira", "in the absence of a catalyst", "The Walking Dead", "Ceramic art", "government monopoly", "Tbilisi, Georgia", "a candidate state must be a free market democracy", "Yondu Udonta", "religious Hindu musical theatre styles", "Rocinante", "6 - 6", "John Roberts", "Ray Charles", "British", "a routing table", "deceased - donor ( formerly known as cadaveric )", "Kevin Spacey", "1995 Mitsubishi Eclipse", "Lana Del Rey", "Kim Basinger", "photoelectric", "4,840", "Dasharatha", "September 19", "President pro tempore", "February 16, 2016", "Wisconsin", "Merry Clayton", "a phenomenon known as cold shortening", "Florida", "a violation of nature", "Nepal", "Hungary", "Word Options", "Australia", "June 2, 2008", "Reverend Timothy \"Tim\" Lovejoy", "Arthur E. Morgan III,", "a Muslim", "Australia", "Oxford Committee for Famine Relief", "a Follies", "Chief Oshkosh"], "metric_results": {"EM": 0.515625, "QA-F1": 0.619204260651629}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.8571428571428571, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.10526315789473684, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.8, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-32", "mrqa_squad-validation-6453", "mrqa_squad-validation-690", "mrqa_squad-validation-80", "mrqa_squad-validation-3473", "mrqa_squad-validation-8062", "mrqa_squad-validation-362", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-10135", "mrqa_triviaqa-validation-5650", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2435", "mrqa_searchqa-validation-11788", "mrqa_searchqa-validation-9115"], "SR": 0.515625, "CSR": 0.6125, "EFR": 1.0, "Overall": 0.747109375}, {"timecode": 15, "before_eval_results": {"predictions": ["chest pains", "thermodynamic theory", "The Lone Ranger", "broken arm", "Arthur Woolf", "roughly equivalent to little Hugos, or those who want Hugo", "Manning", "Warren Buffett", "Malkin Athletic Center", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "generally planktonic", "Lucas Horenbout", "a 3\u20130 lead", "Spektor", "writ of certiorari", "the right of the dinner plate", "Joanne Wheatley", "flawed democracy", "Southwest Florida International Airport ( RSW )", "1987", "the temporal lobes", "Missi Hale", "their son", "a Norwegian town circa 1879", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Television demonstrations", "57 days", "1937", "the chant", "gastrocnemius", "a god of the Ammonites", "336", "24th match", "1 atm pressure", "Qutab - ud - din Aibak", "Isaiah Amir Mustafa", "dry lake beds", "The photoelectric ( optical ) smoke detector", "New York University", "the Carnaval de Qu\u00e9bec", "the ball is fed into the gap between the two forward packs", "on the microscope's stage", "9 February 2018", "31 October 1972", "Jesse McCartney", "4.25 inches", "Lady Gaga", "Director of National Intelligence", "April 1979", "the Isthmus of Corinth", "must be at least 18 or 21 years old ( or have a legal guardian present )", "the Royal Air Force ( RAF )", "three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "sometimes ambiguous designation of two classes of organic compounds", "7,926 miles", "Alberich", "Katharine Juliet Ross", "\"The Brothers\"", "about 100", "58", "ulna", "to compare", "Monday night", "fight outside of an Atlanta strip club"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5854617776608109}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 0.7368421052631579, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.25, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6341463414634146, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.30769230769230765]}}, "before_error_ids": ["mrqa_squad-validation-3190", "mrqa_squad-validation-845", "mrqa_squad-validation-9399", "mrqa_squad-validation-4636", "mrqa_squad-validation-800", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-4064", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-6481", "mrqa_triviaqa-validation-2131", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-1447", "mrqa_newsqa-validation-1789", "mrqa_searchqa-validation-7968", "mrqa_searchqa-validation-10092", "mrqa_newsqa-validation-85"], "SR": 0.484375, "CSR": 0.6044921875, "EFR": 0.9393939393939394, "Overall": 0.7333866003787879}, {"timecode": 16, "before_eval_results": {"predictions": ["427", "Michael Mullett", "impact process effects", "since 2001", "double or triple non-French linguistic origins", "German", "a course of study, lesson plan, or a practical skill", "Super Bowl XXXIII", "a protest that blocked heavy traffic passing over the A13, Brenner Autobahn, en route to Italy.", "P is not equal to PSPACE", "charging their students tuition fees", "directly every four years", "parabola", "Mission San Juan Capistrano", "crawdads", "Lewis and Clark", "a cross", "The Age of Innocence", "Logan International Airport", "a thirst", "parabola", "Canada", "an asylum", "parabola", "Judy Garland", "the names of God", "airplanes", "Kansas City", "bay", "John", "the opera", "a parabola", "carbon dioxide", "San Francisco", "the Venus landing", "Saturn", "the devil", "a lichen", "Lake Baikal", "Brazil", "a depression", "Laos", "Chang Apana", "a British accent", "Never go to a doctor whose office plants have died.", "Clinton", "the UK", "Herod", "Iran", "a serve", "Touch of Evil", "the paroxide hair", "Nightingale", "the Pooh", "Daryl Sabara", "commemorating fealty and filial piety", "William Shakespeare", "Nick Berry", "a split 7\"", "the area of the reservation", "Turkey", "sumo wrestling", "more than 26,000", "Fat Man"], "metric_results": {"EM": 0.390625, "QA-F1": 0.47672566833751046}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5263157894736842, 0.2, 0.5714285714285715, 0.8571428571428571, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4267", "mrqa_squad-validation-4065", "mrqa_squad-validation-1844", "mrqa_squad-validation-4332", "mrqa_squad-validation-1795", "mrqa_squad-validation-6983", "mrqa_squad-validation-964", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-3450", "mrqa_searchqa-validation-3445", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-16782", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-3214", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-9973", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-15522", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-15665", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-1397", "mrqa_searchqa-validation-12143", "mrqa_searchqa-validation-519", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-3174", "mrqa_newsqa-validation-1120", "mrqa_hotpotqa-validation-5388"], "SR": 0.390625, "CSR": 0.5919117647058824, "EFR": 1.0, "Overall": 0.7429917279411764}, {"timecode": 17, "before_eval_results": {"predictions": ["forceful taking of property", "Robert Iger", "Central business districts", "ouravers", "prime number theorem", "nonphotosynthetic eukaryote engulfed a chloroplast-containing alga but failed to digest it", "Santa Clara Marriott", "Trevathan", "The View and The Chew", "being drafted into the Austro-Hungarian Army in Smiljan", "friction", "a statue", "Titanic", "Queen Hatshepsut", "Heroes", "Prince of Denmarke", "a dog eat dog world", "Sir Anthony Eden", "Defending Your Life", "an inquire", "Jalisco state", "Santa Fe", "the Space Coast Convention Center", "Muddy Waters", "the First Telegraphic Message", "King County Democrats", "Manfred von Richthofen", "cowboys", "General Michael Collins", "John J. Pershing", "a balloon", "La Crosse", "the Jesuit order", "Javier", "The Titan", "an anatomical animation", "( Susan) Lahrman", "the University of Massachusetts Amherst", "less than 60 beats per minute", "a poacher", "a coal", "the heart", "rabbit", "livestock", "The Call of the Wild", "Vladimir Putin", "Hillary Clinton", "Nikola Tesla", "a gingerbread men", "Elza", "a trestle", "a light-gathering mirror", "David Tyree", "statistical modeling and statistical estimation or statistical inference", "The federal government", "the Greater Antilles", "Doncaster Rovers", "the City of Onkaparinga", "an Anglo-Saxon tumulus (or \"barrow\")", "Monday night", "Immigration Minister Eric Besson", "needle - like teeth", "3", "relieve families who had difficulty finding jobs during the Great Depression in the United States"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4933201058201059}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.962962962962963]}}, "before_error_ids": ["mrqa_squad-validation-3106", "mrqa_squad-validation-315", "mrqa_squad-validation-5938", "mrqa_squad-validation-1232", "mrqa_squad-validation-10287", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-7483", "mrqa_searchqa-validation-8063", "mrqa_searchqa-validation-10653", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-8777", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-6969", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-15293", "mrqa_searchqa-validation-10372", "mrqa_searchqa-validation-9231", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-222", "mrqa_hotpotqa-validation-3804", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-9856"], "SR": 0.34375, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.740234375}, {"timecode": 18, "before_eval_results": {"predictions": ["308", "Trajan's Column", "the solution", "the Danube", "parliamentary systems", "Ralph Woodward", "major cities", "19th Century", "high density", "the Ten Commandments", "amyotrophic lateral sclerosis (ALS)", "The Jefferson Memorial", "Magic Johnson", "between 7,500 and 40,000", "The Soloist", "bronze", "Odawa", "feats of exploration", "Abdul Razzak Yaqoob", "Norwegian", "1868", "Churchill", "Hopeless Records", "Saturday Night Live", "the National Basketball Association (NBA)", "Ang Lee", "near North Chicago", "1971", "Dutch", "Arkansas", "2009", "The Ministry of Utmost Happiness", "Uzumaki", "northeastern Alabama", "March 30, 2025", "Miller Brewing", "Michael Lewis Greenwell", "Daimler-Benz", "1994", "John of Gaunt", "classical realism", "1,521", "a royal residence", "one", "fantasy role-playing game", "Leofric", "The Division of Fawkner", "Argentinian Americanos", "the International Hotel", "American black bear", "Mot\u00f6rhead", "Richa Sharma", "An impresario", "Lake Michigan", "Tulsa", "homeless", "Jessica Smith", "Hungary", "five", "Maryland", "all correct", "Ulysses S. Grant", "The Maracot Deep", "kidnapping"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6558035714285714}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.28571428571428575, 0.33333333333333337, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9479", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-4297", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-2241", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-4001", "mrqa_newsqa-validation-4077", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-10375"], "SR": 0.578125, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.740234375}, {"timecode": 19, "before_eval_results": {"predictions": ["toward the end of his life", "motivated students", "native tribes", "Cuba", "Dai \u00d6n Ulus, also rendered as Ikh Yuan \u00dcls or Yekhe Yuan Ulus", "Parliamentary time", "Private Bill Committees", "photolysis of ozone", "Eliot Ness", "Joseph Stalin", "Bobby Eli", "Eurasian Plate", "Anglican", "10.5 %", "the donor organ", "the South Pacific Ocean", "silk floss", "Tom Waits", "Cherbourg in France", "Olympic - class ocean liners", "T.J. Miller", "full '' sexual intercourse", "1995", "Paradise, Nevada", "Mike Nesmith", "the country club pool", "Office of Inspector General", "Skat", "1878", "Rockwell", "Scott Bakula", "October 2008", "the Two Brothers", "the theory of T\u0101\u1e47\u1e0dava dance ( Shiva )", "Heather Stebbins", "Spanish surname", "supervillains who pose catastrophic challenges to the world", "1999", "Geraldine Margaret Agnew - Somerville", "Joe Lawrence", "October 2, 2017", "Stephen Curry of Davidson", "December 15, 2016", "a star", "AMX - 50", "the Overlook Hotel", "Marty Robbins", "Albert Einstein", "2017", "Ed Sheeran", "state ownership of the means of production", "HTTP / 1.1", "A substitute good", "Midsomer Murders", "Honolulu, Hawaii", "Hopeless Records", "death sentence", "Seasons of My Heart", "wife Linda", "To Build a Fire", "an Emergency Tracheotomy", "business school", "Vancouver", "Hudson River"], "metric_results": {"EM": 0.453125, "QA-F1": 0.566421568627451}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.11764705882352941, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9275", "mrqa_triviaqa-validation-6696", "mrqa_hotpotqa-validation-4897", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-568"], "SR": 0.453125, "CSR": 0.571875, "EFR": 1.0, "Overall": 0.738984375}, {"timecode": 20, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-2139", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2467", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10135", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2025", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-3628", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1789", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11788", "mrqa_searchqa-validation-12143", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-16723", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-3450", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6061", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6844", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-6969", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8063", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_squad-validation-10052", "mrqa_squad-validation-10107", "mrqa_squad-validation-10125", "mrqa_squad-validation-10149", "mrqa_squad-validation-10186", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10333", "mrqa_squad-validation-10341", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10445", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10493", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-116", "mrqa_squad-validation-1193", "mrqa_squad-validation-1257", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1527", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1684", "mrqa_squad-validation-1754", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2092", "mrqa_squad-validation-2166", "mrqa_squad-validation-2288", "mrqa_squad-validation-2302", "mrqa_squad-validation-232", "mrqa_squad-validation-2322", "mrqa_squad-validation-2324", "mrqa_squad-validation-2344", "mrqa_squad-validation-2406", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2559", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2737", "mrqa_squad-validation-2778", "mrqa_squad-validation-2827", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-32", "mrqa_squad-validation-3217", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3506", "mrqa_squad-validation-3617", "mrqa_squad-validation-362", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3923", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-401", "mrqa_squad-validation-4086", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4248", "mrqa_squad-validation-4287", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4836", "mrqa_squad-validation-4974", "mrqa_squad-validation-5012", "mrqa_squad-validation-5088", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5379", "mrqa_squad-validation-5451", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5950", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6069", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6250", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6605", "mrqa_squad-validation-6671", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6843", "mrqa_squad-validation-6846", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7476", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-789", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-800", "mrqa_squad-validation-805", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8193", "mrqa_squad-validation-8197", "mrqa_squad-validation-8307", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-845", "mrqa_squad-validation-852", "mrqa_squad-validation-8580", "mrqa_squad-validation-8696", "mrqa_squad-validation-8771", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8798", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8935", "mrqa_squad-validation-8953", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9141", "mrqa_squad-validation-9254", "mrqa_squad-validation-9270", "mrqa_squad-validation-929", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9457", "mrqa_squad-validation-9479", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9630", "mrqa_squad-validation-964", "mrqa_squad-validation-9718", "mrqa_squad-validation-9766", "mrqa_squad-validation-9768", "mrqa_squad-validation-985", "mrqa_squad-validation-9968", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1751", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2106", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.85546875, "KG": 0.46953125, "before_eval_results": {"predictions": ["Behind the Sofa", "Madame de Pompadour", "the Privy Council", "During the Second World War", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "an assembly center", "Leonardo da Vinci", "end of 1350", "two Nobel Peace Prizes", "an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "June 12, 2017", "Prussian statesman", "London Heathrow", "27 November 1956", "Dan Conner", "supernatural psychological horror film", "Billy Joel", "The Times Higher Education Guide", "The conversation", "Al D'Amato", "nuclear weapons", "Lush Ltd.", "Port Macquarie", "Coca-Cola", "Lonestar", "Brea, California", "World War II", "Brazilian Jiu-Jitsu", "Biola University", "Columbia Pictures", "February 22, 1968", "Erreway", "2015", "Kim Bauer", "the Joint Chiefs of Staff", "Give Up", "1955", "1993", "near Philip Billard Municipal Airport", "John Duigan", "John Boyd Dunlop", "Iran", "Cherokee\u2013American wars", "a creek", "Edward Trowbridge Collins Sr.", "Province of New York", "Archie Andrews", "January 28, 2016", "a championship and an NBA Finals Most Valuable Player Award", "Steve Martin", "Israeli Declaration of Independence in 1948", "Sleepy Hollow", "Mumbai", "1996", "Scheria", "The National Council for the Unmarried Mother and her Child", "Doubting Castle", "Noida", "genocide", "Jane Austen", "Howard Hughes Jr.", "Dairy Queen", "AC/DC", "the Iberian Peninsula"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7567773600668337}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.4210526315789474, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8189", "mrqa_squad-validation-5061", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-3393", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-5047", "mrqa_hotpotqa-validation-5727", "mrqa_triviaqa-validation-5418", "mrqa_newsqa-validation-3563", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11847", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-12943"], "SR": 0.671875, "CSR": 0.5766369047619048, "EFR": 1.0, "Overall": 0.729546130952381}, {"timecode": 21, "before_eval_results": {"predictions": ["Several thousand", "Paris", "Islamic Republic", "different subject specialists each session during the week", "Sonia Shankman Orthogenic School", "inverse proportionality of acceleration", "alternating current", "1002", "Guardians of the Galaxy Vol. 2", "1987", "arts manager", "technical director", "44", "their unusual behavior", "UK garage", "Ireland", "Valley Falls", "U\u00ed \u00cdmair", "Ry\u016bkyuan people", "Dallas", "a leg injury", "Hirsch index rating", "neo-Nazi", "Warner Animation Group", "2008", "Michael Crawford", "Isobel", "Nick Offerman", "American", "the Dominican Republic", "29,000", "Wes Unseld", "Mr. Church", "1990", "Ronald Wilson Reagan", "Ray Romano", "The Dressmaker", "Kennedy Road", "Israel", "Peter Yarrow", "\"From Here to Eternity\"", "Slugger Field", "The Royal Family", "Backstreet Boys", "Lynn Minmei", "King James I of England", "Derek Jacobi", "August 14, 1848", "Catwoman", "The Hindu Group", "Detroit Lions and the Los Angeles Rams", "Audi", "1886", "Phillip Schofield and Christine Bleakley", "USS Chesapeake", "Jupiter", "Seal", "CNN", "Somali forces and Islamic insurgents", "Katrina & the Waves", "Joe Jackson", "Michael Arrington", "Krishna Rajaram", "at least 12 months"], "metric_results": {"EM": 0.5625, "QA-F1": 0.658407738095238}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.8, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1894", "mrqa_squad-validation-10424", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-2473", "mrqa_naturalquestions-validation-1786", "mrqa_triviaqa-validation-2997", "mrqa_newsqa-validation-3178", "mrqa_searchqa-validation-6538", "mrqa_searchqa-validation-939", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-274"], "SR": 0.5625, "CSR": 0.5759943181818181, "EFR": 1.0, "Overall": 0.7294176136363637}, {"timecode": 22, "before_eval_results": {"predictions": ["1,230 kilometres (764 miles)", "anti-colonial movements", "The Middle and Modern Family", "An increase in imported cars", "immunomodulators", "inequality", "Duval County", "Univision", "Pieter van Musschenbroek", "\"Beauty and the Beast\"", "every aspect of public and private life", "Missouri Tigers", "Acela Express", "German", "Cartoon Network", "Aamir Khan", "trans-Pacific flight", "Kristina Ceyton and Kristian Moliere", "ginger Rogers", "Dan Castellaneta", "from 1995 to 2012", "Saint Petersburg Conservatory", "Harlequin", "one", "Colonel", "Ars Nova Theater", "Donna Paige Helmintoller", "Detroit, Michigan", "CBS News", "1838", "Assistant Director Neil J. Welch", "near Philip Billard Municipal Airport", "ZZ Top", "2", "The Handmaid's Tale", "Rick and Morty", "second largest", "Bamyan Province", "Melbourne", "1590", "247,597", "jurisdiction", "Yoruba people", "Madonna Louise Ciccone", "Telugu", "1800000 sqft", "Malayalam cinema", "New York State Route 907E", "Man Booker Prize for Fiction", "teenage actor or teen actor", "October 21, 2016", "Taeko Ikeda", "Claire Rhiannon Holt", "NATO or laser", "Thomas Chisholm", "Brazil", "Superman", "Wales", "since 1983", "three", "threatening messages", "China", "a urkast picture", "SS Mayaguez"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7452008928571429}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9302", "mrqa_hotpotqa-validation-3366", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-2446", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-1384", "mrqa_naturalquestions-validation-4308", "mrqa_triviaqa-validation-5792", "mrqa_newsqa-validation-377", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-6136"], "SR": 0.65625, "CSR": 0.5794836956521738, "EFR": 1.0, "Overall": 0.7301154891304348}, {"timecode": 23, "before_eval_results": {"predictions": ["28.5\u00b0E", "the House of Hohenstaufen", "seven", "BBC Dead Ringers", "Mnemiopsis", "Dr. George E. Mueller", "a salmon", "Gerald", "heating", "Bob Newhart", "Constellations", "opossums", "a map", "Gerald Dostoyevsky", "The Plaza Hotel", "Stephen Hawking", "Rodeo", "Oahu", "a sustained pull", "a Relief Sculpture", "Gerald Sharpe", "an M1 Abrams", "Earth", "\"Egg in a bottle\"", "Gerald II", "Jean Foucault", "\"Saturday Night Live\"", "Variety.com", "Indian School of the College", "Who's Afraid of Virginia Woolf", "aluminum", "jamais", "Barnard College", "\"The Baths of Caracalla\"", "Gerald Goodstein", "\"Greenleaf\"", "a chicken (chicken)", "a stone", "George W. Bush", "anemones", "James Cook", "Bosom Buddies", "Alexander Calder", "Gerald Retriever", "Walter Crawford Kelly, Jr.", "Edith Wharton", "Rapa Nui National Park", "Nike+ iPod Sport Kit", "gravity", "an oblate spheroid", "West Darfur", "a chile", "Bay of Montevideo", "The U.S. state of Georgia", "in the 1820s", "a palindrome", "a numeric keypad", "a chock", "Las Vegas Boulevard", "Slaughterhouse-Five", "United Nations Global Ambassador for the Food and Agriculture Organization", "participate in Iraq's government.", "an enormous step forward in Dana Gas' strategy across the Middle East, North Africa and South Asia.", "2050"], "metric_results": {"EM": 0.375, "QA-F1": 0.4971153846153846}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1045", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4758", "mrqa_searchqa-validation-15407", "mrqa_searchqa-validation-5554", "mrqa_searchqa-validation-12149", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-13231", "mrqa_searchqa-validation-9639", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-14893", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-2225", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-596", "mrqa_searchqa-validation-14432", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-4374", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-7572", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-15964", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7047", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-3087", "mrqa_hotpotqa-validation-5890", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-1639"], "SR": 0.375, "CSR": 0.5709635416666667, "EFR": 1.0, "Overall": 0.7284114583333334}, {"timecode": 24, "before_eval_results": {"predictions": ["ca. 2 million", "Catholic", "The waxy cuticle of many leaves", "a theta intermediary form", "subsequent long-run economic growth", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law.\"", "two-state solution", "CNN", "a Taliban member who had come for the talks about peace and reconciliation,", "lizard", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "eight", "South of Port", "Rima Fakih", "South States, NATO member states, Russia and India", "43,000", "18", "Sunday", "a rally", "Herman Cain", "humans", "Friday", "Malmo", "murder", "the Swat Valley", "two years,", "228", "21 percent", "serving its fast burgers in the Carrousel du Louvre", "Steve Williams", "computer-generated animated film", "Nazi Germany", "\"It's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "30 years ago", "five", "July", "Oprah: A Biography", "100 percent", "whites", "still unidentified group of bandits.", "The Palm", "There's no chance", "Egypt", "Jeddah, Saudi Arabia", "poor families", "Russia", "2011", "use of torture and indefinite detention", "a skull", "Iran", "a review of state government practices completed in 100 days.\"", "dancing in short pants.", "Michael Schumacher", "West Virginia", "Massachusetts", "phi", "the natural world", "The Lion King", "7pm", "1966", "Eavan Boland", "Twelfth Night", "wine", "yellow"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6096440574942792}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.10526315789473685, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.875, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.08695652173913043, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6435", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2201", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1419", "mrqa_naturalquestions-validation-4207", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-173", "mrqa_searchqa-validation-2076"], "SR": 0.515625, "CSR": 0.56875, "EFR": 0.967741935483871, "Overall": 0.7215171370967742}, {"timecode": 25, "before_eval_results": {"predictions": ["primes", "UNESCO's World Heritage list", "lower incomes,", "Wankel engine", "five or more seats in the Parliament", "the Bronx.", "in northwest Pakistan", "digging ditches.", "Oregon", "Democrats and Republicans are saying Meehan shouldn't be using a 9/11 image to make a political point.", "Fullerton, California,", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "The minister later apologized, telling CNN his comments had been taken out of context.", "Carl Froch", "10 below in Chicago, Illlinois.", "\"have no problems about the school, they are happy about everything.\"", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "The man who shot him,", "The pilot, whose name has not yet been released,", "humanitarian transport.", "the Iraqi and U.S. soldiers were attacked by small-arms, machine-gun and RPG fire from buildings overlooking the road.", "air support.", "Marie-Therese Walter.", "neither Sudanese nor orphans,", "free enterprise in history", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "Swansea Crown Court,", "U.S. Court of Appeals for the District of Columbia.", "Michael Jackson", "Kurdistan Freedom Falcons,", "insurgent small arms fire.", "schools better", "a lump in Henry's nether regions", "in Florida", "glamour and hedonism", "smile softly at the powerless bodies they hold.", "a peace sign.", "the Southeast,", "Diego Milito's", "The man who was killed had been part of a hunting party of three men,", "Colombian President Alvaro Uribe", "Florida", "Christopher Savoie", "rebels", "three Ghanaians, two Liberians and a Togo national", "\"How I Met Your Mother,\"", "British capital's other two airports, Stansted and Gatwick,", "The Casalesi Camorra clan", "Stratfor", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "CBS, CNN, Fox and The Associated Press.", "Ashley \"A.J. Jewell,", "The stratum lucidum", "Gettysburg College", "John Cabot", "Celsius", "75", "supreme religious leader of the Israelites", "Charles Quinton Murphy", "Rio Gavin Ferdinand", "more than 110 films", "Athens", "(especially politicians) who misuse words", "artesian"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4840289051226552}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.25, 0.0, 0.5, 0.0, 1.0, 0.4, 0.2666666666666667, 0.0, 0.33333333333333337, 0.0, 0.2666666666666667, 0.125, 0.2222222222222222, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.8, 0.0, 0.0, 0.6666666666666666, 0.375, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9459", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-3680", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-561", "mrqa_naturalquestions-validation-8585", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-1390", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-4004", "mrqa_searchqa-validation-1916", "mrqa_searchqa-validation-16961"], "SR": 0.359375, "CSR": 0.5606971153846154, "EFR": 1.0, "Overall": 0.7263581730769231}, {"timecode": 26, "before_eval_results": {"predictions": ["enhanced transit infrastructure, possible shuttles open to the public, and park space which will also be publicly accessible.", "Katy\u0144 Museum", "two", "a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship.", "David Anthony O'Leary", "1822", "Stephanie Plum", "Dominican", "Evgeni Platov", "Knoxville, Tennessee", "Enkare Nairobi", "Field Marshal Stapleton Cotton,", "Shari Shattuck", "Si Da Ming Bu", "The Blue Album", "Apsley George Benet Cherry-Garrard", "A basilica", "Blender", "The Odawa", "Columbus Crew SC", "Yoo Seung-ho", "1989 until 1994", "Peter & Gordon", "Wildhorn, Bricusse and Cuden", "pornographicstar", "Montreal", "Domingo \"Sam\" Samudio", "John Nicholas Galleher", "San Francisco 49ers", "Prince of Cambodia Norodom Sihanouk", "Durham, North Carolina", "Double Crossed", "19th", "Donald McNichol Sutherland", "provides its services in the Japanese market", "Security Management", "14,372", "musician", "Cersei Lannister", "Fort Worth", "Dutch", "shorthand writing", "North Atlantic Treaty Organisation (NATO)", "The Young Ones", "Tabasco", "Sunday, November 2, 2003", "1812", "1.5 million", "Neymar da Silva Santos J\u00fanior", "Plymouth Regional High School", "Tainted Love", "Dissection", "Cecil Lockhart", "MercyMe", "Five years later", "a ride cymbal", "Al Jazeera", "Anabaptists and the non-sectarians", "38 feet", "ties to paramilitary groups,", "Shenzhen", "Brigham Young", "Silly Putty", "Ginnie Mae"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5989679339477727}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8387096774193548, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6426", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3749", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-111", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-34", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3889", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-2131", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-5740", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-3954", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2495", "mrqa_searchqa-validation-10970"], "SR": 0.484375, "CSR": 0.5578703703703703, "EFR": 1.0, "Overall": 0.7257928240740741}, {"timecode": 27, "before_eval_results": {"predictions": ["In low-light conditions", "1550", "60 days", "$60,000", "four", "Steve Goodman", "Jonathan Goldstein", "Cristeta Comerford", "Kirstjen Nielsen", "1996", "Tristan Rogers", "Mase Dinehart", "counter clockwise direction", "Paul Hogan", "tennis", "Matt Monro", "IIII", "Fa Ze Rug", "8ft", "Robert Irsay", "to manage the characteristics of the beer's head", "a moral tale", "2019", "Carrie Hegele", "the south", "1988", "his guilt in killing the bird", "1", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "December 1, 2017", "James Watson and Francis Crick", "Sara Gilbert", "Athens", "the courts", "John Travolta", "Djokovic", "Rigg", "Ant & Dec", "Nepal", "Joanne Wheatley", "UNESCO / ILO", "September 24, 2012", "lowest air temperature record was set on 21 July 1983", "summer", "between the Eastern Ghats and the Bay of Bengal", "Daniel Suarez", "1 - 2 spinal nerve segments above the point of entry", "16 August 1975", "useless, time - wasting activity", "Kyla Pratt", "Tim McGraw", "Beorn", "Supreme Commander Gen. Douglas MacArthur", "Daily Mail", "Scotland", "Forbes", "2006", "Fife", "on China, Taiwan, Hong Kong and Mongolia,", "Russia and the United States", "one American diplomat to a \"prostitute\"", "unassisted triple play", "Ben Kingsley", "(not Paris)"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6732579500472646}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.8, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.7692307692307692, 1.0, 1.0, 0.0, 0.06451612903225806, 1.0, 0.0, 1.0, 0.09090909090909093, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9, 1.0, 0.8, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.4, 1.0, 0.4, 0.26666666666666666, 0.4, 0.9090909090909091, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1313", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-7158", "mrqa_triviaqa-validation-3649", "mrqa_triviaqa-validation-6967", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-506", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-2350", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-12174"], "SR": 0.546875, "CSR": 0.5574776785714286, "EFR": 0.9310344827586207, "Overall": 0.7119211822660099}, {"timecode": 28, "before_eval_results": {"predictions": ["Elisabeth Sladen", "1206", "Genghis Khan", "Shenzhen in southern China.", "24", "Monday night where it left off in September with what Sedgwick called \"a fantastic five episodes.\"", "a delegation of American Muslim and Christian leaders", "September 21.", "1,500", "Romney for his \"solid credentials,\"", "269,000", "Auckland, setting up a 19-12 victory in Auckland,", "The Ski Train is a 68-year-old local favorite that shuttles about 750 people between Denver and Winter Park.", "sailboat matching the description of the missing 38-foot boat was found overturned about 5:15 p.m. Saturday,", "on the side, looking at the young man in the middle of John Brown Avenue, which is usually one of the busiest streets in the capital city.", "has \"committed acts of terrorism, it is committed to the obliteration of the state of Israel, and its statement last week that it was legitimate to kill Jewish children", "dogs who walk on ice in Alaska.", "his father's parenting skills.", "some of the Awa before killing them with knives.", "Haitians", "President Obama", "Chevron", "about 10 toasters,", "can roam freely, largely feed and shelter themselves and interact with others, often after years living alone in captivity.", "to secure more funds", "to fire a missile toward Hawaii", "he retired from the Army after nearly 40 years of service with the rank of lieutenant general on April 28 -- one day before he was sworn in as ambassador,", "defaulted on the mortgage and the house fell into foreclosure.", "A member of the group dubbed the \"Jena 6\"", "state governments", "Anil Kapoor", "Illlinois.", "by raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond.", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Nazi Germany", "has been widely praised and adopted as a model around the world.", "gun", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "don't believe the U.S. assertion that the system is needed to guard against imminent threats from Iran or North Korea.", "David McKenzie", "citizenship because he was depriving his wife of the liberty to come and go with her face uncovered,", "heavy brush,", "through a facility in Salt Lake City, Utah,", "insinuate that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis,", "remains unknown,", "can I", "Alwin Landry's supply vessel Damon Bankston", "along the equator between South America and Africa.", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case is recovering well and wants to see the ocean and a pop concert,", "an empty tub, his face blue and purple and a chain around his neck,", "in a tenement in the Mumbai suburb of Chembur,", "California, Texas and Florida,", "Rigg", "the efferent nerves that directly innervate muscles", "Spain", "The Duchess, which opens on September 5,", "Bahrain", "Tokyo International Airport", "Danny Lebern Glover", "William Shakespeare", "British", "an assault rifle", "rookoe", "Turtle Wax"], "metric_results": {"EM": 0.28125, "QA-F1": 0.4660646242692317}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.23529411764705882, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.2222222222222222, 0.13333333333333333, 0.0, 0.0, 0.25, 0.6666666666666666, 0.5454545454545454, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.7499999999999999, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.3076923076923077, 0.4615384615384615, 1.0, 0.8333333333333333, 1.0, 0.0909090909090909, 0.08333333333333333, 1.0, 0.1111111111111111, 0.0, 1.0, 0.967741935483871, 1.0, 0.0, 0.5, 0.8750000000000001, 0.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.5, 0.0, 0.2857142857142857, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-301", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-2338", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-7484", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-1993", "mrqa_hotpotqa-validation-1922", "mrqa_searchqa-validation-9212", "mrqa_searchqa-validation-6288"], "SR": 0.28125, "CSR": 0.5479525862068966, "EFR": 1.0, "Overall": 0.7238092672413794}, {"timecode": 29, "before_eval_results": {"predictions": ["Disney\u2013ABC Domestic Television", "2011", "Soviet", "naltrexone", "sense of smell", "Brigit Forsyth", "Florence", "Wrigley", "Oprah Winfrey", "2004", "a Great Dane", "Director General of the Security Service", "Caracas", "Rock Follies of \u201977", "Richard Rodgers - Stephen Sondheim", "Nobel Prize in Literature", "Celtic", "Northwestern University", "the best value diamond for your money", "The Star Spangled Banner", "(a)", "Ibrox Stadium", "New York", "Sir Stirling Craufurd Moss", "Micael Caine", "Llyn Padarn", "Little Dorrit", "Tacitus", "apples", "Chekhov", "Chris Evans", "John Keats", "Declaration of Independence", "Lome", "a condor", "Belgium", "Pilgrim's Progress", "Xenophon", "Fulham Football Club", "graphite", "Australia", "Alaska", "Cain", "the town of Jerez de la Frontera", "Michelin-Bibendum", "Tesco", "Deep Throat", "Patrick Newell", "Clio Awards", "Pygmalion", "Watford", "Trainspotting", "English", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Vicente Fox", "Central-Eastern Europe", "1919", "August 24, 1983", "ensuring that all prescription drugs on the market are FDA approved,", "International Polo Club Palm Beach in Florida.", "more than 200.", "Sappho", "Washington, DC", "Steely Dan"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6763880621693122}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.4, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.4, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.9600000000000001, 1.0, 0.0, 1.0, 0.5, 0.07407407407407408, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7651", "mrqa_triviaqa-validation-2203", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-4186", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-566", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-1069", "mrqa_triviaqa-validation-2611", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-45", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-3697", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-4547", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-4122", "mrqa_searchqa-validation-8687"], "SR": 0.578125, "CSR": 0.5489583333333333, "EFR": 0.9259259259259259, "Overall": 0.7091956018518519}, {"timecode": 30, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2030", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-3249", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3655", "mrqa_hotpotqa-validation-3701", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-5047", "mrqa_hotpotqa-validation-505", "mrqa_hotpotqa-validation-5213", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5645", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-988", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4308", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-994", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2553", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-919", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-14268", "mrqa_searchqa-validation-14735", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-1664", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_squad-validation-10052", "mrqa_squad-validation-10125", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10308", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-1159", "mrqa_squad-validation-1193", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-1368", "mrqa_squad-validation-1503", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2166", "mrqa_squad-validation-2324", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2778", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-3259", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3831", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3916", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-4065", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4191", "mrqa_squad-validation-4248", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4746", "mrqa_squad-validation-4836", "mrqa_squad-validation-5009", "mrqa_squad-validation-5088", "mrqa_squad-validation-5108", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5180", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5521", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5964", "mrqa_squad-validation-6001", "mrqa_squad-validation-6069", "mrqa_squad-validation-6082", "mrqa_squad-validation-6158", "mrqa_squad-validation-6256", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6592", "mrqa_squad-validation-6605", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-709", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7476", "mrqa_squad-validation-7485", "mrqa_squad-validation-7502", "mrqa_squad-validation-7578", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-8159", "mrqa_squad-validation-8213", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8580", "mrqa_squad-validation-8681", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8935", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9141", "mrqa_squad-validation-9270", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9510", "mrqa_squad-validation-9569", "mrqa_squad-validation-964", "mrqa_squad-validation-9759", "mrqa_squad-validation-9766", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1329", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-576", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-855", "mrqa_triviaqa-validation-874", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-996"], "OKR": 0.86328125, "KG": 0.45078125, "before_eval_results": {"predictions": ["1947", "d'Alen\u00e7on", "$125 per month", "helen shapiro", "Newbury", "duchess", "plato", "a Righteous Man", "Lisieux", "Astor family", "ymdrechion", "Canned Heat", "John Huston", "Daniel Casey", "Christopher Lee", "caboto", "the Advisory Council of Science and Industry", "Patrick Kielty", "Norfolk Island", "d'Ivoire", "Mexico", "Worcester Cathedral", "The French Connection", "Albert Finney", "Carrefour", "Ken Russell", "in the garden of Gethsemane", "John Galliano", "William Walmsley", "Bartlett Sher", "Mickey Mouse", "protection from UVB rays", "Xenophon", "Bugsy Malone", "1812", "llanar", "basil", "llanet", "crested weir", "lyoness", "raclette", "the AllStars", "hk", "Copenhagen", "japan", "jazz", "George Walker Bush", "Nicky Henderson", "David Mitchell", "Zachary Taylor", "Old Ironsides", "furrow", "Total Drama World Tour", "General George Washington", "Real Madrid", "two", "Balvenie Castle", "January 18, 1977", "threatening messages", "London", "of \"The Real Housewives of Atlanta\" reunion special,", "a lemur", "Communist Party", "misfortune"], "metric_results": {"EM": 0.5, "QA-F1": 0.583110119047619}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5491", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2299", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5311", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-6074", "mrqa_triviaqa-validation-3052", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4499", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-7608", "mrqa_triviaqa-validation-2611", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3285", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-3517", "mrqa_hotpotqa-validation-1351", "mrqa_newsqa-validation-450", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-3674"], "SR": 0.5, "CSR": 0.5473790322580645, "EFR": 1.0, "Overall": 0.7133039314516129}, {"timecode": 31, "before_eval_results": {"predictions": ["nine", "along the Lower Rhine", "Pet Shop Boys", "rugby", "geologic chronological term", "Budapest", "Mikhail Baryshiro", "a doodle stick", "joan crawford", "I'm Sorry, I Haven't a Clue", "Jean-Paul Gaultier", "javelin throw", "silvery blue", "Chickens", "French", "finland", "buttock", "Isambard", "Turandot", "b'Stard", "joan crawford", "Pablo Picasso", "apples", "440 hertz", "the Spanish", "York Minster", "a karst cave", "mexico", "Belarus", "Russia", "sense of taste", "chile", "Boojum", "b - whale", "finland", "senior", "Paul Merton", "Anna", "Paris", "gethse", "Venado Tuerto, Argentina", "Charlie Chaplin", "The Perfect Storm", "lacrosse", "pottery", "finisher", "Newbury Racecourse", "Abraham", "vice-admiral", "2016", "joan crawford", "joan crawford", "Rachel Kelly Tucker", "the BBC", "pilgrimages to Jerusalem", "a farmers' co-op", "Tim Allen", "Anatoly Lunacharsky", "an empty water bottle down the touchline following a disallowed goal for Arsenal.", "Japan's fisheries agency", "The Rev. Alberto Cutie", "Eurasian Economic Union", "glenn crawford", "Ibn Saud"], "metric_results": {"EM": 0.296875, "QA-F1": 0.4040865384615384}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9344", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-4054", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-4765", "mrqa_triviaqa-validation-5237", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-1633", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-1261", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-5259", "mrqa_triviaqa-validation-7645", "mrqa_triviaqa-validation-5549", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-6730", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5728", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-103", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-12801", "mrqa_searchqa-validation-2441"], "SR": 0.296875, "CSR": 0.53955078125, "EFR": 0.9777777777777777, "Overall": 0.7072938368055556}, {"timecode": 32, "before_eval_results": {"predictions": ["TeacherspayTeachers.com", "provisional elder/deacon", "fibers", "Prince Harry", "Mujib", "b\u00e9la Bart\u00f3k", "Denver", "sailor", "Salt Lake City", "four", "secretary", "brazil", "Canada", "Peter Nichols", "American Family Publishers", "seven suits", "Microsoft", "Brigit Forsyth", "Celsius", "curvature", "amalthea", "leicestershire", "Boris Johnson", "HMS Conqueror", "Tamar", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "irons", "inner ear", "Gloucestershire", "Norway", "gymnastics", "ginger Rogers", "Ishmael", "bluebell", "ned", "Prokofiev", "Cyclone", "Dan Brown", "horses", "Newcastle United", "Thank you", "cgs", "second-year", "William Neil Connor", "boston", "charles chaplin", "horse-racing", "slow", "ball-and- socket", "dragon", "peregrines", "1938", "Anirudh Sinha", "Tim Rice", "five", "The Walking Dead", "1979", "1999", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "Apple employees", "suicide", "the Battle of Marston Moor", "February", "Athol Fugard"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6919070512820513}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-2608", "mrqa_triviaqa-validation-1838", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-7671", "mrqa_triviaqa-validation-2258", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-6726", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-5666", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-6815", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-10161", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-1889", "mrqa_searchqa-validation-8011"], "SR": 0.640625, "CSR": 0.5426136363636364, "EFR": 1.0, "Overall": 0.7123508522727272}, {"timecode": 33, "before_eval_results": {"predictions": ["1,100", "since 2001", "horseshoe", "(Barry) Briggs", "table tennis", "lord andrew haddock", "bart\u00f3k", "19", "Harold Shipman", "The Undertones", "michael hordern", "norway", "yeast", "michael hold", "(359-299 Ma)", "Thank you", "nipples", "queen Mary", "lola", "muscle tissue", "Surrealism", "shinto", "sewing machines", "Morgan Spurlock", "john Buchan", "algae", "norway", "norway", "shorthand", "Altamont", "fourteen", "jack Sprat", "?Operation Dynamo?", "(Tony Washington, Willie Woods and Victor Thomas)", "john Brooke", "Praseodymium", "pickled", "50p", "al rosmarino", "Alan Greenspan", "Wicked Witch", "Rita Hayworth", "The Observer", "Sir Isaac Newton", "Turnbull & Asser, Hawes & Curtis, Thomas Pink, Harvie & Hudson, Charles Tyrwhitt and T. M. Lewin", "bullfighting", "Arthur C. Clarke", "Marc Warren", "entropy", "Chad", "Earring", "Taggart", "on the microscope's stage", "Ole Einar Bj\u00f8rndalen", "Norway", "Virgin", "Philadelphia Naval Shipyard", "afro-Caribbean", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "cross-country skiers", "was \"very important that Brazil and the United States work closely in this field,\"", "a ladder", "Hill Street Blues", "grey"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6112780448717949}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5384615384615384, 1.0, 0.42857142857142855, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-1410", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-7142", "mrqa_triviaqa-validation-368", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-1431", "mrqa_triviaqa-validation-7043", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3060", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-1369", "mrqa_naturalquestions-validation-182", "mrqa_hotpotqa-validation-1813", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-7985"], "SR": 0.546875, "CSR": 0.5427389705882353, "EFR": 0.9655172413793104, "Overall": 0.7054793673935091}, {"timecode": 34, "before_eval_results": {"predictions": ["second Gleichschaltung", "two of Tesla's uncles", "Renault", "kinks", "Massachusetts", "lyonesse", "6", "Bono", "ink", "Toy Story", "house sparrow", "The Lion", "intestines", "Independence Day", "Charlie Brooker", "Oxford", "number 13", "Glasgow", "thomas france france", "Florence", "Wat Tyler", "Tony Meo", "Black Wednesday", "vomiting", "Ennio Morricone", "NBA", "enid blyton", "1749", "horseradish", "chile", "1066", "edward Emanuel", "$1", "edward Margaret College", "Dian Fossey", "port", "checkers", "Norman Mailer", "an action figure", "jura", "edward deves", "chatsworth House", "pongo", "quant pole", "Scooby-Doo", "The Pennine Way", "peter bowlles", "lorne Greene", "thomas Jefferson", "Frans Hals", "alpha Bravo Charlie", "richmond", "the University of Oxford", "Matt Monro", "Greensleeves", "Belarus", "\"Big Fucking German\"", "Welterweight", "Harrison Ford", "The EU naval force", "Brooklyn, New York,", "upperglacial", "The 39 Steps", "dynamo"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6007965686274509}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-1015", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-2232", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-3840", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6896", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-4210", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-587", "mrqa_hotpotqa-validation-1891", "mrqa_newsqa-validation-1335", "mrqa_searchqa-validation-14545", "mrqa_searchqa-validation-2191"], "SR": 0.5625, "CSR": 0.5433035714285714, "EFR": 0.9642857142857143, "Overall": 0.7053459821428572}, {"timecode": 35, "before_eval_results": {"predictions": ["Tracy Wolfson and Evan Washburn", "2013", "Russ Conway", "blue", "london", "York", "Austria", "a poster", "j john tommie Connor", "barium, magnesium, iodine, chlorine, and potassium", "cuthbert", "one hundred", "soybeans", "dennis taylor", "Pinot Noir", "Seal", "london", "King Henry VI", "Dick Whittington", "peacock", "Pisces", "Ishmael", "smell", "Brad Pitt", "Eleanor Rigby", "The Simpsons", "One Direction", "yellows", "Cornell University", "nipper", "vinegar jo", "london", "Follicle-stimulating hormone", "paramitas", "step-by-step solution to", "ned arthur", "france", "london", "racecar", "Costa Concordia", "the Dominican Republic", "Gary Gibbon", "london", "an insect", "horseshoes", "King George III", "and is one of only two presidents\u2014the other being John Quincy Adams\u2014to be the son of a former president and made an unsuccessful run for president in 2016.", "oil capital of Europe", "an orange", "jays", "Yassir Arafat", "the Black Sea", "Jennifer O'Neill", "49 cents", "1", "Sim\u00f3n Bol\u00edvar", "Wings of Desire", "actor and former fashion model", "Airbus A330-200", "an account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "an FAA-certified physician every year; those over 40,", "Roland Garros", "rhinoplasty", "global village"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4703125}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-584", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-4807", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-168", "mrqa_triviaqa-validation-1757", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-6664", "mrqa_triviaqa-validation-3593", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-5010", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-121", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-5128", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-276", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11091"], "SR": 0.421875, "CSR": 0.5399305555555556, "EFR": 1.0, "Overall": 0.711814236111111}, {"timecode": 36, "before_eval_results": {"predictions": ["independent schools", "cymbals", "Djibouti and Yemen,", "the Nile", "Barcelona", "Meg Ryan", "llandudno", "a beggar woman", "Mt. McKinley", "Dover", "rabbit", "crow", "The Flintstones", "the Great Chicago Fire", "gelatine", "Ecuador", "Cyprus", "a charity-boy", "homeless", "Dublin", "london", "walker", "rent doesn't include additional costs such as insurance or business rates", "Google", "lulu", "Pembrokeshire Coast National Park", "Tripoli", "jimmy Burns", "Dreamgirls", "Opus Dei", "Belize", "an undergraduate academic degree", "The Mary Whitehouse Experience", "Liberator", "the James Gang", "a goat", "Dubai", "Sydney", "orange", "Bank of England", "self-serving and loopy.", "Ordovices", "Robert Devereux", "Dealings with the Firm of Dombey and Son: Wholesale, Retail and for Exportation", "mexico", "pascal", "jimmy boyd", "Boris Becker", "Dr. Julius No", "Amsterdam", "Peter Ustinov", "24", "Ford", "between 27 July and 7 August 2021", "94", "Dan Castellaneta", "La Liga", "environmental", "five minutes before commandos descended from ropes that dangled from helicopters,", "St. Louis, Missouri,", "cheese", "Jezebel", "Goodson-Todman", "city"], "metric_results": {"EM": 0.5, "QA-F1": 0.544140625}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.625, 0.8, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-5503", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-6236", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7186", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-7594", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-562", "mrqa_naturalquestions-validation-5647", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-621", "mrqa_searchqa-validation-11332", "mrqa_searchqa-validation-12212"], "SR": 0.5, "CSR": 0.5388513513513513, "EFR": 1.0, "Overall": 0.7115983952702702}, {"timecode": 37, "before_eval_results": {"predictions": ["uncivilized", "jimmy boy,", "9", "m\u00e1laga", "hugh hefner", "Tiananmen", "noises off", "julius johnson", "Till Death Us Do Part", "javier Bardem", "1720", "australia", "endometriosis", "The Hague", "red", "circular line", "henry johnson", "svetlana Savitskaya", "g\u00e9rard Depardieu", "jimmy johnson", "julium", "herpes zoster", "small faces", "slow", "Angela dothea Kasner", "crawbert", "lung cancer", "henry johnson", "spectator", "2", "Aslan", "jethro", "Vancouver Island", "purdy", "blues Brothers", "six", "double-hung", "Marie Trepanier", "Stockholm", "st Helens", "beta", "henry johnson", "Basil Fawlty Towers", "blue", "henry west", "boreas", "Rumble in the Jungle", "violins", "Lady Gaga", "Burgundy", "colleen McCullough", "retinal ganglion cell axons and glial cells", "2026", "Don McMillan", "50th anniversary of the founding of the National Basketball Association (NBA)", "The Cherokee Nation", "r\u014dm\u0101num", "45th anniversary.", "Edie", "the man facing up, with his arms out to the side.", "stomatrini", "Santo Versace", "yearwood", "burt Bacharach"], "metric_results": {"EM": 0.40625, "QA-F1": 0.47617442810457516}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.35294117647058826, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6964", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-2675", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4963", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-5616", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-4129", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-856", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-614", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-6638", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-1958", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-1855", "mrqa_searchqa-validation-12111", "mrqa_searchqa-validation-15107", "mrqa_searchqa-validation-3037", "mrqa_naturalquestions-validation-6125"], "SR": 0.40625, "CSR": 0.5353618421052632, "EFR": 1.0, "Overall": 0.7109004934210527}, {"timecode": 38, "before_eval_results": {"predictions": ["71", "(Cecil) Rhodes", "constant", "noun", "a triangle", "jedoublen/jeopardy", "(Wy) Earp", "Anne", "root beer", "honey Nut Cheerios", "Venus", "duc de Berry", "Jesse James", "secretary of state", "density", "merlusconi", "g Clooney", "Barack Obama", "jedoublen/jeopardy", "Brave Little Toaster", "jubarb and pear ginger", "macau", "60 mph", "jaffa", "Dan Marino", "Munich", "viola", "duchy of Luxembourg", "butterflies are __", "yamazaki whisky", "Tower of London", "you Bet Your Life", "deuteronomy", "zounds", "bix", "yellow", "Crimean War", "opium", "nave", "concave", "caspian sea", "Gretchen Wilson", "Scooby-Doo", "(Dean) Cain", "hypnotic", "South Dakota", "polly", "four", "hautboy", "orson welles", "roxy", "subduction zone", "Erica Rivera", "Julia Roberts", "wuthering Heights", "Craggy Island", "greece", "Idaho", "Belgian", "Esteban Ocon", "Roberto Micheletti", "greece", "19", "Oahu"], "metric_results": {"EM": 0.5, "QA-F1": 0.546875}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-3544", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-15961", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-11212", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-13762", "mrqa_searchqa-validation-16868", "mrqa_searchqa-validation-8389", "mrqa_searchqa-validation-7647", "mrqa_searchqa-validation-10062", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-11967", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-5544", "mrqa_searchqa-validation-9662", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-11364", "mrqa_naturalquestions-validation-6285", "mrqa_hotpotqa-validation-4023", "mrqa_newsqa-validation-267", "mrqa_hotpotqa-validation-4625"], "SR": 0.5, "CSR": 0.5344551282051282, "EFR": 0.96875, "Overall": 0.7044691506410257}, {"timecode": 39, "before_eval_results": {"predictions": ["inequality in wealth and income", "Princess Aisha bint Hussein", "Baugur Group", "Westchester County", "City Mazda Stadium", "841", "American", "CinemaScope", "Kaep", "Charmed", "Southern Rhodesia", "Alonso L\u00f3pez", "broadcast internationally", "Voni Morrison", "Galleria Vittorio Emanuele II", "Coalwood, West Virginia", "Aubrey Posen", "neuro-orthopaedic", "City of Westminster", "British", "Albert", "6,241", "Dan Bilzerian", "STS-51-C.", "an American business magnate, investor, and philanthropist", "Crackle", "Kristy Lee Cook", "Perth", "Love Streams", "1935", "5.3 million", "New York City", "Dara Torres", "The dyers of Lincoln", "actress", "a role-playing game or wargame campaign", "May 5, 2015", "Red and Assiniboine Rivers", "methylenedioxy metha", "Neymar", "The Jefferson Memorial", "Strange Interlude", "Bothtec", "2,099", "a body of water", "35,672", "2004 Paris Motor Show", "1996", "33 of the 100 seats", "1999", "Axl Rose", "5 - 7", "John Corker", "George Harrison", "Bruno Mars", "Wyre", "Azzurri", "Heshmatollah Attarzadeh", "a one-shot victory in the Bob Hope Classic", "700", "the Untouchables", "Monopoly", "a cookie jar", "2002"], "metric_results": {"EM": 0.546875, "QA-F1": 0.670498511904762}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.8, 0.5, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.4, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-3495", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2707", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-404", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-5316", "mrqa_triviaqa-validation-1097", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1230"], "SR": 0.546875, "CSR": 0.534765625, "EFR": 1.0, "Overall": 0.7107812499999999}, {"timecode": 40, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1616", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3708", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-612", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12704", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15107", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-2178", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7865", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9870", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10149", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1844", "mrqa_squad-validation-1967", "mrqa_squad-validation-2049", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3428", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3815", "mrqa_squad-validation-3836", "mrqa_squad-validation-3837", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4135", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-503", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5338", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-5859", "mrqa_squad-validation-5893", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6435", "mrqa_squad-validation-6506", "mrqa_squad-validation-6671", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7002", "mrqa_squad-validation-7193", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7704", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8084", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8935", "mrqa_squad-validation-902", "mrqa_squad-validation-9254", "mrqa_squad-validation-9300", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9479", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3640", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3874", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4239", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4527", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6152", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-657", "mrqa_triviaqa-validation-6762", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7400", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-996"], "OKR": 0.8671875, "KG": 0.49140625, "before_eval_results": {"predictions": ["2006", "Ghana", "Dublin", "Ribhu Dasgupta", "George Gordon Byron", "\"Catch Me If You Can\"", "Irish", "Sulfur mustard", "12\u201318", "Mike Biden", "Jeffrey William Van Gundy", "The Zombies", "Ballarat Bitter", "Bank of China Building", "(January 11, 1881 \u2013 March 24, 1938)", "26,000", "TransAd Adelaide", "Gillian Leigh Anderson", "Steve Carell", "The Second City", "Lauren Alaina", "National Football League", "1943", "Fountains of Wayne", "Saint Paul, Minnesota", "composer", "and Newfound Lake", "October 20, 2017", "Zimbabwe", "Larry Eustachy", "Douglas Jackson", "Richard Strauss", "Rabat", "Straits of Gibraltar", "American burlesque", "Russell T Davies", "Jay Schottenstein", "600", "April 30, 1982", "Martin \"Marty\" McCann", "Nine Inch Nails", "Labour Party", "Orlando\u2013Kissimmee\u2013Sanford,", "Prussia", "Boston, Massachusetts", "Bambi, a Life in the Woods", "from 1993 to 1996", "Watertown, New York", "green and yellow", "ice hockey", "Kids", "3 September", "the Gaget, Gauthier & Co. workshop", "200 to 500 mg", "Paul Gauguin", "Ynys M\u00f4n", "Saturday Night Live", "an antihistamine and an epinephrine auto-injector", "General Motors", "outside his house in Najaf's Adala neighborhood", "a Panic's", "Backstreet Boys", "Momentum", "hemoglobin"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5582228535353535}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.2, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-5509", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-507", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-883", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2527", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-4639", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-1008", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-2482", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-8555", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-5417", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-1606", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-3357"], "SR": 0.421875, "CSR": 0.5320121951219512, "EFR": 1.0, "Overall": 0.7257774390243903}, {"timecode": 41, "before_eval_results": {"predictions": ["his sons and grandsons", "Memory Recognition", "Inundate", "the camera attachment", "Bears", "Parris Island", "Bagels", "She was married a staggering eight times to seven men", "She is also one of my favorite 80's actress.", "Mick Taylor", "White blood cells", "Al Capone", "Matsu", "Stardust", "a opera", "Three Little Pigs", "Quinn the Eskimo", "Medical Malpractice", "a grizzly bear", "Kareem Abdul-Jabbar", "The Police", "Travel Channel", "Henry Clay Frick", "Gorbachev", "a ghost", "Bloomberg", "Amateur Radio", "a pagoda", "The Pythian Games", "a feisty and kind-hearted passenger", "Franklin D. Roosevelt", "Aries", "the Golden Fleece", "Alzheimer's disease", "Chuck Yeager", "penguin", "\"PANT\"s", "a toilet", "Vermont", "by. She was named World Swimmer of the Year", "a bazooka", "zenith", "the Whig", "Vietnam", "Ectoplasm", "by the German label Behind s.r.b.)", "Old North Church", "binocular", "She is an American vocal quartet", "Legally Blonde", "Scorpio", "the Rashidun", "Kaley Christine Cuoco", "H ions", "Salix", "Slaghoople", "Fun Advice Trivia", "Canning", "New Orleans Saints", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "said the claims are unfounded, and it's the union that has harassed him.", "Aldgate East.", "Awa", "crossword"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48870192307692306}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2, 0.07692307692307691, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-13266", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-9985", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-9793", "mrqa_searchqa-validation-1329", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-12679", "mrqa_searchqa-validation-16343", "mrqa_searchqa-validation-591", "mrqa_searchqa-validation-12899", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-1471", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-1035", "mrqa_triviaqa-validation-987"], "SR": 0.421875, "CSR": 0.5293898809523809, "EFR": 1.0, "Overall": 0.7252529761904761}, {"timecode": 42, "before_eval_results": {"predictions": ["Westminster", "Ratatouille", "the Swift", "Catherine de Medici", "Wenceslas", "Ecuador", "Microsoft", "Katharine Hepburn", "binocular", "forest", "London", "Little Boy Blue", "cotton", "Alexander", "Seinfeld", "John Paul Jones", "the Bell X-1", "Spider-Man", "Camelot", "Hudson Bay", "Hamlet", "an axiom", "St Patrick\\'s Cathedral", "Frosted Flakes", "King George III", "Solomon", "Wheaton", "Heart of Darkness", "Rastafari", "Beverly Cleary", "a pizza sin.", "Heartbreak Hotel", "Spain", "angels", "Pisa", "slapstick", "Bangkok", "Cuba Gooding Jr.", "Russia", "Burt Lancaster", "Reuleaux triangle", "the Chinese Communist Party", "sacristy", "Israel", "Othello", "Alabama", "Making the Band 3", "Martinique", "Sure Deodorant", "the Pathfinder", "the Lion King", "Florida", "Norman", "After tentatively courting each other in `` Entropy ''", "dragonfly", "is our children learning", "firethorn", "alt-right", "Australian", "Scott Saiki", "Negotiators for Zelaya and Roberto Micheletti,", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson.", "Daytime Emmy Lifetime Achievement Award", "Australia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6041666666666667}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-1830", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-854", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-8318", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-1179", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-314", "mrqa_searchqa-validation-8416", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-8515", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-1452", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10320", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-616", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-755", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-1351"], "SR": 0.515625, "CSR": 0.5290697674418605, "EFR": 1.0, "Overall": 0.7251889534883721}, {"timecode": 43, "before_eval_results": {"predictions": ["jellyfish", "enforcing racially separated educational facilities", "the Primal rib", "late January or early February", "Monk's Caf\u00e9", "Christopher Lloyd", "Morgan Freeman", "1910", "over 74 languages", "John Vincent Calipari", "Floyd", "2019", "James Madison", "either by voting or voice vote", "1917", "Icarus", "Mike Alstott", "the Red Sea", "1982", "1956", "Harry", "Monk's", "on the Atchafalaya River", "Pete Seeger", "depression", "Optimus", "in sequence with each heartbeat", "Kevin Sumlin", "Bonnie Lipton", "American drama film", "two", "gastrocnemius", "transmissions", "Roy Eberhardt moves to Florida and into the town of Coconut Cove, where his classmate Dana Matherson starts bullying him", "junior enlisted sailor", "March 9, 2018", "937 total weeks", "Johannes Gutenberg", "cadmium", "1961", "2 September 1990", "Eastern Redbud", "cephalopods", "1939", "March 26, 1973", "Stephen Foster", "Charles Carson", "Bee Gees", "through Brazil, Bolivia, Paraguay and Argentina", "The White House Executive Chef", "3.9 and 5.5 mmol / L ( 70 to 100 mg / dL )", "Mike Danger", "conductor", "James Bond", "Manchester United", "Skipton Castle", "Duncan Kenworthy", "Elena Kagan", "a cardio for 45 minutes,", "Egyptian State TV ran footage Thursday of the assassination of President Mohamed Anwar al-Sadat", "Ernesto \"Che\" Guevara", "pinnipeds", "Amanda Seyfried", "Arizona Health Care Cost Containment System"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5022763694638694}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5000000000000001, 0.4, 1.0, 1.0, 0.0, 0.8, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5384615384615384, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307691, 1.0, 1.0, 0.5, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-2630", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-946", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-912", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-15077", "mrqa_hotpotqa-validation-1803"], "SR": 0.390625, "CSR": 0.5259232954545454, "EFR": 0.9230769230769231, "Overall": 0.7091750437062936}, {"timecode": 44, "before_eval_results": {"predictions": ["Neo-Confucianism", "April 2011", "James Intveld", "frontal lobe", "Haliaeetus", "Orange", "a low concentration in pigmentation", "Carol Ann Susi", "osseo - cartilaginous", "a premalignant flat ( or sessile ) lesion of the colon", "Arnold Schoenberg", "a jazz funeral without a body", "asexually", "mid November", "Deposition", "Andy Cole and Shearer", "George Strait", "boiling water reactor", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "October 2012", "201", "Rafael Nadal", "the International Border", "Melissa Disney", "the Hebrew Bible", "Gene MacLellan", "the fingers on either side of the mouth ( usually with the knuckles facing the observer )", "the world's a stage", "water ice", "Michael Schumacher", "on the microscope's stage", "living - donor", "North Atlantic Ocean", "John Hancock", "silk floss tree", "around 100,000 writes", "Triple threat", "Clarence Darrow", "alpha efferent neurons", "in teaching elocution", "the optic disc to the optic chiasma", "Butter Island off North Haven, Maine in the Penobscot Bay", "a combination of genetics and the male hormone dihydrotestosterone", "British Columbia, Canada", "saliva", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Frankie Valli", "908 mbar ( hPa ; 26.81 inHg )", "1939", "Pyeongchang County, Gangwon Province, South Korea", "Utah, Arizona, Wyoming, and Oroville, California", "Kuala Lampur", "Lidice", "Augustus Caesar", "Ars Nova Theater", "French mathematician and physicist", "1902", "Najaf.", "a judge to order the pop star's estate to pay him a monthly allowance,", "(3 degrees Fahrenheit),", "cutlery", "Caesar", "Possession is nine-tenths of the law", "last summer."], "metric_results": {"EM": 0.484375, "QA-F1": 0.6109699328449328}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true], "QA-F1": [0.25, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.25, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.3333333333333333, 0.4615384615384615, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.4615384615384615, 1.0, 0.6, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3076923076923077, 0.0, 0.0, 0.6666666666666666, 0.33333333333333337, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8160", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-9196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-1662", "mrqa_hotpotqa-validation-2863", "mrqa_newsqa-validation-1954", "mrqa_newsqa-validation-1076", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-6194"], "SR": 0.484375, "CSR": 0.525, "EFR": 0.9393939393939394, "Overall": 0.7122537878787878}, {"timecode": 45, "before_eval_results": {"predictions": ["adaptive immune system", "New Delhi", "the engineer \u00c9mile Gagnan and Naval Lieutenant ( `` lieutenant de vaisseau '' ) Jacques Cousteau", "a mid-size four - wheel drive luxury Mercedes -Benz GL - Class", "Wilhelm Groener", "compasses", "Lake Wales, Florida", "Anakin Skywalker", "Megan Park", "Dan Sogliuzzo", "Tulsa, Oklahoma", "Broken Hill and Sydney", "John Goodman", "the right side of the heart", "during the period of rest ( day )", "Oklahoma", "11 January 1923", "Rocky Mountains in southwestern Colorado and northwestern New Mexico", "Ann Gillespie", "the Himalayas", "Master Christopher Jones", "832 BCE", "Claudia Grace Wells", "Jerry Leiber and Mike Stoller", "1995 Mitsubishi Eclipse", "Natural - language processing", "Sir Alex Ferguson", "around 1872", "Jane Addams", "1990", "In December 1971", "Cairo, Illinois", "comic", "Abanindranath Tagore CIE", "Coldplay", "retina", "Empiricism", "1,149 feet ( 350 m )", "Lana Del Rey", "The Jewel of the Nile", "six degrees of freedom", "December 1886", "Ludacris", "A costume", "950 pesos ( approximately $ 18 ) in the Philippines or $60 abroad", "Frankie Muniz", "Freddie Highmore", "the nerves and ganglia outside the brain and spinal cord", "Andy Cole", "1966", "Scar's henchmen", "a visual defect in which colored objects appear to be tinged with color", "Perth", "a bramble fruit", "England", "two", "6,241", "Roy Foster", "share personal information.", "Stephen Tyrone Johns", "\"Austen\"tatious novel", "flat", "the Golden Hind", "Charles"], "metric_results": {"EM": 0.5, "QA-F1": 0.6192567415223665}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.5714285714285715, 0.7777777777777778, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.16666666666666669, 1.0, 0.4, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 1.0, 0.375, 0.4, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3018", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5041", "mrqa_hotpotqa-validation-5438", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-2549", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-6979", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-2528"], "SR": 0.5, "CSR": 0.5244565217391304, "EFR": 1.0, "Overall": 0.724266304347826}, {"timecode": 46, "before_eval_results": {"predictions": ["New England Patriots", "2.5 %", "an ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "Cliff Richard", "Frederik Barth in his review of this system of social stratification in Pakistan suggested that these are castes", "The British Indian Association", "foreign investors", "Redenbacher family", "British and French Canadian fur traders", "a line of committed and effective Sultans", "Jules Shear", "from 13 to 22 June 2012", "at Tandi, in Lahaul", "Thomas Jefferson", "Janie Crawford", "West Norse sailors", "the following year", "in 2005, against the Chicago White Sox, in which they were swept in four games", "in the front of the body", "Dottie West", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "Jesus Himself referenced the flower, saying `` Consider the lilies how they grow : they toil not, they spin not ; and yet I say unto you, that Solomon in all his glory was not arrayed", "its vast territory was divided into several successor polities", "Buffalo Lookout", "Aristotle", "December 1800", "Hemingway", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Cristeta Comerford 2005 -- present", "104 colonists and Discovery", "6 - 7 % average GDP growth annually", "Arnold Schoenberg", "Identification of alternative plans / policies", "The Outback", "quartz", "Wisconsin", "85 %", "Long Island", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "seven", "NFL owners", "Stan and Cartman accidentally destroy a dam, causing the town of Beaverton to be destroyed", "Juliet compares Romeo to a rose saying that if he was not named Romeo he would still be handsome and be Juliet's love", "a certified question or proposition of law from one of the United States Courts of Appeals", "gathering money from the public, which circumvents traditional avenues of investment", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "2018", "the contestant makes a thirty - second call to one of a number of friends ( who provide their phone numbers in advance )", "San Jose, California", "Gary Player, Nick Faldo, and Phil Mickelson", "Indo - Pacific", "Charles Cioffi", "Denise van Outen", "West Virginia", "Syracuse", "Girls' Generation", "Manchester, England,", "Authorities in Fayetteville, North Carolina,", "three out of four Americans are angry about the way things are going in the country.", "If a security officer were to pull a gun on an armed individual in a mall, it could result in \"the gunfight at the 'OK corral,'", "Jericho", "James Garfield Davis", "Catherine", "\"The Real Housewives of Atlanta,\""], "metric_results": {"EM": 0.40625, "QA-F1": 0.556052902650961}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.08333333333333333, 1.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.9090909090909091, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5283018867924527, 0.044444444444444446, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.08695652173913045, 0.11764705882352941, 0.8695652173913044, 0.4615384615384615, 1.0, 0.7499999999999999, 0.0, 0.0, 0.05714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.5, 0.0, 1.0, 0.4, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-259", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-5406", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-2578", "mrqa_triviaqa-validation-3771", "mrqa_hotpotqa-validation-4117", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-982", "mrqa_searchqa-validation-16615", "mrqa_newsqa-validation-4163"], "SR": 0.40625, "CSR": 0.5219414893617021, "EFR": 0.9473684210526315, "Overall": 0.7132369820828667}, {"timecode": 47, "before_eval_results": {"predictions": ["mumps", "The Last King of Scotland", "Kazakhstan", "Wilkie Collins", "pulmonary ligament", "Knutsford", "Burma", "Ewan McGregor", "a falcon", "South Park", "Shylock", "Canada", "Phil Spector", "Champagne", "shaftet", "the Surrealist movement", "bemidji", "Roddy Doyle", "climate", "Operation Frequent Wind", "Berlin", "Charlie Chan", "in 1871, Wanderers became the first winners of the FA Cup, beating Royal Engineers 1-0 at The Oval.", "Pinwright's Progress", "a winter fur hat", "Lady Gaga", "one of the first winter waterfowl to arrive in this area", "Christian Wulff", "the Kinks Are the Village Green Preservation Society", "the Queen of Comedy", "Debbie Rowe", "(William) Gladstone", "a centaur", "iodine deficiency", "fourteen", "Margaret Beckett", "James Hogg", "Welsh", "George Bernard Shaw", "table tennis", "Woolton Pie", "the Florida Current", "Boston Legal", "Brighton", "Gandalf", "1907", "Motown", "Canadian", "Pope Benedict XVI", "a dove", "John T. Cable", "translocation Down syndrome", "the American Civil War", "$2 million in 2011, with a winner's share of $315,600", "Amy Poehler", "Eric Allan Kramer", "Koninklijke Ahold N.V.", "via YouTube days after 35 bodies were found in two trucks during rush hour in the city of Boca del Rio.", "the Dutch patent office", "debris", "Sweden", "Nancy Drew", "schizophrenia", "it's hot out there today so remember to keep them"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6216875572344323}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.0, 0.5, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 0.09523809523809523, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-2153", "mrqa_triviaqa-validation-3098", "mrqa_triviaqa-validation-2077", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-1527", "mrqa_triviaqa-validation-7274", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-961", "mrqa_triviaqa-validation-304", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-10537", "mrqa_hotpotqa-validation-4091", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-111", "mrqa_searchqa-validation-14439"], "SR": 0.578125, "CSR": 0.5231119791666667, "EFR": 1.0, "Overall": 0.7239973958333333}, {"timecode": 48, "before_eval_results": {"predictions": ["Eukarya", "eight", "April 1st", "June 1992", "won", "Katharine Hepburn -- Ethel Thayer", "William J. Bell", "March 31, 2017", "gogh", "New York City", "American country music singer George Strait", "John Adams", "a major fall in stock prices", "to Lands End", "Charles Path\u00e9", "Phillip Paley", "from statute or the Constitution itself", "18", "2007", "those colonists of the Thirteen Colonies who rebelled against British control during the American Revolution and in July 1776 declared the United States of America an independent nation", "Abraham Gottlob Werner", "IMS", "18th century", "American singer Lesley Gore", "sometime between 124 and 800 CE", "The Ed, Edd n Eddy animated television series", "mongrel female", "Teri Hatcher", "John Quincy Adams", "August 1991", "Uralic languages", "dromedary", "Bhupendranath Dutt", "2011", "a substance that fully activates the receptor that it binds to )", "Bill Russell", "Battle of Antietam", "sport utility vehicles", "Hunter Tylo", "Frank Theodore `` Ted '' Levine", "by the early 3rd century", "James Rodr\u00edguez", "around 10 : 30am", "Jack Barry", "White Sox", "approximately 11 %", "reduces the back pressure, which in turn reduces the steam consumption, and thus the fuel consumption, while at the same time increasing power and recycling boiler - water", "Bill Russell", "1984", "Identification of alternative plans / policies", "the longest rotation period ( 243 days ) of any planet in the Solar System and rotates in the opposite direction to most other planets ( meaning the Sun would rise in the west and set in the east )", "Northern", "cilla black", "de Reims", "her grandmother Hanako Muraoka", "Taoiseach", "Theatre Ventures, Inc.", "Kurdistan Freedom Falcons,", "AbdulMutallab", "don Draper", "a typewriter", "calico", "mandolin", "Gary Player"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6799852881493507}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.12500000000000003, 0.10714285714285715, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.0, 0.16, 0.6666666666666666, 0.4799999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0625, 0.5714285714285715, 1.0, 1.0, 0.0606060606060606, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-255", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-8157", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-183", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-1029", "mrqa_newsqa-validation-1506"], "SR": 0.578125, "CSR": 0.5242346938775511, "EFR": 1.0, "Overall": 0.7242219387755101}, {"timecode": 49, "before_eval_results": {"predictions": ["Napoleon", "iron", "Patrick Warburton", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "pneumonoultramicroscopicsilicovolcanoconiosis", "Joseph M. Scriven", "Andrew Garfield", "July 4, 1776", "Keith Thibodeaux", "Jesus Christ", "Charles Path\u00e9", "eleven", "the federal judiciary", "Johannes Gutenberg", "O'Meara", "first published in the United States by Melvil Dewey in 1876", "Marley & Me", "fourth season", "four seasons", "It was a Confederate victory, followed by a disorganized retreat of the Union forces", "the probability of rejecting the null hypothesis given that it is true", "slavery", "The Royalettes", "Employers", "The Outback", "it failed to enforce its rule, and its vast territory was divided into several successor polities", "Louis XV", "Super Bowl XXXIX in Jacksonville", "genome", "Beorn", "authority", "Confederate forces", "in response to the Weimar Republic's failure to continue its reparation payments in the aftermath of World War I", "Indirect rule", "Zachary John Quinto", "the governor of West Virginia", "Wednesday, September 21, 2016, on NBC and finished on Wednesday, May 24, 2017, with a two - hour season finale", "ninth", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "milling process removes material by performing many separate, small cuts", "in North America by several companies : Seven Seas Entertainment licensed the manga, Sentai Filmworks the anime, and was simulcast outside Japan by Crunchyroll", "1939", "1992", "Millerlite", "Felix Baumgartner", "Donald Fauntleroy Duck", "c. 3000 BC", "Bart Howard", "Paris", "1966", "the vascular cambium", "child of The 1980\u2019s", "Leeds", "the Netherlands", "Coleman Hawkins", "Zero Mostel", "Ellesmere Port, United Kingdom", "Genocide Prevention Task Force", "a grizzly bear", "Maersk Line Ltd.", "deborah", "Treasure Island", "Pablo Picasso", "sailing"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6616788105305547}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.16666666666666669, 0.0, 1.0, 1.0, 0.0, 1.0, 0.72, 1.0, 0.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.8837209302325582, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-6888", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-6021", "mrqa_triviaqa-validation-460", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2428", "mrqa_searchqa-validation-9956", "mrqa_searchqa-validation-15480"], "SR": 0.5625, "CSR": 0.525, "EFR": 0.9642857142857143, "Overall": 0.7172321428571429}, {"timecode": 50, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1373", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10692", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5387", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8709", "mrqa_naturalquestions-validation-8819", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5744", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99"], "OKR": 0.853515625, "KG": 0.4953125, "before_eval_results": {"predictions": ["Jason, son of Athamas", "Trainspotting", "rock and roll", "caulking", "spark", "Concorde", "Al Jazeera", "French", "government", "squid", "1925", "Goldfinger", "Chicago", "Flower", "Ford", "Dengue fever", "Japan", "Oprah Winfrey", "phobia", "Cowslip", "Mount Everest", "Strangeways", "Carthage", "Wensum", "Robben Island", "British", "Taekwondo", "foot", "apple", "sixth Wimbledon championship", "Mandela", "George Orwell", "Andrew Jackson", "Ms. Spark", "table tennis", "Entwistle Reservoir", "DeLorean", "six", "Perseus", "Yakutat", "United Nations of Football", "syndicate", "muscle", "transuranic elements", "John Buchan", "Tesco", "Lolita", "Barbara Eden", "Indus Valley", "duck", "Pickwick", "Nancy Jean Cartwright", "Watson and Crick", "Authority", "his superhero roles", "German", "Che Guevara", "Robert Barnett", "five", "Jet Republic", "the Lexus LS 460", "John Lennon", "Murder by Death", "Republicans"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7208333333333333}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-139", "mrqa_triviaqa-validation-6499", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-6476", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-4108", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-5363", "mrqa_triviaqa-validation-3982", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-756", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3249", "mrqa_naturalquestions-validation-3468", "mrqa_hotpotqa-validation-881", "mrqa_searchqa-validation-5409"], "SR": 0.65625, "CSR": 0.5275735294117647, "EFR": 1.0, "Overall": 0.7166865808823528}, {"timecode": 51, "before_eval_results": {"predictions": ["Flatbush", "Australian", "\"Traumnovelle\" (\"Dream Story\")", "Denmark", "Bad Meets Evil", "Bellagio and The Mirage", "George Mikan", "more than 20 principal operations", "Guthred", "The New Yorker", "skip Tyler", "St. Louis Cardinals", "NXT Tag Team Championship", "Lee Byung-hun", "24", "February 1", "capital crimes or capital offences", "\"We've Only Just Begun\"", "March", "Chuck Noll", "cate Blanchett", "Oregon", "Atlas ICBM", "Democratic", "Kim So-hyun", "Rolling Stones", "22,500 acres", "Trey Parker", "Kew", "Brava, Cape Verde", "XII", "Wembley Stadium", "Shameless", "Brigadier General Raden Panji", "skiing and mountaineering", "Indian", "all Things Comedy", "cruiserweight", "five", "Leofric", "Bigfoot", "March 17, 2015", "yubin", "5249", "fourth-ranking", "three", "28 November 1973", "\"O\", \"La Nouba\", \"Myst\u00e8re\", \"Alegr\u00eda\", and \"Quidam\"", "Londonderry", "Santiago Herrera", "jewelry designer", "Steve Russell", "a surname, derived from `` Wat '', or `` Wa'ter '', an old pronunciation of Gaultier or Walter, and similarly derived from the surname Watson", "a play about a man whose choice to send out faulty airplane parts for the good of his business and family", "Pegasus", "sunday", "crawson", "15-year-old's", "July 23.", "Baghdad", "knott\\'s Berry Farm", "circum", "t.S. Eliot", "new wilson"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5309257075471698}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false], "QA-F1": [0.25, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.5, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 0.3333333333333333, 0.25, 1.0, 0.0, 1.0, 1.0, 0.09523809523809525, 0.6792452830188679, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-3712", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-3752", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-1886", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-3651", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-7514", "mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-4557", "mrqa_searchqa-validation-834", "mrqa_triviaqa-validation-4216"], "SR": 0.40625, "CSR": 0.5252403846153846, "EFR": 1.0, "Overall": 0.7162199519230769}, {"timecode": 52, "before_eval_results": {"predictions": ["Frank Ocean", "Brookhaven", "2010", "Ryukyuan people", "Robert L. Stone", "Mexican", "The King of Hollywood", "five times", "1968", "Charles Eug\u00e8ne Jules Marie Nungesser", "Kim Yoon-seok and Ha Jung-woo", "Jennifer Grey", "1978", "M2M", "Mark Neveldine and Brian Taylor", "Starship Planet", "Beauty and the Beast", "\"Odorama\", whereby viewers could smell what they saw on screen through scratch and sniff cards", "The 8th Habit", "Larnelle Steward Harris", "Total Nonstop Action Wrestling", "Lambic", "Bit Instant", "Tom Jones", "Charles Guiteau", "Secrets and Lies", "Hard rock", "Ludwig van Beethoven", "Peter Kay's Car Share", "Orph\u00e9e et Eurydice", "Dirt track racing", "Frederick Barbarossa", "Kegeyli tumani", "Walldorf, Baden-W\u00fcrttemberg", "Jack Ryan", "Campbellsville University", "Shinjuku", "1933", "Delphi Lawrence", "Philadelphia", "December 13, 2015", "a governor of the New York Stock Exchange", "Paradise, Nevada", "Russell T Davies", "four", "Kensan-Devan Wildlife Sanctuary", "charlie wilbur", "2018\u201319 UEFA Europa League group stage", "Argentinian", "76,416", "Burning Man", "the president", "Ra\u00fal Eduardo Esparza", "October 30, 2017", "Live and Let Die", "Carrie", "on Mars", "Reggae legend Lucky Dube, one of South Africa's most famous musicians,", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "the Internet", "a cursor amok", "Yes", "East Germany", "heart"], "metric_results": {"EM": 0.625, "QA-F1": 0.7331745077838827}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-847", "mrqa_hotpotqa-validation-1263", "mrqa_naturalquestions-validation-321", "mrqa_triviaqa-validation-7133", "mrqa_newsqa-validation-594", "mrqa_searchqa-validation-15932", "mrqa_triviaqa-validation-3362"], "SR": 0.625, "CSR": 0.5271226415094339, "EFR": 1.0, "Overall": 0.7165964033018868}, {"timecode": 53, "before_eval_results": {"predictions": ["badminton", "new York", "in the Outer Hebrides", "God", "nippon Sangyo", "binder", "James Hogg", "Sarajevo", "John Darby and his wife Joan", "The Hurt Locker", "Full Metal jacket", "Blur", "chicken Marengo", "Neil Gordon Kinnock", "Sir Herbert Kitchener", "green", "a bodice or corset", "grizzly", "bukwus", "a cardinal", "rowing", "his death in 1975", "Nowhere Boy", "Donald Trump", "Mikhail S. Gorbachev", "Popeye", "John Key", "Charlie Brooker", "University of Nebraska\u2013Lincoln", "Gulf of Mexico", "Rev. John Trigilio, Jr., Rev. Kenneth Brighenti", "Aceso", "dynamite", "Take That", "Jean Alexander", "Norman Brookes", "David Hockney", "La Toya Jackson", "Jimmy Carter", "fred quimby", "the United States", "Edinburgh", "Today", "bellingham", "Bolton", "Norwegian", "Super Bowl Sunday", "\"This Is the Chorus\"", "vowing to change Russia\u2019s economy to a free market economy endorsed by liberalization and privatization,", "Donna Jo Napoli", "a bear market", "Augustus Waters", "1967", "silk, hair / fur ( including wool )", "Ghana", "Peter Kay's Car Share", "Miller Brewing", "The son of Gabon's former president", "more than two years,", "auction off one of the earliest versions of the Magna Carta later this year,", "a Scott", "a judgment, viewpoint, or statement that is not conclusive", "mary becker", "Unseeded Frenchwoman"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5581371753246753}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6562", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-5000", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-7162", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-2385", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5531", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-5344", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-3499", "mrqa_newsqa-validation-3285"], "SR": 0.46875, "CSR": 0.5260416666666667, "EFR": 1.0, "Overall": 0.7163802083333334}, {"timecode": 54, "before_eval_results": {"predictions": ["Tony Curtis", "The Great Gatsby", "germany", "jiangsu", "the First World War", "Georgia", "prince andrew", "Sarah Ferguson", "putin", "ford costello", "Appalachian", "Skylab", "John poulson", "germany", "shoes", "the Great Depression", "bores", "Queen Anne", "Alpha Orionis", "dicken's Dream", "Swansea City", "krypton", "silurian", "meatloaf", "jesuit", "j.M.W.", "The Lone Gunmen", "gold", "jason wales", "at the north-west corner of the central business district", "Wonderwall", "basketball", "carburetor", "germany", "corsets", "germany", "sigmund Freud", "winged horse", "lillita McMurray", "bat", "dora peggotty", "germany", "germany", "Arthur C. Clarke", "Buzz Aldrin", "a power outage", "Napoleon Bonaparte", "fingers", "jukes of Marlborough", "Rihanna", "at kyshtym in Russia", "28 July 1914 to 11 November 1918", "Fix You", "the spectroscopic notation for the associated atomic orbitals", "Copa Airlines", "my Beautiful Dark Twisted Fantasy", "Soha Ali Khan Khemu", "people hoping to keep their skin looking young.", "debris", "work for Grayback Forestry in Medford, Oregon,", "typhoid", "germany", "the Edict of Nantes", "in Austin and Pflugerville"], "metric_results": {"EM": 0.5, "QA-F1": 0.5561011904761904}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-7740", "mrqa_triviaqa-validation-2312", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-5635", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-5896", "mrqa_triviaqa-validation-2243", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-4795", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-1212", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1910", "mrqa_triviaqa-validation-7712", "mrqa_hotpotqa-validation-303", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-4060", "mrqa_searchqa-validation-2415", "mrqa_naturalquestions-validation-3995"], "SR": 0.5, "CSR": 0.5255681818181819, "EFR": 1.0, "Overall": 0.7162855113636363}, {"timecode": 55, "before_eval_results": {"predictions": ["aviva plc", "Venezuela", "Mozart", "anacapri", "welles", "Catherine Cookson", "almonds", "Barack Obama,", "florence", "shatner", "peter pelenburgh", "Arabian Gulf", "durlach", "ascot", "seine", "muppets From Space", "sheryl Crow", "winnie Mae", "spain", "come quietly", "the amorites", "graphite", "estonia", "moby Dick", "The Scream", "gingerbread", "The Cream of Manchester", "king jones", "good luck", "raspberries", "est Thomas", "surfer", "oakum", "blancmange", "rochdale", "penhaligon", "Black September", "197", "Germany", "shoe", "gold", "car ferry", "Professor Brian Cox", "Meow Mix", "8", "joints", "Bolivia", "jewish communities", "Jordan", "Hans Lippershey", "india", "Daisy - May Bates", "a statistical advantage for the casino that is built into the game", "if a vehicle towing a trailer skids, the trailer can push the towing vehicle from behind until it spins the vehicle around and faces backwards", "Teriade", "Japan", "the National Aviation Hall of Fame class of 2001", "Megan Lynn Touma,", "Madeleine K. Albright", "three", "walrus", "grotesque", "Liam Neeson", "a centaur"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5952037545787545}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.7692307692307693, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-4616", "mrqa_triviaqa-validation-5454", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-1324", "mrqa_triviaqa-validation-1674", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-3231", "mrqa_triviaqa-validation-6866", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3275", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-3897", "mrqa_triviaqa-validation-2442", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-4164", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5510", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-2524", "mrqa_searchqa-validation-2027"], "SR": 0.53125, "CSR": 0.5256696428571428, "EFR": 0.9666666666666667, "Overall": 0.7096391369047619}, {"timecode": 56, "before_eval_results": {"predictions": ["joan mccain", "Dorothy Gale", "william hartnell", "Friedrich Nietzsche", "Ben Affleck", "jamaican", "magical Mystery Tour", "Rio de Janeiro", "Cyclopes", "purple", "1961", "sooner state", "florence", "Steve Davis", "George Bernard Shaw", "Antoine Lavoisier", "30th anniversary", "merkat", "tara", "Henri Rousseau", "united states", "john lenn", "united states", "Detroit", "rhryophyta", "paddington bear", "tarn", "the M6", "tidal Bay", "united buzzards", "alastair Cook", "petronas Towers", "cribbage", "1960s", "Yorkshire", "LMFAO", "Emma Chambers", "kinks", "Tony Blackburn", "spain", "rebecca", "united states", "Pink Floyd", "bobby brown", "miles Morales", "sorbie coltrane", "British", "touto", "hyphenated", "mono", "Augustus Caesar", "the south coast of eastern New Guinea", "Lady Gaga", "revenge and karma", "\"Secrets and Lies\"", "October 3, 2017", "Morris Barney Dalitz", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "Unseeded", "Kingman Regional Medical Center", "Walter payton", "the fairway", "the Provisional Irish Republican Army", "2018"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5791666666666666}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-223", "mrqa_triviaqa-validation-454", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-346", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-3252", "mrqa_triviaqa-validation-188", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-4757", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6935", "mrqa_triviaqa-validation-4550", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-3716", "mrqa_naturalquestions-validation-4719", "mrqa_hotpotqa-validation-4161", "mrqa_newsqa-validation-3287", "mrqa_searchqa-validation-11582"], "SR": 0.5625, "CSR": 0.5263157894736843, "EFR": 1.0, "Overall": 0.7164350328947368}, {"timecode": 57, "before_eval_results": {"predictions": ["Wild Bunch", "Illinois", "Edward hopper", "robocop", "pere-Lluis", "Quentin Blake", "bazaar", "shrove Tuesday", "new york", "Hamlet", "Chris Smalling", "007", "north atlantic", "hobbie", "jordan", "Tangled", "Brothers in Arms", "the United States", "crossword puzzle", "Sheree Murphy", "puff puff", "Robin Ellis", "tomato Basil Conchiglie Pasta", "davy crockett", "dmitri", "paphos", "three", "east of Eden", "dutiful daughter", "zaragoza", "nick germany", "germany", "king Edward VIII", "Belgium", "a Christmas Carol", "bridge", "elliptical", "trier", "germany kelly", "blood", "zips", "amisbach", "Roman history", "mj\u00f6llnir", "Admiral Vernon", "florence", "woodstock", "birds", "nijinsky", "goosnargh", "drogba", "a major role in guiding national economic and monetary policy, and is therefore one of the most important public officials in the United Kingdom", "B.J. Thomas", "65,535 bytes", "Prince Amedeo, Duke of Aosta", "San Antonio", "R\u00e5ndan", "the Obama administration", "Senate Democrats", "some of the Awa", "Manhattan", "Patrick Henry", "sleep apnea", "the Eagles"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5375744047619048}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3686", "mrqa_triviaqa-validation-4243", "mrqa_triviaqa-validation-910", "mrqa_triviaqa-validation-4367", "mrqa_triviaqa-validation-6237", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-3645", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1569", "mrqa_triviaqa-validation-1563", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-6016", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7378", "mrqa_triviaqa-validation-6841", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-3218", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-2177", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-1787", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-2399", "mrqa_newsqa-validation-1550"], "SR": 0.515625, "CSR": 0.5261314655172413, "EFR": 0.967741935483871, "Overall": 0.7099465552002224}, {"timecode": 58, "before_eval_results": {"predictions": ["the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "10th Cavalry Regiment", "Hong Kong First Division League club Happy Valley", "John Robert Cocker", "Taylor Swift", "mountaineer", "Lonely", "Garrett Morris", "October 5, 1937", "1692", "Jay Hanna \"Dizzy\" Dean", "Target Corporation", "British Labour Party", "Bandai", "Bill Ponsford", "Patricia Neal", "Code#02Pretty pretty", "every Rose Has its Thorn", "Cleveland Browns", "Jacking", "1901", "my Beautiful Dark Twisted Fantasy", "Broadcasting House in London", "20", "Amway", "Congo River", "Minneapolis", "Alemannic", "illnesses", "XVideos", "1967", "1967", "pinball", "Lawrence of Arabia", "The Fault in Our Stars", "Gareth Jones", "head of the Cabinet of Bluhme I", "J35-A-23", "Scotty Grainger Jr.", "balloon Street, Manchester", "Somerset County, Pennsylvania", "Italy", "Psych", "Gateways", "Iran", "Veneto", "Empire Falls", "Fitzroya cupressoides", "Vernon Lomax Smith", "Dan Rowan", "Oakdale", "March 18, 2005", "1978", "Austria - Hungary", "Switzerland", "the Treaty of Waitangi", "1930-1939", "a U.S. military helicopter", "African National Congress Deputy President Kgalema Motlanthe", "new DNA evidence", "blue whale", "bitter Empire", "books that are no longer being published", "bullfight"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6277777777777778}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.5, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-2209", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-4494", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-2338", "mrqa_naturalquestions-validation-6040", "mrqa_triviaqa-validation-5517", "mrqa_newsqa-validation-1382", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-1481", "mrqa_triviaqa-validation-6175"], "SR": 0.546875, "CSR": 0.5264830508474576, "EFR": 1.0, "Overall": 0.7164684851694915}, {"timecode": 59, "before_eval_results": {"predictions": ["the first integrated circuit", "Oracle Corporation", "Levittown", "the Teenage Mutant Ninja Turtles", "seven", "Ashanti Region", "1934", "Cheshire", "1980", "bullfighting", "Dachshund", "Cheshire, England, which in the 11th century extended to over 60 sqmi, stretching from the Mersey in the north almost to Nantwich in the south, and from the", "Duncan Kenworthy", "the Stern-Plaza in Potsdam", "the Netherlands", "Continental Army", "Norse mythology", "Henry Lau", "1", "Russia", "The Catholic Church in Ireland", "people working in film and the performing arts", "Fast & Furious 8", "1989", "Gareth Barry", "1999", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "Margarine Unie", "David Naughton", "A123 Systems, LLC", "Ian Fleming", "Minnesota", "14", "an anvil", "50 Greatest Players in National Basketball Association History", "James G. Kiernan", "Dizzy Dean", "Magnus Carlsen", "BBC Focus", "\"Home\"", "1958", "World War II", "Jenn Brown", "\"Glee\"", "Purdue University", "Indianapolis", "Australian hard rock band, formed in Sydney in 1973 by brothers Malcolm and Angus Young", "Bury, Greater Manchester, England", "\"Agent Vinod\"", "a Marxist and a Leninist", "George Timothy Clooney", "Alison", "when the cell is undergoing the metaphase of cell division ( where all chromosomes are aligned in the center of the cell in their condensed form )", "2011", "giraffe", "ghee", "Wagner", "to step up.\"", "former Pakistani Prime Minister Nawaz Sharif", "Christina Romete,", "Home on the Range", "1 meter", "Donnie Wahlberg", "1918"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6335416102994228}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, true, false], "QA-F1": [0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0625, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-4420", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-1609", "mrqa_hotpotqa-validation-1148", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-2810", "mrqa_hotpotqa-validation-1055", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-8159", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-3068", "mrqa_searchqa-validation-12933", "mrqa_newsqa-validation-2789"], "SR": 0.53125, "CSR": 0.5265625, "EFR": 1.0, "Overall": 0.716484375}, {"timecode": 60, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4534", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-4728", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-637", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-768", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-921", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-10651", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4312", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2415", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1403", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1768", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2243", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3432", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5228", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6098", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6536", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7606", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-997"], "OKR": 0.82421875, "KG": 0.49375, "before_eval_results": {"predictions": ["September,", "five minutes before commandos descended", "Arsene Wenger", "going through a metamorphosis from blobs of orange to art as night falls.", "a devastating impact on the city's population causing enormous suffering and massive displacement,\" the U.N. refugee agency said.", "to reach beyond their individual capabilities and build a practical framework that could help the U.S. government better respond to threats of genocide and mass atrocities.", "allegedly involved in forged credit cards and identity theft led authorities to a $13 million global crime ring, Queens County District Attorney's Office reported.", "4.6 million people", "Ferraris, a Lamborghini and an Acura NSX", "Vicente Carrillo Leyva,", "Columbian mammoth's", "Communist Party of Nepal (Unified Marxist-Leninist)", "581 points", "Molotov cocktails, rocks and glass.", "1994,", "10 years in prison", "the Gulf", "an average of 25 percent", "Orbiting Carbon Observatory,", "then-Sen. Obama", "Claude Monet", "4,000", "apartment building", "Pittsburgh", "Herman Thomas", "Daytime Emmy Lifetime Achievement Award.", "South Africa", "Barack Obama's", "dual nationality", "Knox's parents", "Cash for Clunkers program", "will not answer questions.", "Jennifer Arnold and husband Bill Klein,", "prostate cancer,", "Zimbabwe", "Britain.", "a fire or punch a hole through the aircraft structure,\"", "10 percent", "eight or nine young girls, some younger then 18,", "Jaipur", "cancer", "Alan Graham", "salutes the \"People of Palestine\" and calls on them to fight back against Israel in Gaza.", "forgery and flying without a valid license,", "George Washington", "Los Angeles County Fire Department", "three out of four questioned say that things are going well for them personally.", "poems", "Fourth time lucky in Atlanta in 1996.", "think are the best.", "South Africa", "2008", "William Jennings Bryan", "The neck", "Barry Humphries", "Mozambique Channel", "Ede & Ravenscroft", "Adelaide", "punk rock", "Great Northern Railway", "United Technologies", "Iberian Peninsula", "George Balanchine", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\""], "metric_results": {"EM": 0.53125, "QA-F1": 0.64452149552956}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.06451612903225806, 0.14285714285714288, 0.8, 0.0, 1.0, 0.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.3333333333333333, 0.923076923076923, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-3297", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-56", "mrqa_triviaqa-validation-2468", "mrqa_hotpotqa-validation-3984", "mrqa_searchqa-validation-15121"], "SR": 0.53125, "CSR": 0.5266393442622951, "EFR": 1.0, "Overall": 0.7079841188524589}, {"timecode": 61, "before_eval_results": {"predictions": ["Donald Duck", "Iran's parliament speaker", "Department of Homeland Security Secretary Janet Napolitano", "18", "India", "World leaders", "Mafia", "managing his time.", "his club", "we seek a new way forward, based on mutual interest and mutual respect.", "$50", "collaborating with the Colombian government,", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "1,500", "Jada,", "200", "Karen Floyd", "Space shuttle Discovery,", "Brazil", "EU naval force", "to help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "Harrison Ford", "Sunday", "28", "The Falklands, known as Las Malvinas in Argentina,", "New York City Mayor Michael Bloomberg", "Department of Homeland Security Secretary Janet Napolitano", "two", "Too many glass shards left by beer drinkers in the city center,", "30-minute", "338", "UNICEF", "eight", "Daniel Radcliffe", "Department of Homeland Security", "Portuguese water dog", "a tanker that sailed under a Saudi flag,", "lightning strikes", "If  your ex's loved ones ask why you broke up,", "Afghan lawmakers", "Dalai Lama", "Colombia.", "nine-wicket", "nearly 100", "1616.", "to sniff out cell phones.", "Casey Anthony,", "people look at the content of the speech, not just the delivery.", "2005", "root out terrorists within its borders.", "a point for Bayern Munich as the German Bundesliga leaders were held to a 1-1 draw by Cologne on Saturday.", "March 31 to April 8, 2018", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "St Paul's Cathedral", "15", "Rome", "University of Vienna", "Dutch", "Naomi Campbell", "Earhart", "Thunder", "Ronco", "the courts"], "metric_results": {"EM": 0.5, "QA-F1": 0.6222609969526088}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.25, 0.2857142857142857, 0.4444444444444445, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.7272727272727273, 0.0, 0.2857142857142857, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 0.5, 0.7999999999999999, 0.21052631578947367, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-2145", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4915", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-880", "mrqa_hotpotqa-validation-3500", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7617"], "SR": 0.5, "CSR": 0.5262096774193548, "EFR": 0.96875, "Overall": 0.701648185483871}, {"timecode": 62, "before_eval_results": {"predictions": ["Golden Valley, Minnesota,", "two", "small forward", "Southern Rock Allstars", "Araminta Ross", "the Mach number", "eight", "November 13, 2015", "Alphonse Gabriel Capone", "atomic bomb", "St Augustine's Abbey", "Vilyam \"Willie\" Genrikhovich Fisher", "minister", "Tony Stewart", "\"the most influential private citizen in the America of his day\"", "dance", "Standard Oil", "over 1.6 million", "British Labour Party", "September 8, 2017", "Obafemi Akinwunmi Martins", "Charles Edward Stuart", "HackThis Site", "Steve Carell", "Saint Motel", "Melissa Rauch", "Flyweight", "Levon Helm", "Jean Acker", "From Here to Eternity", "Fountains of Wayne", "Nicholas \" Nick\" Offerman", "Sam Raimi", "SAS Fr\u00f6sundavik", "Double Crossed", "Edmonton, Alberta.", "8,211", "KXII", "Wikimedia Foundation", "Greek", "Mika H\u00e4kkinen", "Debbie Isitt", "Los Angeles", "1999", "Outside is an American magazine focused on the outdoors.", "the Food and Agriculture Organization", "Wojtek", "Edward Longshanks and the Hammer of the Scots", "Los Angeles", "West Point", "New York City", "Speaker of the House of Representatives", "13", "Elaine Stritch", "an arrowhead", "Corin Redgrave", "a son of Amram and Jochebed,", "UNICEF", "Bob Bogle,", "she was humiliated by last month's incident, in which she was forced to remove the piercings", "a visiting Northern Black detective", "mead", "David", "6"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6846354166666666}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.5, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1420", "mrqa_hotpotqa-validation-4237", "mrqa_hotpotqa-validation-4866", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-821", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-3375", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-3473", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-298", "mrqa_naturalquestions-validation-839", "mrqa_triviaqa-validation-6039", "mrqa_triviaqa-validation-5231", "mrqa_newsqa-validation-390", "mrqa_searchqa-validation-12442"], "SR": 0.59375, "CSR": 0.527281746031746, "EFR": 1.0, "Overall": 0.7081125992063492}, {"timecode": 63, "before_eval_results": {"predictions": ["African National Congress", "Ronald Cummings", "two Israeli soldiers,", "Bob Bogle,", "Bob Bogle", "the creation of an Islamic emirate in Gaza,", "suppress the memories and to live as normal a life as possible;", "Caster Semenya", "her fianc\u00e9,", "the BBC's central London offices", "African National Congress Deputy President Kgalema Motlanthe,", "as he tried to throw a petrol bomb at the officers,", "Karl Eikenberry", "1959", "South Korea", "Elena Kagan", "Harrison Ford", "Christmas parade", "a facility in Salt Lake City, Utah,", "Nearly eight in 10", "your ex's loved ones ask why", "2", "racial intolerance.", "acid attack by a spurned suitor.", "1 km tall.", "part", "bicycles", "landed in Cameroon,", "Akshay Kumar", "August 19, 2007.", "a strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television.", "should have met with the Dalai Lama.", "would require an act of Congress,", "the test results", "is an employed man.", "\"Dancing With the Stars.\"", "Brown-Waite", "we had no real procedure for sectioning off the rear-frame rails,", "40", "strife in Somalia,", "protest child trafficking and shout anti-French slogans", "colonel", "1918-1919.", "in the mouth.", "cancer", "Susan Atkins", "137", "in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "pulling on the top-knot of an opponent,", "at the University of Alabama in Huntsville,", "a secretary of state", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "kerogen Type III", "a water - soluble binder medium ( usually glutinous material such as egg yolk or some other size )", "Touchstone Gold test", "Ennio Morricone", "Auric Goldfinger", "Thomas Mawson", "Al D'Amato", "Peel Holdings", "Java", "contrite", "ecliptic", "Harriet the Spy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.57797541000666}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3, 0.0, 0.0, 0.7499999999999999, 0.8205128205128205, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-412", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2724", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-5940", "mrqa_triviaqa-validation-2958", "mrqa_searchqa-validation-15537"], "SR": 0.484375, "CSR": 0.526611328125, "EFR": 1.0, "Overall": 0.707978515625}, {"timecode": 64, "before_eval_results": {"predictions": ["Abbot and Costello", "Great British Bake Off", "Gary Havelock", "Lance Corporal", "Fiji", "Natty Bumppo", "Ben Whishaw", "Guinea-Bissau, Senegal,", "Henry Jermyn,", "Jon Stewart", "\"Barefoot Bandit\"", "Ytterby", "fox-like", "neos", "Boston Braves", "kitsunes", "Blanche", "1825", "argentina", "Lisieux", "Ascot", "Mark Twain", "Charlie Cairoli", "sunil Gavaskar", "The Blue Danube", "florida", "William Caxton", "Buzz Aldrin", "highball", "Xyloid", "Dutch", "Reform Club", "TUC", "Saint Cecilia", "the Netherlands", "Michael Keaton", "pistil", "philosopher", "Thomas Cranmer", "the Mad Hatter", "Nick Clegg", "Norfolk, Virginia.", "Leonard Nimoy", "the largest buttock muscle", "Nikola Tesla", "Adrian Edmondson", "Macedonia", "forelimb", "Boyle\u2019s law", "J. S. Bach", "Bachelor of Science", "lamina dura", "July 2014", "the French CYCLADES project directed by Louis Pouzin", "Bath, Maine", "actress", "supply chain management", "Dr. Jennifer Arnold and husband Bill Klein,", "Lance Cpl. Mike Mullen,", "that Birnbaum had resigned \"on her own terms and own volition.\"", "Spmi", "Private Benjamin", "the Rhine & the Main", "Amber Heard"], "metric_results": {"EM": 0.46875, "QA-F1": 0.47916666666666663}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-1020", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-6566", "mrqa_triviaqa-validation-5536", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-84", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-195", "mrqa_triviaqa-validation-4258", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-1949", "mrqa_triviaqa-validation-31", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5009", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-1171", "mrqa_hotpotqa-validation-100", "mrqa_hotpotqa-validation-1001", "mrqa_newsqa-validation-1862", "mrqa_searchqa-validation-7466", "mrqa_hotpotqa-validation-652"], "SR": 0.46875, "CSR": 0.5257211538461539, "EFR": 1.0, "Overall": 0.7078004807692307}, {"timecode": 65, "before_eval_results": {"predictions": ["hemlock", "peter greenaway", "roosevelt", "eye", "spain", "david hockney", "sierra leone", "preston", "Walmer Castle", "Scorpio", "spain", "Coalbrookdale", "borgia", "Periodic Table", "antonio leone", "bread", "jones", "jones", "come quietly", "Mitsubishi", "the Panama Canal", "1960", "feet", "apples", "lug", "paul Rudd", "Hamelin", "Harold II", "Kuwait", "leicestershire", "sprint", "green", "The Grapes of Wrath", "Coldplay", "the first news periodical", "Rugrats", "president of the Exchequer", "domino", "fat", "Austria", "sergeant (Lee Ingleby)", "crawk", "paul sarah", "Markus Aemilius Lepidus", "business", "tall", "Lorenzo da Ponte", "thrushchev", "ivy", "high-F fructose corn Syrup (HFCS)", "molecular structure of nucleic acids", "Schwarzenegger", "Wakanda", "lamina dura", "Jeff Davis", "Robert L. Stone", "Haitian Revolution", "sierra leone", "to launch a group that will serve as an alternative to the Organization of American States.", "64,", "kolkata", "Tuna tune-up Casserole", "gwen stefani", "Old English pyrige ( pear tree )"], "metric_results": {"EM": 0.5, "QA-F1": 0.5482175207039337}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 0.34782608695652173, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-5514", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-5504", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4521", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3962", "mrqa_triviaqa-validation-4255", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6015", "mrqa_hotpotqa-validation-4757", "mrqa_newsqa-validation-3226", "mrqa_newsqa-validation-2224", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-16126"], "SR": 0.5, "CSR": 0.5253314393939394, "EFR": 1.0, "Overall": 0.7077225378787879}, {"timecode": 66, "before_eval_results": {"predictions": ["a valid passport", "Sheffield Wednesday", "Buddhist", "Andrew Jackson", "Unfortunate Events", "Daniel Craig", "Red sea", "red", "sesame seeds", "grizzly", "davoisier", "Swiss", "Poem Hunter", "Wars of the Roses", "terence Edward \" Terry\" Hall", "butyl acetate", "San Francisco", "Paris", "sewing machine", "Atlas", "adanagan", "eye", "jennifer anovelli", "nymphet", "peter Principle", "Video", "Frank McCourt", "Little Jack Horner", "mark-girl", "blancmange", "mark Rhodes", "king Louis XVIII", "stand-up", "Happy Birthday to You", "1952", "Pride & Prejudice", "william golding", "ad", "narnia", "Mr. Brainwash", "calypso", "one-eyed", "phrenology", "anna boleyn", "stingy", "laryngeal prominence", "driver", "Joan Rivers", "lord fancourt Babberley", "katherine Mansfield Beauchamp", "heineken", "1987", "1996", "Ella Mitchell", "\"Wicked Twister\"", "Lerotholi Polytechnic Football Club", "3730 km", "mild to moderate depression", "Saturday", "scout", "Bering Sea", "Charlottetown", "Agatha Christie", "Former Beatles"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6770833333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-3718", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6164", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-5391", "mrqa_triviaqa-validation-1042", "mrqa_triviaqa-validation-1749", "mrqa_triviaqa-validation-1067", "mrqa_hotpotqa-validation-758", "mrqa_newsqa-validation-150", "mrqa_searchqa-validation-140"], "SR": 0.640625, "CSR": 0.5270522388059702, "EFR": 1.0, "Overall": 0.708066697761194}, {"timecode": 67, "before_eval_results": {"predictions": ["Yuri Andropov", "michelle mcdonald", "joseph mortimer", "Camino Franc\u00e9s", "fox", "Victoria Coren Mitchell", "\"sound and light\"", "coffee", "tomato", "fred west", "steel", "st Columba", "tintoretto", "1215", "1937", "12", "michelle humphett", "king george IV", "nahuatl", "Venice", "adnams", "Massachusetts", "nikkei 225", "Nutbush", "robert humph", "jape", "NASCAR", "Jordan", "british humph", "llanfairfechan", "Battle Marengo", "darshaan", "drakis", "Nicaragua", "The Female Brain", "par-5", "Oklahoma", "Jason Bourne", "Venus", "bauxite", "low-carb carbohydrate", "dwarf antelopes", "Nevada", "SW20", "Mizrahi", "evans", "Cowboy Builders", "british", "Swansea Bay", "london", "arquaman", "Justin Bieber", "2017 season", "eleven", "Club Deportivo Castell\u00f3n, S.A.D.", "East Knoyle", "Jan Kazimierz", "2.5 million", "Vivek Wadhwa,", "Wednesday", "parodiable", "Delta", "Lake Victoria", "air support"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5430803571428571}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-565", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-2021", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-5176", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-7545", "mrqa_triviaqa-validation-1943", "mrqa_triviaqa-validation-506", "mrqa_triviaqa-validation-7710", "mrqa_hotpotqa-validation-5588", "mrqa_newsqa-validation-860", "mrqa_searchqa-validation-15285", "mrqa_searchqa-validation-11044"], "SR": 0.4375, "CSR": 0.5257352941176471, "EFR": 0.9444444444444444, "Overall": 0.6966921977124183}, {"timecode": 68, "before_eval_results": {"predictions": ["fort boyard", "Richard Seddon", "16", "farmer Phil Archer", "st james cephalonia", "top cat", "Fotheringhay Castle", "tungsten", "New Zealand", "Fenn Street School", "Kristiania", "South Pacific", "kurt hummel", "mozart", "thalia", "paddy mcinness", "woodstock", "mel Blanc", "Chicago", "Bruce Alexander", "dog sport", "alfresco", "Sarajevo", "Hokkaido", "Norman Mailer", "david boyard", "florence", "apple", "braille", "PC", "Stockholm", "george w", "Switzerland", "fort boyard", "pressure", "st thomas\u2019s tower", "fort boyard", "peter boyard", "dr ichak adizes", "1936", "honda", "anoushka shankar", "Dunfermline Athletic Football Club", "cribbage", "midtown Manhattan", "Texas A&M", "stave", "copper", "pear", "cunard", "elton john", "peptide bond", "William the Conqueror", "Aslan", "Gregory Carlton \" Greg\" Anthony", "\"Pete and Gladys\"", "Lowe's Companies, Inc.", "India", "Cash for Clunkers", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Nassau", "michelle jarre", "degaussing", "10 Years"], "metric_results": {"EM": 0.578125, "QA-F1": 0.675}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-4762", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-3851", "mrqa_triviaqa-validation-5077", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-1862", "mrqa_triviaqa-validation-5611", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-5498", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-536", "mrqa_triviaqa-validation-7144", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-7518", "mrqa_naturalquestions-validation-3016", "mrqa_hotpotqa-validation-3119", "mrqa_hotpotqa-validation-1129", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-3611", "mrqa_searchqa-validation-3627"], "SR": 0.578125, "CSR": 0.5264945652173914, "EFR": 1.0, "Overall": 0.7079551630434783}, {"timecode": 69, "before_eval_results": {"predictions": ["victoria plum Brit", "Ronald Searle", "tommy Hendry", "Loki Laufeyi Larson", "The Avengers", "snakes", "insulin", "lilac", "Matt Kowalski", "Adam\u2019s apple", "Andes", "banshee", "hawaii", "eutrophication", "heraldry", "good life", "japan", "british", "Sherlock Holmes", "Ida noddack", "Rocky and Bullwinkle Show", "Vindaloo", "South Africa", "bror", "mark Twain", "Holly Johnson", "bolognese", "khaki uniforms", "berry", "joseph w", "Dunfermline Athletic Football Club", "4x3", "joseph caiaphas", "penrhyn", "australia", "African violet", "ourselves alone", "James Dean", "Eva Herzigov\u00e1", "drizzle", "Chiropractic", "The Wicker man", "stieg Larsson", "tommy biederman", "orecchiette", "Harvard", "hypertext", "Croatia", "cete", "greyfriars", "katherine Bridges", "John Locke", "2017 season", "Matt Monro", "comic", "Disha Patani", "USS \"Enterprise\"", "berry lock", "Kenneth Cole", "Donald Duck", "solar eclipse", "Cher", "the Black Sea", "Crank Yankers"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6276041666666666}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2531", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-1801", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-2824", "mrqa_triviaqa-validation-4485", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1322", "mrqa_triviaqa-validation-6999", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-335", "mrqa_triviaqa-validation-4829", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-7614", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-110", "mrqa_searchqa-validation-9522", "mrqa_searchqa-validation-14235"], "SR": 0.5625, "CSR": 0.5270089285714286, "EFR": 1.0, "Overall": 0.7080580357142857}, {"timecode": 70, "UKR": 0.708984375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3054", "mrqa_hotpotqa-validation-306", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-242", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3680", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-975", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1461", "mrqa_squad-validation-147", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2564", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3473", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3923", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5884", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6670", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-6981", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7083", "mrqa_squad-validation-7094", "mrqa_squad-validation-7339", "mrqa_squad-validation-78", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-9002", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9344", "mrqa_squad-validation-9411", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1514", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1801", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4615", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5117", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6285", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.853515625, "KG": 0.51796875, "before_eval_results": {"predictions": ["the Philippines", "silurian", "r Ricky Gervais", "sanderson", "tequila", "sixth", "perry pear", "lyon", "gold", "Tina Turner", "Sparks", "nissan", "washing", "mexico", "Benjamin Britten", "Eric Coates", "st Pancras", "beer", "Toronto", "main tributaries", "Lady Gaga", "phil Glenister", "carbon copy", "1979", "Donald Trump", "volume", "Tomorrow Never Dies", "tea", "toptenz", "Melbourne", "bullfighting", "Autobahn", "Kiss Me, Kate", "watches", "Hindenburg", "Andre Agassi", "sandra", "Tangled", "spanish", "Morrissey", "stockings", "cooperative", "smallpox", "tommy Steerpike", "Leicester City", "violin", "nipples", "maine taylor", "the Temple of Artemis", "abietic acid", "Achille Lauro", "Frank Langella", "anion", "Valene Kane", "Hilo", "Objectivism", "16,116", "mayor", "Daniel Radcliffe", "Eleven", "Harold M. Ickes", "bone marrow", "Smilla", "France"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6473958333333334}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-4689", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-6736", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-5521", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-2633", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-2648", "mrqa_triviaqa-validation-1591", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-2372", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-2351", "mrqa_hotpotqa-validation-4382", "mrqa_newsqa-validation-3688", "mrqa_searchqa-validation-15744"], "SR": 0.578125, "CSR": 0.5277288732394366, "EFR": 0.9629629629629629, "Overall": 0.71423211724048}, {"timecode": 71, "before_eval_results": {"predictions": ["31536000", "Suez Canal", "robert b Boyle", "manzig", "spain", "john poulson", "breadfruit", "1963", "Richard Strauss", "spock", "sandi Tok svig", "london le Vau", "robert davis", "About Eve", "lord lownie", "ireland", "Arabah", "nicknamed lord lee", "Ut\u00f8ya", "lesley Garrett", "Ty Hardin", "b\u00e4umer", "2240 gallons", "dolomites", "b Bristol Aeroplane Company", "charliesheen", "british Jewish parents", "bitter", "endometriosis", "philip yordan", "d", "eight", "Pizza Express", "Lilo & Stitch", "Hugh quarshie", "b. Baltimore", "Hindi", "st leger", "Eric Morley", "sandstone trail", "Assault on Precinct 13", "james chastain", "barge", "war", "antelopes", "relativistic mass", "james", "muskets", "bajan", "malted barley", "hoagland", "Jonny Buckland", "Laura Haddock", "Janie Crawford", "teen brains Go!", "Milk Barn Animation", "nursery rhyme", "the Russian air force,", "President Obama and Britain's Prince Charles", "byproducts", "Captains Courageous", "Ivan III", "kayak", "Wisconsin"], "metric_results": {"EM": 0.328125, "QA-F1": 0.41770833333333335}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.26666666666666666, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-4932", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-64", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-2746", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3511", "mrqa_triviaqa-validation-3506", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-4467", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-5256", "mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-4048", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-4979", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-2128", "mrqa_triviaqa-validation-7629", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-5018", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-564", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5346", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1051", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-3830"], "SR": 0.328125, "CSR": 0.5249565972222222, "EFR": 0.9767441860465116, "Overall": 0.7164339066537468}, {"timecode": 72, "before_eval_results": {"predictions": ["miller", "wichterle", "Poland", "d.C.", "apple", "high jump", "a horizontal desire", "germany", "port Talbot", "pantagruel", "leggings", "Loose ends", "australian", "charlie chaplin", "smell", "russell", "judy holliday", "cedars", "mary connelly", "Yellowstone", "blue ivy", "peter", "Israel", "wanderers", "1943", "liz taylor", "daimler", "duchess", "Bosnia and Herzegovina", "james hargreaves", "antonia pinter", "peter stuyvesant", "south africa", "d Dirk Bikembergs", "Berlin", "mark Twain", "surfer", "ever decreasing circles", "quito", "Sensurround", "roland", "sandown", "yule buck", "lady", "Turkish Empire", "bb", "phoenician", "halloween", "Kajagoogoo", "Carly Simon", "robbie holliday", "March 12, 2013", "sorrow regarding the environment", "into the intermembrane space", "Ronnie Schell", "La Familia Michoacana", "Starlite", "in the neighboring country of Djibouti,", "cervical cancer vaccine,", "Asia", "Warsaw", "City Slickers", "an ex-wife", "Richa Sharma"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6067708333333334}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-849", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-1196", "mrqa_triviaqa-validation-4914", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-7446", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-5510", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-4148", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-249", "mrqa_triviaqa-validation-2640", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-4498", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-5471", "mrqa_naturalquestions-validation-180", "mrqa_hotpotqa-validation-1782", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2784", "mrqa_searchqa-validation-1212"], "SR": 0.5625, "CSR": 0.5254708904109588, "EFR": 1.0, "Overall": 0.7211879280821918}, {"timecode": 73, "before_eval_results": {"predictions": ["tale of gosta Berling", "c\u00e9vennes", "lilo and stitch", "Tacitus", "george best", "loki", "bagram collection Point", "pink", "charlie chaplin", "ostrich", "ireland", "Louren\u00e7o marques", "silk warp", "swaziland", "cartoons", "Dracula", "jaws", "dodo", "imola", "album", "brazil", "sydney", "america", "worcester", "poincar\u00e9 conjecture", "Superman", "wales", "christian republic", "mary square garden", "The Equals", "Frobisher bay", "Woodstock", "molybdenum", "permian", "hungary", "apollon", "Matterhorn", "hallmarks", "tide-wise", "genesis", "trumpet", "south Carolina", "ourselves alone", "james chadwick", "coffee house", "Apocalypse Now", "fasting", "boris becker", "althorp", "Pyrenees", "noah", "Richmond, BC", "October 1, 2015", "near Flamborough Head", "25 November 2015", "Jesper Myrfors", "Vancouver", "Rod Blagojevich,", "Mary Procidano,", "opium", "Louis XIV of France", "Minnesota", "quid", "Ugly Betty"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7369791666666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-80", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-1967", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-605", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-606", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-1499", "mrqa_naturalquestions-validation-4092", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-5018", "mrqa_newsqa-validation-3632", "mrqa_searchqa-validation-7502"], "SR": 0.71875, "CSR": 0.5280827702702703, "EFR": 0.9444444444444444, "Overall": 0.7105991929429429}, {"timecode": 74, "before_eval_results": {"predictions": ["jamaica", "comedy playhouse", "julius taylor", "norway", "halloween", "Compundyne", "Samson", "copenhagen", "selene", "western Caribbean Sea", "bathtub curve", "john napier", "silvergrass", "macbeth", "Eton College", "geomagnetic field", "Diego Garc\u00eda", "keeper of the Longstone (Fame Islands) lighthouse", "ritchie Valens", "robert boyle", "phoibos", "sphinx", "madison king", "William Morris", "pennsylvania state university", "Father Brown", "thomas taylor", "aircraft", "dihydrogen monoxide", "dennis wilson", "rudolph Cromwell", "alison krauss", "raspberries", "4", "neurons", "copenhagen", "banjo", "cricketer", "time bandits", "The Hague", "one Foot in the Grave,", "philip armstrong", "copper", "Pyotr tchaikovsky", "speed camera", "food, water, sleep, and warmth", "blue", "passport", "florence", "fancy dress shop", "jabba hutt", "the 15th century", "1937", "turkey", "business", "boxer", "The Handmaid's Tale", "Stanford University", "Sgt. Barbara Jones of the Orlando Police Department.", "because the Indians were gathering information about the rebels to give to the Colombian military.", "George Babbitt", "Emanuel Swedenborg", "nod", "Thorleif Haug"], "metric_results": {"EM": 0.453125, "QA-F1": 0.546875}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-4919", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-1912", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-4433", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-3070", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-4603", "mrqa_triviaqa-validation-53", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-4192", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-5806", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-957", "mrqa_naturalquestions-validation-2830", "mrqa_hotpotqa-validation-2388", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3332", "mrqa_naturalquestions-validation-2509"], "SR": 0.453125, "CSR": 0.5270833333333333, "EFR": 1.0, "Overall": 0.7215104166666667}, {"timecode": 75, "before_eval_results": {"predictions": ["rugby", "actual life", "North by Northwest", "Danelaw", "mahatma Gandhi", "for Gallantry", "filibustering", "colette", "willow", "eurozone", "Separate Tables", "Percy Spencer", "Ulysses S. Grant", "1929", "caribbean", "Antarctica", "The Hurt Locker", "general macarthur", "carl", "zager and Evans", "c\u00e9vennes", "Genesis", "sirhan Sirhan", "volleyball", "JeSuis Charlie", "judy garland", "dark blood", "lowestoft", "washington", "teaching evolution in violation of a Tennessee state law", "lulu", "erinyes", "faggots", "caribwin austen", "Angus Deayton", "david bowie", "Chester", "tchaikovsky", "faversham", "Jimmy Knapp", "caribos", "republic", "caribbutz", "butcher", "James Mitchell", "priesthood", "violins", "carl taylor", "eucalyptus", "1883", "herald of free enterprise", "3000 BC", "lacteal", "Renishaw Hall", "Jefferson Memorial", "96", "New Orleans, Louisiana", "JBS Swift Beef Company,", "Silicon Valley.", "10-person", "Trotsky", "beta blockers", "ono", "a oracle"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7613906926406926}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 0.7272727272727273, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-4040", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-2328", "mrqa_naturalquestions-validation-10408", "mrqa_hotpotqa-validation-1123", "mrqa_newsqa-validation-1820", "mrqa_searchqa-validation-7050"], "SR": 0.703125, "CSR": 0.5293996710526316, "EFR": 1.0, "Overall": 0.7219736842105264}, {"timecode": 76, "before_eval_results": {"predictions": ["Ruth Bader Ginsburg", "propeller", "joe louis", "George Clooney", "Andrew Wyeth", "the Louvre", "feminism", "potatoes", "Wallace & Gromit", "anorthos", "Mozambique", "blue Nile", "troy", "\"Timber!\"", "reptiles", "car fair", "coconut milk", "christie woodward", "Russian merchants", "Lord Bill Astor", "Finland", "Making the Band 3", "pennies", "Colorado", "chiaroscuro", "onomatopoeia", "library of congress", "Hawaii", "edward garcia", "Georgetown University", "kidney disease", "difference", "Colin Kaepernick", "Madison County", "Kennebunkport", "pensions", "a leg", "africa", "ingnue", "Notre Dame de Paris", "eRA", "peppermint pelt", "Sir James Paul McCartney, MBE", "Iberian peninsula", "bionic woman", "jeopardy", "baccarat", "Drums Along the Mohawk", "Wallis warfield Simpson", "grapevine", "Coco Chanel", "December 14, 2017", "Virginia Dare", "man", "seppuku", "the Panama Canal", "Will Smith", "1950", "The Bonnie Banks o' Loch Lomond", "An aircraft", "Russia and China", "Alwin Landry's supply vessel Damon Bankston", "Obama administration", "funchal"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6168154761904763}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-16515", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-5110", "mrqa_searchqa-validation-8417", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-13605", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12916", "mrqa_searchqa-validation-737", "mrqa_searchqa-validation-9899", "mrqa_searchqa-validation-15516", "mrqa_searchqa-validation-4056", "mrqa_searchqa-validation-6024", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-13006", "mrqa_searchqa-validation-12225", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10309", "mrqa_searchqa-validation-635", "mrqa_searchqa-validation-5273", "mrqa_searchqa-validation-15174", "mrqa_naturalquestions-validation-9781", "mrqa_triviaqa-validation-5219", "mrqa_hotpotqa-validation-2065", "mrqa_hotpotqa-validation-2730"], "SR": 0.484375, "CSR": 0.528814935064935, "EFR": 1.0, "Overall": 0.721856737012987}, {"timecode": 77, "before_eval_results": {"predictions": ["Graceland", "Bob Fosse", "\"S\"", "mexico", "Miles Davis", "Volleyball", "Havana", "Edwin Hubble", "Einstein", "Lhasa", "the Census", "New Kids on the Block", "Manila Bay", "Lady Chatterley's lover", "molasses", "Hard Knock Life", "a crumpet", "Douglas MacArthur", "(Fred) Thompson", "Sappho", "indigo", "Texas", "Donald Trump", "the Hippocratic Oath", "the Taliban", "Solidarity", "Kookaburra", "the Battle of Hastings", "the examination of one's own thought and feeling", "Crafts", "Shift", "W.H. Auden", "Chuck Berry", "Cal Ripken", "diaphragm", "the Marquis de Sade", "Louis Comfort Tiffany", "a tornado", "the joker", "New Zealand", "Mitt Romney", "Jutland", "Kindergarten Cop", "tentacles", "Titanic", "San Francisco", "Gulliver's Travels", "a carriage", "Billy Bathgate", "Richmond", "steel", "the pop duo Boy Meets Girl in 1988", "Cheryl Campbell", "an Ohio newspaper", "Joe Brown", "Funchal", "Virgil", "Timothy Dowling", "WANH", "southwest Denver, Colorado near Bear Creek", "FBI Special Agent Daniel Cain,", "staff sergeant", "\"procedure on her heart,\"", "55th district"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7341145833333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14018", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-1687", "mrqa_searchqa-validation-1661", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-1963", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14160", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-11728", "mrqa_searchqa-validation-3821", "mrqa_searchqa-validation-388", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-12470", "mrqa_searchqa-validation-13652", "mrqa_searchqa-validation-10670", "mrqa_naturalquestions-validation-8528", "mrqa_naturalquestions-validation-6665", "mrqa_hotpotqa-validation-5500", "mrqa_newsqa-validation-2547", "mrqa_hotpotqa-validation-5006"], "SR": 0.640625, "CSR": 0.5302483974358974, "EFR": 0.9565217391304348, "Overall": 0.7134477773132665}, {"timecode": 78, "before_eval_results": {"predictions": ["Flint, Michigan.", "a president who understands the world today, the future we seek and the change we need.", "5:20 p.m.", "Former Mobile County Circuit Judge Herman Thomas", "\"Top Gun\"", "Daniel Radcliffe", "Graham", "Tennessee", "Intensifying", "the death of Prince George's County police Cpl. Richard Findley,", "Dubai", "gun", "Louisiana's Larry King", "talk show queen Oprah Winfrey.", "bankruptcies", "repression and dire economic circumstances.", "African National Congress", "1.2 million people.", "Haiti.", "police", "Zuma", "TV", "US Airways Flight 1549", "the home, stopping to wonder at the stately main hall and gliding their hands along the same banister that supported the likes of the Marquis de Lafayette.", "forcibly drugging", "an auxiliary lock", "The Rosie Show", "Ninety-two", "Tuesday", "Moscow and Moscow,", "6-1", "The harder they squeeze and squish that breast, the less tissue the X-rays have to go through and the more likely they are to find something.\"", "Diego Milito's", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith", "romantic", "Tennessee Gov. Tim Kaine", "\"a striking blow to due process and the rule of law.\"", "Friday.", "a judge to order the pop star's estate", "consumer confidence", "Afghanistan and India", "Hugo Chavez", "Expedia", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "bronze", "nine", "JBS Swift Beef Company, of Greeley, Colorado,", "as many as 250,000 unprotected civilians", "state senators", "Friday,", "Jaime Andrade", "headdresses", "the head of the United States Department of Justice", "the Sons of Liberty", "Arkansas", "Adam Smith", "the troposphere", "Manchester Victoria station in air rights space", "communist", "1896", "the ceiling", "Gulliver", "John Molson", "Sir Adrian Boult"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6019775222135956}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false], "QA-F1": [0.0, 0.9565217391304348, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.43750000000000006, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-806", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-368", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-642", "mrqa_triviaqa-validation-6410", "mrqa_hotpotqa-validation-1531", "mrqa_hotpotqa-validation-375", "mrqa_searchqa-validation-15735", "mrqa_triviaqa-validation-5099"], "SR": 0.46875, "CSR": 0.5294699367088608, "EFR": 0.9411764705882353, "Overall": 0.7102230314594192}, {"timecode": 79, "before_eval_results": {"predictions": ["an Italian and six Africans", "Daniel Radcliffe", "an independent homeland since 1983.", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Samoa", "BET", "in the Iraq's autonomous region of Kurdistan.", "Adam Yahiye Gadahn,", "mental health and recovery.", "75.", "co-wrote", "2005.", "12", "Lana Clarkson", "Iran", "attempted burgl stemming from a fatal encounter with police officer Daniel Enchautegui.", "70,000", "severe flooding", "56,", "frozen world located in the Gaslight Theater.", "AbdulMutallab", "a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Saturday's Hungarian Grand Prix.", "Sub-Saharan Africa", "Manny Pacquiao", "Aung San Suu Kyi", "Aniston, Demi Moore and Alicia Keys", "hand-painted Swedish wooden clogs", "Michael Schumacher", "Austin Wuennenberg,", "1983", "the U.S. Holocaust Memorial Museum,", "Monterrey, Texas.", "an African-American woman", "a secret U.S. program to assassinate terrorists in Iraq.", "misdemeanor", "Thursday", "the man facing up, with his arms out to the side.", "golf", "stopping militant rocket fire into Israel.", "prostate cancer,", "Iraqi and U.S. soldiers were attacked by small-arms, machine-gun and RPG fire", "$1.45 billion", "people look at the content of the speech, not just the delivery.", "Television's Eco Solutions", "strategy, plans and policy on the Army staff.", "walk on ice in Alaska.", "a city of romance, of incredible architecture and history.", "regulators in the agency's Colorado office", "Chancellor Angela Merkel", "\"Nothing But Love\"", "number of times a pitcher pitches in a season", "Audrey II", "access to US courts", "the troposphere", "arthur ashe", "Hippos", "2004", "Tim \"Ripper\" Owens", "Brad Silberling", "Mother Vineyard", "mars", "the Capitol", "during World War II"], "metric_results": {"EM": 0.671875, "QA-F1": 0.736603776125835}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.9523809523809523, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615383, 0.33333333333333337, 0.9411764705882353, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2835", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-1706", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7987", "mrqa_triviaqa-validation-4441", "mrqa_searchqa-validation-15009"], "SR": 0.671875, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.7223437500000001}, {"timecode": 80, "UKR": 0.67578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-512", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11854", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5646", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7035", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-984"], "OKR": 0.828125, "KG": 0.49609375, "before_eval_results": {"predictions": ["Tyler \"Ty\" Mendoza", "\"The Chronicles of Narnia\"", "four", "professional footballer", "five", "Julie Taymor", "Greg Anthony", "Drifting", "Logar", "Rebirth", "the President's Volunteer Service Award", "\"Big\" Harpe,", "Bedknobs and Broomsticks", "Dougray Scott, Jessica De Gouw and Martin McCann", "Yubin, Yeeun", "Herbert Ross", "Elena Verdugo", "melodic hard rock", "nine", "Chancellor of Austria", "Taylor Swift", "SARS", "William F. Buckley Jr.", "1345 to 1377", "\"Cs\u00e1sz\u00e1ri \u00e9s Kir\u00e1lyi Hadsereg\"", "Noel", "India Today", "North Dakota", "2006", "\"Histoires ou contes du temps pass\u00e9\"", "Bernice Pauahi Bishop", "MMA", "Yarrow and Stookey", "Mathieu Kassovitz", "Eileen Atkins", "Summerlin, Nevada", "Jean- Marc Vall\u00e9e", "Klasky Csupo", "1950s", "Prussia", "Newfoundland and Labrador", "Tom Kartsotis", "Knowlton Hall", "shock cavalry", "Manhattan", "Professor Frederick Lindemann, Baron Cherwell", "dementia", "The 5 foot 9 inch tall twins", "seven", "July 11, 2016", "Vision of Love", "5", "40 %", "May 2017", "trumpet", "Top Cat", "G\u00e9rard Depardieu", "is a businessman, team owner, radio-show host and author.", "dozens more", "14", "Rhizo", "Emperor Maximillian", "Lake Michigan", "Hot Chocolate"], "metric_results": {"EM": 0.5, "QA-F1": 0.6186383928571428}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.28571428571428575, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-1975", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-3650", "mrqa_hotpotqa-validation-3898", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-399", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-4131", "mrqa_naturalquestions-validation-8301", "mrqa_naturalquestions-validation-190", "mrqa_newsqa-validation-781", "mrqa_searchqa-validation-1349", "mrqa_searchqa-validation-6921", "mrqa_triviaqa-validation-85"], "SR": 0.5, "CSR": 0.5308641975308642, "EFR": 1.0, "Overall": 0.7061728395061728}, {"timecode": 81, "before_eval_results": {"predictions": ["views", "The White Shadow", "Hungary", "the HIV/AIDS", "Nepal", "nounseven", "Sanjaya", "Fauvism", "Dresden", "Turkish", "Shirley Temple", "flavor Flav", "Mike Nichols", "backcountry", "blue blood", "acetylene", "32", "Harriet the Spy", "Michigan", "a drop of Roses lime", "Amsterdam", "Grover Cleveland", "Clyde", "James Naismith", "The Battle of Hastings", "North Carolina", "Job", "1969", "a cassava plants", "pickles", "Stand by Me", "Lead", "Nokia", "Eddie Waitkus", "Cyprus", "alpine skiing", "Neil Diamond", "Munich", "Babe Ruth", "wildebeest", "Sicilian", "Pirates of the Caribbean", "tuna", "Arts and Crafts", "lm", "Subclue 2", "Uvula", "Biloxi", "Treasure Island", "Robots", "the trousseau", "comprehend and formulate language", "Robber baron", "NFL owners", "Jane Seymour", "Guy", "George Bernard Shaw", "Michael Greif", "Tom Ewell", "Wolfgang Amadeus Mozart", "Senate Democrats", "seven", "16", "an cancerous tumor."], "metric_results": {"EM": 0.6875, "QA-F1": 0.6979166666666666}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-6682", "mrqa_searchqa-validation-7539", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-14093", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-1314", "mrqa_searchqa-validation-14091", "mrqa_searchqa-validation-7545", "mrqa_searchqa-validation-15450", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-16773", "mrqa_searchqa-validation-7174", "mrqa_searchqa-validation-2290", "mrqa_hotpotqa-validation-2337", "mrqa_newsqa-validation-1546"], "SR": 0.6875, "CSR": 0.5327743902439024, "EFR": 1.0, "Overall": 0.7065548780487805}, {"timecode": 82, "before_eval_results": {"predictions": ["Jaws 2", "the femur", "Ovid", "The Power Hour News", "a squier noun", "Tudor", "Australia", "sugar cane juice", "sheep", "Washington, D.C.", "lily", "Hammurabi", "the Isle of Wight", "gung ho", "Dale Earnhardt", "Johns Hopkins", "Hashemite Kingdom of Jordan", "Lindsay Davenport", "North Africa", "a timpani", "Stephen Hawking", "James Madison", "an X-Ray", "Disturbia", "Michael Moore", "The Indianapolis 500", "I, Daniel Blake", "Tap Queen", "T. Leaves", "Johannesburg", "carbon", "Philistines", "Deep brain stimulation", "Louis Chevy", "Morocco", "M&M\\'s", "Hieronymus Bosch", "Neil Diamond", "Cardinal Richelieu", "Malaysia", "bionic", "Hamlet", "Lance Armstrong", "a chicken Fried Steak", "Edith Wharton", "the Berlin Wall", "Uranus", "George Costanza", "a telephone operator", "a bonnet", "Henry Moore", "wintertime", "the Great Crash", "Elena Anaya", "Brisbane Road", "vinegar Joe", "austria", "Girl Meets World", "three", "\"Pour le M\u00e9rite\"", "The meter reader", "New York-based Human Rights Watch", "used", "Don Draper"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7367187500000001}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-734", "mrqa_searchqa-validation-7268", "mrqa_searchqa-validation-802", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-15533", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-14165", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-13445", "mrqa_searchqa-validation-14350", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-319", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8837", "mrqa_triviaqa-validation-5807", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2964"], "SR": 0.65625, "CSR": 0.5342620481927711, "EFR": 0.9545454545454546, "Overall": 0.6977615005476452}, {"timecode": 83, "before_eval_results": {"predictions": ["Winnipeg", "Dmitri Mendeleev", "calligraphy", "Kennedy", "Maria Sharapova", "Chile", "glow", "John Waters", "Aristophanes", "the Clean Air Act", "freelance", "y Yahoo", "Thurman Munson", "a barrel", "Chippewa Indians", "Rooster Cogburn", "15", "Richard Burton", "gears", "a pie", "The Dying Swan", "the Big Bang", "winter", "Alyssa Milano", "Tahiti", "Herbert Hoover", "Keith Urban", "3", "smoking", "John Herschel Glenn Jr.", "The Netherlands", "Kelly Clarkson", "Michael Douglas", "an aquiline nose", "Troy weight", "Neil Simon", "the Newbury", "trespass", "Ronald Reagan", "Patrick Henry", "the light bulb", "war", "the viola", "ostrich", "I love rock and roll", "the American Mind", "America", "Ziploc", "Hannibal", "Anne Wiggins Brown", "Beethoven", "Gene Barry", "the normal resting rate", "a 1993 American comedy - drama film directed by Fred Schepisi", "the American Civil War", "mexico", "Denise Richards", "India Today", "the Distinguished Service Cross", "GQ", "rape and murdering a woman in Missouri.", "Jennifer Aniston", "the supreme court,", "New York Islanders"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7229166666666667}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8000000000000002, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15272", "mrqa_searchqa-validation-4451", "mrqa_searchqa-validation-9367", "mrqa_searchqa-validation-6597", "mrqa_searchqa-validation-7651", "mrqa_searchqa-validation-1729", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-9041", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-15147", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-16490", "mrqa_searchqa-validation-3237", "mrqa_searchqa-validation-6308", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-4354", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-435", "mrqa_newsqa-validation-3882"], "SR": 0.65625, "CSR": 0.5357142857142857, "EFR": 1.0, "Overall": 0.7071428571428571}, {"timecode": 84, "before_eval_results": {"predictions": ["Hoffmann", "Ford", "zymase", "Grover\\'s Corner", "President Lincoln's second inaugural address", "topaz", "Universal City", "surrender", "New York Times Best Seller", "subtraction", "Harpy", "Macon", "3.1", "fur", "Titan", "KNOLL", "quick picks", "Fidel Castro", "a glacier", "makrama", "Toy Story", "fight", "the Ark of the Covenant", "the French Legion of Honour", "granite", "Emperor", "Klondike", "a dove", "GALCIT Rocket Research", "Francis Scott Key", "Eminem", "Tarzan", "Diebold", "mozzarella", "New Guinea", "Queen Latifah", "the Liberty Bell", "anchovy", "a Saint", "Clarence Thomas", "the day of Mars", "nacreous", "whimper", "Prison Break", "Iberia", "the ceiling", "a kart", "Kilimanjaro", "Koalas", "Circus World Museum", "Extradition", "1979", "Steve Carell", "Alan Holdsworth", "pasta carbonara", "finger", "Robert Schumann", "Pacific Place", "1941", "yellow fever", "help the convicts find calmness in a prison culture fertile with violence and chaos.", "Polo", "Friday,", "2009"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5313244047619048}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-10313", "mrqa_searchqa-validation-9264", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-3217", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-14803", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-12518", "mrqa_searchqa-validation-3882", "mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9830", "mrqa_searchqa-validation-4630", "mrqa_searchqa-validation-8767", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-7275", "mrqa_searchqa-validation-14709", "mrqa_searchqa-validation-10541", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-5264", "mrqa_triviaqa-validation-7214", "mrqa_hotpotqa-validation-3149", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-1008", "mrqa_naturalquestions-validation-1856"], "SR": 0.484375, "CSR": 0.5351102941176471, "EFR": 0.9696969696969697, "Overall": 0.7009614527629233}, {"timecode": 85, "before_eval_results": {"predictions": ["15", "cancer", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "Picasso's muse and mistress, Marie-Therese Walter.", "a motor scooter", "Pixar's \"Toy Story\"", "supermodel", "Graeme Smith", "Missouri.", "AbdulMutallab", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "the equator,", "at least 13", "Immigration and Customs Enforcement", "9 a.m.-1 p.m.", "acid attack by a spurned suitor.", "Bowie", "has been weakened by this latest economic crisis,\" said Robert Pastor, who was a Latin America national security adviser for former President Carter.", "a number of calls,", "summer", "Tuesday afternoon,", "an experiment of 25 years of research on the condition, which is characterized by bouts of diarrhea and constipation.", "Molotov cocktails, rocks and glass.", "Sen. Evan Bayh of Indiana and Virginia Gov. Tim Kaine", "last April,", "2008", "\"fusion teams,\"", "three", "the L'Aquila earthquake, which killed nearly 300 people and devastated the city when it struck last year,", "At least 22", "Cash for Clunkers", "$199", "cancerous tumor.", "American", "CNN's \"Piers Morgan Tonight\"", "several weeks,", "$1.4 million,", "next year", "the death of", "the 11th year in a row.", "will look at how the universe formed by analyzing particle collisions.", "Saturn owners", "dead", "11 healthy eggs", "a former Navy captain whose boyish looks and deceitful ways earned him the nickname the \"Blonde Angel of Death.\"", "drug cartels", "some of the Awa", "two remaining crew members", "Sabina Guzzanti", "more than 4,000 commercial farmers off their land, destroying Zimbabwe's once prosperous agricultural sector.", "U.S. forces in Afghanistan are doing everything possible to free Bergdahl, Defense Secretary Robert Gates said Monday.", "the national flag of the United States", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "\"IRL\"", "Gaston Leroux", "Volkswagen", "\u00c6thelwald Moll", "Robert \"Bobby\" Germaine, Sr.", "white and orange", "an epiphyte", "Reanimator", "a stride", "Girls' Generation"], "metric_results": {"EM": 0.5, "QA-F1": 0.6735856951758896}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.05882352941176471, 1.0, 1.0, 1.0, 0.0625, 1.0, 0.9523809523809523, 0.6666666666666666, 1.0, 0.0, 1.0, 0.19047619047619044, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.9473684210526316, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.6923076923076924, 0.1818181818181818, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3186", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-164", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-3537"], "SR": 0.5, "CSR": 0.5347020348837209, "EFR": 1.0, "Overall": 0.7069404069767442}, {"timecode": 86, "before_eval_results": {"predictions": ["cement (or concrete)", "Cayman", "orsche", "coax", "haiku", "waive", "China", "loverly", "economics", "Graceland", "funnel", "Beverly Hills", "coffee", "a chicken", "automobiles", "Isaac Newton", "Billy Budd", "John Brown", "Communist", "Gene Krupa", "Skulls", "Cain", "Smashing Pumpkins", "cruller", "I", "Ma Barker", "Northanger Abbey", "Wyatt Earp", "Star Trek", "Mensa", "febreze", "a portrait", "mutton", "Philip Seymour Hoffman", "opni belief", "Wayne Gretzky", "amu", "Michael Irvin", "Gap", "salt", "Tower of London", "Arbor Day", "Westinghouse Electric Company", "a salad Dressing bottle", "The Fugitive", "Sisyphus", "Java", "Puerto Rico", "bioluminescence", "Rococo", "the First Barbary War", "Pakistan", "961", "Linda Davis", "dodo", "Northumberland", "Louis XVI", "July 16, 1971", "pastels and oil painting", "9", "monarchy's", "Africa", "Donald Trump", "Johnny cage"], "metric_results": {"EM": 0.6875, "QA-F1": 0.73828125}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15136", "mrqa_searchqa-validation-16965", "mrqa_searchqa-validation-8424", "mrqa_searchqa-validation-1300", "mrqa_searchqa-validation-12385", "mrqa_searchqa-validation-6460", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-14681", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-6245", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-13204", "mrqa_searchqa-validation-2161", "mrqa_searchqa-validation-5579", "mrqa_searchqa-validation-5867", "mrqa_hotpotqa-validation-680", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1586", "mrqa_hotpotqa-validation-4514"], "SR": 0.6875, "CSR": 0.5364583333333333, "EFR": 1.0, "Overall": 0.7072916666666667}, {"timecode": 87, "before_eval_results": {"predictions": ["Washington, Jay and Franklin", "no more than 4.25 inches ( 108 mm )", "the external genitalia", "The empire gains the Mandate of Heaven", "seven", "the third ventricle", "Cody Fern", "12", "in 2007 and 2008", "New York City", "joy", "longitude", "Midnight Mass in Rome", "Johannes Gutenberg", "The Mecca", "Rocky Dzidzornu", "Jennifer Grey", "Experimental neuropsychology", "Yosemite National Park", "April 3, 1973", "Jane Lynch", "administrative supervision", "1997", "Emma Watson", "near Flamborough Head", "Big Boi and Sleepy Brown", "the President of India", "Pedro Espada", "nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "the House of Representatives", "Session Initiation Protocol", "bowel obstruction, short bowel syndrome, gastroschisis, prolonged diarrhea regardless of its cause, high - output fistula, very severe Crohn's disease or ulcerative colitis, and certain pediatric GI disorders", "presbyters", "Tessa Peake - Jones", "Jennifer O'Neill", "September 19 - 22, 2017", "the 2001 -- 2002 season", "1773", "Randy VanWarmer", "senators", "Teri Garr", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors", "13", "10.5 %", "Gene MacLellan", "from shore to shore", "Brad Dourif", "during a game in 1988", "Sanchez Navarro", "in the late 1970s", "23 September 1889", "My Fair Lady", "Armageddon", "thammasat", "\u00c6thelstan", "Eugene", "The Highwaymen", "Israel", "Former Mobile County Circuit Judge Herman Thomas", "the abduction of minors.", "Dag Hammarskjld", "Erin Go Bragh", "a BS degree", "three"], "metric_results": {"EM": 0.5, "QA-F1": 0.596918174138569}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true], "QA-F1": [0.4210526315789474, 1.0, 0.0, 0.7499999999999999, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.4, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.9189189189189189, 0.0, 1.0, 0.2631578947368421, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2777777777777778, 1.0, 1.0, 1.0, 0.0, 1.0, 0.46153846153846156, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-6157", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-8061", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4571", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-7138", "mrqa_naturalquestions-validation-7074", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-4707", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-687", "mrqa_searchqa-validation-3012", "mrqa_searchqa-validation-3589"], "SR": 0.5, "CSR": 0.5360440340909092, "EFR": 0.9375, "Overall": 0.6947088068181818}, {"timecode": 88, "before_eval_results": {"predictions": ["2013", "Triple Alliance of Germany, Austria - Hungary, and Italy", "John von Neumann", "Joanne Wheatley", "Alicia Vikander as Lara Croft", "the Federated States of Micronesia and the Indonesia ( which consists of thousands of islands )", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "Mickey Rourke", "4.37 light - years ( 1.34 pc )", "the 135th meridian west of the Greenwich Observatory", "John Joseph Patrick Ryan", "12 to 36 months old", "1988", "Malayalam", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned, and to give his or her advice and opinion upon questions of law when required by the", "the Old Testament", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "Kryptonite", "the Fly Girls", "a crust of mashed potato", "the New Testament", "digestive systems", "the employer", "speech", "1973", "fresh nuclear fuel", "Nancy Jean Cartwright", "Germany", "total cost", "the National Football League ( NFL )", "1978", "geologist Charles Lyell", "1956", "Little G minor symphony", "Thomas Middlingitch", "the author of V\u1e5bksayurveda", "The track is built around an organ accompanied by slow tempo drums and vocals", "2011", "during initial entry training", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "Jeff Barry and Andy Kim", "2003", "Simone Vangsness", "Ludacris", "vehicles inspired by theJeep that are suitable for use on rough terrain", "tissues in the vicinity of the nose", "Javier Fern\u00e1ndez", "Felicity Huffman", "RAF, Fighter Command", "1974", "National Industrial Recovery Act", "Mercury", "Bob Marley & the Wailers", "Ronseal", "model", "between the 8th and 16th centuries", "Blue Origin", "2,700-acre", "to provide security as needed.", "forcibly injecting them with psychotropic drugs", "AkAk", "time", "the Cherokee", "the elections are expected to provide Mugabe with the toughest challenge yet in his nearly 28 years of rule."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5913037004901229}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.48275862068965514, 1.0, 0.7499999999999999, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.8311688311688311, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.8, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1111111111111111]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-2272", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-954", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-6001", "mrqa_newsqa-validation-2756", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-12140", "mrqa_searchqa-validation-11445", "mrqa_newsqa-validation-1133"], "SR": 0.453125, "CSR": 0.5351123595505618, "EFR": 0.9714285714285714, "Overall": 0.7013081861958266}, {"timecode": 89, "before_eval_results": {"predictions": ["Paul Lynde", "the naos", "In England", "420", "the fourth ventricle", "Doc '' Brown", "Steve Russell", "if the occurrence of one does not affect the probability of occurrence of the other", "full '' sexual intercourse", "the Archies", "the government - owned Panama Canal Authority", "to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "Comancheria", "Jay Baruchel", "Plank", "the early 20th century", "Thomas Hobbes in his Leviathan", "in the red bone marrow of large bones", "Sarah Silverman", "Janie Crawford", "New England", "3", "supervillains who pose catastrophic challenges to the world", "in 1932", "After World War II", "reproductive", "the NFL", "Biotic -- Biotic", "1975", "the British colonists", "March 1995", "Austin, Texas", "Muhammad", "July 21, 1861", "Part 2", "the 1984 Summer Olympics in Los Angeles", "the `` 0 '' trunk code", "David Joseph Madden", "December 12, 2017", "8 December 1985", "British Indian Association", "inner epithelia", "on the microscope's stage", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "The uvea", "he hosted a short - lived talk show in WCW called A Flair for the Gold", "in 1971", "Moton Field", "1995 Mitsubishi Eclipse", "Beijing", "sport utility vehicles", "Nowhere Boy", "grayson", "kidney", "Luigi Segre", "Ronald Lyle \" Ron\" Goldman", "Adrian Lyne", "543", "from Geraldine Ferraro to Bill Clinton.", "stay on track and get me through prison,\"", "tanks", "the Army of the Potomac", "Horn", "Basketball"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7380284645909645}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-699", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-10250", "mrqa_naturalquestions-validation-2209", "mrqa_triviaqa-validation-519", "mrqa_hotpotqa-validation-2410", "mrqa_newsqa-validation-2027", "mrqa_searchqa-validation-1282"], "SR": 0.640625, "CSR": 0.5362847222222222, "EFR": 0.9565217391304348, "Overall": 0.6985612922705314}, {"timecode": 90, "UKR": 0.673828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3812", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8093", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.806640625, "KG": 0.515625, "before_eval_results": {"predictions": ["Eriksson", "Tiananmen Square", "Colonel Sebastian Moran", "December", "Milli Vanilli", "Chrysler", "jigdal Dagchen Sakya", "Real Madrid", "York", "Norman Hartnell", "A Beautiful Mind", "Red Admiral", "The Meadows", "23.5\u00b0", "Verona", "Ishmael", "Christmas in Hollis", "Macbeth", "throw", "physics", "poland", "Easter Island", "Peter Sellers", "fever", "Milton Keynes", "a meteorite", "1954", "China", "river wimpole", "fishes", "Independence Day", "English", "Keane", "Sarkozy", "The Princess bride", "porphyry", "Jack Ruby", "a base stock", "website", "Helen Gurley Brown", "australia", "Groucho Marx", "Exile", "1664", "Shanghai", "Stieg Larsson", "five", "Saskatchewan", "Priam", "Denise van Outen", "argument", "775", "septum", "August 8, 1508", "Dallas", "four months in jail", "Christmas Day, December 25, 2009", "L'Aquila earthquake,", "Janet and La Toya", "that he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted.", "2C", "rain", "I Will Remember You", "Hohenschwangau"], "metric_results": {"EM": 0.609375, "QA-F1": 0.66484375}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-2755", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-5187", "mrqa_triviaqa-validation-7317", "mrqa_triviaqa-validation-2576", "mrqa_triviaqa-validation-7092", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-2330", "mrqa_triviaqa-validation-4760", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-3004", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5474", "mrqa_hotpotqa-validation-4546", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-2839", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-3057"], "SR": 0.609375, "CSR": 0.5370879120879121, "EFR": 1.0, "Overall": 0.7066363324175824}, {"timecode": 91, "before_eval_results": {"predictions": ["diddle", "lusitania", "Bild Newspaper", "japan", "blind side", "korea", "Imola", "twenty-second", "Herald of Free Enterprise", "bridge", "norway", "le Leicester", "Jo Moore", "ormolu", "yellow", "Burkina Faso", "mortadella", "Wembley", "phil archer", "The Telegraph", "palladium", "leander", "a national militia", "o2", "dorset", "1825", "(Bito) Mussolini", "portugal", "ochre", "ophthalmologist", "Donald Trump", "ruritania", "birds", "london", "lily Allen", "portugal", "Blofeld", "mozart", "flipper", "Cardiff", "one", "lolo soetoro", "don quijote", "tla\u010denica", "zipporah", "carousel", "michael hordern", "Mary Poppins", "The Quatermass Experiment", "(Lord) Beaconsfield", "cappella", "small fission systems", "disputes between two or more states", "2014 Olympic Games in Sochi, Krasnodar Krai, Russia", "L\u00edneas A\u00e9reas", "tragedy", "Eastern College Athletic Conference", "one evening last week", "\"The Cycle of Life,\"", "July 23.", "a tartar sauce", "Enron", "selenium", "2010"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5709325396825397}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.42857142857142855, 0.8, 0.2222222222222222, 0.8, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-4425", "mrqa_triviaqa-validation-6622", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-684", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-3194", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-7358", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-766", "mrqa_triviaqa-validation-4458", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-4865", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-2585", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2013", "mrqa_searchqa-validation-6143"], "SR": 0.484375, "CSR": 0.5365149456521738, "EFR": 1.0, "Overall": 0.7065217391304348}, {"timecode": 92, "before_eval_results": {"predictions": ["maarten tromp", "indonesia", "Brooklyn", "Arizona Diamondbacks", "Dan Dare", "rudolph", "fat", "Singapore", "birds", "Stephenie Meyer", "Hebrew", "heisenberg", "carlsberg", "Cumberland", "Billy Connolly", "spanish", "kiel Canal", "australia", "faggots", "madison square garden", "Jeffery deaver", "John Flamsteed", "pangram", "croquet", "kinks", "Spearchucker", "reservoirs", "Botticelli", "zebras", "king arthur", "Renaissance", "south africa", "blackfriars", "jimmy re Reeves", "sandra wilsons", "philadelphia", "haute", "archers", "paris", "green Lion", "kiki", "mary taylor", "sheep", "Siberia", "Astronaut", "poirot", "three", "jet streams", "ditz", "Barbra Streisand", "kipps: The Story of a Simple Soul", "the 2001 -- 2002 season", "the NFL", "Identity Theory", "Melbourne Storm", "A Rush of Blood to the Head", "1993", "1994", "Derek Mears", "11", "a truss", "Lake Titicaca", "crossword puzzle", "Austin, Texas,"], "metric_results": {"EM": 0.625, "QA-F1": 0.6791666666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-3842", "mrqa_triviaqa-validation-6743", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-5230", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-525", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-4934", "mrqa_triviaqa-validation-1601", "mrqa_naturalquestions-validation-1507", "mrqa_newsqa-validation-406", "mrqa_searchqa-validation-4717", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-6744", "mrqa_newsqa-validation-3021"], "SR": 0.625, "CSR": 0.5374663978494624, "EFR": 0.875, "Overall": 0.6817120295698925}, {"timecode": 93, "before_eval_results": {"predictions": ["star", "julius taylor", "spain", "japan", "james Garner", "philadelphia boult", "thumper", "kerry kitten", "dill bannatt", "sinus node", "red", "darts checkout table", "Dutch", "indonesia", "spain", "germany", "paul reubens", "gluteal region", "majorca (Mallorca)", "12", "carry on leo", "Alexander Borodin", "Hector BERLIOZ", "king arthur", "spain", "st aidan", "john Virgo", "Richard Seddon", "moles", "stiefbeen en zoon", "Prince Andrew", "parma", "cryonic suspension", "a reclining or couchant sphinx", "Takifugu rubripes", "sodor", "the Porteous Riots", "Willie Nelson", "datello", "wittle", "island of spain", "paris", "dennis", "germany", "patsy Williams", "Austria", "Essex Eagles", "Kaiser Chiefs", "philistine", "nicolas cage", "Ralph Vaughan Williams", "long - standing policy of neutrality", "April 1917", "Saint Alphonsa", "Takura Tendayi", "Whitesnake", "310", "Robert Barnett,", "Google's Android phones,", "The switch had been scheduled for February 17, but Congress delayed the conversion -- which had been planned for years -- to accommodate people like Richter who had not been able to update their TVs.", "an orchid", "nubia", "lira", "Joanna Moskawa"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5243055555555556}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.05555555555555555, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2634", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-6873", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-4113", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4135", "mrqa_triviaqa-validation-5975", "mrqa_triviaqa-validation-4323", "mrqa_triviaqa-validation-3070", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-6743", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-6771", "mrqa_triviaqa-validation-6485", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-3276", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-1424", "mrqa_searchqa-validation-14194"], "SR": 0.484375, "CSR": 0.5369015957446808, "EFR": 1.0, "Overall": 0.7065990691489361}, {"timecode": 94, "before_eval_results": {"predictions": ["the Interpreter", "Frida Khalo", "the Kite Runner", "Pope John Paul II", "Louisa May Alcott", "Rock Island", "Turandot", "the Bolsheviks", "cloning", "Signs", "(Edward) the Black Prince", "forgery", "the Police", "a carrots", "Manhattan", "Rehab", "ballpoint", "tap", "Ernie Banks", "Christopher Columbus", "Olivia Newton-John", "the Great White Way", "shrewd", "her mother", "peter shaffer", "the Ubangi River", "Lovecraft", "reptiles", "a gizzard", "Bangkok", "Reform", "Catwoman", "bats", "Puccini", "Omaha", "the Monitor", "magnesium", "silver", "go where no man has gone before", "Takana", "the Silk Road", "dreams", "Yahoo!", "Cruise Addicts", "Hairspray", "(William) Shakespeare", "a palace", "a blood type O", "Italy", "green", "Humperdinck", "1940", "three", "January 11, 2014, and on April 16, 2014 on Super Channel in Canada", "Velvet Revolution", "taka", "Hampton Court Palace", "Marvel Comics", "three", "Donald Wayne Johnson", "Marcell Jansen", "At least 88 people had been hurt, 28 of them seriously enough to go to a hospital,", "African-Americans", "rocket"], "metric_results": {"EM": 0.625, "QA-F1": 0.7000773225957049}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9550", "mrqa_searchqa-validation-10265", "mrqa_searchqa-validation-5390", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-14368", "mrqa_searchqa-validation-12939", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-11609", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-14428", "mrqa_searchqa-validation-3284", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-6100", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-9613", "mrqa_naturalquestions-validation-4594", "mrqa_hotpotqa-validation-4578", "mrqa_newsqa-validation-2068"], "SR": 0.625, "CSR": 0.537828947368421, "EFR": 1.0, "Overall": 0.7067845394736842}, {"timecode": 95, "before_eval_results": {"predictions": ["Clifford Roberts", "a cobra", "New York", "MEXICO", "Peter Paul Rubens", "(Tom) Parker", "Belgium", "the mu-koan", "cholesterol", "his only begotten Son", "the Miami Dolphins", "the MIM-104 Patriot", "a utensil", "Marcia Cross", "Northern Exposure", "Pocahontas", "Easy Rider", "Gatun Lake", "\"blessed\"", "Ned Kelly", "Jakarta", "the Cherokee Nation", "Jim Bunning", "brood", "Kennedy", "Arby's", "Albert Einstein", "lysogenic", "\"Les Feuilles d'automnes\"", "fudge", "a dot", "a cattle prod", "Henry Bessemer", "stimulation", "an egg", "Ken Russell", "The Crucible", "the United Farm Workers", "a zenith", "apogee", "Vancouver", "a semaphore", "a reverse", "Coors Field", "Edgar Rice Burroughs", "\"All for Our Country\"", "Gatherer", "a Canadian philosopher", "3.14159", "\"The Postman Always Rings Twice\"", "Kansas City", "1979", "Mockingjay -- Part 2 ( 2015 )", "Massachusetts", "Illinois", "South Africa", "Vader", "George Adamski", "five", "Marine Corps", "two", "to put the \"black box\" warning on Cipro and other fluoroquinolones, and also to warn doctors.", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "black"], "metric_results": {"EM": 0.59375, "QA-F1": 0.68046875}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3709", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-4716", "mrqa_searchqa-validation-838", "mrqa_searchqa-validation-7412", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-7113", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-9198", "mrqa_searchqa-validation-3964", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-7194", "mrqa_naturalquestions-validation-1427", "mrqa_triviaqa-validation-3778", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3198"], "SR": 0.59375, "CSR": 0.5384114583333333, "EFR": 1.0, "Overall": 0.7069010416666666}, {"timecode": 96, "before_eval_results": {"predictions": ["Flickr", "Van Allen radiation belts", "air", "Leontyne Price", "a dragonfly", "Charles I", "Casey Kasem", "San Juan", "sheep", "The Witch of Eastwick", "Joseph Smith", "the Rose", "May", "aOreo", "Cyrano de Bergerac", "Alaska", "birds", "the European Union", "Verdi", "Matt Lauer", "the Kremlin", "\"How the firebrand Shi'ite cleric became a major power broker in the new Iraq\"", "Frogs", "heracles", "a Clerk", "giovanni bologna", "ACL", "the Sacred Cod", "Agatha Christie", "a truck back", "Esther", "cat scratch fever", "New Kids on the Block", "Iraq", "country", "The Crucible", "Lincoln", "center of gravity", "Moriarty", "Simon Cowell", "sodium", "Lenin", "a fruitcake", "nests", "The Firebird", "Kansas", "a radical", "Air France", "Louis Brandeis", "a tooth", "David", "In the 1979 -- 80 season", "Virgil Ogletree", "Tom Brady", "Dick Whittington", "leeds", "gloster", "1828", "Elijah Wood", "Dizzy Dean", "The Ski Train", "Israel", "a rapist", "skirts"], "metric_results": {"EM": 0.625, "QA-F1": 0.7042410714285714}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-13498", "mrqa_searchqa-validation-3088", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-862", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-12287", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4540", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9745", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-6912", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-3093", "mrqa_hotpotqa-validation-4621", "mrqa_newsqa-validation-1176"], "SR": 0.625, "CSR": 0.5393041237113403, "EFR": 1.0, "Overall": 0.707079574742268}, {"timecode": 97, "before_eval_results": {"predictions": ["Louis: The French Prince Who Invaded England", "a pig", "Fear of Flying", "War Admiral", "Abraham Lincoln", "the B horizon", "Donald Duck", "Czech Republic", "Roussimoff", "Spanish", "Buddhism", "Theodore Roosevelt", "deluge", "Cold Mountain", "murder mysteries", "boxing", "A Night at the Roxbury", "King Henry II", "the Claddagh Ring", "Keith Richards", "the Hydra", "Name Name Name", "Central Park Zoo", "Marcia Clark", "the Lincoln Tunnel", "anbatross", "Bob Fosse", "Dictum", "Georgia", "(Richard) Nixon", "Madame Tussaud", "Monsters", "Shakespeare", "Mother Jones", "James Gandolfini", "HatfieldMcCoy", "Walter Scott", "Pig Latin", "the Nile", "air traffic control", "mutton", "Latin", "New Wave", "Patrick Ewing", "Vienna", "Darwin", "New Orleans", "parody", "ccoli", "arteries", "Carol Burnett", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "expressing disagreement with the majority opinion of the court which gives rise to its judgment", "9 February 2018", "18", "Jeremy Thorpe", "blue", "A Song of Ice and Fire", "British Labour Party", "Balvenie Castle", "Animal Planet", "St. Louis, Missouri.", "strawberry", "ITV"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6184640522875816}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.47058823529411764, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-15628", "mrqa_searchqa-validation-2353", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-16083", "mrqa_searchqa-validation-5057", "mrqa_searchqa-validation-13412", "mrqa_searchqa-validation-15943", "mrqa_searchqa-validation-16528", "mrqa_searchqa-validation-11879", "mrqa_searchqa-validation-8337", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-4255", "mrqa_searchqa-validation-16590", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-8347", "mrqa_searchqa-validation-13547", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-2490"], "SR": 0.515625, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.70703125}, {"timecode": 98, "before_eval_results": {"predictions": ["Franklin, Indiana", "Nelson County", "47", "Sir Hiram Stevens Maxim", "15 October 1988", "5 February 1976", "Boston Celtics", "Hermione Youlanda Ruby Clinton-Baddeley", "45,698", "The Netherland's Andries Jonker", "Ashanti Region of Ghana", "Groupe PSA", "Texas Tech University", "brigadier general", "Omega SA", "South Australia", "Apatosaurus", "Resorts World Genting", "the Beatles", "British", "1950", "six", "Future", "London", "Cuyler Reynolds", "The original News Corporation or News Corp.", "Toxics Release Inventory", "Figaro", "South African", "Alleyne v. United States", "Bambi: Eine Lebensgeschichte aus dem Walde", "close to 50 million", "Whoopi Goldberg", "Transporter 3 (French: Le Transporteur 3)", "Disco", "Frederick I", "Afghanistan", "Thriller", "Antonio Lippi", "March 30, 2025", "American", "Warcraft", "Isabella II", "3 Days to Kill", "Vitor Belfort", "11 November 1821", "villanelle", "Emilia-Romagna", "Who's That Girl", "Alan Young", "Cristiano Ronaldo", "Virginia Dare", "David Tennant", "inefficient", "Simeon Williamson", "Baffin Island", "Moffitt", "Alwin Landry's", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "the Sri Lankan cricket team in the Pakistani city of Lahore.", "raising", "Banana", "Tallahassee", "Brooke Wexler"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6075520833333334}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 0.8, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.25, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-5381", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-4963", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1669", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-3578", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-2364", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-6791", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-1102", "mrqa_searchqa-validation-5295", "mrqa_searchqa-validation-3489"], "SR": 0.515625, "CSR": 0.5388257575757576, "EFR": 1.0, "Overall": 0.7069839015151516}, {"timecode": 99, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5687", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-1037", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-1440", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15285", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15680", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3057", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4454", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1734", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2791", "mrqa_triviaqa-validation-2844", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.861328125, "KG": 0.5265625, "before_eval_results": {"predictions": ["Robert Arthur Mould", "1926", "Antonio Lippi", "1993", "The Allies of World War I", "Logan International Airport", "1979", "Best Sound", "The Suite Life of Zack & Cody", "Switzerland", "2017", "Stern-Plaza", "Pakistan", "jazz homeland section of New Orleans", "Mick Schumacher", "Darkroom", "1972", "Royce da 5'9\" (Bad) and Eminem (Evil)", "Christianity Today", "Lionel Eugene Hollins", "water", "Harlem neighborhood", "Bardot", "Love Actually", "Commanding General", "George Orwell", "five", "20 March to 1 May 2003", "1993", "imp My Ride", "1886", "relations", "Syracuse", "Godspell", "1755", "The Big Bang Theory", "1966", "Johnny Herbert", "Annales de chimie et de\u58eb", "British", "punk rock", "Atlantic", "Adelaide Laetitia \" Addie\" Miethke", "Red", "Theodore Robert Bundy", "Matthew Ryan Kemp", "Epic Records", "Roslin Castle", "2.1 million members", "the Rothschild banking dynasty", "Corendon Dutch Airlines", "Claudia Grace Wells", "Hugo Weaving", "Freedom Day", "Misery", "Henry Hudson", "geometry", "Adidas", "whether to close some entrances, bring in additional officers, and make security more visible.", "the 3rd Platoon, A Company, 2nd Light Cruiser Reconnaissance Battalion", "Syria", "cherries", "the Philippines", "Bullnose"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7427218614718615}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.5, 0.8, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-5238", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-2900", "mrqa_hotpotqa-validation-1711", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-1822", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-735", "mrqa_hotpotqa-validation-1640", "mrqa_naturalquestions-validation-9150", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1859", "mrqa_searchqa-validation-14664"], "SR": 0.640625, "CSR": 0.53984375, "EFR": 0.9565217391304348, "Overall": 0.7202105978260869}]}