{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5500, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["67.9", "Mike Carey", "rapidly raising population and traffic in cities along SR 99, as well as the desirability of Federal funding", "their greatest common divisor is one", "the Official Report", "immunoinformatics", "lupus erythematosus", "Kublai Khan", "New Testament", "1926", "other ctenophores", "60%", "he was illiterate in Czech", "architect or engineer", "British", "Gateshead Council", "Book of Genesis", "Shing-Tung Yau", "complexity classes", "Mexico", "cabinet", "after its 1977 merger with Radcliffe College", "The Master", "chastity", "Mark Woods", "one another", "100% oxygen", "Steam engines", "the wedding banquet", "the Ohio Company of Virginia", "CBS and NBC", "aircraft manufacturing", "a school or other place of formal education", "Times Square Studios", "Normans and Norman", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "decreases", "Duke Richard II of Normandy, and King Ethelred II of England", "Royal Shakespeare Company", "books and articles", "between 1835 and 1842", "Edmonton, Canada", "rises in sea levels", "it is neither zero nor a unit", "University of Chicago College Bowl Team", "algorithms have been written that solve the problem in reasonable times in most cases", "13.34%", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "49\u201315", "The Mongols' extensive West Asian and European contacts", "east-west", "the kip", "Croatia", "railway locomotives", "the law is no longer to be taught to Christians but belonged only to city hall", "Josh Norman", "1964 and 1968", "expelled Jews", "Elton Rule", "gravel", "11:28", "Super Bowl XLVII", "a fee per unit of information transmitted", "seven"], "metric_results": {"EM": 0.875, "QA-F1": 0.9101325145442792}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1187", "mrqa_squad-validation-2853", "mrqa_squad-validation-6986", "mrqa_squad-validation-2704", "mrqa_squad-validation-1891", "mrqa_squad-validation-1161", "mrqa_squad-validation-1089", "mrqa_squad-validation-2473"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["\"Fourth\"", "The input string", "European Parliament", "the Meuse", "overseas colonies", "Kings Canyon Avenue and Clovis Avenue", "entertainment", "Astra 2A", "poison", "atoon", "Orange", "the BBC", "Dolby Digital", "shocked", "Executive Vice President of Football Operations", "Thomas Edison", "British colonists would not be safe as long as the French were present.", "least prejudiced", "inferior", "Johann Tetzel", "planktonic", "main porch", "the courts of member states", "The Shah's army was split by diverse internecine feuds and by the Shah's decision to divide his army into small groups concentrated in various cities.", "1st century BC", "Iberia", "Silas B. Cobb", "Aristotle", "The Swahili", "the sheepshanks Gallery", "1968", "heavy/highway, heavy civil or heavy engineering", "type III secretion system", "Commission v Italy", "Gary Kubiak", "The European Court of Justice", "silicates", "1933", "Endosymbiotic gene transfer", "proplastids", "Jurassic Period", "Conrad of Montferrat", "sacramental union", "reciprocating Diesel", "the Arabs and much of the rest of the Third World", "mineral deposits", "Grand Canal d'Alsace", "Vince Lombardi Trophy", "\u00dcberseering BV v Nordic Construction GmbH", "first 15 years", "Neoclassical economics", "Variable lymphocyte receptors", "exploration is still continuing to determine if there are more reserves.", "Elder", "internal migration and urbanisation", "high risk preparations and some other compounding functions", "water level", "two", "James `` Jamie '' Dornan ( born 1 May 1982 ) is an Irish actor, model, and musician. He played Axel von Fersen in Sofia Coppola's Marie Antoinette ( 2006 )", "A driver's license is an official document permitting a specific individual to operate one or more types of motorized vehicles", "radioisotope heater unit provides heat from radioactive decay of a material and can potentially produce heat for decades", "Mickey Mantle", "A lymphocytes is one of the subtypes of white blood cell in a vertebrate's immune system", "concentration camps"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7201914098972922}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.5, 0.2857142857142857, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7689", "mrqa_squad-validation-1767", "mrqa_squad-validation-3922", "mrqa_squad-validation-6029", "mrqa_squad-validation-2672", "mrqa_squad-validation-6211", "mrqa_squad-validation-1902", "mrqa_squad-validation-375", "mrqa_squad-validation-10186", "mrqa_squad-validation-4636", "mrqa_squad-validation-6163", "mrqa_squad-validation-8927", "mrqa_squad-validation-3370", "mrqa_squad-validation-7635", "mrqa_squad-validation-8273", "mrqa_squad-validation-2315", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-9342"], "SR": 0.6875, "CSR": 0.78125, "EFR": 1.0, "Overall": 0.890625}, {"timecode": 2, "before_eval_results": {"predictions": ["six", "1883", "1870 to 1939", "2010", "unit-dose, or a single doses of medicine", "2003", "Budapest Telephone Exchange", "an induction motor", "markets", "to avoid being targeted by the boycott", "socialist realism", "Ten", "education, sanitation, and traffic control", "4000", "St. Bartholomew's Day massacre", "early twentieth century homes", "Thesis 86", "five or more", "in a glass case", "1,320 kilometres (820 miles)", "force-free magnetic fields", "a Tatar chieftain, Tem\u00fcjin-\u00fcge", "The Three Doctors", "San Francisco Bay Area's Levi's Stadium", "a prime", "St. Lawrence and Mississippi watersheds", "2010", "cholera", "168,637", "four", "faith", "25", "Deabolis", "ten", "to spearhead the regeneration of the North-East", "the late 1920s", "2007", "1936", "1968", "the European Parliament and the Council of the European Union", "typhus, smallpox and respiratory infections", "California", "Spanish", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "the sheepshanks Gallery", "Four thousand", "Prime ideals", "Narrow alleys or 'chares', most of which can only be traversed by foot", "a lack of understanding of the legal ramifications, or due to a fear of seeming rude", "MHC class I molecules", "the Great Fire of London", "in his lab and elsewhere", "Rudy Clark", "Art Carney", "Ren\u00e9 Verdon 1961 -- 1965   Henry Haller 1966 -- 1987   Jon Hill 1987", "honey bees may be the state's most valuable export", "July 4, 1776", "May 5, 1904", "The Lykan Hypersport is a Lebanese limited production supercar built by W Motors, a United Arab Emirates based company, founded in 2012 in Lebanon with the collaboration of Lebanese, French and Italian engineers", "E \u00d7 12, A \u00d7 9, I \u00d7 9, O \u00d7 8, N \u00d7 6, R \u00d7 4, S \u00d7 4", "the digitization of social systems", "Trace Adkins", "he checked himself into a Los Angeles mental institution in an effort to kick the habit", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7890697004608295}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.12903225806451613, 0.0, 1.0, 1.0, 0.761904761904762, 0.3]}}, "before_error_ids": ["mrqa_squad-validation-9855", "mrqa_squad-validation-6526", "mrqa_squad-validation-1277", "mrqa_squad-validation-7246", "mrqa_squad-validation-5751", "mrqa_squad-validation-133", "mrqa_squad-validation-9012", "mrqa_squad-validation-4546", "mrqa_squad-validation-5135", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-1173", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-3164"], "SR": 0.734375, "CSR": 0.765625, "EFR": 0.9411764705882353, "Overall": 0.8534007352941176}, {"timecode": 3, "before_eval_results": {"predictions": ["the Hamiltonian path problem", "Thomas Edison", "systematic economic inequalities", "July 31, 1995", "vitamin D", "Thomas Edison", "1987", "terra nullius", "Leukocytes", "research, exhibitions and other shows", "Erg\u00e4nzungsschulen", "along the coast", "\"principal hostile country\"", "17", "The Book of Discipline", "67.9", "Turkey", "over 100%", "Saffir-Simpson Scale", "New Orleans", "Nearly 3,000", "Earth", "Bruno Mars", "an archipelago-like estuary", "Antigone", "Soviet Union", "the Dodge D-50", "over the age of 18", "the ozone generated in contact with the skin", "polynomial-time", "the WMO", "several years", "Danny Trevathan", "Albert of Mainz", "patient care rounds drug product selection", "Thomas Coke", "Colony of Victoria Act 1855", "Anglo-Saxon populations", "John Debney", "Geordie", "Ancient Greeks", "Budget cuts", "Systemic acquired resistance", "one darkened lens; the picture would look normal to those viewers who watched without the glasses", "the Legislative Assembly", "the desire to prevent things that are indisputably bad", "Any member can put their name forward to be First Minister, and a vote is taken by all members of Parliament", "Tyrion", "Manchuria", "David H. Splane", "To capitalize on her publicity", "1937", "All Things Must Pass", "James W. Marshall", "1979", "January 12, 2017", "a loop", "Kyrie Irving", "Charles Carson", "Morgan Freeman", "Eda Reiss Merin", "The Seven Cities of Cibola", "is fully wired, tested and U.L. listed for electrical", "The fibula is a long, thin bone running parallel to the tibia"], "metric_results": {"EM": 0.75, "QA-F1": 0.7878014989714075}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789473, 1.0, 1.0, 0.18181818181818182, 0.19999999999999998, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.19999999999999998]}}, "before_error_ids": ["mrqa_squad-validation-1760", "mrqa_squad-validation-9764", "mrqa_squad-validation-3770", "mrqa_squad-validation-100", "mrqa_squad-validation-1436", "mrqa_squad-validation-1764", "mrqa_squad-validation-7713", "mrqa_squad-validation-7838", "mrqa_squad-validation-9559", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-8657", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-6338"], "SR": 0.75, "CSR": 0.76171875, "retrieved_ids": ["mrqa_squad-train-4382", "mrqa_squad-train-67602", "mrqa_squad-train-49659", "mrqa_squad-train-1049", "mrqa_squad-train-58508", "mrqa_squad-train-69523", "mrqa_squad-train-32669", "mrqa_squad-train-23545", "mrqa_squad-train-40306", "mrqa_squad-train-22217", "mrqa_squad-train-51019", "mrqa_squad-train-30594", "mrqa_squad-train-3665", "mrqa_squad-train-25019", "mrqa_squad-train-61687", "mrqa_squad-train-3918", "mrqa_squad-validation-2704", "mrqa_squad-validation-6211", "mrqa_naturalquestions-validation-6573", "mrqa_newsqa-validation-1030", "mrqa_squad-validation-10186", "mrqa_squad-validation-8927", "mrqa_squad-validation-6986", "mrqa_squad-validation-6526", "mrqa_naturalquestions-validation-9342", "mrqa_squad-validation-1277", "mrqa_squad-validation-3922", "mrqa_squad-validation-2473", "mrqa_naturalquestions-validation-5378", "mrqa_squad-validation-6029", "mrqa_squad-validation-2315", "mrqa_naturalquestions-validation-4586"], "EFR": 1.0, "Overall": 0.880859375}, {"timecode": 4, "before_eval_results": {"predictions": ["immune surveillance", "Arizona Cardinals", "10,000", "3 January 1521", "Scotland Act 1998", "AC", "Broncos quarterback", "1\u20133 \u03bcm thick", "Air Force", "Meiji Restoration", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "842 pounds", "June 4, 2014", "recast as decision problems", "Gateshead Council", "more than 70", "Gamal Abdul Nasser", "the Bible", "rolling circle mechanism", "the mantle", "Hangzhou", "genetic branches", "12 December 1963", "UNESCO World Heritage Site", "January 27, 1967", "cortisol and catecholamines", "constituency seats", "drummes", "Tower Theatre", "MinE", "coach", "North American Aviation", "banded iron", "Arizona Cardinals", "Super Bowl XXXVIII", "major business districts", "HO", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d", "two", "destruction of the forest", "the Commission", "the net mechanical energy", "Paul Greene", "Eddie Murphy", "1923", "southwestern Colorado and northwestern New Mexico", "Paul", "Carol Worthington", "1997", "accomplish the objectives of the organization", "3 October 1990", "The federal government", "Jerry Leiber", "Heroes and Villains", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "Directed distance is a positive, zero, or negative scalar quantity", "June 12, 2018", "tracy lawrence", "22 \u00b0 00 \u2032 N 80 \u00b0 00", "James Martin Lafferty", "La Dame aux cam\u00e9lias", "Cadillac Stingrays", "Polo", "the mind at birth as a clean slate"], "metric_results": {"EM": 0.625, "QA-F1": 0.7268353174603175}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.5714285714285715, 1.0, 0.8, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.3333333333333333, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-635", "mrqa_squad-validation-8750", "mrqa_squad-validation-6327", "mrqa_squad-validation-3811", "mrqa_squad-validation-1649", "mrqa_squad-validation-4949", "mrqa_squad-validation-8901", "mrqa_squad-validation-73", "mrqa_squad-validation-2632", "mrqa_squad-validation-6614", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-5451", "mrqa_triviaqa-validation-4831", "mrqa_hotpotqa-validation-5838", "mrqa_newsqa-validation-1003", "mrqa_searchqa-validation-5338"], "SR": 0.625, "CSR": 0.734375, "EFR": 0.875, "Overall": 0.8046875}, {"timecode": 5, "before_eval_results": {"predictions": ["orientalism", "Director", "the east end", "NL and NC", "1534", "April 1887", "\u00d6gedei Khan", "nearby open spaces", "on the West Side", "John Pap, Lord of Pelham Manor", "Dai Setsen", "821,784", "the two political parties would share power equally", "buoyancy", "ancestors", "divergence problem", "Islamist", "satellite television", "he ordered the deportation of the French-speaking Acadian population from the area", "accepting Jesus as your personal Lord and Savior", "131", "investigated the other alleged mistakes", "Maria Sk\u0142odowska-Curie Institute of Oncology", "winter of 1973\u201374", "P", "Best Drama Series", "Off-Off Campus", "the General Conference", "Super Bowl XLIV", "Sam Chisholm", "the mother", "LDS Church", "the most cost efficient bidder", "non-governmental", "non-Catholics", "southern and central parts", "the Aveo", "Luther's education", "1998", "administrative supervision", "XXXX, is a London underworld criminal who has established himself as one of the biggest cocaine suppliers in the city, with effective legitimate cover", "Thunder Road", "Sylvester Stallone", "In the present ( 2016 -- 2018, contemporaneous with airing ) and a storyline taking place at a set time in the past ; but some episodes are set in one time period or use multiple flashback time periods", "Mohammad Reza Pahlavi", "Thomas Mundy Peterson", "Hebrew", "to be produced with constant technology and resources per unit of time", "season seven", "TLC", "The Spanish brought the European tradition to Mexico, although there were similar traditions in Mesoamerica, such as the god Huitzilopochtli in mid December", "1987", "Asuka", "4 September 1936, she took off from Abingdon, England", "Amy Wong", "Ice Age : The meltdown in 2006", "Pasek and Paul", "U.S. service members who have died without their remains being identified", "Western cultures", "Ovid", "William Bradford", "Bryant Purvis", "40-40", "40,400 members"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6901599702380952}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.761904761904762, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.4, 0.0909090909090909, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3319", "mrqa_squad-validation-3492", "mrqa_squad-validation-10273", "mrqa_squad-validation-9989", "mrqa_squad-validation-9416", "mrqa_squad-validation-8579", "mrqa_squad-validation-1759", "mrqa_squad-validation-7819", "mrqa_squad-validation-6752", "mrqa_squad-validation-2975", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-471", "mrqa_searchqa-validation-13515", "mrqa_hotpotqa-validation-1989"], "SR": 0.59375, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 6, "before_eval_results": {"predictions": ["monophyletic", "computational power", "1971", "rapid combustion", "Stanford", "pressure swing adsorption", "chloroplast lineage", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "Citadel Media", "lands west of the Appalachian Mountains", "two", "ten", "clerical marriage", "average workers", "Catholic", "1886", "potentially dangerous", "Golovin", "the applied force", "near the center of the chloroplast", "the last 7000 years", "3.6%", "God", "C. J. Anderson", "Robert R. Gilruth", "1469", "regions along the border with Somalia and Ethiopia", "$200,000", "atmospheric pressure", "with observations", "6800", "after their second year", "oxygen concentration is too high", "Giuliano da Sangallo", "two", "Chuck Connors", "Eva Cassidy", "Lincoln Logs", "a tin star", "the muscle tissue of vertebrates", "georgebeest", "Promenade", "Hamlet", "a george symbol of liberty", "President Abraham Lincoln", "Martina Hingis", "tuna", "TESLAR Satellite", "a kind of superman", "Gentlemen Prefer Blondes", "Decoupage", "george dot symbols", "Bouvier", "Paris", "Titanic", "temperature", "New Zealand", "george Stevens", "January 15, 2010", "Mark Neveldine and Brian Taylor", "four", "george sabanekh", "Edward Kenway", "1770 BC"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6799479166666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.4, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3676", "mrqa_squad-validation-8643", "mrqa_squad-validation-2754", "mrqa_squad-validation-8900", "mrqa_squad-validation-8399", "mrqa_squad-validation-3479", "mrqa_squad-validation-8529", "mrqa_squad-validation-8924", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-5627", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1643", "mrqa_naturalquestions-validation-8909", "mrqa_searchqa-validation-9828", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-8659"], "SR": 0.609375, "CSR": 0.6964285714285714, "retrieved_ids": ["mrqa_squad-train-36231", "mrqa_squad-train-80601", "mrqa_squad-train-8061", "mrqa_squad-train-124", "mrqa_squad-train-4731", "mrqa_squad-train-47382", "mrqa_squad-train-56672", "mrqa_squad-train-43018", "mrqa_squad-train-15732", "mrqa_squad-train-67943", "mrqa_squad-train-52814", "mrqa_squad-train-14391", "mrqa_squad-train-63014", "mrqa_squad-train-41093", "mrqa_squad-train-83917", "mrqa_squad-train-16655", "mrqa_searchqa-validation-8172", "mrqa_naturalquestions-validation-4247", "mrqa_squad-validation-2853", "mrqa_squad-validation-6526", "mrqa_squad-validation-7246", "mrqa_squad-validation-3370", "mrqa_naturalquestions-validation-392", "mrqa_squad-validation-1089", "mrqa_squad-validation-9989", "mrqa_squad-validation-5135", "mrqa_searchqa-validation-5338", "mrqa_naturalquestions-validation-6573", "mrqa_squad-validation-9855", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-6248", "mrqa_squad-validation-2704"], "EFR": 1.0, "Overall": 0.8482142857142857}, {"timecode": 7, "before_eval_results": {"predictions": ["free", "Oireachtas funds", "May 21, 2013", "Warsaw", "the Mongols' extensive West Asian and European contacts", "3", "Spanish moss", "the frequency and severity of micrometeorite impacts", "Charles Avison", "colonel", "Pedro Men\u00e9ndez de Avil\u00e9s", "a mainline Protestant Methodist denomination", "Capital Cities Communications", "40 km", "meritocracy", "reactive allotrope of oxygen", "bitstrings", "markets", "a difference in earnings between women and men", "2014", "By 9000 BP", "Neoclassical economics", "d'Hondt method", "home viewers who made tape recordings of the show", "the Kenyan coastal town of Kilifi", "first set of endosymbiotic events", "Abe Silverstein", "embroidery", "three", "1823", "British Sky Broadcasting Group plc", "Pylos and Thebes", "Supreme Court", "Minnesota Democratic\u2013Farmer\u2013Labor Party", "Syracuse", "artist and graffiti writer", "General Manager", "Bergen", "John Lennon", "David Irving", "an album", "Croatan, Nantahala, and Nebo", "15,000 people", "2016 World Indoor Championships", "he was a recipient of the \"Pour le M\u00e9rite\"", "psilocybin", "Washington", "Hero", "Saint-Domingue", "Jena Malone", "1865", "National Hockey League", "one", "2009", "Walldorf", "Texas Raiders", "nursery rhyme", "Necator americanus and Ancylostoma duodenale", "Hamelin", "lizards", "a shuttle bus", "a neurological disorder causing an attractive urge to move the body while relaxing or trying to get to sleep", "C. S. Forester", "6"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6562542925824175}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.125, 1.0, 0.8, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5351", "mrqa_squad-validation-10174", "mrqa_squad-validation-3497", "mrqa_squad-validation-7447", "mrqa_squad-validation-9436", "mrqa_squad-validation-9532", "mrqa_squad-validation-8453", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-5701", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-5346", "mrqa_naturalquestions-validation-6200", "mrqa_newsqa-validation-4024", "mrqa_searchqa-validation-7219", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-221"], "SR": 0.546875, "CSR": 0.677734375, "EFR": 1.0, "Overall": 0.8388671875}, {"timecode": 8, "before_eval_results": {"predictions": ["a rock concert", "50-yard line", "a decrease in the price of skilled labor", "12 December 1964", "speed-up theorem", "Schr\u00f6dinger equation", "Guinness World Records", "nominate speakers", "Louis Agassiz", "Annual Status of Education Report", "locomotion", "geophysical surveys", "The later accidental introduction of Beroe", "socially", "one of the most common forms of school discipline", "the Romantic Rhine", "British colonists", "adjustable spring-loaded valve", "complicated", "Arabic numerals", "7\u20134\u20132\u20133 system", "much higher school fees", "WatchESPN", "Economist", "Richard Lindzen", "emigration", "Islamism", "between 25-minute episodes", "Figaro", "Mazda", "US Naval Submarine Base New London submarine school", "Secretary of Defense", "Minyue", "Vyd\u016bnas", "1989", "American", "Anne of Green Gables", "the twelfth title", "Lovejoy", "his death", "Walldorf", "Kings Point, New York", "Bill Miner", "Agent 99", "Outside", "Christopher Nolan", "Umina Beach", "Chicago", "Thriller", "1992", "Let's Make Sure We Kiss Goodbye", "Andrzej Go\u0142ota", "Alonso L\u00f3pez", "post\u2013World War II", "Waylon Albright \"Shooter\" Jennings", "The New Yorker", "Type 10 tank", "Aemento, homo, quia pulvis es, et in pulverem reverteris", "nirvana", "Jared Polis", "a dull and invisible gas", "Speaker of the House of Representatives", "1947, 1956, 1975, 2015 and 2017", "49"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6459687881562881}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 0.0, 0.07692307692307691, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7436", "mrqa_squad-validation-8369", "mrqa_squad-validation-10386", "mrqa_squad-validation-4327", "mrqa_squad-validation-2085", "mrqa_squad-validation-7131", "mrqa_squad-validation-3124", "mrqa_squad-validation-7707", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-2481", "mrqa_naturalquestions-validation-4556", "mrqa_newsqa-validation-3169", "mrqa_searchqa-validation-14617", "mrqa_naturalquestions-validation-5865"], "SR": 0.5625, "CSR": 0.6649305555555556, "EFR": 1.0, "Overall": 0.8324652777777778}, {"timecode": 9, "before_eval_results": {"predictions": ["Konwiktorska Street", "glass", "39", "1760", "CALIPSO", "the state", "The European Court of Justice", "if the head of government of a country were to refuse to enforce a decision of that country's highest court", "lymphokines", "solar", "no more (or no less) meaning", "Finsteraarhorn", "1015 kelvins", "Aaron Spelling", "1770", "833,500", "1851", "Canada", "Northern Chinese", "between 1859 and 1865", "use the proceedings as a forum to inform the jury and the public of the political circumstances surrounding the case and their reasons for breaking the law via civil disobedience", "Gap", "\"missile gap\"", "Kensington", "Port of Long Beach", "he is known to have sentenced only two men to hang, one of whom escaped", "made into a TV series", "50 million", "George Clooney, Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "Matt Groening", "Hern\u00e1n Crespo", "William Finn", "Kenny Young", "British Overseas Territories", "poetry, theater, art, music, the media, and books", "The Rebirth", "the sulfur mustards", "Hearts", "Martin Scorsese", "the Joseon Dynasty", "The Terminator", "the Saint Petersburg Conservatory", "Michael Fred Phelps II", "Bolton, England", "Quasimodo", "Cuban", "Cleveland Browns", "Dhivehi Raa'jeyge Jumhooriyya", "\u00c9cole des Beaux-Arts", "the Kentucky Music Hall of Fame", "American 3D computer-animated comedy", "Agra", "Polka", "Esteban Ocon", "actress", "Kassie DePaiva", "Jaydev Shah", "Stephen Lang", "Doris Lessing", "April", "voluntary manslaughter", "Sodra nongovernmental organization", "the boy", "American"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7141592261904761}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3173", "mrqa_squad-validation-6814", "mrqa_squad-validation-6837", "mrqa_squad-validation-5294", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-1980", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-9926"], "SR": 0.65625, "CSR": 0.6640625, "retrieved_ids": ["mrqa_squad-train-40235", "mrqa_squad-train-3082", "mrqa_squad-train-8480", "mrqa_squad-train-24222", "mrqa_squad-train-7440", "mrqa_squad-train-74213", "mrqa_squad-train-57999", "mrqa_squad-train-31803", "mrqa_squad-train-80496", "mrqa_squad-train-44094", "mrqa_squad-train-39967", "mrqa_squad-train-72054", "mrqa_squad-train-45868", "mrqa_squad-train-18227", "mrqa_squad-train-84844", "mrqa_squad-train-31742", "mrqa_squad-validation-8927", "mrqa_squad-validation-9989", "mrqa_squad-validation-1187", "mrqa_squad-validation-9764", "mrqa_naturalquestions-validation-2130", "mrqa_triviaqa-validation-5627", "mrqa_naturalquestions-validation-661", "mrqa_squad-validation-7838", "mrqa_squad-validation-9436", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-3032", "mrqa_squad-validation-8750", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-5451", "mrqa_squad-validation-2672", "mrqa_hotpotqa-validation-5838"], "EFR": 1.0, "Overall": 0.83203125}, {"timecode": 10, "UKR": 0.787109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-2362", "mrqa_hotpotqa-validation-2481", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-3898", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-476", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4903", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5162", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5512", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5701", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-965", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6604", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-9536", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9842", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-9926", "mrqa_squad-validation-10001", "mrqa_squad-validation-10059", "mrqa_squad-validation-10063", "mrqa_squad-validation-1008", "mrqa_squad-validation-10123", "mrqa_squad-validation-10148", "mrqa_squad-validation-10174", "mrqa_squad-validation-10181", "mrqa_squad-validation-10186", "mrqa_squad-validation-10273", "mrqa_squad-validation-10316", "mrqa_squad-validation-10386", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-1078", "mrqa_squad-validation-1089", "mrqa_squad-validation-1161", "mrqa_squad-validation-1177", "mrqa_squad-validation-1177", "mrqa_squad-validation-1182", "mrqa_squad-validation-1187", "mrqa_squad-validation-1219", "mrqa_squad-validation-1254", "mrqa_squad-validation-1277", "mrqa_squad-validation-133", "mrqa_squad-validation-134", "mrqa_squad-validation-1356", "mrqa_squad-validation-1423", "mrqa_squad-validation-1432", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1593", "mrqa_squad-validation-1613", "mrqa_squad-validation-1614", "mrqa_squad-validation-1640", "mrqa_squad-validation-1649", "mrqa_squad-validation-1665", "mrqa_squad-validation-1678", "mrqa_squad-validation-168", "mrqa_squad-validation-1681", "mrqa_squad-validation-175", "mrqa_squad-validation-1759", "mrqa_squad-validation-1767", "mrqa_squad-validation-1779", "mrqa_squad-validation-1815", "mrqa_squad-validation-185", "mrqa_squad-validation-1859", "mrqa_squad-validation-1891", "mrqa_squad-validation-1898", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-1980", "mrqa_squad-validation-2079", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2144", "mrqa_squad-validation-215", "mrqa_squad-validation-2186", "mrqa_squad-validation-2197", "mrqa_squad-validation-2200", "mrqa_squad-validation-2214", "mrqa_squad-validation-2248", "mrqa_squad-validation-2272", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-242", "mrqa_squad-validation-2473", "mrqa_squad-validation-2490", "mrqa_squad-validation-2568", "mrqa_squad-validation-2586", "mrqa_squad-validation-2612", "mrqa_squad-validation-2632", "mrqa_squad-validation-2666", "mrqa_squad-validation-2672", "mrqa_squad-validation-270", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-271", "mrqa_squad-validation-2725", "mrqa_squad-validation-2765", "mrqa_squad-validation-2775", "mrqa_squad-validation-2807", "mrqa_squad-validation-2811", "mrqa_squad-validation-2853", "mrqa_squad-validation-2873", "mrqa_squad-validation-2893", "mrqa_squad-validation-2950", "mrqa_squad-validation-2975", "mrqa_squad-validation-2986", "mrqa_squad-validation-3007", "mrqa_squad-validation-3076", "mrqa_squad-validation-3096", "mrqa_squad-validation-3124", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3140", "mrqa_squad-validation-3143", "mrqa_squad-validation-3173", "mrqa_squad-validation-3275", "mrqa_squad-validation-3319", "mrqa_squad-validation-3327", "mrqa_squad-validation-3370", "mrqa_squad-validation-3407", "mrqa_squad-validation-3464", "mrqa_squad-validation-3479", "mrqa_squad-validation-3492", "mrqa_squad-validation-3497", "mrqa_squad-validation-3511", "mrqa_squad-validation-3537", "mrqa_squad-validation-3550", "mrqa_squad-validation-3581", "mrqa_squad-validation-3676", "mrqa_squad-validation-3723", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3788", "mrqa_squad-validation-38", "mrqa_squad-validation-3802", "mrqa_squad-validation-3811", "mrqa_squad-validation-3813", "mrqa_squad-validation-3842", "mrqa_squad-validation-3852", "mrqa_squad-validation-3871", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3923", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-3945", "mrqa_squad-validation-402", "mrqa_squad-validation-4034", "mrqa_squad-validation-4179", "mrqa_squad-validation-420", "mrqa_squad-validation-4227", "mrqa_squad-validation-4246", "mrqa_squad-validation-4292", "mrqa_squad-validation-4327", "mrqa_squad-validation-4430", "mrqa_squad-validation-4437", "mrqa_squad-validation-4473", "mrqa_squad-validation-4484", "mrqa_squad-validation-4607", "mrqa_squad-validation-4612", "mrqa_squad-validation-4636", "mrqa_squad-validation-4660", "mrqa_squad-validation-4737", "mrqa_squad-validation-4750", "mrqa_squad-validation-476", "mrqa_squad-validation-4882", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-4981", "mrqa_squad-validation-5007", "mrqa_squad-validation-5019", "mrqa_squad-validation-5030", "mrqa_squad-validation-5042", "mrqa_squad-validation-5085", "mrqa_squad-validation-5135", "mrqa_squad-validation-5147", "mrqa_squad-validation-5196", "mrqa_squad-validation-5198", "mrqa_squad-validation-5276", "mrqa_squad-validation-5294", "mrqa_squad-validation-5325", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5351", "mrqa_squad-validation-5389", "mrqa_squad-validation-5434", "mrqa_squad-validation-5531", "mrqa_squad-validation-5621", "mrqa_squad-validation-5634", "mrqa_squad-validation-5671", "mrqa_squad-validation-5699", "mrqa_squad-validation-5724", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-5788", "mrqa_squad-validation-5797", "mrqa_squad-validation-5869", "mrqa_squad-validation-5875", "mrqa_squad-validation-5947", "mrqa_squad-validation-5961", "mrqa_squad-validation-6029", "mrqa_squad-validation-6089", "mrqa_squad-validation-611", "mrqa_squad-validation-6159", "mrqa_squad-validation-6163", "mrqa_squad-validation-624", "mrqa_squad-validation-6318", "mrqa_squad-validation-6341", "mrqa_squad-validation-635", "mrqa_squad-validation-6351", "mrqa_squad-validation-6381", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6546", "mrqa_squad-validation-6555", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6594", "mrqa_squad-validation-6628", "mrqa_squad-validation-6636", "mrqa_squad-validation-6648", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6716", "mrqa_squad-validation-6752", "mrqa_squad-validation-679", "mrqa_squad-validation-6814", "mrqa_squad-validation-682", "mrqa_squad-validation-6837", "mrqa_squad-validation-6838", "mrqa_squad-validation-6873", "mrqa_squad-validation-6877", "mrqa_squad-validation-6924", "mrqa_squad-validation-6960", "mrqa_squad-validation-6978", "mrqa_squad-validation-6981", "mrqa_squad-validation-6986", "mrqa_squad-validation-7126", "mrqa_squad-validation-7131", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7168", "mrqa_squad-validation-7194", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7277", "mrqa_squad-validation-729", "mrqa_squad-validation-73", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7384", "mrqa_squad-validation-7436", "mrqa_squad-validation-7447", "mrqa_squad-validation-7476", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7585", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7652", "mrqa_squad-validation-7656", "mrqa_squad-validation-7689", "mrqa_squad-validation-7707", "mrqa_squad-validation-7713", "mrqa_squad-validation-773", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-7819", "mrqa_squad-validation-7838", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-8028", "mrqa_squad-validation-8043", "mrqa_squad-validation-8049", "mrqa_squad-validation-8066", "mrqa_squad-validation-8118", "mrqa_squad-validation-8139", "mrqa_squad-validation-816", "mrqa_squad-validation-824", "mrqa_squad-validation-8253", "mrqa_squad-validation-8273", "mrqa_squad-validation-8283", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8386", "mrqa_squad-validation-8399", "mrqa_squad-validation-8453", "mrqa_squad-validation-850", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8505", "mrqa_squad-validation-8523", "mrqa_squad-validation-8529", "mrqa_squad-validation-8555", "mrqa_squad-validation-8579", "mrqa_squad-validation-8601", "mrqa_squad-validation-8603", "mrqa_squad-validation-8643", "mrqa_squad-validation-8680", "mrqa_squad-validation-8683", "mrqa_squad-validation-8750", "mrqa_squad-validation-8801", "mrqa_squad-validation-889", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8924", "mrqa_squad-validation-8927", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-8969", "mrqa_squad-validation-8987", "mrqa_squad-validation-9048", "mrqa_squad-validation-9097", "mrqa_squad-validation-9135", "mrqa_squad-validation-9157", "mrqa_squad-validation-9165", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9328", "mrqa_squad-validation-9367", "mrqa_squad-validation-9416", "mrqa_squad-validation-9436", "mrqa_squad-validation-9459", "mrqa_squad-validation-9470", "mrqa_squad-validation-9531", "mrqa_squad-validation-9543", "mrqa_squad-validation-9553", "mrqa_squad-validation-9559", "mrqa_squad-validation-9608", "mrqa_squad-validation-9764", "mrqa_squad-validation-9787", "mrqa_squad-validation-9819", "mrqa_squad-validation-9868", "mrqa_squad-validation-9900", "mrqa_squad-validation-9901", "mrqa_squad-validation-9943", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-471", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5627", "mrqa_triviaqa-validation-5960", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6577", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7683"], "OKR": 0.923828125, "KG": 0.34765625, "before_eval_results": {"predictions": ["hydrogen and helium", "supervisory church body", "AC", "Jerricho Cotchery", "Isaac Komnenos", "a certain number of teacher's salaries are paid by the State", "cylinders and valve gear", "ABC1", "ATP energy", "San Andreas fault system", "Hulu", "one", "social unrest and violence.", "Toyota Corona", "Metro Light Rail system", "receiver", "A Turing machine", "Luther", "antibodies", "1978", "George Sylvester Viereck", "1804", "seven", "goth and BDSM", "David Hilbert", "India", "Julia Sawalha", "a jingle", "anaerobe", "Kiss Me", "Sean Connery", "Mumbai", "Dada", "Jordan", "Thailand", "Bertrand Russell", "A-ha", "N Africa", "William Holden", "Charlie Brooker", "racing", "gold", "aniline dyes", "Al Ahly", "tennis", "A small New Mexico town", "John Perry", "Norns", "steel", "Old Trafford", "Hermann Engels", "a rabbit", "The King and I", "Lisbeth Salander", "Standard Oil Company", "an attorney", "Janis Joplin", "1983 and 1984", "over 1.6 million", "Mark Neveldine and Brian Taylor", "Alina Cho", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves", "paddington bear", "paul mcc McCartney"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5551339285714285}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false], "QA-F1": [0.42857142857142855, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.33333333333333337, 0.5, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3667", "mrqa_squad-validation-2468", "mrqa_squad-validation-1402", "mrqa_squad-validation-780", "mrqa_squad-validation-5025", "mrqa_squad-validation-5669", "mrqa_squad-validation-3708", "mrqa_squad-validation-336", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-921", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-2372", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-3159", "mrqa_triviaqa-validation-4982", "mrqa_triviaqa-validation-4005", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-1461", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-7580", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9135", "mrqa_hotpotqa-validation-1526", "mrqa_newsqa-validation-2422", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-10099"], "SR": 0.453125, "CSR": 0.6448863636363636, "EFR": 0.9714285714285714, "Overall": 0.734981737012987}, {"timecode": 11, "before_eval_results": {"predictions": ["high than normal O2 exposure for a fee", "toward the center of the curving path", "polynomial", "1206", "Dutch East India Company", "Newton", "after the Franco-German War", "chloroplasts", "June 4, 2014", "energy", "December 2014", "18 February 1546", "19 of 28", "meritocracy", "photooxidative damage", "tangential force", "St John the Baptist", "more or less rounded, sometimes nearly spherical and other times more cylindrical or egg-shaped", "adenosine triphosphate", "Johann Eck", "egypt", "Civil War", "way can we get women to smoke on the street? They\u2019re smoking indoors", "marx", "egypt", "pheromones", "John Flamsteed", "bison", "Anita Roddick", "Clyde", "Tamar", "Nizhny Novgorod", "The Word", "Left Book Club", "Tchaikovsky", "Tony Blackburn", "egypt", "spa", "butterfly", "Tarzan", "James Hanratty", "Middlesbrough", "pyle", "London Pride", "pakland", "pinafore", "Milton Keynes", "November", "koftas", "Coventry to Leicester Motorway", "marx", "Little Arrows", "legion", "Saint Vitus", "Syriza", "one bridge has a longer span than another it does not necessarily mean that the bridge is longer from shore to shore", "in the idiom `` ( well ) I 'll be a monkey's uncle '', is used to express complete surprise, amazement or disbelief", "Channel 4", "Cartoon Network Too", "pattern matching", "to wear Islamic dress", "Mojave", "paddington Reservoir", "egypt"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5522772366522366}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.4, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.09523809523809523, 0.0909090909090909, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3610", "mrqa_squad-validation-8229", "mrqa_squad-validation-361", "mrqa_squad-validation-8931", "mrqa_squad-validation-6391", "mrqa_squad-validation-4469", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-3014", "mrqa_triviaqa-validation-828", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-3045", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-4400", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-6665", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-18", "mrqa_searchqa-validation-12699", "mrqa_searchqa-validation-5935", "mrqa_newsqa-validation-3918"], "SR": 0.484375, "CSR": 0.6315104166666667, "EFR": 1.0, "Overall": 0.7380208333333333}, {"timecode": 12, "before_eval_results": {"predictions": ["Marburg Colloquy", "a negative long-term impact", "with known magnitudes of force", "Southern California Megaregion, one of the 11 megaregions of the United States", "Joe Barton, chairman of the House Committee on Energy and Commerce wrote joint letters with Ed Whitfield,", "Basel", "Levi's Stadium", "Miasma theory", "a freshwater lake", "Monte Gargano", "50-yard line", "National Galleries of Scotland", "Arts & Entertainment Television (A&E)", "gravity", "Paul Samuelson, the first American to win the Nobel Memorial Prize in Economic Sciences,", "fans", "pathogen attack", "a not-for-profit United States computer networking consortium", "1850", "at CFB Trenton", "Spanish explorers", "eight hoops", "Columbia University", "Waylon Jennings", "orange", "Melissa Disney", "five", "1989", "Palmer Williams Jr. as Floyd   Patrice Lovely as Hattie   Tony Hightower as Frank   Alexis Jones as Diane   Maurice Lauchner as Lewis   Kislyck Halsey as Rose", "1987", "Tyler, Ali, and Lydia having fun at Tyler's little sister's birthday party", "May 18, 2018", "2015", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "$75,000", "272", "Hudson Bay", "an expression of unknown origin", "accomplish the objectives of the organization", "The name -- elevator pitch -- reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "1775", "badge limit remained in effect", "a normally inaccessible mini-game", "Charles Darwin", "insulated shipping containers", "dromedary", "1896", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Richard Carpenter", "Eliot", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Joe Spano", "1964 Republican National Convention in San Francisco", "north end of the contiguous beach stretch that starts from Sinquerim, Candolim, leads to Calangute and then to Baga", "Paul Lynde", "sweet and sour", "Ross Bagdasarian", "Gracie Mansion", "Peter Seamus O'Toole", "Arthur E. Morgan III, who is wanted for questioning in the death of a two-year-old girl", "Felipe Calderon", "Jeopardy!", "hartman", "Arkansas"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5855330433455433}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.8, 0.2857142857142857, 0.4615384615384615, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.33333333333333337, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.761904761904762, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4444444444444445, 1.0, 0.0, 0.09999999999999999, 1.0, 0.0, 0.0, 1.0, 0.8, 0.13333333333333333, 0.8, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5213", "mrqa_squad-validation-10321", "mrqa_squad-validation-2553", "mrqa_squad-validation-8620", "mrqa_squad-validation-4877", "mrqa_squad-validation-10352", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-627", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-1575", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-3299", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-3444", "mrqa_searchqa-validation-8089", "mrqa_searchqa-validation-15790"], "SR": 0.4375, "CSR": 0.6165865384615384, "retrieved_ids": ["mrqa_squad-train-18370", "mrqa_squad-train-12561", "mrqa_squad-train-82769", "mrqa_squad-train-27869", "mrqa_squad-train-72080", "mrqa_squad-train-77822", "mrqa_squad-train-25007", "mrqa_squad-train-56447", "mrqa_squad-train-1087", "mrqa_squad-train-17696", "mrqa_squad-train-59128", "mrqa_squad-train-33617", "mrqa_squad-train-18025", "mrqa_squad-train-39251", "mrqa_squad-train-23522", "mrqa_squad-train-24389", "mrqa_hotpotqa-validation-1767", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-7468", "mrqa_squad-validation-2632", "mrqa_squad-validation-4469", "mrqa_naturalquestions-validation-6573", "mrqa_squad-validation-6752", "mrqa_squad-validation-1891", "mrqa_triviaqa-validation-1461", "mrqa_hotpotqa-validation-5587", "mrqa_squad-validation-9436", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-5370", "mrqa_hotpotqa-validation-2925", "mrqa_triviaqa-validation-4445", "mrqa_hotpotqa-validation-4961"], "EFR": 1.0, "Overall": 0.7350360576923076}, {"timecode": 13, "before_eval_results": {"predictions": ["phlogiston theory of combustion and corrosion", "1974", "Andrew Alper", "shaping ideas about the free market", "learning", "The Prospect Studios", "Milton Friedman Institute", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "1972", "polytechnics became new universities", "conservation of momentum", "October 1973", "Frontex", "a dispute over control of the confluence of the Allegheny and Monongahela rivers", "720p high definition", "1924", "Sippin' on Some Syrup", "Apple Lisa", "Raden Panji Nugroho Notosusanto", "Howard Cosell", "October 21, 2016", "Donald Duck", "February 12, 2014", "Eureka Hall", "Aircraft  An aircraft", "University of Vienna", "March 21, 2004", "Golden Calf", "2012 Summer Olympics", "February 13, 1946", "Les Clark", "how well a watch is sealed against the ingress of water", "Nathan Rothschild", "The Grandmaster", "Montana State University", "37", "Shashi Kapoor", "beer", "Hank Azaria", "Supergirl", "aun of the Dead", "1816", "Mauritian", "mixed martial arts", "Cape Cod", "Mike Farrell", "Humberside Airport", "Henry Luce", "energy trading", "George I", "Melanie Owen", "Book of Judges", "12", "35", "the length of suspended roadway between the bridge's towers", "the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age", "HMS Thetis", "Vinegar Joe", "Ralph Lauren", "Lashkar-e-Jhangvi", "Detroit", "The Parent Trap", "Larry King", "terminal brain cancer"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5514520202020201}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7763", "mrqa_squad-validation-5889", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-3736", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-3057", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-405", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-2844", "mrqa_newsqa-validation-1095", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-15877", "mrqa_newsqa-validation-2128"], "SR": 0.515625, "CSR": 0.609375, "EFR": 0.967741935483871, "Overall": 0.7271421370967742}, {"timecode": 14, "before_eval_results": {"predictions": ["Chu'Tsai", "second-largest", "17", "Ed Mangan", "suspended", "send aid", "nearly two-thirds", "a dam turbine", "SAP Center in San Jose", "Niels Jerne", "Central Bridge", "Vistula River", "an enzyme called rubisco", "Spanish moss at a nearby mattress factory was quickly engulfed in flames", "Louis Pasteur", "\"The Tales of Hoffmann\"", "Lake Placid, New York", "four", "Elton John", "1978", "Victorian England", "Indian", "the R-8 Human Rhythm Composer", "1874", "Brian Yorkey", "Outside", "Forbes", "acidic", "Tampa", "Saoirse Ronan", "newspapers, television, radio, cable television, and other businesses", "University of Kansas", "Prescription Drug User Fee Act", "S\u00f8nderjyskE Ishockey", "Foxborough", "Cyclic Defrost", "Lancashire Combination side Stalybridge Celtic", "John Kevin Delaney", "\"General Hospital\"", "1955", "2016", "2 March 1972", "Londonderry", "11 November 1869", "Sam Bettley", "Larry Richard Drake", "Pim Fortuyn List", "Eisstadion Davos", "Sex Drive", "Toxics Release Inventory", "St Augustine's Abbey", "a fictional character", "Vince Vaughan and Paul Giamatti", "La Boito's \"Mefistofele\"", "its genome", "1998", "cholesterol", "Israel", "Juan Martin Del Potro", "Rev. Alberto Cutie", "the Hungarian artist", "the Lochner-.... son clerks reunion, hosted by new Chief Justice Rehnquist", "Ward Robe in Spare Oom", "a beloved horse named Snickers"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6748536186036187}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.7692307692307693, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.8, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-9695", "mrqa_squad-validation-457", "mrqa_squad-validation-7233", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-4780", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-866", "mrqa_hotpotqa-validation-4899", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-5714", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-16559", "mrqa_searchqa-validation-13152", "mrqa_searchqa-validation-6300"], "SR": 0.578125, "CSR": 0.6072916666666667, "EFR": 1.0, "Overall": 0.7331770833333333}, {"timecode": 15, "before_eval_results": {"predictions": ["11", "1968", "teacher who stays with them for most of the week and will teach them the whole curriculum", "Westinghouse Electric", "nine months", "more than 70 pioneers in science and engineering", "Inherited wealth", "Super Bowl XXXIII", "Alan Turing", "North America", "Conservative", "contrasts with the newer areas of tract homes", "Clair Cameron Patterson", "Esp\u00edrito Santo Financial Group", "Europe", "the Saddledome", "The String Cheese Incident, Ralph Stanley, Steve Martin & the Steep Canyon Rangers, and Rhonda Vincent", "S Pictures' \"Veyyil\" (2006)", "PewDie Pie", "Seoul, South Korea", "Pittsburgh", "2012", "Chinese Coffee", "John Gotti", "$7.3 billion", "a French natural philosopher, mathematician, physicist", "Umina Beach, New South Wales", "Port Macquarie", "\"Anomalisa\"", "Edinburgh", "Anthony Lynn", "Levi Weeks", "the Mayor of the City of New York", "soccer", "Memphis Minnie", "The Five", "James Mitchum", "Candice Susan Swanepoel", "Bhaktivedanta Manor", "Thomas Christopher Ince", "American college football", "U.S.", "south", "historic buildings, arts, and published works", "June 24, 1935", "James K. Polk", "March 31, 1944", "Linux Format", "Na Na", "Koch Industries", "Rymill Park", "Eliot Cutler", "Black Swan", "Thomas Jefferson", "Albert Brown", "Billy Idol", "Stanley Kubrick's Full Metal jacket", "Bombay Stock Exchange", "eight or nine", "Hussein's Revolutionary Command Council", "Mastino", "Manfred von Richthofen", "a farmer\u2019s crops", "objects appear to be abnormally colored or tinged with color"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7197950487012987}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.1, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.4, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1892", "mrqa_squad-validation-1640", "mrqa_squad-validation-4671", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-5125", "mrqa_hotpotqa-validation-1192", "mrqa_naturalquestions-validation-3641", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5715", "mrqa_searchqa-validation-8148", "mrqa_searchqa-validation-1728", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-581"], "SR": 0.640625, "CSR": 0.609375, "retrieved_ids": ["mrqa_squad-train-32874", "mrqa_squad-train-36979", "mrqa_squad-train-1903", "mrqa_squad-train-31155", "mrqa_squad-train-77208", "mrqa_squad-train-68275", "mrqa_squad-train-30600", "mrqa_squad-train-20535", "mrqa_squad-train-76475", "mrqa_squad-train-49561", "mrqa_squad-train-74803", "mrqa_squad-train-57012", "mrqa_squad-train-79091", "mrqa_squad-train-60046", "mrqa_squad-train-79326", "mrqa_squad-train-10063", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4596", "mrqa_squad-validation-6526", "mrqa_squad-validation-7447", "mrqa_searchqa-validation-6300", "mrqa_newsqa-validation-872", "mrqa_triviaqa-validation-828", "mrqa_triviaqa-validation-6089", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-812", "mrqa_squad-validation-2754", "mrqa_triviaqa-validation-4382", "mrqa_squad-validation-2468", "mrqa_squad-validation-8620", "mrqa_searchqa-validation-7219"], "EFR": 1.0, "Overall": 0.73359375}, {"timecode": 16, "before_eval_results": {"predictions": ["Aston Webb", "Jin", "in his lab", "1253", "the clinical pharmacy movement initially began inside hospitals and clinics", "RNA silencing", "50", "lectured on the Psalms", "Yes\u00fcgei", "a renewed version of the Russian imperialism and colonialism", "the American Philosophical Society", "Albert Einstein", "Bristol", "Javier Bardem", "fastest steam engine", "Ben Whishaw", "2", "law", "Richard Noble", "Skylab", "Philistine", "Spain", "milk", "law", "rue", "free booter", "law", "bats", "Styal", "The Three Pigs", "sergeant", "Margaret Beckett", "Brad Pitt", "a whitehead", "Canada", "The Curse of Scotland", "a marble campanile", "law", "Sesame Street", "Big Brother", "Avro Lancaster", "Leeds", "Jon Naismith", "Joseph Priestley", "white", "a new Eurasian union", "St Asaph", "Fernando Torres", "ADNAMS", "egypt", "aircraft carrier", "leicestershire", "Wyoming", "Socrates", "O'Meara", "3 total", "Paper Trail", "2013", "Mexico", "Newcastle retained fourth place", "Hugh Grant", "law", "Tony Manero", "1 draw"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6781250000000001}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6547", "mrqa_squad-validation-10128", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-5830", "mrqa_triviaqa-validation-5856", "mrqa_triviaqa-validation-5844", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-6246", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-7155", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3685", "mrqa_triviaqa-validation-5052", "mrqa_naturalquestions-validation-8087", "mrqa_newsqa-validation-2467", "mrqa_searchqa-validation-9683", "mrqa_hotpotqa-validation-3874"], "SR": 0.609375, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.73359375}, {"timecode": 17, "before_eval_results": {"predictions": ["pattern recognition receptors", "Pole Mokotowskie", "in the courtyard", "six", "1527", "Zhongdu", "British culture", "the Chicago Bears", "Bill Clinton", "differences in value added by labor, capital and land", "the geochemical evolution of rock units", "an assemblage", "an eagle", "red", "Pink Floyd", "Constantinople", "tendon", "the diamond", "Mesozoic", "riots", "ethanethiol", "coral", "forty-niners", "Clyde", "New Orleans", "a signal", "liver", "right-to-left", "nonfiction", "a Reformed theologian", "mask", "the money he receives becomes part of his cash", "Mediolanum", "the Diet of Worms", "rupture of membranes", "emperor", "white blood cells", "Green Lantern", "Zeus", "the New York Giants", "prince Bernhard", "Yves Saint Laurent", "the UAE", "Macy's", "Austria-Hungary", "Carmen", "choppers", "prince", "Bali", "a fruit", "the Crimean War", "oresteia", "pesto", "providing telecommunication services to enterprises and offices", "1834", "Pantagruel", "the Savoy", "Leonard Cohen", "S7", "American soldiers", "leaky valve", "Tommy James", "the tax rate paid by a small business", "The Inn at Newport Ranch"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5345982142857143}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.38095238095238093]}}, "before_error_ids": ["mrqa_squad-validation-9400", "mrqa_squad-validation-5429", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-6670", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-4065", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-4154", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-15504", "mrqa_searchqa-validation-14923", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-8417", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-11410", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-1387", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2978", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-2250"], "SR": 0.46875, "CSR": 0.6015625, "EFR": 0.9705882352941176, "Overall": 0.7261488970588236}, {"timecode": 18, "before_eval_results": {"predictions": ["Apollo Applications Program", "5.3%", "seizures", "Mercury", "a setup phase in each involved node", "Infrastructure", "MHC class I molecules", "the European Court of Human Rights", "it is neither zero nor a unit", "neuronal dendrites", "The Lost Symbol", "President Robert Mugabe", "November 26", "The Al Nisr Al Saudi", "two paintings", "Marcus Schrenker", "too many glass shards left by beer drinkers", "Chancellor Angela Merkel", "Tibet's independence", "drug trafficking", "The Delta Queen", "Osama bin Laden", "2002", "Yusuf Saad Kamel", "general secretary", "byproducts emitted during the process of burning and melting raw materials", "September", "Transport Workers Union", "introducing legislation Thursday to improve the military's suicide-prevention programs", "Eintracht Frankfurt", "Jacob", "5 1/2-year-old son, Ryder Russell", "canyon in the path of the blaze", "eco-horror scenarios", "Bob Dole", "Kenyan Defense Minister Yusuf Haji", "the new kid on the block in the modern art scene", "senators", "April", "Chesley \"Sully\" Sullenberger", "Africa's largest producer", "Thursday", "emergency aid", "A third beluga whale belonging to the world's largest aquarium has died", "Martin Aloysius Culhane", "Washington", "more effectively restore the vehicles to their pre-accident condition", "Dubai", "Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets", "the cancellation of more than 650 flights at London's Heathrow airport", "At least 38", "Juan Martin Del Potro", "the last single text", "E-2s and E-3s", "two easily observed features", "The Comedy of Errors", "xenophon", "devotional", "14th Street", "Lord Byron", "a pitch", "Clark Irwin", "a cape", "a hickey"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5934154665155535}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.8421052631578948, 0.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.5, 0.28571428571428575, 1.0, 0.8571428571428571, 0.875, 1.0, 1.0, 0.0, 0.8333333333333333, 0.0, 1.0, 0.25, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.07142857142857144, 1.0, 0.23529411764705882, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-961", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3365", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-1482", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1184", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2901", "mrqa_searchqa-validation-15244", "mrqa_searchqa-validation-9803", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-8188", "mrqa_searchqa-validation-15495"], "SR": 0.453125, "CSR": 0.59375, "retrieved_ids": ["mrqa_squad-train-33382", "mrqa_squad-train-75380", "mrqa_squad-train-63452", "mrqa_squad-train-83256", "mrqa_squad-train-26342", "mrqa_squad-train-1444", "mrqa_squad-train-50791", "mrqa_squad-train-27512", "mrqa_squad-train-49072", "mrqa_squad-train-46078", "mrqa_squad-train-48555", "mrqa_squad-train-30402", "mrqa_squad-train-8692", "mrqa_squad-train-83079", "mrqa_squad-train-8187", "mrqa_squad-train-41371", "mrqa_squad-validation-2473", "mrqa_hotpotqa-validation-3775", "mrqa_squad-validation-7233", "mrqa_searchqa-validation-8172", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1473", "mrqa_naturalquestions-validation-9419", "mrqa_squad-validation-3770", "mrqa_squad-validation-7246", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-4982", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4005", "mrqa_triviaqa-validation-6381", "mrqa_squad-validation-5889", "mrqa_triviaqa-validation-3159"], "EFR": 0.9714285714285714, "Overall": 0.7247544642857143}, {"timecode": 19, "before_eval_results": {"predictions": ["one of the most common forms of school discipline throughout much of the world.", "April 1, 1963", "Islamism", "he did not want disloyal men in his army.", "conservation of momentum", "farther west, through the Waal and then, via the Merwede and Nieuwe merwede (De Biesbosch)", "154", "17 February 1546", "1996", "1960", "Friday", "Anil Kapoor", "2005", "2008", "Ben Roethlisberger", "President Obama", "Matthew Chance", "11th year in a row", "suspended a student who admitted to hanging a noose in a campus library,", "\"The e-mails] are almost like reading a novel that you would embarrassed to buy,\"", "a Ukrainian -- is accused of involvement during World War II in killings at a Nazi German death camp in Poland.", "Lashkar-e-Tayyiba (LeT), an Islamic militant group based in Pakistan.", "an Omani national who has close ties to Mugabe and his top officials, to \"enable Mugabe... to maintain access to, and derive personal benefit from, various mining ventures in the Democratic Republic of the Congo.", "Steven Chu", "Wednesday at the age of 95", "\"a whole new treasure collection of fossils\"", "island stronghold of the Islamic militant group Abu Sayyaf", "\"It seemed to be kind of laid-back -- it didn't seem to be that dangerous,\"", "2,000 euros ($2,963)", "buckling under pressure from the ruling party.", "Russian air company Vertikal-T,", "The Everglades, known as the River of Grass,", "a striking blow to due process and the rule of law", "federal officers' bodies", "Long troop deployments in Iraq, above, and Afghanistan have been cited in the rise in military suicides.", "in a ravine a week after losing control of his car on a rural road and plunging 500 feet down an embankment into heavy brush", "his mother", "South African police have opened a criminal investigation into allegations that a dorm parent mistreated students at the school.", "hundreds", "girls", "blind Majid Movahedi, the man who blinded her.", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "was targeting the latest effort by the government to foster national reconciliation between religious and ethnic groups.", "Amy Winehouse", "2007", "\"Wolfman,\" starring Benicio del Toro, grossed an estimated $30.6 million", "Vernon Forrest", "The man ran out of bullets and blew himself up.", "Jason Chaffetz", "St. Louis, Missouri", "Alicia Keys", "Oxbow", "\"Most of those who managed to survive the incident hid in a boiler room and storage closets during the massacre.", "1038", "Johannes Gutenberg", "marriage that includes more than two partners", "a crystal ball", "Lithuania", "two years", "Mark Hillman", "Craig Ferguson", "spectacled bear", "chili", "Constellation family"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5446103896103897}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.18181818181818182, 0.0, 0.1818181818181818, 0.0, 0.0, 1.0, 0.33333333333333337, 0.8333333333333334, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.47619047619047616, 0.32, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.19999999999999998, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2085", "mrqa_squad-validation-9228", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-2000", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-4927", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-9391"], "SR": 0.46875, "CSR": 0.5875, "EFR": 1.0, "Overall": 0.72921875}, {"timecode": 20, "UKR": 0.802734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1334", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1484", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1696", "mrqa_hotpotqa-validation-1750", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1840", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1912", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2456", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2574", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3057", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4780", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5838", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-748", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-4876", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9545", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3297", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3365", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-872", "mrqa_searchqa-validation-11130", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-14923", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-16559", "mrqa_searchqa-validation-1926", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5531", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-7219", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-9391", "mrqa_searchqa-validation-9683", "mrqa_squad-validation-100", "mrqa_squad-validation-10001", "mrqa_squad-validation-10059", "mrqa_squad-validation-10063", "mrqa_squad-validation-1008", "mrqa_squad-validation-10186", "mrqa_squad-validation-10256", "mrqa_squad-validation-10273", "mrqa_squad-validation-10316", "mrqa_squad-validation-10346", "mrqa_squad-validation-10352", "mrqa_squad-validation-10370", "mrqa_squad-validation-10386", "mrqa_squad-validation-10399", "mrqa_squad-validation-1042", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-10475", "mrqa_squad-validation-10484", "mrqa_squad-validation-1078", "mrqa_squad-validation-1177", "mrqa_squad-validation-12", "mrqa_squad-validation-1207", "mrqa_squad-validation-1219", "mrqa_squad-validation-1254", "mrqa_squad-validation-1263", "mrqa_squad-validation-133", "mrqa_squad-validation-1330", "mrqa_squad-validation-134", "mrqa_squad-validation-1402", "mrqa_squad-validation-1432", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1565", "mrqa_squad-validation-1612", "mrqa_squad-validation-1640", "mrqa_squad-validation-168", "mrqa_squad-validation-1764", "mrqa_squad-validation-1813", "mrqa_squad-validation-185", "mrqa_squad-validation-185", "mrqa_squad-validation-1891", "mrqa_squad-validation-1980", "mrqa_squad-validation-2018", "mrqa_squad-validation-2085", "mrqa_squad-validation-2085", "mrqa_squad-validation-2109", "mrqa_squad-validation-215", "mrqa_squad-validation-2167", "mrqa_squad-validation-2214", "mrqa_squad-validation-2314", "mrqa_squad-validation-2370", "mrqa_squad-validation-246", "mrqa_squad-validation-2500", "mrqa_squad-validation-2559", "mrqa_squad-validation-2586", "mrqa_squad-validation-2666", "mrqa_squad-validation-2672", "mrqa_squad-validation-269", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-2754", "mrqa_squad-validation-2853", "mrqa_squad-validation-2913", "mrqa_squad-validation-2939", "mrqa_squad-validation-2964", "mrqa_squad-validation-2975", "mrqa_squad-validation-3096", "mrqa_squad-validation-3124", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3140", "mrqa_squad-validation-3143", "mrqa_squad-validation-3275", "mrqa_squad-validation-3319", "mrqa_squad-validation-336", "mrqa_squad-validation-3370", "mrqa_squad-validation-3407", "mrqa_squad-validation-3492", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-350", "mrqa_squad-validation-3535", "mrqa_squad-validation-3550", "mrqa_squad-validation-36", "mrqa_squad-validation-3667", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3802", "mrqa_squad-validation-3811", "mrqa_squad-validation-3813", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3904", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-4227", "mrqa_squad-validation-4246", "mrqa_squad-validation-4469", "mrqa_squad-validation-4473", "mrqa_squad-validation-4546", "mrqa_squad-validation-4591", "mrqa_squad-validation-4636", "mrqa_squad-validation-4669", "mrqa_squad-validation-468", "mrqa_squad-validation-4737", "mrqa_squad-validation-4754", "mrqa_squad-validation-4799", "mrqa_squad-validation-4860", "mrqa_squad-validation-4877", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5007", "mrqa_squad-validation-5019", "mrqa_squad-validation-5135", "mrqa_squad-validation-5294", "mrqa_squad-validation-5325", "mrqa_squad-validation-5351", "mrqa_squad-validation-5613", "mrqa_squad-validation-5699", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5716", "mrqa_squad-validation-5721", "mrqa_squad-validation-5724", "mrqa_squad-validation-5797", "mrqa_squad-validation-5875", "mrqa_squad-validation-5933", "mrqa_squad-validation-5961", "mrqa_squad-validation-6025", "mrqa_squad-validation-6089", "mrqa_squad-validation-6116", "mrqa_squad-validation-6157", "mrqa_squad-validation-6159", "mrqa_squad-validation-6163", "mrqa_squad-validation-6341", "mrqa_squad-validation-635", "mrqa_squad-validation-6502", "mrqa_squad-validation-6526", "mrqa_squad-validation-6579", "mrqa_squad-validation-6614", "mrqa_squad-validation-6628", "mrqa_squad-validation-6669", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6705", "mrqa_squad-validation-6803", "mrqa_squad-validation-6873", "mrqa_squad-validation-6986", "mrqa_squad-validation-7036", "mrqa_squad-validation-7168", "mrqa_squad-validation-719", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7277", "mrqa_squad-validation-7294", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7384", "mrqa_squad-validation-7428", "mrqa_squad-validation-7456", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7585", "mrqa_squad-validation-76", "mrqa_squad-validation-7652", "mrqa_squad-validation-7671", "mrqa_squad-validation-7689", "mrqa_squad-validation-7707", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-780", "mrqa_squad-validation-7805", "mrqa_squad-validation-7838", "mrqa_squad-validation-7909", "mrqa_squad-validation-791", "mrqa_squad-validation-8028", "mrqa_squad-validation-8043", "mrqa_squad-validation-8045", "mrqa_squad-validation-8073", "mrqa_squad-validation-8283", "mrqa_squad-validation-8386", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8529", "mrqa_squad-validation-8555", "mrqa_squad-validation-8603", "mrqa_squad-validation-8680", "mrqa_squad-validation-8754", "mrqa_squad-validation-8830", "mrqa_squad-validation-8834", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8905", "mrqa_squad-validation-891", "mrqa_squad-validation-8927", "mrqa_squad-validation-8931", "mrqa_squad-validation-8939", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-8987", "mrqa_squad-validation-9200", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9302", "mrqa_squad-validation-9322", "mrqa_squad-validation-9328", "mrqa_squad-validation-9436", "mrqa_squad-validation-95", "mrqa_squad-validation-9531", "mrqa_squad-validation-9532", "mrqa_squad-validation-9543", "mrqa_squad-validation-9695", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_squad-validation-9931", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-1873", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3278", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4850", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5025", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5960", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6246", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-828"], "OKR": 0.904296875, "KG": 0.49765625, "before_eval_results": {"predictions": ["2002", "1,548", "American Institute of Electrical Engineers", "a deficit", "Apollo 20", "Virgin Media", "in a number of stages", "light energy", "128", "voting directly or elect representatives from among themselves to form a governing body,", "Mary Ellen Mark", "Australian Defence Force", "Moon shot", "1296", "1970", "Vilnius", "Rounders", "music of pre-Hispanic and contemporary music of the Andes", "Robert John Day", "Rio Gavin Ferdinand", "National Lottery", "Battle of Prome", "M2M", "Austria", "Citizens for a Sound Economy", "right-hand", "Heathrow", "Australian", "Darkroom", "House of Commons", "La Familia Michoacana", "five", "1983", "James Douglas Packer", "Northern Ireland", "Erich Maria Remarque", "Best Musical", "Floyd Mutrux and Colin Escott", "Frederick Dewey Smith", "Mikoyan design bureau", "American Indian", "A Little Princess", "1943", "TD Garden", "Voyager 2 spacecraft in January 1986.", "Vixen", "4", "Uchinaanchu", "Chicago", "J35", "Axl Rose", "Prudential Center", "Homer Hickam, Jr.", "CBS All Access", "Diary of a Wimpy Kid : The Long Haul", "dal\u00b7ton\u00b7ism (dawl\u0491t\u0259n-iz-\u0259m)", "July 28, 1948.", "Department of Homeland Security Secretary Janet Napolitano", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "Beatles", "Marmaduke", "Operation Cast Lead", "future relations with Washington", "650"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6327473958333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.2, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.5, 0.8, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-5692", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1812", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-2486", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-1867", "mrqa_naturalquestions-validation-6298", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-3423", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-8155", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-571"], "SR": 0.53125, "CSR": 0.5848214285714286, "EFR": 0.9666666666666667, "Overall": 0.751235119047619}, {"timecode": 21, "before_eval_results": {"predictions": ["1622", "wars", "all age groups", "Sudden cabin pressure loss", "largest gold rushes the world has ever seen", "1985", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "Suleiman the Magnificent", "cruiserweight", "A Hard Day's Night", "Lord Chancellor of England", "Ben Elton", "West Cheshire Association Football League", "KB", "1,467 rooms", "pioneering New Zealand food writer", "Old Executive Office Building", "\"Bad Reputation\"", "John R. Leonetti", "The Legend of Sleepy Hollow", "Martin Amis", "Harmony Korine", "137th", "Texas Tech University", "Croatatan, Nantahala, and Uwharrie", "703 rooms", "King's College London", "Rochdale", "most influenced by Tudor music and English folk-song", "Northern Ireland", "from \u03b2\u03ad\u03bd\u03b8\u03bf\u03c2 \"(sea) depth\" and \u03ba\u1fe6\u03bc\u03b1 \"wave\")", "Larry Gatlin & the Gatlin Brothers Band", "the Beatles", "Lady Frederick Windsor", "Monica Seles", "Umberto I", "American record for the most time in space (381.6 days)", "NYPD's 83rd Precinct", "Patterns of Sexual Behavior", "Peel Holdings", "560", "wrestler, actor, and hip hop musician", "Theodor W. Adorno", "Gianna (also \"La Staffetta Gianna\")", "City of Newcastle", "Jim Kelly", "gender queer", "Kramer", "Rod Smith (politician)", "Minette Walters", "2004", "Tamil", "67,038", "73", "Barbara Windsor", "Stephenie Meyer", "Jane Austen", "Pakistan's intelligence agency", "Michelle Rounds", "A Doll's House", "Florida", "horse", "auction houses with watch departments and specialist watch-only auctioneers", "Mickey Mouse"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6409515831390831}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.75, 0.6666666666666666, 0.0, 1.0, 0.7692307692307693, 1.0, 0.2, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3601", "mrqa_squad-validation-2987", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-334", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-984", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-3707", "mrqa_hotpotqa-validation-5052", "mrqa_naturalquestions-validation-2159", "mrqa_triviaqa-validation-6256", "mrqa_newsqa-validation-1218", "mrqa_triviaqa-validation-2227", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-7724"], "SR": 0.53125, "CSR": 0.5823863636363636, "retrieved_ids": ["mrqa_squad-train-76981", "mrqa_squad-train-51207", "mrqa_squad-train-48780", "mrqa_squad-train-80164", "mrqa_squad-train-27892", "mrqa_squad-train-23838", "mrqa_squad-train-68114", "mrqa_squad-train-58434", "mrqa_squad-train-76115", "mrqa_squad-train-13700", "mrqa_squad-train-57860", "mrqa_squad-train-79044", "mrqa_squad-train-61275", "mrqa_squad-train-27206", "mrqa_squad-train-66771", "mrqa_squad-train-74621", "mrqa_triviaqa-validation-4982", "mrqa_triviaqa-validation-1808", "mrqa_searchqa-validation-1728", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-4435", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-3523", "mrqa_naturalquestions-validation-5451", "mrqa_squad-validation-100", "mrqa_searchqa-validation-11410", "mrqa_naturalquestions-validation-661", "mrqa_squad-validation-8927", "mrqa_hotpotqa-validation-5203", "mrqa_triviaqa-validation-828", "mrqa_searchqa-validation-10395", "mrqa_squad-validation-3497"], "EFR": 0.9666666666666667, "Overall": 0.750748106060606}, {"timecode": 22, "before_eval_results": {"predictions": ["$960 billion", "European Court of Human Rights", "kidney and bladder stones, and arthritis", "NP", "the Augustinian friars", "1948", "New Orleans", "mafia clan", "Charles Dickens", "Vaccine", "1984", "ravens", "Alfred Gilbert", "insulin", "bullfight", "17 pink", "12", "The Pennine Way", "Muriel Spark", "basil", "La Mancha", "Martin Van Buren", "Bonnie and Clyde", "three other musicians", "Hillary Clinton", "Gettysburg", "Tom Hanks", "a problem", "sound and light", "Panama", "mushrooms", "Harrods", "Usain Bolt", "Mead", "In her graceful, understated style", "Solo", "Sudan", "inanimate object or abstraction", "Russia", "Amy", "Steve Jobs", "Christmas", "Rajasthan", "in love with a young man called Jenik", "lawn games", "David Hockney", "bone", "Barnaby Rudge", "Surficial", "1861", "Thomas Jefferson", "Ceredigion", "10 : 30am", "2014 Winter Olympics in Sochi, Russia", "Seattle, Washington", "Capture of the Five Boroughs", "79 AD", "Wilmette", "Trevor Rees,", "last week", "Robert Park", "Sounder", "Syracuse", "4"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5738553113553113}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.2857142857142857, 0.3076923076923077, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3577", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2366", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-6681", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-4040", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6192", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-6458", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-7443", "mrqa_hotpotqa-validation-1874", "mrqa_newsqa-validation-2959", "mrqa_searchqa-validation-14932", "mrqa_searchqa-validation-2394"], "SR": 0.53125, "CSR": 0.5801630434782609, "EFR": 0.9666666666666667, "Overall": 0.7503034420289855}, {"timecode": 23, "before_eval_results": {"predictions": ["1759-60", "tyrosinase", "Keraites", "Israelis", "Konstantin Mereschkowski", "disease", "gounod and Reyer", "Vienna", "March 10, 1997", "pouched", "meat", "spain", "cop detective drama", "1985", "whitsun", "John Steinbeck", "drew Carey", "beta", "poland", "whitsun", "luthier", "Cheshire", "Rebecca Adlington", "silks", "power station", "Notts County", "the Tigris", "John Fitzgerald Kennedy", "just off the side of Saint", "cows", "poland", "islands", "Hydra agent", "whitsun", "Frank Harris", "Charles Atlas", "poland", "harrow", "Isle of Wight", "violin", "Roberto Cammarelle", "elephant", "whitsun", "whitsun", "poland", "restless leg", "spleen", "Olympics", "whitsun", "poland", "spain", "pangaea", "Pebe Sebert and Hugh Moffatt", "October 29, 2015", "in the season - five premiere episode `` Second Opinion ''", "Leslie Knope", "First Balkan War", "copenhagen", "next year", "heavy turbulence", "workers went on strike early Tuesday in Philadelphia,", "poland", "Senator Schumer", "the mouth"], "metric_results": {"EM": 0.40625, "QA-F1": 0.48004807692307694}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.923076923076923, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8488", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5563", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-7333", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-6080", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-6876", "mrqa_naturalquestions-validation-3440", "mrqa_naturalquestions-validation-2818", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-5271", "mrqa_newsqa-validation-1893", "mrqa_searchqa-validation-7791", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-7996"], "SR": 0.40625, "CSR": 0.5729166666666667, "EFR": 0.9736842105263158, "Overall": 0.7502576754385964}, {"timecode": 24, "before_eval_results": {"predictions": ["comb jelly", "grubs", "gaseous oxygen", "There is no known case of any U.S. citizens buying Canadian drugs for personal use with a prescription", "9th century", "Teha'amana", "Gerald R. Ford", "buegel", "lyndon johnson", "I Don't Want to Miss A Thing", "Sunday, November 6", "3 characters", "saddle oxfords", "NAFTA", "earth", "lyndon", "le Duc Tho", "venial sin", "Toronto", "Irish", "wolverine Gulo gulo", "fish", "ricky martin", "Alfred Nobel", "Smith & Wesson", "600 nm", "Emma Watson", "Pan Am", "Cardinal Richelieu", "Robert E. Lee", "hip roof", "OK", "Nikita Khrushchev", "koolsla", "an American lawyer and author", "silicon carbide", "Sonny Corleone", "morphine", "Mars", "ice age", "one hand", "lyndon johnson", "farr's Fort", "an American Tail", "pelican", "Specialist", "Mt. Kenya", "george johnson", "a Canadian", "king david", "Death Valley", "johnson depp", "Wembley Stadium", "Mike Higham, who had previously worked with Sondheim on Sweeney Todd : The Demon Barber of Fleet Street", "MGM Resorts International", "high jump", "bats", "halide", "Ferdinand Magellan", "November 23, 2011", "Famous Ghost Stories", "a man who said he had found it in the desert five months before.", "haitians", "leaders of more than 30 Latin American and Caribbean nations"], "metric_results": {"EM": 0.296875, "QA-F1": 0.3918678977272727}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.45454545454545453, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-9119", "mrqa_squad-validation-6383", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1375", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-16821", "mrqa_searchqa-validation-12291", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-11213", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-1004", "mrqa_searchqa-validation-16836", "mrqa_searchqa-validation-14524", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-7094", "mrqa_searchqa-validation-9261", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-7326", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-8944", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-263", "mrqa_searchqa-validation-6770", "mrqa_searchqa-validation-3135", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-10911", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6439", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-4468", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2225"], "SR": 0.296875, "CSR": 0.561875, "retrieved_ids": ["mrqa_squad-train-14090", "mrqa_squad-train-38785", "mrqa_squad-train-11837", "mrqa_squad-train-33691", "mrqa_squad-train-82910", "mrqa_squad-train-85883", "mrqa_squad-train-79163", "mrqa_squad-train-41256", "mrqa_squad-train-80506", "mrqa_squad-train-36416", "mrqa_squad-train-33460", "mrqa_squad-train-13145", "mrqa_squad-train-15671", "mrqa_squad-train-25985", "mrqa_squad-train-11434", "mrqa_squad-train-31189", "mrqa_squad-validation-10386", "mrqa_hotpotqa-validation-4855", "mrqa_squad-validation-73", "mrqa_triviaqa-validation-5714", "mrqa_searchqa-validation-12699", "mrqa_squad-validation-8927", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-1526", "mrqa_squad-validation-457", "mrqa_searchqa-validation-15504", "mrqa_squad-validation-3708", "mrqa_newsqa-validation-3164", "mrqa_triviaqa-validation-6514", "mrqa_naturalquestions-validation-10615", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-1872"], "EFR": 1.0, "Overall": 0.7533125}, {"timecode": 25, "before_eval_results": {"predictions": ["Khitan rulers", "over 100%", "vaccination", "natural grass", "three", "Kgalema Motlanthe", "\"The idea that this was a fly-by-night [is]... not true.", "a cancerous tumor", "a bag", "\"This is robbery. he went to rob the people. He went to steal money -- American dollars,\"", "gun charges", "the area where the single-engine Cessna 206 went down", "sought Cain's help finding a job", "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "dead", "three different videos", "September 21", "stand down", "hot and humid", "KBR", "Mother Nature", "\"Twilight\"", "private client", "Venus Williams", "Booches Billiard Hall", "northwest Pakistan", "23-year-old", "three", "creating and distributing affordable, durable and solar-powered laptops to the world's poorest children", "\"There is no exclusively military solution to the issues we and our partners confront in Afghanistan.\"", "Sovereign Wealth Funds", "Eric Besson", "Department of Homeland Security Secretary Janet Napolitano", "prison inmates.", "\"Larry King Live\"", "death squad killings", "hours", "a full garden and pool, a tennis court, or several heli-pads", "Bill", "Terry Marcus", "Caylee Anthony", "completely changed the business of music", "Raiders of the Lost Ark", "45 minutes, five days a week.", "to secure more funds", "upper respiratory infection", "\"It hurts my heart to see him in pain,", "Brown-Waite", "Mao Zedong", "he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted.", "the Italian Serie A title", "in his favor", "In the year 2026", "Mitch Murray, who offered it to Adam Faith and Brian Poole", "Iron River Ranch", "Captain Mark Phillips", "Naomi Watts", "Bruce Alexander", "vice president", "Pittsburgh Steelers", "the attack on Pearl Harbor", "Star Trek", "Chris Matthews", "Monticello"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5985466403722983}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 0.9600000000000001, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14814814814814814, 0.09523809523809522, 1.0, 0.6666666666666666, 0.4444444444444445, 0.6666666666666666, 0.0, 0.42857142857142855, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.4736842105263158, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-1137", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-7896", "mrqa_triviaqa-validation-1508", "mrqa_hotpotqa-validation-350", "mrqa_searchqa-validation-5521"], "SR": 0.453125, "CSR": 0.5576923076923077, "EFR": 1.0, "Overall": 0.7524759615384615}, {"timecode": 26, "before_eval_results": {"predictions": ["David G. Booth", "pr\u00e9tendus r\u00e9form\u00e9s", "the true Islamic system", "$20,000", "intention to set up headquarters in Dublin.", "is requiring the label warnings and a medication guide for fluoroquinolone drugs", "Itawamba County School District", "Jada", "Elisabeth,", "Tuesday", "it is provocative action,\"", "waterboarding at least 266 times on two top al Qaeda suspects,", "Ricardo Urbina", "\"bad apples\"", "Christopher Savoie", "near Garacad, Somalia,", "people thought this was a small problem,\"", "either ignore signs of depression or lie about their use of medication for fear of losing their licenses to fly", "How I Met Your Mother", "preserved corpses having sex", "teenager", "inmates", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "U.S. State Department and British Foreign Office", "the two remaining crew members", "at a Little Rock military recruiting center", "up to $50,000", "Arthur E. Morgan III,", "Yusuf Saad Kamel", "Tom Hanks", "baseball bat", "a one-of-a-kind navy dress with red lining", "$60 million", "test-launched a rocket capable of carrying a satellite", "three", "Governor Sanford", "gun", "$1.5 million", "off the coast of Dubai", "Davidson college students", "Diego Milito", "upper respiratory infection", "officers", "McDonald's", "collapsed apartment building", "the Richmond students did nothing because of the \"bystander effect\"", "leftist Workers' Party", "protest child trafficking and shout anti-French slogans", "full garden and pool, a tennis court,", "October 3,", "Lou and Wilson", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "the head of the Imperial Family", "the 2015 season", "November 2016", "sewing machines", "exploits on the Island", "King Macbeth of Scotland,", "half a million acres", "hiphop", "four", "Stanford", "a rough", "a synthetic chemical element"], "metric_results": {"EM": 0.34375, "QA-F1": 0.48724579818329816}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.07407407407407408, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.42857142857142855, 0.4, 0.5, 0.18181818181818182, 0.8, 0.6666666666666666, 0.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-3307", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-9119", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5002", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-10210"], "SR": 0.34375, "CSR": 0.5497685185185186, "EFR": 1.0, "Overall": 0.7508912037037037}, {"timecode": 27, "before_eval_results": {"predictions": ["theoretical", "its own culture and atmosphere", "Labor", "Children of Earth", "a broken pelvis", "\"Airing too much of your dirty laundry to them will make you look like a psycho.", "28", "one count of attempted murder in the second degree in the October 12 attack.", "Sri Lanka", "Muslim", "an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "200", "July", "innovative, exciting skyscrapers", "pro-democracy activists clashed Friday with Egyptian security forces.", "1519", "Glasgow, Scotland", "10 to 15 percent", "Kearny, New Jersey", "19-12", "four life jackets", "a number of calls,", "Mark Obama Ndesandjo", "New York City Mayor Michael Bloomberg", "The great paddlewheel turned the Ohio River water to a froth", "At least 15 people", "opposition parties", "July 23.", "2004", "citizenship", "\"The most affecting thing about this whole wheelchair for children is when the parents realize the gift that is being given to their children and they reach out to hug you.\"", "Gustav's top winds weakened to 110 mph,", "National September 11 Memorial Museum", "curfew", "Osama bin Laden's sons", "Tim Masters", "surgical anesthetic propofol", "the Iranian consulate,", "Steve Jobs", "The Da Vinci Code", "martial arts", "Bahrami's", "producing rock music with a country influence.", "\"The American president toured a mosque, laid a wreath at the grave of the founder of the Turkish republic,", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "$8.8 million", "near his home in Peshawar", "the refusal or inability to \"turn it off\"", "fighting charges of Nazi war crimes for well over two decades.", "Minerals Management Service Director Elizabeth Birnbaum", "20%", "U.S.-flagged Maersk Alabama", "`` planted '' into the bracket in a manner that is typically intended so that the best don't meet until later in the competition", "1994", "all cases affecting ambassadors, other public ministers and consuls, and those in which a state shall be party", "shorthand", "Carmen Miranda", "Lesley Garrett", "Denmark", "the tissues of the outer third of the vagina", "Mark Neveldine and Brian Taylor.", "Ulysses S. Grant", "Kevin Bacon", "yellow"], "metric_results": {"EM": 0.421875, "QA-F1": 0.549760843731432}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.16666666666666669, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.08, 0.0, 1.0, 1.0, 1.0, 0.7058823529411764, 0.5, 0.5, 0.0, 0.0392156862745098, 0.5, 0.9714285714285714, 1.0, 1.0, 1.0, 0.5, 0.1818181818181818, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1704", "mrqa_squad-validation-2640", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-1028", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-8092", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-4194", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420"], "SR": 0.421875, "CSR": 0.5452008928571428, "retrieved_ids": ["mrqa_squad-train-56913", "mrqa_squad-train-50183", "mrqa_squad-train-71143", "mrqa_squad-train-5903", "mrqa_squad-train-46518", "mrqa_squad-train-36595", "mrqa_squad-train-61078", "mrqa_squad-train-24466", "mrqa_squad-train-6976", "mrqa_squad-train-3853", "mrqa_squad-train-30347", "mrqa_squad-train-10691", "mrqa_squad-train-83777", "mrqa_squad-train-1155", "mrqa_squad-train-338", "mrqa_squad-train-54941", "mrqa_newsqa-validation-2586", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-7614", "mrqa_searchqa-validation-9828", "mrqa_newsqa-validation-1030", "mrqa_naturalquestions-validation-627", "mrqa_newsqa-validation-2791", "mrqa_searchqa-validation-4977", "mrqa_naturalquestions-validation-4247", "mrqa_hotpotqa-validation-412", "mrqa_searchqa-validation-10210", "mrqa_newsqa-validation-4054", "mrqa_hotpotqa-validation-1991", "mrqa_squad-validation-8620", "mrqa_triviaqa-validation-2372", "mrqa_triviaqa-validation-4376"], "EFR": 0.972972972972973, "Overall": 0.7445722731660231}, {"timecode": 28, "before_eval_results": {"predictions": ["Ex post facto laws,", "temperature and light", "He could feel a sharp stinging pain where it entered his body, and again at the place where it passed out.\"", "He won it with a clear strategy that was stuck to with remarkably little internal drama.", "Hillary Clinton", "volatile updrafts and downdrafts", "an older generation", "Ken Plunkett,", "800,000", "Monday and Tuesday", "a full garden and pool, a tennis court, or several heli-pads.", "the couple's surrogate", "immediately releasing all civilians and laying down arms,\"", "two-hour finale.", "describing her as \"wacko.\"", "prisoners", "Ashley \"A.J.\" Jewell,", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "July", "Iran of trying to build nuclear bombs,", "The U.S. State Department and British Foreign Office", "former U.S. secretary of state", "misdemeanor assault charges", "The Tupolev Tu-160 strategic bombers", "Argentine", "your ex's loved ones ask why", "Piers Morgan Tonight", "$50,000", "Leo Frank", "Osama's son,", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "U.S.", "New York City", "children's hospital in St. Louis, Missouri.", "David Design", "The comprehensive response has extended the lives of tens of thousands of Brazilians and saved the government billions,", "Pope Benedict XVI", "\"The National Telecommunications and Information Administration offered a program to help people buy converter boxes that make old TVs work in the new era.", "three searches", "Basel", "45", "first grand Slam", "suppress the memories and to live as normal a life as possible;", "$1.4 million", "Jennifer Arnold and husband Bill Klein,", "murder", "role as a bride in the 2007 movie \"License to Wed\"", "no reason", "2009", "Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "Burrell Edward Mohler Sr.", "Robert Barnett", "Javier Fern\u00e1ndez", "7.6 mm", "Scopes Trial in the United States", "Tinie Tempah", "Petula Clark", "Lord Marmaduke", "Roman Kostomarov", "August 1973", "ten", "The Stranger", "The Los Angeles Times", "George Orwell"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5358529011593104}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 0.7333333333333334, 0.5, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5333333333333333, 0.0, 0.20689655172413796, 0.0, 0.08695652173913042, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.923076923076923, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4096", "mrqa_squad-validation-1389", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2291", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1663", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2030", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-6701", "mrqa_searchqa-validation-1869"], "SR": 0.4375, "CSR": 0.5414870689655172, "EFR": 1.0, "Overall": 0.7492349137931035}, {"timecode": 29, "before_eval_results": {"predictions": ["fully consistent with the conceptual definition of force offered by Newtonian mechanics.", "peer tuitions", "six", "to drive from Texas to Africa", "Tennyson", "1028", "Francis Drake", "COLEGE COURSES", "(Candice Bergen)", "aluminum", "a chasm", "Soviet", "a greater kudu", "Brazil", "Idi Amin", "(C) 2007", "pink", "Tom", "anemia", "Frank Sinatra", "fishes", "Peter", "Vienna", "the Russian fleet", "Schmitt", "Nanosecond", "Citizen Kane", "\"The Stag\"", "a jolly Roger", "a bear", "Shirley Jackson", "JetBlue", "a bret", "a shot glass", "Peru", "a ghost", "an eye", "(1618-48)", "Hawaii", "Bob Newhart", "shorthand", "cologne", "burdock", "Utah", "a", "bustopher", "Sam Shepard", "petroleum", "August Wilson", "Dr. Jack Shephard, Kate Austen, Sayid Jarrah, Hugo \" Hurley\"", "a bay leaf", "the Blue Nile", "China ( formerly the Republic of China ), Russia (formerly the Soviet Union ), France, the United Kingdom, and the United States", "Roman Reigns", "Mexican Seismic Alert System", "violin", "(Caballero)", "Simeon Williamson", "11 November 1869", "Ten Walls", "Nia Kay", "new kidney", "surgical anesthetic propofol", "to advance the narrative that Arizona has no choice but to do its own immigration enforcement"], "metric_results": {"EM": 0.484375, "QA-F1": 0.522172619047619}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10284", "mrqa_searchqa-validation-13463", "mrqa_searchqa-validation-1839", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-14158", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14060", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-12860", "mrqa_searchqa-validation-9533", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-1595", "mrqa_searchqa-validation-11292", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-16060", "mrqa_naturalquestions-validation-975", "mrqa_triviaqa-validation-2394", "mrqa_triviaqa-validation-6791", "mrqa_hotpotqa-validation-4819", "mrqa_newsqa-validation-1445"], "SR": 0.484375, "CSR": 0.5395833333333333, "EFR": 1.0, "Overall": 0.7488541666666666}, {"timecode": 30, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1484", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-1750", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1912", "mrqa_hotpotqa-validation-1914", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2281", "mrqa_hotpotqa-validation-2310", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2440", "mrqa_hotpotqa-validation-2456", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-3310", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-5002", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-748", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-918", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4876", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-5390", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-975", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3154", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-86", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-12242", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-1265", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12897", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14932", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15244", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1595", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1696", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1773", "mrqa_searchqa-validation-1926", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5478", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6770", "mrqa_searchqa-validation-6958", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8791", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-9391", "mrqa_searchqa-validation-9683", "mrqa_squad-validation-100", "mrqa_squad-validation-10001", "mrqa_squad-validation-1002", "mrqa_squad-validation-10063", "mrqa_squad-validation-10174", "mrqa_squad-validation-10186", "mrqa_squad-validation-10256", "mrqa_squad-validation-10316", "mrqa_squad-validation-10370", "mrqa_squad-validation-10386", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-1078", "mrqa_squad-validation-1177", "mrqa_squad-validation-1219", "mrqa_squad-validation-1263", "mrqa_squad-validation-133", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1612", "mrqa_squad-validation-1613", "mrqa_squad-validation-168", "mrqa_squad-validation-175", "mrqa_squad-validation-1759", "mrqa_squad-validation-1779", "mrqa_squad-validation-1813", "mrqa_squad-validation-1891", "mrqa_squad-validation-1980", "mrqa_squad-validation-2018", "mrqa_squad-validation-2085", "mrqa_squad-validation-2085", "mrqa_squad-validation-2109", "mrqa_squad-validation-2129", "mrqa_squad-validation-2167", "mrqa_squad-validation-2214", "mrqa_squad-validation-2302", "mrqa_squad-validation-246", "mrqa_squad-validation-2586", "mrqa_squad-validation-2666", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-2754", "mrqa_squad-validation-2913", "mrqa_squad-validation-2939", "mrqa_squad-validation-2975", "mrqa_squad-validation-3037", "mrqa_squad-validation-313", "mrqa_squad-validation-3213", "mrqa_squad-validation-3370", "mrqa_squad-validation-3479", "mrqa_squad-validation-350", "mrqa_squad-validation-3535", "mrqa_squad-validation-3581", "mrqa_squad-validation-3667", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3811", "mrqa_squad-validation-3842", "mrqa_squad-validation-385", "mrqa_squad-validation-3922", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-4096", "mrqa_squad-validation-4179", "mrqa_squad-validation-4246", "mrqa_squad-validation-4292", "mrqa_squad-validation-4469", "mrqa_squad-validation-4591", "mrqa_squad-validation-4669", "mrqa_squad-validation-468", "mrqa_squad-validation-4860", "mrqa_squad-validation-4877", "mrqa_squad-validation-4898", "mrqa_squad-validation-5007", "mrqa_squad-validation-5030", "mrqa_squad-validation-5042", "mrqa_squad-validation-5135", "mrqa_squad-validation-5325", "mrqa_squad-validation-5351", "mrqa_squad-validation-5613", "mrqa_squad-validation-5699", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5716", "mrqa_squad-validation-5721", "mrqa_squad-validation-5809", "mrqa_squad-validation-5875", "mrqa_squad-validation-5933", "mrqa_squad-validation-6025", "mrqa_squad-validation-6116", "mrqa_squad-validation-6157", "mrqa_squad-validation-6163", "mrqa_squad-validation-6274", "mrqa_squad-validation-6341", "mrqa_squad-validation-6383", "mrqa_squad-validation-6579", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6705", "mrqa_squad-validation-6803", "mrqa_squad-validation-7036", "mrqa_squad-validation-7168", "mrqa_squad-validation-7233", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7294", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7428", "mrqa_squad-validation-7491", "mrqa_squad-validation-7516", "mrqa_squad-validation-7531", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7707", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-780", "mrqa_squad-validation-7805", "mrqa_squad-validation-7838", "mrqa_squad-validation-7909", "mrqa_squad-validation-8386", "mrqa_squad-validation-850", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8523", "mrqa_squad-validation-8555", "mrqa_squad-validation-8601", "mrqa_squad-validation-8603", "mrqa_squad-validation-8680", "mrqa_squad-validation-8861", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8905", "mrqa_squad-validation-8927", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9302", "mrqa_squad-validation-9322", "mrqa_squad-validation-9328", "mrqa_squad-validation-9412", "mrqa_squad-validation-9436", "mrqa_squad-validation-95", "mrqa_squad-validation-9590", "mrqa_squad-validation-9695", "mrqa_squad-validation-9901", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-1052", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1840", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3045", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3577", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4430", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5226", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7613"], "OKR": 0.86328125, "KG": 0.4515625, "before_eval_results": {"predictions": ["climate change", "coronary thrombosis", "a Qutb", "The Hurricane", "D.C.", "the Medusa", "The Tin Drum", "kerosene", "England", "Shirley Temple", "flagellation", "Hubert Humphrey", "Bart Simpson", "Wesley Clark", "Frasier", "Hispanic heritage", "fish", "Sherlock Holmes", "motor neurons", "President Bill Clinton", "D'Annunzo", "Mexico", "cosmology", "copper", "Welsh", "a decoupage", "(1925)", "Arethusa", "Randolph", "Bucharest", "Down syndrome", "manager", "Wheat", "(2)", "Dusty Old Dust", "insulin", "a Ninja", "World War II", "a pear", "Mark Twain", "Luther Powell", "The World Through More Than One lens", "the Galapagos", "a Knesset", "Administrative Professionals", "Time", "Dante", "a bull", "Lulu Kennedy-Cairns", "the Great Hunger", "a eulogy", "Dahl", "Massachusetts", "third", "the leaves of the plant species", "(Abrams) Patmore", "alpestrine", "Michael Sheen", "St. Patrick's Day in 1988", "the Kona coast", "Santiago Herrera", "three", "Secretary of State", "(CNN)"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5473958333333333}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3977", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-3010", "mrqa_searchqa-validation-3606", "mrqa_searchqa-validation-12276", "mrqa_searchqa-validation-1744", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-875", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-4055", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-12346", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-2416", "mrqa_searchqa-validation-16391", "mrqa_searchqa-validation-12944", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-1913", "mrqa_searchqa-validation-349", "mrqa_searchqa-validation-3032", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-16220", "mrqa_searchqa-validation-3267", "mrqa_searchqa-validation-16274", "mrqa_searchqa-validation-4325", "mrqa_searchqa-validation-8372", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-9172", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-4167", "mrqa_hotpotqa-validation-5627", "mrqa_newsqa-validation-2470"], "SR": 0.421875, "CSR": 0.5357862903225806, "retrieved_ids": ["mrqa_squad-train-64676", "mrqa_squad-train-54283", "mrqa_squad-train-29953", "mrqa_squad-train-29528", "mrqa_squad-train-57433", "mrqa_squad-train-11379", "mrqa_squad-train-57951", "mrqa_squad-train-74952", "mrqa_squad-train-13056", "mrqa_squad-train-69444", "mrqa_squad-train-16296", "mrqa_squad-train-51751", "mrqa_squad-train-71894", "mrqa_squad-train-6109", "mrqa_squad-train-46816", "mrqa_squad-train-42768", "mrqa_newsqa-validation-961", "mrqa_searchqa-validation-12104", "mrqa_squad-validation-1436", "mrqa_searchqa-validation-9803", "mrqa_naturalquestions-validation-9135", "mrqa_searchqa-validation-14979", "mrqa_hotpotqa-validation-2092", "mrqa_newsqa-validation-2040", "mrqa_squad-validation-5025", "mrqa_newsqa-validation-3731", "mrqa_hotpotqa-validation-334", "mrqa_naturalquestions-validation-9119", "mrqa_triviaqa-validation-7724", "mrqa_hotpotqa-validation-4075", "mrqa_searchqa-validation-82", "mrqa_hotpotqa-validation-866"], "EFR": 1.0, "Overall": 0.7173916330645161}, {"timecode": 31, "before_eval_results": {"predictions": ["corrosion", "steamboats", "imperfect", "Eli Lilly", "Manhattan Project", "Maryland", "breakfast", "carioca", "(D Dale)hardt", "(John) DUBBED", "Ford Madox Ford", "the Cuyahoga River", "Barney Miller", "James Lovell", "a coyote", "terminal", "\"The Boss\"", "Satin", "Air Force Academy", "Harry Lime", "shrewd", "Nucleus", "Don Juan", "Texas", "the Computing-Tabulating-Recording Company", "Chuck Yeager", "the C.I.A. in Iran", "radioactive material ha", "vixen", "the Wadi Hanifah valley", "Abnormal Psychology", "goat milk", "Billy Idol", "anaphylactic shock", "copper", "the Ropers", "Bank of America", "the Lampoon", "Terry Bradshaw", "Florence", "foolery", "parasites", "Columbia University", "(John) Clinton", "Don Quixote", "Medium", "Seattle", "insulin", "dlectre", "the Pilcro Hyphen Chinos", "Ricky Martin", "(Frank) Gifford", "1989", "sea water and fresh water", "Toronto", "Napoleon Bonaparte", "power", "Frederick Forsyth", "Daniil Shafran", "67,575", "various names", "(Rihanna)anna", "At least 13", "The Everglades,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6321924603174602}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-757", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-2914", "mrqa_searchqa-validation-9740", "mrqa_searchqa-validation-10027", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-12607", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-4764", "mrqa_searchqa-validation-7211", "mrqa_searchqa-validation-14766", "mrqa_searchqa-validation-9617", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-3155", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-2405", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-8628", "mrqa_triviaqa-validation-5038", "mrqa_newsqa-validation-3090", "mrqa_newsqa-validation-3911"], "SR": 0.546875, "CSR": 0.5361328125, "EFR": 1.0, "Overall": 0.7174609375000001}, {"timecode": 32, "before_eval_results": {"predictions": ["Eric Roberts", "Department for Culture, Media and Sport", "Wigs for Kids", "Tennessee Williams", "Wayne Gretzky", "Pushkin", "Williamsburg", "Pitcairn", "air superiority", "Halloween", "a port-wine stain", "hurricane", "Broadway", "one foot", "the AAA", "the Jutland", "a mutual fund", "the Two Sicilies", "1773", "Atlas", "a Bisque", "four", "Minnesota", "Violetta Chamorro", "Alisa Hamilton", "Pillsbury", "oxygen", "The Last Mimzy", "Victoria\\'s Secret", "Jenna Bush", "Jonathan Demme", "California", "the North Atlantic Treaty Organization", "the ear", "Moulin Rouge", "Cape Cod", "the chondrocra- nium", "febreze", "Boeing-Iran", "the ice age", "a Medal of Honor", "Slavic", "Walter Payton", "Orleans", "the Campus Martius", "the B-52", "nickel", "Tom Ridge", "George Babbitt", "Meg Tilly", "Jeopardy", "a imperative", "18", "George Halas", "Battle of Long Island", "the Cheshire Cat", "imperative", "Kent", "Gareth Jones", "an obsessed and tormented king", "Newcastle upon Tyne, England", "Illness", "The pilot, whose name has not yet been released,", "2,000 euros ($2,963)"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6050347222222222}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16822", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-13952", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-9956", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-16353", "mrqa_searchqa-validation-2878", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-11313", "mrqa_searchqa-validation-2960", "mrqa_searchqa-validation-13215", "mrqa_searchqa-validation-8969", "mrqa_searchqa-validation-10140", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-12784", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-5282", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-1605", "mrqa_newsqa-validation-2296"], "SR": 0.53125, "CSR": 0.5359848484848485, "EFR": 1.0, "Overall": 0.7174313446969698}, {"timecode": 33, "before_eval_results": {"predictions": ["2100", "Apollo 5", "Benazir Bhutto,", "Tim Kaine", "is revolutionizing what scientists know about Earth's closest neighbor.", "the i report form", "Muqtada al-Sadr", "a keystroke", "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"", "is getting an even smaller update.", "success as a recording artist", "Janet Napolitano", "sodium dichromate", "citizenship", "10,000", "4,000", "CNN/Opinion Research Corporation", "California", "the market makers", "Ralph Cifaretto", "Bobby Darin,", "Two", "LulzSec.", "Transportation Security Administration", "Maude", "is death", "Nineteen", "a weight-loss show", "World War I", "Liza Murphy", "Turkey", "the Cowardly Lion", "\"It was bigger than what I had lived in,\"", "Gary Player", "attempted murder", "Nicole", "guard in the jails", "a \"prostitute\"", "Mississippi", "40 militants", "April 22", "bartering", "14", "Nairobi, Kenya", "the war years", "Silicon Valley", "between 1917 and 1924", "June 6, 1944", "giving birth to baby daughter Jada,", "Seoul", "is not enough", "is the worst subway train accident in the history of the Washington Metropolitan Area Transit Authority.", "Charlene Holt", "the gastrocnemius", "Miami Heat", "4", "Finch", "architect", "the Secret Intelligence Service", "Ready Player One", "alcoholic drinks", "Mars", "crushable", "Tartarus"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5635054181929182}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.0, 0.8, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.2857142857142857, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-850", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-3020", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1288", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-4032", "mrqa_triviaqa-validation-3514", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-201"], "SR": 0.46875, "CSR": 0.5340073529411764, "retrieved_ids": ["mrqa_squad-train-4748", "mrqa_squad-train-77098", "mrqa_squad-train-51245", "mrqa_squad-train-3476", "mrqa_squad-train-4188", "mrqa_squad-train-4850", "mrqa_squad-train-82567", "mrqa_squad-train-11611", "mrqa_squad-train-35080", "mrqa_squad-train-9247", "mrqa_squad-train-54092", "mrqa_squad-train-36063", "mrqa_squad-train-72843", "mrqa_squad-train-618", "mrqa_squad-train-27402", "mrqa_squad-train-57202", "mrqa_squad-validation-6986", "mrqa_triviaqa-validation-6080", "mrqa_newsqa-validation-2791", "mrqa_naturalquestions-validation-4365", "mrqa_squad-validation-9400", "mrqa_squad-validation-5213", "mrqa_searchqa-validation-8944", "mrqa_searchqa-validation-14932", "mrqa_naturalquestions-validation-8628", "mrqa_searchqa-validation-9660", "mrqa_newsqa-validation-1928", "mrqa_hotpotqa-validation-5692", "mrqa_triviaqa-validation-6083", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4914", "mrqa_searchqa-validation-3977"], "EFR": 1.0, "Overall": 0.7170358455882353}, {"timecode": 34, "before_eval_results": {"predictions": ["for complicity and to Odinga declaring himself the \"people's president\"", "KGPE", "Lerotholi Polytechnic", "John Delaney", "1979", "The Catholic Church in Ireland", "Trey Parker and Matt Stone", "Canterbury", "310", "Bohemia", "Czech (Bohemian) and German (Franconian)", "people working in film and the performing arts", "Washington, D.C.", "Lady Frederick Windsor", "Portsmouth", "Sam the Sham", "coaxial", "Graham Hill", "Marika Nicolette Green", "Katherine Harris", "Frank Thomas' Big Hurt", "World War I", "John Richard Schlesinger,", "Nikolai Trubetzkoy", "3,500,000", "Rabies", "Big Bad Wolf", "Thomas Jane", "2016", "George Orwell", "July 8, 2014", "Prabh Gill", "Costa del Sol", "Gabriel Jesus Iglesias", "Kolkata", "two", "Roscoe Lee Browne", "23", "video game", "a Peach", "U.S.", "Joe Frazier", "July 11, 2016", "in Srinagar", "Drunken Master II", "San Francisco, California", "\"The Brothers\"", "Daniel Espinosa", "Blake Shelton,", "Benjamin Andrew \" Ben\" Stokes", "Duke University", "The Kennedy Center", "Louis XV", "2,140 kilometres", "to refer to a god of the Ammonites", "Munich", "Admiral Johan van Galen", "a \u201cstupid term\u201d", "New Delhi, India", "Cesar Laurean", "crude oil", "a Rolling Stone", "Nick", "Abraham Lincoln"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5336090686274509}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.11764705882352941, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8422", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5039", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-3889", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4748", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3428", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-2753", "mrqa_searchqa-validation-15973"], "SR": 0.4375, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.7164843750000001}, {"timecode": 35, "before_eval_results": {"predictions": ["downward pressure on wages", "ABC Television Center", "Cincinnati", "North Sea", "Headless Body in Topless Bar", "\"Gliding Dance of the Maidens\"", "Patti Smith", "\"Darconville\u2019s Cat\"", "kitty Hawk", "Tiberius", "the Netherlands", "Eva Ibbotson", "70 m and 90 m events.", "Charles Otto Puth Jr.", "\"Danger Mouse\"", "Don DeLillo", "crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "comedy", "2013\u20132014", "University of Southern California", "New York City", "authoritarian tendencies", "Robert Grosvenor", "Christopher McCulloch", "Marigold Newey", "an album", "The Wachowskis", "a fantasy role-playing game", "WikiLeaks", "top division", "Ramsey County", "fantasy role-playing game", "New Jersey", "east", "Giacomo Puccini", "Thomas Jefferson", "Winchester, Nevada", "classical Carnatic music", "1912", "Eisenhower Executive Office Building", "Edward Ryon Makuahanai Aikau", "American Chopper", "first and second segment", "England", "Hindi", "Netherlands", "soccer", "Kings Point, New York", "a Ballon d'Or", "Westminster system", "Bruce Grobbelaar", "mid-air collision", "seven years earlier", "a cliffhanger", "79", "Alaska", "football", "USA Army Pigeon", "in an effort to make the animals' lives as natural as possible.", "Kurt Cobain", "\"It was always uplifting and happy music,\"", "Our Sea", "a sedimentary rock", "Tchekoff"], "metric_results": {"EM": 0.5, "QA-F1": 0.601604974376114}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.0, 0.6666666666666666, 0.23529411764705882, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7182", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-455", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2773", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-2257", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-9026", "mrqa_triviaqa-validation-6520", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1352", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-282"], "SR": 0.5, "CSR": 0.5303819444444444, "EFR": 1.0, "Overall": 0.7163107638888889}, {"timecode": 36, "before_eval_results": {"predictions": ["comb jellies", "Luger P08", "Lt. Gen. Ulysses S. Grant", "the southern (Dolomitic) Alps in the Veneto region of Northern Italy", "various deities, beings, and heroes", "a role-playing game", "Wandsworth, London", "1967", "New York City", "capital crimes", "around 8000 BC", "Smoothie King Center", "Nan Britton", "1977", "9\u201310 March 1945", "Currer Bell", "Jim Davis", "Ted Bundy", "Crawley Town", "Parlophone", "McDowell County, West Virginia", "Life Is a Minestrone", "Louis Silvie \"Louie\" Zamperini", "an English professional footballer", "New York Shakespeare Festival", "peat moss", "South Australia", "Apple Lisa", "George Gordon Byron, 6th Baron Byron, FRS (22 January 1788 \u2013 19 April 1824)", "Singapore", "1853", "Big Bad Wolf", "seven nights a week", "A Liverpool F.C.", "Manchester\u2013Boston Regional Airport", "Christopher Francis \"Frank\" Ocean", "Gangsta's Paradise", "A one off single by Dutch association football club AFC Ajax,", "in most casinos", "331", "Kristina Ceyton and Kristian Moliere", "1835", "Helsinki", "Francisco P. Felix", "Carrefour", "2014", "HBO miniseries \"Empire Falls\"", "Wet 'n Wild Orlando", "North Queensland", "The iPhone 5", "203 people", "the Caucasus region", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Southwest Atlanta Christian Academy", "a burrow", "Billie Holiday", "the Cumberland", "eight-week plan for low-calorie meals", "American Civil Liberties Union", "three of the bombers", "Three Little Pigs", "Spider-Man", "Cynthia Nixon", "French parliamentary commission recommended a partial ban on any veils that cover the face"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5593277815934066}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [0.25, 1.0, 0.0, 0.7142857142857143, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.3076923076923077, 0.6666666666666666, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4534", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-3301", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-614", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-4653", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-423", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-891", "mrqa_searchqa-validation-15738", "mrqa_newsqa-validation-297"], "SR": 0.4375, "CSR": 0.5278716216216216, "retrieved_ids": ["mrqa_squad-train-67443", "mrqa_squad-train-7978", "mrqa_squad-train-55118", "mrqa_squad-train-2903", "mrqa_squad-train-3641", "mrqa_squad-train-15402", "mrqa_squad-train-37943", "mrqa_squad-train-30528", "mrqa_squad-train-60054", "mrqa_squad-train-32854", "mrqa_squad-train-16166", "mrqa_squad-train-51092", "mrqa_squad-train-72438", "mrqa_squad-train-79062", "mrqa_squad-train-2586", "mrqa_squad-train-10981", "mrqa_hotpotqa-validation-3875", "mrqa_searchqa-validation-3135", "mrqa_searchqa-validation-16205", "mrqa_squad-validation-1612", "mrqa_naturalquestions-validation-5378", "mrqa_hotpotqa-validation-5239", "mrqa_squad-validation-3319", "mrqa_hotpotqa-validation-5701", "mrqa_searchqa-validation-16796", "mrqa_newsqa-validation-3918", "mrqa_triviaqa-validation-2889", "mrqa_hotpotqa-validation-412", "mrqa_newsqa-validation-3365", "mrqa_hotpotqa-validation-2725", "mrqa_triviaqa-validation-1590", "mrqa_squad-validation-1089"], "EFR": 1.0, "Overall": 0.7158086993243243}, {"timecode": 37, "before_eval_results": {"predictions": ["many castles and vineyards", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "2000.", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "American Airlines have been losing money recently while others have profited, but at the end of June, it had more than $5 billion in the bank and it has been able to arrange financing since then.", "(Chadian President Idriss Deby hopes the journalists and the flight crew will be freed,", "walked into the Central Methodist Church in downtown Johannesburg and joined a long queue of people waiting for shelter and food.", "President Paul Biya,", "\"Mad Men's\" Don Draper and his blatantly sexual overtures to female employees", "Pervez Musharraf", "money or other discreet aid for the effort if it could be made available,", "News of the World tabloid.", "The Falklands, known as Las Malvinas in Argentina, lie in the South Atlantic Ocean off the Argentinean coast and have been under British rule since 1833.", "prostate cancer", "July 23.", "Al Gamaa al-Islamiyya, a Muslim Brotherhood offshoot that had failed ambitions of launching an Islamic revolution in the mid-1980s.", "debris", "Bangladesh", "The syndicate, founded by software magnate Larry Ellison,", "150", "Sharon Bialek", "bribing other wrestlers to lose bouts, compounding the view that corruption was prevalent in the sport.", "did not speak to those who had gathered but shadow-boxed to spectators and cameras before meeting his distant relatives.\"", "45th anniversary.", "state's attorney's office", "photos", "\"Twilight\" book series.", "male veterans struggling with homelessness and addiction.", "cities throughout Canada.", "to step up attacks against innocent civilians.\"", "opening of its new restaurant next to the home of Mona Lisa as something completely normal.", "writing and starring in 'The Prisoner' about a former spy locked away in an isolated village who tries to escape each episode.", "Friday", "Belfast, Northern Ireland", "Pakistan's combustible Swat Valley", "Former Beatles", "Washington Redskins fan and loved to travel,", "Asashoryu", "Sicily", "last summer.", "Mesut Oezil", "revelry", "London Heathrow's Terminal 5", "18", "last April", "Casablanca, Morocco,", "Michael Jackson had publicly criticized his father's parenting skills.", "$250,000", "Los Angeles.", "$3 billion", "Manchester United", "7 July", "Bob Dylan", "St Pancras International", "Awning window", "Denver", "Brazil", "Phelan Beale", "sandstone", "2005", "Babe Ruth", "New York Presbyterian Hospital", "coffee", "David Lodge"], "metric_results": {"EM": 0.34375, "QA-F1": 0.47340852941833894}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.0, 0.05263157894736842, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5555555555555556, 0.05714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.35294117647058826, 0.1904761904761905, 0.0, 0.4, 1.0, 0.5, 0.0, 0.5, 0.923076923076923, 0.08695652173913045, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.19999999999999998, 1.0, 0.4444444444444445, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8990", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-925", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-241", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2938", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1516", "mrqa_naturalquestions-validation-5457", "mrqa_triviaqa-validation-2027", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-4180", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-15555"], "SR": 0.34375, "CSR": 0.5230263157894737, "EFR": 1.0, "Overall": 0.7148396381578948}, {"timecode": 38, "before_eval_results": {"predictions": ["death of a heretic.\"", "Gainsbourg", "37", "no interest in mating and would attack females when they were introduced.", "her home", "not yet approved by the International Atomic Energy Agency's board of governors,", "gratitude for his mother.", "84-year-old", "free laundry service.", "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York", "opium", "public opinion", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Galveston, Texas,", "Six", "Carl", "581 points", "make life a little easier", "Yellowstone National Park,", "building bombs,", "forgery and flying without a valid license,", "2050,", "Eintracht Frankfurt", "Juan Martin Del Potro.", "about 50 formal applications", "because its facilities are full.", "stealing the personal credit information of thousands of unknow American and European consumers,", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Akshay Kumar", "133 people", "more than 100", "The Ski Train", "a man's lifeless, naked body", "Flint, Michigan.", "O2 Arena.", "Philippines", "$500,000", "work rule issues.", "not be allowed", "not broadcast a appeal would use pictures which are the same or similar to those we would be using in our news programs but would do so with the objective of encouraging public donations.", "Six members of Zoe's Ark", "second time since the 1990s", "noose incident occurred two weeks after Black History Month", "Congress", "Sen. Joe Lieberman,", "Marcell Jansen", "Gov. Bobby Jindal", "two suicide bombers,", "in the shop at the Form Design Center.", "\"17 Again,\"", "Nechirvan Barzani, Prime Minister of the Kurdish Regional Government,", "writ of certiorari", "New Mexico", "In response, in 1947, U.S. Secretary of State George Marshall devised the `` European Recovery Program ''", "jewellers", "supercontinent", "motorway", "ethereal wave", "Guinness World Records", "Citizens for a Sound Economy", "bees, honey, and the Black Madonna who presides over their household.", "The Prisoner of Zenda", "Jean Lafitte", "Dribbling"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5013519461217756}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.14285714285714285, 1.0, 0.4, 0.8181818181818181, 1.0, 0.6666666666666666, 0.1212121212121212, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.2857142857142857, 0.4, 1.0, 1.0, 0.4, 0.0, 0.21276595744680848, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0588235294117647, 1.0, 0.25, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.18181818181818182, 0.5, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3052", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-3006", "mrqa_naturalquestions-validation-4860", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-1849", "mrqa_hotpotqa-validation-5148", "mrqa_hotpotqa-validation-4020", "mrqa_searchqa-validation-1177", "mrqa_searchqa-validation-3701", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-14613"], "SR": 0.359375, "CSR": 0.5188301282051282, "EFR": 0.9512195121951219, "Overall": 0.70424430308005}, {"timecode": 39, "before_eval_results": {"predictions": ["radicalize the Islamist movement", "Western New York and Central New York", "on the buttocks floor", "Andy Serkis", "Aldis Hodge", "Shaina Taub", "sedimentary", "in soils", "John Young", "degree of independence from the Department of Health", "W. Edwards Deming", "MacFarlane", "Cyndi Grecco", "any untoward medical occurrence in a patient or clinical investigation subject administered a pharmaceutical product", "`` non rigid '' bodies", "Fleetwood Mac", "The chief executive of West Virginia", "December 12, 2017", "South Africa", "1832", "Curtis Armstrong", "fibrous tissue", "Marne - la - Vall\u00e9e -- Chessy", "Jerry Leiber and Mike Stoller", "De Wayne Warren", "Charles Woodson", "Jesse Wesley Williams", "Terry Kath", "276 episodes", "6 : 00 p.m.", "Book of Exodus", "April 25 -- 30 in Park Avenue", "Kristy Swanson", "2001", "1994", "Washington", "Triple Alliance of Germany, Austria - Hungary, and Italy", "New South Wales", "Wisconsin", "The standing rib roast", "Donald Trump", "David Tennant", "Ben Findon, Mike Myers and Bob Puzey", "Charles Path\u00e9", "Barbara Windsor", "1854", "1997", "eleven", "Mel Gibson", "boy", "makes Maria a dress to wear to the neighborhood dance", "Manchester", "The Wrestling Classic", "Piper Aircraft", "Mike Fiers", "Rockland", "Delilah Rene", "three", "inferior,", "Frank Ricci", "Zanzibar", "Mao Zedong", "Alexander Graham Bell", "Drew Kesse,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5648299881436314}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.14634146341463414, 0.0, 0.0, 0.6666666666666665, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.4, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-2365", "mrqa_hotpotqa-validation-2210", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3331"], "SR": 0.484375, "CSR": 0.51796875, "retrieved_ids": ["mrqa_squad-train-51262", "mrqa_squad-train-34393", "mrqa_squad-train-15217", "mrqa_squad-train-37147", "mrqa_squad-train-65409", "mrqa_squad-train-41523", "mrqa_squad-train-79412", "mrqa_squad-train-14054", "mrqa_squad-train-45062", "mrqa_squad-train-12874", "mrqa_squad-train-11377", "mrqa_squad-train-72612", "mrqa_squad-train-14089", "mrqa_squad-train-45945", "mrqa_squad-train-82401", "mrqa_squad-train-15797", "mrqa_triviaqa-validation-1907", "mrqa_newsqa-validation-1425", "mrqa_squad-validation-9559", "mrqa_triviaqa-validation-4748", "mrqa_hotpotqa-validation-3339", "mrqa_searchqa-validation-15790", "mrqa_searchqa-validation-2638", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-3135", "mrqa_naturalquestions-validation-2582", "mrqa_squad-validation-7707", "mrqa_naturalquestions-validation-1814", "mrqa_newsqa-validation-3592", "mrqa_triviaqa-validation-3955", "mrqa_searchqa-validation-4055", "mrqa_searchqa-validation-3010"], "EFR": 0.9393939393939394, "Overall": 0.701706912878788}, {"timecode": 40, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1840", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3310", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5424", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-854", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3525", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-370", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4658", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2470", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-11232", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-1177", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14978", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15004", "mrqa_searchqa-validation-15504", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-16353", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4567", "mrqa_searchqa-validation-4764", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8176", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8366", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8791", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-9803", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-1540", "mrqa_squad-validation-1586", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2167", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2473", "mrqa_squad-validation-2640", "mrqa_squad-validation-2672", "mrqa_squad-validation-270", "mrqa_squad-validation-2757", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3407", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3786", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-3939", "mrqa_squad-validation-4010", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4484", "mrqa_squad-validation-4546", "mrqa_squad-validation-4671", "mrqa_squad-validation-4750", "mrqa_squad-validation-4799", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5019", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5634", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-6029", "mrqa_squad-validation-6318", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6594", "mrqa_squad-validation-6630", "mrqa_squad-validation-6981", "mrqa_squad-validation-7023", "mrqa_squad-validation-7168", "mrqa_squad-validation-7194", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7294", "mrqa_squad-validation-7466", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-7907", "mrqa_squad-validation-7932", "mrqa_squad-validation-8066", "mrqa_squad-validation-8066", "mrqa_squad-validation-8127", "mrqa_squad-validation-8209", "mrqa_squad-validation-8253", "mrqa_squad-validation-8386", "mrqa_squad-validation-8399", "mrqa_squad-validation-8488", "mrqa_squad-validation-8501", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8901", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9855", "mrqa_squad-validation-9868", "mrqa_squad-validation-9901", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1052", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4357", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6418", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6681", "mrqa_triviaqa-validation-6704", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-895"], "OKR": 0.8359375, "KG": 0.509375, "before_eval_results": {"predictions": ["Turkana", "Action Jackson", "Ben Willis", "Phillip Schofield and Christine Bleakley", "Jacksonville", "The Third Five - year Plan", "Hermann Ebbinghaus", "Ronald Reagan", "Baltimore", "to collect menstrual flow", "April 1st", "late 1980s", "Lesley Gore", "Andrew Lincoln", "Joe Spano", "inverted", "political ideology", "Matt Monro", "Wednesday, 5 September 1666", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "response to a perceived harmful event, attack, or threat to survival", "Vice President, Speaker of the House of Representatives, President pro tempore of the Senate", "hypermarkets", "July 2, 1776", "balance sheet", "after the title page, copyright notices, and, in technical journals, the abstract ; and before any lists of tables or figures, the foreword, and the preface", "2007", "Fred Ott", "Tevin Campbell", "October 6, 2017", "early Christians of Mesopotamia", "Anna Murphy", "a mountainous, peninsular mainland jutting out into the Mediterranean Sea", "new version ''", "Santo Domingo", "( 3 ) 1976", "Brad Johnson", "in Egypt", "in front of only 700 fans", "2003", "May 30, 2017", "A turlough", "King Louie", "the national flag of the United States", "The euro", "local authorities", "`` One Son ''", "2017", "Nick Kroll", "Ron Harper", "1,350", "Angel Cabrera", "The Jetsons", "Trinidad", "Point", "Taipei", "England", "Sunday", "Elizabeth Birnbaum", "Karen Floyd", "Lager", "burritos", "(John) Ritter", "Australian actor and film producer"], "metric_results": {"EM": 0.5, "QA-F1": 0.5798696299615418}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5882352941176471, 0.0, 1.0, 0.0, 0.9545454545454545, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-7464", "mrqa_triviaqa-validation-783", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-4084", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-15726"], "SR": 0.5, "CSR": 0.5175304878048781, "EFR": 1.0, "Overall": 0.7288185975609756}, {"timecode": 41, "before_eval_results": {"predictions": ["middle eastern scientists", "23 September 1889", "to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas : Spain ( 1519 -- 1690 )", "ummat al - Islamiyah", "2018", "Saint Alphonsa, F.C., ( born Anna Muttathupadathu ; 19 August 1910 -- 28 July 1946 )", "Peggy Lipton", "in the basic curriculum", "air moisture", "to the left of the dinner plate", "Yondu Udonta", "Terry Kath", "1830s", "November 5, 2017", "medical abnormalities, activation level, or recruitment order, or to analyze the biomechanics of human or animal movement", "\u20b9 39.50 lakh", "Psychomachia", "supervillains who pose catastrophic challenges to the world", "Atlanta", "21 June 2007", "Pakistan", "Bed and breakfast", "Masha Skorobogatov", "an empty line", "Jack Gleeson", "Coriolis effect", "October 6, 2017", "the Sunni Muslim family", "160km / hour", "Escherichia coli", "Roman Reigns", "Andrea Brooks", "Audrey II", "Acid rain", "Kaley Christine Cuoco", "Quantitative psychological research", "Jules Shear", "Darren McGavin", "1961", "Randy VanWarmer", "April 3, 1973", "manta rays and Scorpion fish", "natural language data", "St. Louis Cardinals", "President Friedrich Ebert", "9.1 %", "Coldplay", "between the Mediterranean Sea to the north and the Red Sea in the south", "Lord's", "Kingsford, Michigan", "Honor\u00e9 Mirabeau", "Daniel Defoe", "1961", "birmingham", "First Street", "I write What I Like", "Princes Park", "cancerous tumor.", "Kyra and Violet,", "\"Empire of the Sun,\"", "\" Bulldog\"", "Petsmart", "24", "1970"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6591540666213631}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.9302325581395349, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.1818181818181818, 0.5714285714285715, 0.0, 1.0, 0.0, 0.4, 0.9, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-3757", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-1455", "mrqa_triviaqa-validation-7151", "mrqa_newsqa-validation-3380", "mrqa_searchqa-validation-4755"], "SR": 0.578125, "CSR": 0.5189732142857143, "EFR": 0.9259259259259259, "Overall": 0.714292328042328}, {"timecode": 42, "before_eval_results": {"predictions": ["less partly the product of a declining state of mind", "1861\u20131865", "private", "A shooting guard", "703", "December 1974", "Sufism", "Capellini", "1614", "international association football", "German and American", "thirteen", "McComb, Mississippi", "Santa Fe", "Donald Duck", "Kew Gardens", "the northeastern part", "American Horror Story", "Chicago, Illinois", "Free Range Films", "The Deep Blue Sea", "Vernon Smith", "Australian", "Frank Fertitta, Jr. Station Casinos", "from September 30, 2011", "the Battelle Energy Alliance", "athlete", "Steve Biko", "October 2016", "Sid Vicious", "51,271", "Sun Valley", "Guardians of the Galaxy Vol. 2", "American burlesque", "Pulitzer Prize", "Scott Paul Carson", "IFFHS World's Best Goalkeeper", "Suspiria", "an English singer, songwriter, actress, and radio and television presenter", "Belladonna", "Russell T Davies", "\"Creed\"", "The 2017 Pakistan Super League spot-fixing scandal", "Erich Schmidt-Leichner", "City of Peace", "Alfred in \"Die Fledermaus\"", "Movie Masters", "6,241", "Hennepin County", "Australian coast", "Adelaide Botanic Garden, Hutt Street, and Victoria Park", "Paul Newman", "Michael Buffer", "Presley Smith", "AFC Wimbledon", "Higgs", "Sashimi", "a share in the royalties for the tune.", "Mongolian,", "\"The Real Housewives of Atlanta\"", "Antnio Guterres", "ratify the Constitution of the United States", "letter", "Dairy Queen"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6450892857142857}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2523", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3352", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4296", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-973", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5385", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-4201", "mrqa_hotpotqa-validation-2239", "mrqa_hotpotqa-validation-2395", "mrqa_naturalquestions-validation-5293", "mrqa_triviaqa-validation-3569", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-1125", "mrqa_searchqa-validation-3012", "mrqa_searchqa-validation-732", "mrqa_searchqa-validation-11496"], "SR": 0.546875, "CSR": 0.5196220930232558, "retrieved_ids": ["mrqa_squad-train-30088", "mrqa_squad-train-34555", "mrqa_squad-train-57944", "mrqa_squad-train-45016", "mrqa_squad-train-78588", "mrqa_squad-train-73500", "mrqa_squad-train-52661", "mrqa_squad-train-7307", "mrqa_squad-train-57175", "mrqa_squad-train-55970", "mrqa_squad-train-20836", "mrqa_squad-train-12990", "mrqa_squad-train-4716", "mrqa_squad-train-81919", "mrqa_squad-train-44703", "mrqa_squad-train-79857", "mrqa_newsqa-validation-3120", "mrqa_hotpotqa-validation-3589", "mrqa_squad-validation-1767", "mrqa_squad-validation-10128", "mrqa_searchqa-validation-6670", "mrqa_squad-validation-5025", "mrqa_newsqa-validation-241", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-2018", "mrqa_searchqa-validation-7791", "mrqa_newsqa-validation-2040", "mrqa_triviaqa-validation-4600", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-4798", "mrqa_hotpotqa-validation-4508", "mrqa_squad-validation-2704"], "EFR": 1.0, "Overall": 0.7292369186046511}, {"timecode": 43, "before_eval_results": {"predictions": ["25 percent", "Evey", "1848", "Adelaide", "the George Washington Bridge", "January 26, 1996", "2008", "Jacking", "February 5, 2015", "The Worm", "MGM Resorts International", "Sam Raimi", "StubHub Center", "2001", "balloons Street, Manchester", "Liverpool Bay", "St. Louis, Missouri", "Stephen King", "\"Seducing Mr. Perfect\"", "British Labour Party", "The Five", "Ang Lee", "Taylor Swift", "Objectivism", "\"Traumnovelle\" (\"Dream Story)", "Mandarin", "the lead female role of London Tipton", "KlingStubbins", "the Goddess of Pop", "Chevron Corporation", "Black Panther Party", "Baldwin", "John John Florence", "YouTube", "Kohlberg K Travis Roberts", "Salzkammergut", "Rain Man", "feats of exploration", "video game", "Father Dougal McGuire", "1 million", "Mulberry", "London", "Subway restaurants", "cancer", "Campbellsville", "Mark Helfrich", "three", "Rickie Lee Skaggs", "James Hill", "Owsley Stanley", "Donna Mills", "16 seasons", "gas exchange", "f\u00e8in", "West Ham boss Allardyce", "gin", "almost 9 million", "vitamin injections", "to lose bouts,", "gain exposure to a stock on which you are", "Society & Community", "Carolina Peas and Rice", "Haiti"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6741714015151515}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3333333333333333, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-2770", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4239", "mrqa_hotpotqa-validation-3856", "mrqa_hotpotqa-validation-4501", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-8767", "mrqa_triviaqa-validation-2810", "mrqa_triviaqa-validation-5974", "mrqa_newsqa-validation-3325", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-14771"], "SR": 0.578125, "CSR": 0.5209517045454546, "EFR": 1.0, "Overall": 0.7295028409090909}, {"timecode": 44, "before_eval_results": {"predictions": ["a broken arm", "Mexico", "Missouri", "Franklin D. Roosevelt", "0", "Copenhagen", "the Pulitzer Prize", "the 904 Olympics", "Snares", "the Ziegfeld", "the Honeymooners", "Carnitas", "enamel", "Macy\\'s", "kamergersky Lane", "samuel kunison", "the House of Representatives", "Roman Empire", "Alaska", "Matt Leinart", "(Khan) Khan", "Jeremy Bentham", "the United States", "the Mekong", "Poseidon", "the 1984 Summer Olympics", "\"Ricochet\"", "venezuela", "a liquid", "(Kicks) Slay", "Ivory Coast", "Lord of the Rings: The Return of the King", "the canoe", "Alanis Morissette", "a tie", "soggy", "King Minos", "\"Emma Peel\"", "Schindler\\'s List", "King Henry VIII", "Stephen Crane", "Mississippi", "a chainmaille", "Yellow Brick Road", "madding", "Steely Dan", "Linda Tripp", "the Sierra Nevada", "(Al- Ashraf Khalil", "adios", "the Gadsden Treaty", "New Zealand", "Roman Reigns", "A new ruler unites China", "the Marshall Plan", "\"The Spurious Highwayman\"", "black", "15", "Cymbeline", "Mary-Kay Wilmers", "the shipping industry -- responsible for 5% of global greenhouse gas emissions,", "Hundreds", "2008", "three"], "metric_results": {"EM": 0.453125, "QA-F1": 0.49088541666666663}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-7525", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-3120", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-14186", "mrqa_searchqa-validation-6399", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-12098", "mrqa_searchqa-validation-15476", "mrqa_searchqa-validation-8837", "mrqa_searchqa-validation-15203", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-2669", "mrqa_searchqa-validation-10061", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-11577", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-16207", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-7176", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-11005", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1084", "mrqa_hotpotqa-validation-5093", "mrqa_newsqa-validation-3979"], "SR": 0.453125, "CSR": 0.5194444444444444, "EFR": 1.0, "Overall": 0.7292013888888889}, {"timecode": 45, "before_eval_results": {"predictions": ["the Lunar Roving Vehicle (LRV) increasing the exploration area and allowing televised liftoff of the LM", "the leader of a drug cartel that set off two grenades during a public celebration in September, killing eight people and wounding more than 100.", "Robert Barnett,", "Too many glass shards", "Sunday.", "\"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "Alan Graham", "Islamabad", "Jackson was battling a potentially fatal disease that required a life-saving lung transplant,", "President Thabo Mbeki", "\"Biscuit\" fire", "(Siemionow)", "12 hours in jail.", "the Somali coast.", "the Obama chief of staff", "$273 million", "Wigan Athletic", "\"The most visible, most exciting family in America is this beautiful black family and so people are ready and looking for those kinds of images,\"", "at least 25", "former U.S. secretary of state", "called Israel's actions against Hamas militants \"a gift\" from U.S. President-elect Barack Obama.", "( Brad) Blauser,", "U.S. President-elect Barack Obama", "204,000", "North Korea", "The Da Vinci Code", "Angela Merkel", "for at least 12 months.", "Passers-by", "Almost all British troops in Iraq", "Lance Cpl. Maria Lauterbach", "Picasso's muse and mistress, Marie-Therese Walter.", "80", "Polo", "bartering -- trading goods and services without exchanging money", "central Cairo,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.\"", "fluoroquinolone", "North Korea", "\"The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,\"", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "northwestern province of Antioquia", "The University of California San Diego", "to protect ocean ecology, address climate change and promote sustainable ocean economies.", "Asian", "a brilliant maiden Test century from JP Duminy,", "six", "flooding and debris", "then-presidential candidate Barack Obama", "The 23-year-old Rezai -- who had only claimed WTA Tour titles at Strasbourg and Bali prior to Madrid -- continued her remarkable week with a 6-2 7-5 victory", "Al-Shabaab, the radical Islamist militia that controls the city", "201", "IBM", "Gorakhpur railway station, Uttar Pradesh", "Chief Inspector of Prisons", "Charlie Cairoli", "Sweden", "2017", "Edmund Ironside", "Comme des Gar\u00e7ons", "Theodore Roosevelt", "the Montagues", "the Shang dynasty", "Germany"], "metric_results": {"EM": 0.328125, "QA-F1": 0.4381237599206349}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true], "QA-F1": [0.2666666666666667, 0.0, 1.0, 0.6666666666666666, 1.0, 0.08, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.25, 1.0, 1.0, 0.2222222222222222, 0.4, 1.0, 0.125, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.25, 0.25, 0.6666666666666666, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4027", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1848", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-1257", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-19", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-1863", "mrqa_hotpotqa-validation-3844", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-11067", "mrqa_searchqa-validation-4478"], "SR": 0.328125, "CSR": 0.5152853260869565, "retrieved_ids": ["mrqa_squad-train-40121", "mrqa_squad-train-69293", "mrqa_squad-train-56053", "mrqa_squad-train-80304", "mrqa_squad-train-11605", "mrqa_squad-train-11858", "mrqa_squad-train-35609", "mrqa_squad-train-83176", "mrqa_squad-train-59230", "mrqa_squad-train-16365", "mrqa_squad-train-9230", "mrqa_squad-train-66938", "mrqa_squad-train-62669", "mrqa_squad-train-52232", "mrqa_squad-train-64902", "mrqa_squad-train-22762", "mrqa_newsqa-validation-3097", "mrqa_hotpotqa-validation-2619", "mrqa_naturalquestions-validation-8092", "mrqa_newsqa-validation-3164", "mrqa_squad-validation-8990", "mrqa_newsqa-validation-2897", "mrqa_hotpotqa-validation-5487", "mrqa_naturalquestions-validation-7848", "mrqa_searchqa-validation-14158", "mrqa_newsqa-validation-3377", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-1874", "mrqa_triviaqa-validation-5714", "mrqa_squad-validation-1436", "mrqa_newsqa-validation-1003", "mrqa_searchqa-validation-3863"], "EFR": 1.0, "Overall": 0.7283695652173913}, {"timecode": 46, "before_eval_results": {"predictions": ["Thames River", "an independent homeland for the country's ethnic", "Turkish President Abdullah Gul,", "phone calls or by text messaging,", "hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "in Fayetteville, North Carolina,", "\"surge\" strategy he implemented last year.", "Christopher Savoie", "work together to stabilize Somalia and cooperate in security and military operations.", "not guilty", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "\"You can go from rags to riches there.", "the United States", "Manny Pacquiao", "Garth Brooks", "Omar Bongo,", "super-yacht designers", "a body", "Prince George's County Correctional Center", "two Metro transit trains that crashed the day before,", "Karen Floyd", "opposed the Iraq war and considered Afghanistan the \"good war.\"", "since 1983.", "$20 million to $30 million,", "Daryeel Bulasho Guud", "Zulfikar Ali Bhutto,", "the insurgency,", "Stoke City.", "American Civil Liberties Union against the government.", "three different videos", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.\"", "his past and his future", "the son of the most-wanted man in the world", "travel in cars with tinted windows -- which protected me from identification by terrorists -- or travel with privately armed guards,\"", "mild to moderate depression", "series of poems telling of the pain and suffering of children just like her", "economic growth", "Mitt Romney,", "\"The Sopranos,\"", "The Louvre", "Jared Polis", "a delegation of American Muslim and Christian leaders", "the legitimacy of that race.", "Australian officials", "misdemeanor", "$10 billion", "\"It's very new and involves repairing my leaky valve using a clip device, without open heart surgery, so that my heart will function better,\"", "safety issues in the company's cars", "Newcastle", "54", "1 mile ( 1.6 km )", "Abanindranath Tagore CIE", "1996", "Mozart", "HMS Thetis", "1960's", "the onset and progression of Alzheimer's disease", "The Future", "1770", "Jacob Grimm", "Seasons in the Sun", "Geraldine Farrar", "Michael Harney"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6998213546933121}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.08, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.2, 0.9333333333333333, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 0.09090909090909091, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 1.0, 0.8, 0.0, 1.0, 0.8799999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06896551724137931, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2547", "mrqa_newsqa-validation-247", "mrqa_triviaqa-validation-4949", "mrqa_hotpotqa-validation-5485", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-13873", "mrqa_naturalquestions-validation-3802"], "SR": 0.578125, "CSR": 0.5166223404255319, "EFR": 0.9629629629629629, "Overall": 0.7212295606776989}, {"timecode": 47, "before_eval_results": {"predictions": ["a waffles", "H. Ross Perot", "Harry Potter", "a stagecoach", "Serbian", "a fruitcake", "the Charleston", "the Japanese Spaniel", "Siegfried", "taxonomy", "Anne Rice", "Anne Murray", "the lithosphere", "a billionaire", "Lady Jane Gray", "Santeria", "The Blizzardards \"Your Body, Not Your Soul\"", "the Lincoln penny", "Sydney", "an excessively cheerful or optimistic person", "Belarus", "Airplane", "French toast", "the gingerbread", "Swiss Cheese", "the Sex Pistols", "Samuel Johnson", "Agatha Christie", "Jack Dempsey", "a conglomeratus", "a brood", "Thomas Edison", "Mount Everest", "black-eyed pea dip", "Giacomo Puccini", "Battlestar Galactica", "Yugoslavia", "the Surgeon General", "a Place bet", "War and Peace", "Frank Lloyd Wright", "Falcon Crest", "William the Conqueror", "Grant and Sherman", "Adam Smith", "Yeast", "Pearl S. Buck", "Mona Lisa", "Atlanta", "Ayn Rand", "Sisyphus", "2017", "Nathan Hale", "the United Kingdom", "John F. Kennedy", "aureus", "Gettysburg", "Sam Raimi", "1940s and 1950s", "Freddie Jackson", "because the Indians were gathering information about the rebels to give to the Colombian military.", "a monthly allowance,", "ALS6", "Stevie Wonder"], "metric_results": {"EM": 0.609375, "QA-F1": 0.703515625}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, true], "QA-F1": [0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12474", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-16128", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-3768", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-8160", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10067", "mrqa_searchqa-validation-16434", "mrqa_searchqa-validation-11677", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-15591", "mrqa_searchqa-validation-2263", "mrqa_searchqa-validation-15327", "mrqa_naturalquestions-validation-9246", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-5640", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1952"], "SR": 0.609375, "CSR": 0.5185546875, "EFR": 1.0, "Overall": 0.7290234375}, {"timecode": 48, "before_eval_results": {"predictions": ["Argentine composer Lalo Schifrin", "on what became known to locals as `` Black Monday ''", "the 1971 motion picture", "the date has been as early as January 3, and as late as February 12", "1995 Toyota Supra - Brian handed over the keys to Dominic after he crashed his Dodge Charged Prelude for 2 Fast 2 Furious", "John Adams of Massachusetts, Benjamin Franklin of Pennsylvania, Thomas Jefferson of Virginia, Robert R. Livingston of New York, and Roger Sherman of Connecticut", "the Ute name", "the sixth series", "the Sundance Film Festival", "in season two", "the 1970s", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Phosphorus pentoxide", "the Student League for Industrial Democracy ( SLID )", "Duck", "the Philippians", "the Great Plains and U.S. Interior Highlands", "a burden to be carried as penance", "Michael Phelps", "the British", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W", "Ella Mitchell", "1986", "1932", "Sophocles", "The show is broadcast live on BBC One on Saturday evenings", "the American Revolutionary War", "The Naughty Nineties", "the 2009 model year", "Cyrus", "Saint Alphonsa", "Billie Jean King", "the Battle of Antietam", "C\u03bc and C\u03b4", "American singer Daya", "the recommended.", "the Gentiles", "Donna", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "pools campaign contributions from members and donates those funds to campaign for or against candidates, ballot initiatives, or legislation", "in favour of the bill with more than 50 % of the total members of a house, is required per Article 368", "In the eight episode series", "Jacqueline Bouvier, and Patty and Selma Bouvier", "September 19, 2017", "the song has appeared significantly in film and television. `` Boom ( The Crystal Method remix ) '' was included on the remix album Community Service and as a bonus track on the special edition re-release of Satellite available August 27, 2002", "Chelsea", "San Antonio, Texas", "Luther Ingram", "Omar Khayyam", "1984", "Joseph Nye Welch", "Fur Elise", "Puck", "Quatar Holdings", "Adolfo Rodr\u00edguez Sa\u00e1", "Tool", "seven", "a satellite.", "Ali Bongo", "a neck injury", "The Sisters Rosensweig", "Cops", "Vatican City", "Timothy Dalton"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5852690916716277}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.88, 0.25, 0.16, 0.3636363636363636, 0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.8421052631578948, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.8387096774193548, 1.0, 0.16, 0.4, 0.0, 1.0, 0.05714285714285715, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-156", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-74", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3122", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-9281", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-1449", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-1309", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-2167", "mrqa_searchqa-validation-4594"], "SR": 0.40625, "CSR": 0.5162627551020409, "retrieved_ids": ["mrqa_squad-train-82128", "mrqa_squad-train-45103", "mrqa_squad-train-75259", "mrqa_squad-train-58281", "mrqa_squad-train-46725", "mrqa_squad-train-3608", "mrqa_squad-train-67145", "mrqa_squad-train-51076", "mrqa_squad-train-48954", "mrqa_squad-train-17124", "mrqa_squad-train-59130", "mrqa_squad-train-6178", "mrqa_squad-train-82418", "mrqa_squad-train-37565", "mrqa_squad-train-72550", "mrqa_squad-train-19493", "mrqa_squad-validation-6029", "mrqa_hotpotqa-validation-2327", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-538", "mrqa_naturalquestions-validation-3651", "mrqa_newsqa-validation-3191", "mrqa_squad-validation-2987", "mrqa_hotpotqa-validation-4753", "mrqa_searchqa-validation-13176", "mrqa_newsqa-validation-2842", "mrqa_naturalquestions-validation-7464", "mrqa_triviaqa-validation-4376", "mrqa_naturalquestions-validation-1864", "mrqa_searchqa-validation-3386"], "EFR": 0.9736842105263158, "Overall": 0.7233018931256713}, {"timecode": 49, "before_eval_results": {"predictions": ["Louisa May Alcott", "Davy Crockett", "Sugar Ray", "Suspicious Minds", "Jupiter", "fmina", "the Wise Men", "Ladies Professional Golf Association", "Henry VIII", "Corpulent", "Kate Chopin", "Copacabana", "Krakow", "Daredevil", "Algiers", "emerald", "d'Allevo DOP", "Mauna Loa", "John Lennon", "Macy\\'s", "Sonny", "Fred Claus", "the Philosopher\\'s Stone", "the Kremlin", "\"The Raven\"", "New York Cosmos", "Richard Feynman", "John Grisham", "Positron emission tomography", "Catherine the Great", "Eisenhower", "Eleanor Roosevelt", "rheumatoid arthritis", "the Hermitage Museum", "Crispix", "Gabriela Sabatini", "St. Louis", "Yesterday Henry loved me", "the Caspian tern", "the Beagle", "John Wayne", "the Wing-T", "Luzon", "Henry Hudson", "a diamond", "Roger Brooke Taney", "the United Nations", "high crimes", "The Unbearable Lightness of Being", "the Appian Way", "Joseph", "`` Nearer, My God, to Thee ''", "marriage officiant, solemniser, or `` vow master ''", "1 October 2006", "Wikipedia", "Kenny Everett", "Eminem", "Jack White", "Isabella II", "villanelle", "Revolutionary Armed Forces of Colombia", "Oaxacan countryside of southern Mexico", "$5.5 billion", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.6469178391053392}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.9090909090909091, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6521", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-9168", "mrqa_searchqa-validation-14716", "mrqa_searchqa-validation-9072", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-1620", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-2632", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-6765", "mrqa_searchqa-validation-12896", "mrqa_searchqa-validation-14906", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-1953", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-2231", "mrqa_naturalquestions-validation-8217", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3761"], "SR": 0.546875, "CSR": 0.516875, "EFR": 1.0, "Overall": 0.7286874999999999}, {"timecode": 50, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2302", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2458", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-870", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3062", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3757", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-9778", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2470", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10752", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-10981", "mrqa_searchqa-validation-11232", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-12474", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15203", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3295", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4567", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6765", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7287", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8176", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-9803", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1540", "mrqa_squad-validation-1586", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2523", "mrqa_squad-validation-2640", "mrqa_squad-validation-270", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3770", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4546", "mrqa_squad-validation-4671", "mrqa_squad-validation-4750", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-6029", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6630", "mrqa_squad-validation-7023", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8066", "mrqa_squad-validation-8209", "mrqa_squad-validation-8253", "mrqa_squad-validation-8386", "mrqa_squad-validation-8488", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9819", "mrqa_squad-validation-9868", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-4949", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-895"], "OKR": 0.828125, "KG": 0.4578125, "before_eval_results": {"predictions": ["Russia offered money or other discreet aid for the effort if it could be made available,", "recall notices", "American", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "0-0", "President Obama", "Adam Lambert and Kris Allen,", "Arabic, French and English", "Saturday", "snakes -- and one snake in particular.", "a skilled hacker could disrupt the system and cause a blackout.", "in a Starbucks this summer.", "a violent government crackdown seeped out.\"", "the former Massachusetts governor in an ad Sunday in Iowa's The Des Moines Register newspaper.", "Amanda Knox's parents,", "warning to management.", "her fianc\u00e9,", "\"His treatment met the legal definition of torture. And that's why I did not refer the case\" for prosecution.", "a woman", "Haiti.", "228", "to Nazi Germany", "\"The oceans are kind of the last frontier for use and development,\"", "J.Crew outfits", "India", "2009", "President Obama", "At least 38", "death squad killings carried out during his rule in the 1990s.", "Illinois Reform Commission", "10", "Dr. Maria Siemionow,", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "her boyfriend,", "flying", "14", "Britain. He holds a Saudi passport.", "Dogpatch Labs", "two", "\"The situation is pretty much resolved,\"", "16", "\"bystander effect\":", "Ewan McGregor", "hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "\"Empire of the Sun,\"", "the United States", "June 2002", "President Mohamed Anwar al-Sadat", "At least 15", "three", "London's 20,000-capacity O2 Arena.\"", "a continent", "the inner core", "a turducken", "Bonnie and Clyde", "WeaponsMan", "a latte", "wine", "Belgian", "Guthred", "a soothsayer", "Coca-Cola (3) (1962)", "The Waldorf Astoria New York", "Fringillidae"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5723977913533835}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false], "QA-F1": [0.9333333333333333, 0.0, 1.0, 0.5263157894736842, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.4, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8571428571428571, 0.22222222222222224, 0.8, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-3228", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-3629", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-3651", "mrqa_naturalquestions-validation-10209", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-1389", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-471", "mrqa_searchqa-validation-7167", "mrqa_searchqa-validation-15063", "mrqa_triviaqa-validation-4377"], "SR": 0.453125, "CSR": 0.515625, "EFR": 1.0, "Overall": 0.7032812500000001}, {"timecode": 51, "before_eval_results": {"predictions": ["Thursday night,", "a judge to order the pop star's estate", "\"Red Lines,\"", "tax credits", "Herman Thomas", "hawks of tiny pine beetles are munching away at Colorado's forests,", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "2,000 euros ($2,963)", "Christopher Savoie", "a song about freedom of speech", "eight-week", "\"Oprah is an angel, she is God-sent,\"", "D.J. Knight of Pearlman, Texas,", "CNN.com", "January", "to protect the tourist industry,", "A member of the group dubbed the \"Jena 6\"", "in Japan", "Venus Williams", "two", "Monday", "Seoul", "swimsuit", "collaborating with the Colombian government,", "she also believed police were trying to cover up the truth behind her daughter's murder,", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "in an artificial coma", "Pakistan's", "Charles Lock", "\"This is not something that anybody can reasonably anticipate,\"", "Somali-based", "gun charges", "\"Hairspray,\"", "more than 20 times", "bipartisan", "helping on the sandbags", "to sniff out cell phones.", "2,000", "41,", "a \" happy ending\" to the case.", "Dharamsala, India.", "Liza Murphy,", "Arsene Wenger", "four months ago", "India", "that Birnbaum had resigned \"on her own terms and own volition.\"", "hundreds", "photos", "Arizona", "raping and killing a 14-year-old Iraqi girl.", "from Spain to the Caribbean", "`` A.R.M. ''", "Warren Hastings", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "bactrian", "pork\\'s frying", "vanilla", "a creek", "2007", "Aldosterone", "Amtrak Vacations", "vice presidential running mate", "Animal Crackers", "the Ethiopian Eastern Highlands"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5523613972832724}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.7999999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7692307692307693, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1188", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-2985", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-784", "mrqa_newsqa-validation-475", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1488", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-397", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-7781", "mrqa_triviaqa-validation-4912"], "SR": 0.46875, "CSR": 0.5147235576923077, "retrieved_ids": ["mrqa_squad-train-34988", "mrqa_squad-train-9595", "mrqa_squad-train-65396", "mrqa_squad-train-43222", "mrqa_squad-train-64595", "mrqa_squad-train-4764", "mrqa_squad-train-35529", "mrqa_squad-train-33609", "mrqa_squad-train-54967", "mrqa_squad-train-58280", "mrqa_squad-train-2486", "mrqa_squad-train-69030", "mrqa_squad-train-43793", "mrqa_squad-train-67707", "mrqa_squad-train-61385", "mrqa_squad-train-77113", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1537", "mrqa_hotpotqa-validation-4201", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-11558", "mrqa_newsqa-validation-971", "mrqa_naturalquestions-validation-5865", "mrqa_triviaqa-validation-3350", "mrqa_searchqa-validation-3863", "mrqa_naturalquestions-validation-6597", "mrqa_squad-validation-7635", "mrqa_searchqa-validation-11313", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2705"], "EFR": 1.0, "Overall": 0.7031009615384616}, {"timecode": 52, "before_eval_results": {"predictions": ["Pope Paul Biya", "mild to moderate depression", "Summer", "reverse the Taliban's momentum and stabilize the country's government.\"", "VBS.TV", "the children of street cleaners and firefighters.", "Switzerland", "2009", "Tim Clark, Matt Kuchar and Bubba Watson", "Araceli Valencia,", "400 farmers", "Friday,", "Lisa Brown", "President Robert Mugabe", "one of five Lebanese prisoners", "planned attacks", "\" Teen Patti\"", "Seasons of My Heart", "an angry mob.", "improve the environment", "Jared Polis", "dancing against a stripper's pole.", "bragging about his sex life on television", "\"Three Little Beers,\"", "12.3 million people worldwide", "the 6.2-mile Moffat Tunnel,", "English", "Arkansas", "Millvina Dean,", "a pure meritocracy,", "Bill Haas", "the war of words in the Republican Party centered around Rush Limbaugh.", "Tuesday in Los Angeles.", "after giving birth to baby daughter Jada,", "co-writing credits", "sailing", "Jeffrey Jamaleldine", "2.5 million", "the state's first lady,", "Somali", "NATO fighters", "U.S. Vice President Dick Cheney", "5:20 p.m.", "Former Beatles", "The Rosie Show", "remote highway in Michoacan state,", "helicopters and unmanned aerial vehicles", "a \"prostitute\"", "Hanin Zoabi", "two", "April 2010.", "1985", "1599", "`` Seeing Red ''", "shoji screen", "PPTH", "the different levels of importance of human psychological and physical needs", "the Dutch Empire", "1692", "Cushman", "a bobtail squid", "past", "Whitewater Development Corporation", "\"WHO\": Steve Cauthen & Kent Descormeaux"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6670472756410256}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, false], "QA-F1": [0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.33333333333333337, 0.4, 0.6666666666666666, 1.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1590", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-6545", "mrqa_triviaqa-validation-934", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-2536", "mrqa_searchqa-validation-14214", "mrqa_searchqa-validation-9278", "mrqa_searchqa-validation-4725", "mrqa_searchqa-validation-5273"], "SR": 0.578125, "CSR": 0.5159198113207547, "EFR": 1.0, "Overall": 0.703340212264151}, {"timecode": 53, "before_eval_results": {"predictions": ["anvil firing", "Abigail", "Encore Las Vegas", "David Jolly", "9 November 1955", "Ch\u014dfu, Tokyo, Japan", "Adrian Peter McLaren", "Anthony John Herrera", "2008\u201309", "Lev Ivanovich Yashin", "Lu\u00eds Carlos Almeida da Cunha,", "Mot\u00f6rhead", "Solace", "1943", "Vivendi S.A.", "\"The Heavens Over Berlin\"", "May 27, 2016", "the Orange Bowl", "Kristoffer Kristofferson", "Hudson Bay Mining and Smelting Company", "Cherokee County", "9.58 seconds", "Acela Express", "5249", "Johnnie Ray", "Harry Booth", "John McClane", "James Packer", "black nationalism", "USC Marshall School of Business", "A/J Jackson", "\"You're Next\"", "its eclectic mix of musical styles", "Christian", "Fife", "Peter 'Drago' Sell,", "Marktown", "Hidden America with Jonah Ray", "Bangkok", "Gregg Popovich", "Roy Spencer", "Chicago", "constant support from propaganda campaigns", "2012", "torpedoes", "Saint Paul", "1501", "the Man Booker Prize", "Wayne Rooney", "Nic Cester", "Kim Carnes", "a convergent plate boundary", "the 2013 -- 14 television season", "1947", "the Chief Whips", "bifrost", "Manchester", "Sri Lanka's", "Felipe Massa.", "\"It seemed to be kind of laid-back -- it didn't seem to be that dangerous,\"", "Admiral Hyman Rickover", "\"In a Nutshell\"", "Vestal Virgins", "Aaron Lewis"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6593513257575757}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.6666666666666666, 0.0, 1.0, 0.18181818181818182, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-4945", "mrqa_naturalquestions-validation-9330", "mrqa_triviaqa-validation-4355", "mrqa_triviaqa-validation-2833", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-2941", "mrqa_searchqa-validation-127", "mrqa_searchqa-validation-7762"], "SR": 0.578125, "CSR": 0.5170717592592593, "EFR": 0.9259259259259259, "Overall": 0.6887557870370371}, {"timecode": 54, "before_eval_results": {"predictions": ["Betty Crocker", "Frank Sinatra", "khaki", "Frank Sinatra", "Antarctica", "robota", "Easter Island", "the troposphere", "Huntsville, Alabama", "Khrushchev", "Venus", "the retina", "Jordan", "German", "toga", "George Washington", "Columbia River", "Hairspray", "The Prince and the Pauper", "Bangkok", "Nitrogen, as a diatomic molecule", "Ponzi", "Jay Martin", "Islamic Republic", "John Lennon's", "the Spanish Republic", "Three Coins in the Fountain", "George Wallace", "sushi", "in Heaven", "the Byzantine Empire", "Fall Out Boy", "freezing", "silver", "beak", "William Jennings Bryan", "The Blues Brothers", "Ford Madox Ford", "the catcher", "Copenhagen", "flower", "blues", "Flight of the Bumblebee", "zero", "Panama Canal", "Dan Marino", "Crustaceans", "Hestia", "Band of Brothers", "France", "this nation", "Profit maximization happens when marginal cost is equal to marginal revenue", "Havana Harbor", "2017 - 12 - 10 )", "Thai", "Heather Stanning and Helen Glover", "redheads", "April 1, 1949", "Lantern Waste", "glam-style funk metal", "Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "Abhisit Vejjajiva", "prisoners at the South Dakota State Penitentiary", "1989"], "metric_results": {"EM": 0.625, "QA-F1": 0.7084535256410256}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6583", "mrqa_searchqa-validation-4968", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-7142", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-15081", "mrqa_searchqa-validation-16507", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-4922", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-12780", "mrqa_searchqa-validation-7435", "mrqa_searchqa-validation-10730", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-7321", "mrqa_hotpotqa-validation-495", "mrqa_newsqa-validation-3407"], "SR": 0.625, "CSR": 0.5190340909090909, "retrieved_ids": ["mrqa_squad-train-8767", "mrqa_squad-train-68381", "mrqa_squad-train-49910", "mrqa_squad-train-26770", "mrqa_squad-train-39382", "mrqa_squad-train-33631", "mrqa_squad-train-30034", "mrqa_squad-train-54663", "mrqa_squad-train-11983", "mrqa_squad-train-9676", "mrqa_squad-train-44871", "mrqa_squad-train-57044", "mrqa_squad-train-58888", "mrqa_squad-train-40899", "mrqa_squad-train-57850", "mrqa_squad-train-973", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-2342", "mrqa_searchqa-validation-1986", "mrqa_triviaqa-validation-5563", "mrqa_hotpotqa-validation-4350", "mrqa_newsqa-validation-3231", "mrqa_naturalquestions-validation-9963", "mrqa_hotpotqa-validation-1606", "mrqa_triviaqa-validation-5052", "mrqa_newsqa-validation-2253", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3942", "mrqa_newsqa-validation-1035", "mrqa_triviaqa-validation-7151", "mrqa_searchqa-validation-679"], "EFR": 1.0, "Overall": 0.7039630681818182}, {"timecode": 55, "before_eval_results": {"predictions": ["Transportation Security Administration", "an open window", "helping to plan the September 11, 2001, terror attacks,", "Iran", "1960s", "Janet Napolitano", "legislation Wednesday requiring federal oil industry regulators to wait at least two years after leaving government service before going to work for companies they helped regulate.", "Stanford University,", "\"How I Met Your Mother,\"", "Opryland", "three", "April 2", "Arabic, French and English", "Fernando Gonzalez", "The minister later apologized, telling CNN his comments had been taken out of context. He said the murder of the boss could never be justified.", "10 below", "KBR", "L'Aquila earthquake,", "crocodile eggs", "200", "Sam Raimi,", "The Palm", "82", "1975", "Leo Frank,", "50", "inspector-general of the House of Representatives", "Friday,", "two", "France's famous Louvre museum", "Ashley \"A.J.\" Jewell,", "be silent.", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "two weeks after Black History Month", "Two people", "one of Africa's most stable nations.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "federal officers' bodies", "Hundreds of contraband cell phones were found behind bars or in transit to", "CEO of an engineering and construction company", "$50,000", "a national telephone survey", "\"The oceans are kind of the last frontier for use and development,\"", "Michelle Rounds", "Daryeel Bulasho Guud", "a strict interpretation of the law,", "Monday and Tuesday", "\"A good vegan cupcake has the power to transform everything for the better,\"", "sailing", "246", "Patrick McGoohan,", "provides the public with financial information about a nonprofit organization", "President Gerald Ford", "thicker consistency and a deeper flavour than sauce", "windmill", "France", "Mt Kenya", "R&B", "Donald Duck", "American", "the Edo era", "the Grand Canal", "pennies", "merengue"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7167898478835979}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4444444444444445, 0.9600000000000001, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.8, 1.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.29629629629629634, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.25, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-2666", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-3183", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-2943", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5565", "mrqa_searchqa-validation-2063"], "SR": 0.59375, "CSR": 0.5203683035714286, "EFR": 1.0, "Overall": 0.7042299107142858}, {"timecode": 56, "before_eval_results": {"predictions": ["and aid agencies", "$150 billion", "Kellogg Brown and Root,", "Thursday", "\"Empire of the Sun,\"", "Buenos Aires.", "Baltimore Orioles,", "Sonia Sotomayor", "Herman Cain,", "There's no chance", "15-year-old", "137", "22-year-old", "\"bystander effect\":", "Bryant Purvis", "570 billion pesos ($42 billion)", "Ralph Lauren (Rizzoli)", "will not support the Stop Online Piracy Act,", "Screen Actors Guild Awards,", "Indra Ramasesham, 69, and his 19-year-old son Krishna Rajaram,", "helping to plan the September 11, 2001,", "The pilot,", "$10 billion", "people who have had sex will have one type of HPV at some time in their lives.", "Washington State's", "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Pakistan", "a canyon in the path of the blaze Thursday.", "left the medical engineering company where she worked.", "an antihistamine and an epinephrine auto-injector for emergencies,", "U.S. Holocaust Memorial Museum,", "racial intolerance.", "Jobs,", "Al-Shabaab,", "in an appearance last week in Broward County Circuit Court.", "3-0", "Tuesday", "Taher Nunu", "off the coast of Dubai", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Dr. Jennifer Arnold and husband Bill Klein,", "80 percent of the woman's face", "five", "British Prime Minister Gordon Brown", "Pacific Ocean territory of Guam", "Angola,", "drama", "May 4", "a \"happy ending\" to the case.", "Harry Nicolaides,", "ALS6,", "a judicial officer, of a lower or puisne court, elected or appointed by means of a commission ( letters patent ) to keep the peace", "autompne ( automne in modern French ) or autumpne in Middle English", "Because Binding of a ligand to the extracellular region causes a series of structural rearrangements in the RTK that lead to its enzymatic activation", "hound", "a feisty creature", "Cambodia", "West Point Iron and Cannon Foundry", "February 13, 1946", "Arrowhead Stadium", "Thomas Jefferson", "a refrigerator", "X-Files", "Gov. Eric McWilliams"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7074089513427749}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false], "QA-F1": [0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.8571428571428571, 0.3636363636363636, 1.0, 1.0, 1.0, 0.1, 0.5, 1.0, 0.0, 0.923076923076923, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8000000000000002, 0.7142857142857143, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1764705882352941, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-933", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-4113", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-9271", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-1316", "mrqa_searchqa-validation-7077"], "SR": 0.59375, "CSR": 0.5216557017543859, "EFR": 0.9615384615384616, "Overall": 0.6967950826585695}, {"timecode": 57, "before_eval_results": {"predictions": ["$55.7 million", "led the weekend box office,", "Kurdish militant group in Turkey", "Dr. Maria Siemionow,", "Alwin Landry's", "a city of romance, of incredible architecture and history.", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "free services.", "the number of new cases is falling.", "Amitabh Bachchan", "\"Rent,\" \"Cabaret\" and \" Proof,\"", "Dancy-Power Automotive Group", "closure of Guant Bay prison and CIA \"black site\" prisons,", "anyone wanting to harm them.", "Screen Actors Guild", "11 healthy eggs", "in Salt Lake City, Utah,", "Guinea, Myanmar, Sudan and Venezuela.", "four", "\" Michoacan Family,\"", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus -- and went on a 100-day killing rampage.", "$60 billion on America's infrastructure.", "bullet wounds in the back of a friend's head, seeing friends grabbing their arms, and blood just everywhere.", "Tim Clark, Matt Kuchar and Bubba Watson", "Mark Fields of Ford", "three", "Al Qaeda", "Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia.", "A Colorado prosecutor", "near his home in Peshawar", "U.S.-based venture capital company Polaris Venture Partners,", "California Highway Patrol.", "cross-country skiers", "no one is sure", "HBO series \"The Sopranos,\"", "a one-shot victory in the Bob Hope Classic", "peace with Israel", "Stratfor,", "Adam Yahiye Gadahn,", "an antihistamine and an epinephrine auto-injector", "EU naval force", "hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Deputy Treasury Secretary", "heavy turbulence", "recording artist", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "Emmy-winning Patrick McGoohan,", "Henrik Stenson", "Three", "Dore Gold, former Israeli ambassador to the United Nations", "using recreational drugs", "Roger Federer", "Iran", "December 2, 2013", "Hayes", "overprotective clownfish", "Morgan Spurlock", "football", "\"War & Peace\"", "Eugene Levy", "Floyd Patterson", "Jan & Dean", "Sweeney Todd: The Demon Barber of Fleet Street", "bass"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5997388251983841}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.5, 0.0, 0.0, 1.0, 0.0, 0.9411764705882353, 0.8918918918918919, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.3333333333333333, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4444444444444445, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-1509", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-1270", "mrqa_naturalquestions-validation-2170", "mrqa_triviaqa-validation-6757", "mrqa_hotpotqa-validation-3321", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-10144"], "SR": 0.453125, "CSR": 0.5204741379310345, "retrieved_ids": ["mrqa_squad-train-24368", "mrqa_squad-train-36587", "mrqa_squad-train-85847", "mrqa_squad-train-32664", "mrqa_squad-train-32931", "mrqa_squad-train-10858", "mrqa_squad-train-48904", "mrqa_squad-train-5963", "mrqa_squad-train-32932", "mrqa_squad-train-7467", "mrqa_squad-train-30669", "mrqa_squad-train-68095", "mrqa_squad-train-40713", "mrqa_squad-train-49553", "mrqa_squad-train-24586", "mrqa_squad-train-31578", "mrqa_newsqa-validation-2791", "mrqa_squad-validation-8901", "mrqa_triviaqa-validation-4831", "mrqa_newsqa-validation-3779", "mrqa_searchqa-validation-14771", "mrqa_newsqa-validation-733", "mrqa_naturalquestions-validation-6849", "mrqa_newsqa-validation-1147", "mrqa_squad-validation-7635", "mrqa_newsqa-validation-2959", "mrqa_naturalquestions-validation-3783", "mrqa_searchqa-validation-12184", "mrqa_hotpotqa-validation-2725", "mrqa_triviaqa-validation-3569", "mrqa_newsqa-validation-1904", "mrqa_hotpotqa-validation-4969"], "EFR": 0.9714285714285714, "Overall": 0.6985367918719212}, {"timecode": 58, "before_eval_results": {"predictions": ["at a depth of about 1,300 meters in the Mediterranean Sea.", "three machine guns and two silencers", "the 3rd District of Utah.", "Kenneth Cole", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "President Bush", "Stephen Tyrone Johns", "Transport Workers Union leaders", "eight Indian army troopers, including one officer, and 17 militants,", "23-year-old", "prisoners at the South Dakota State Penitentiary", "has been fighting to impose its interpretation of Islamic law, or sharia, on Somalia.", "The food, music, culture and language of Latin America", "Workers' Party.", "is fighting an unjust war for an America that went too far when it invaded Iraq", "Pope", "boyhood experience in a World War II internment camp", "Secretary of State", "Mother's", "the home of NFL's Chargers", "opium poppies", "1979", "Alexandre Caizergues, of France,", "sportswear,", "military trial system", "Paul McCartney and Ringo Starr", "many years of extensive technical research.", "iTunes,", "Indonesian", "eight-week", "order", "Natalie Cole", "Somali-based", "folding table", "The Obama administration", "Ventures", "British", "Long Island", "former Procol Harum bandmate Gary Brooker", "Former Mobile County Circuit Judge Herman Thomas", "Sen. Barack Obama", "July 23.", "suicides", "The finding of the body might still be alive,", "Sunday.", "a preliminary injunction against a Mississippi school district and high school in federal court Tuesday over the April 2 prom.", "105-year", "helping consumers move beyond these hard times", "a man's lifeless, naked body", "iTunes,", "eight", "summer of 1990", "Cheitharol Kummaba", "Ricky Nelson", "Cream", "Portugal,", "sphinx", "John Wilkes Booth", "Charles Nungesser", "The United States of America (USA),", "cherry almond cake", "often", "bananas", "Leonard Bernstein"], "metric_results": {"EM": 0.5, "QA-F1": 0.6193073062558356}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6428571428571428, 0.0, 0.7000000000000001, 1.0, 0.5, 1.0, 0.6, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.888888888888889, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2135", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-2617", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4798", "mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-3070", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-722", "mrqa_searchqa-validation-14467", "mrqa_searchqa-validation-7269"], "SR": 0.5, "CSR": 0.5201271186440678, "EFR": 0.96875, "Overall": 0.6979316737288136}, {"timecode": 59, "before_eval_results": {"predictions": ["Supplemental oxygen", "John Smith", "McFerrin, Robin Williams, and Bill Irwin", "7th century", "March 6, 2018", "multiple", "Egypt", "Alan Tudyk", "United States military", "the 1840s", "the 1940s", "1985", "helps scientists better understand the spread of pollution around the globe", "electors", "The resulting molecule, now mature insulin, is stored as a hexamer in secretory vesicles", "the Holy See", "Carpenter", "The current Secretary of Homeland Security is Kirstjen Nielsen", "$1.84 billion", "8", "crossbar", "February 7, 2018", "White Sox", "`` Reveille ''", "the United States emerged from World War II as a global superpower", "Tara / Ghost of Christmas Past", "Stephen Foster", "1975", "6 -- 14 July", "majority of members present at that time", "off the northeast coast of Australia", "`` Blood is the New Black ''", "the continent of Antarctica", "the courts", "the primal rib", "in positions Arg15 - Ile16", "Steve Valentine", "a political ideology", "Anakin Skywalker", "American singer Lesley Gore", "Tom Hanks", "\u01c3ke e : \u01c0xarra", "peninsular", "Peter Cetera", "16 best - selling religious novels", "Beorn", "Johnny Cash", "pre-Columbian times", "human induced greenhouse warming", "Wally Palmar, Mike Skill and Jimmy Marinos", "Gorakhpur Junction", "Maarten Tromp", "sternum", "horseracing", "good character", "travel", "\"Nebo Zovyot\"", "Caster Semenya", "Berga", "North Korean Foreign Ministry spokesman described U.S. Vice President Dick Cheney as a \"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "Nellie Bly", "Red Adair", "faith", "a mixed drink"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5614430708180709}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.5, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.23076923076923078, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-9881", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-255", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8093", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-2280", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-2405", "mrqa_searchqa-validation-8408", "mrqa_searchqa-validation-3666"], "SR": 0.484375, "CSR": 0.51953125, "EFR": 0.9696969696969697, "Overall": 0.6980018939393939}, {"timecode": 60, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2302", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-2850", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-3485", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-870", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3062", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-9281", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-9778", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1590", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1678", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1708", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3867", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10752", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12763", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14969", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16642", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2791", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3295", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4219", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4922", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7317", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-9176", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1540", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2523", "mrqa_squad-validation-270", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4546", "mrqa_squad-validation-4750", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-6029", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6630", "mrqa_squad-validation-7023", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7531", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-8066", "mrqa_squad-validation-8209", "mrqa_squad-validation-8386", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8931", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9868", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4371", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7683"], "OKR": 0.830078125, "KG": 0.48515625, "before_eval_results": {"predictions": ["hydrogen", "Sax Rohmer", "Nicole Brown Simpson", "Castro and the Cuban Revolution", "Barbara Mandrell", "carry On Cleo", "St. Barnabus", "iodine deficiency disorders", "C\u00f4te d'Or", "Wisconsin", "Damson plums", "\"perfect\" upper-middle-class", "Wonga", "Secretary of State William H. Seward", "\u201ccryosleep\u201d", "The World is Not Enough", "Barnaby Rudge", "James Chadwick", "Gary Barlow", "a meteoroid", "Mary Quant", "California", "cesium", "Picasso", "brain production of the abnormal protein beta-amyloid,", "piano", "King George I", "whipped cream", "index fingers", "earache", "90%", "muezzin", "the moon", "watts", "Alan Ladd", "soy", "Virginia", "Darwin", "PJ Harvey", "Jim Jones", "decathlon", "Runcorn", "Three Worlds", "Amnesty International", "Maxwell Scherrer Cabelino Andrade", "Naomi Watts", "Arthur Miller", "Carousel", "samuel vixen", "potassium", "Athens", "headdresses", "Janie Crawford", "Julia Roberts", "the Kenya-Uganda Railway", "The Seduction of Hillary Rodham", "Apsley George Benet Cherry-Garrard", "\"A Bug's Life,\"", "86", "allegations that a dorm parent mistreated students at the school.", "a daiquiri", "apples", "Morocco", "the nerves and ganglia outside the brain and spinal cord"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6020833333333333}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3345", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-4827", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4497", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-6726", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-2637", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-6285", "mrqa_hotpotqa-validation-3308", "mrqa_newsqa-validation-2618"], "SR": 0.5625, "CSR": 0.5202356557377049, "retrieved_ids": ["mrqa_squad-train-56879", "mrqa_squad-train-56443", "mrqa_squad-train-6770", "mrqa_squad-train-48458", "mrqa_squad-train-28444", "mrqa_squad-train-27084", "mrqa_squad-train-68008", "mrqa_squad-train-27902", "mrqa_squad-train-36905", "mrqa_squad-train-67133", "mrqa_squad-train-13007", "mrqa_squad-train-78400", "mrqa_squad-train-46590", "mrqa_squad-train-46135", "mrqa_squad-train-58133", "mrqa_squad-train-56579", "mrqa_newsqa-validation-2950", "mrqa_hotpotqa-validation-5606", "mrqa_newsqa-validation-2205", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6876", "mrqa_squad-validation-2315", "mrqa_newsqa-validation-1832", "mrqa_naturalquestions-validation-8429", "mrqa_newsqa-validation-1277", "mrqa_searchqa-validation-12267", "mrqa_newsqa-validation-2897", "mrqa_naturalquestions-validation-9650", "mrqa_squad-validation-3492", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-3177", "mrqa_searchqa-validation-8285"], "EFR": 1.0, "Overall": 0.715531506147541}, {"timecode": 61, "before_eval_results": {"predictions": ["a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal", "Mugabe and Tsvangirai", "\"a violent and brutal extremist group", "Samuel Herr", "checkposts and military camps in the Mohmand agency,", "Cannes Film Festival,", "customers are lining up for vitamin injections that promise to improve health and beauty.", "Caster Semenya", "Andrew Morris,", "Alberto Espinoza Barron,", "22 million", "41,280", "Casey Anthony,", "18", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "Phoenix, Arizona,", "\"The Adventures of Superman\"", "\"A Lion Among Men,\"", "between 1917 and 1924", "Mississippi", "$3 billion,", "Steve Williams", "school in South Africa", "Dr. Jennifer Arnold and husband Bill Klein,", "\"Up,\"", "\"The Real Housewives of Atlanta\"", "Bollywood", "Wigan Athletic", "40-year-old", "1,500 Marines", "California-based Current TV", "civilians,", "Michelle Obama", "Adriano", "returning combat veterans", "Kit of Elsinore", "\"The first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.", "Phay Siphan, secretary of the Cambodian Council of Ministers.", "staff sergeant", "nine", "Jason Voorhees", "George Washington", "chairman of the House Budget Committee", "Kenyan", "Hamburg.", "discusses his roots as he castigates U.S. policies and deplores Israel's offensive in Gaza that started in late December 2008 and continued into January.", "HSH Nordbank Arena", "Israel", "thriller writer", "1994", "the Carrousel du Louvre,", "Prem Lata Agarwal", "Ms. Stout", "Terry Kath", "charles", "William Shakespeare", "Nicolas Sarkozy", "Melanie Owen", "Thor", "Queenston Delta", "George III", "Taj Mahal", "colon", "Larry Storch"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5845456309994352}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false], "QA-F1": [0.5454545454545454, 1.0, 0.0, 1.0, 0.0, 1.0, 0.782608695652174, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.23999999999999996, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.0, 0.8, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-702", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1929", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-3139", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1704", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-2955", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-400", "mrqa_triviaqa-validation-4714", "mrqa_hotpotqa-validation-4692", "mrqa_naturalquestions-validation-9545"], "SR": 0.515625, "CSR": 0.5201612903225806, "EFR": 0.9032258064516129, "Overall": 0.6961617943548387}, {"timecode": 62, "before_eval_results": {"predictions": ["500 feet down an embankment", "\"We hold the group and its leader fully responsible for what is happening in Gaza, and we offer our condolences to everyone who was killed during the clashes,\"", "1960s song \"A Whiter Shade of Pale\"", "Ciudad Juarez,", "Kenneth Cole", "protective shoes", "David McKenzie", "$8.8 million", "\"I'm just getting started.\"", "Saturday,", "\"brain hacking\"", "14", "planning processes are urgently needed", "Mandi Hamlin", "Malcolm X", "for financial gain,", "a face-to-face interview with the president", "11 people", "Steve Jobs", "a head injury.", "Daniel Radcliffe", "Tillakaratne Dilshan", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "a construction site in the heart of Los Angeles.", "Jaipur", "Al-Aqsa mosque", "Ameneh Bahrami", "criminals", "whether to close some entrances, bring in additional officers, and make security more visible,\"", "\"We clearly believe that no option should be removed from the table and this is our policy, but we cannot dictate it to anyone.\"", "Transportation Security Administration", "E. coli", "Three French journalists, a seven-member Spanish flight crew and one Belgian", "U.N. Secretary of State Hillary Clinton", "100 percent", "Brian Mabry", "150", "\"Watchmen\"", "autonomy.", "\"She was focused so much on learning that she didn't notice,\"", "and renewable energy at home everyday,\"", "two", "responsibility for the abductions.", "Brian David Mitchell,", "Omar bin Laden", "as part of its 18-month journey around the world.", "Italian Serie A title", "an independent homeland since 1983.", "Tutsis and moderate Hutus", "They are co-chairs of the Genocide Prevention Task Force.", "\"We were petitioned and have been looking into it for the past two years,\"", "William Shakespeare's As You Like It", "DeWayne Warren", "1954", "jackstones", "a fool", "gemini", "designated hitter", "\"Principle-Centered Leadership\"", "11 June 1959", "Plutarch", "Buddhist", "cab", "seven nights a week"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6449771582584083}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.12121212121212123, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8571428571428571, 0.0, 0.14285714285714288, 0.7692307692307692, 1.0, 0.4, 0.0, 0.0, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3617", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-6383", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-6505", "mrqa_hotpotqa-validation-2543", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-3266", "mrqa_searchqa-validation-16570"], "SR": 0.546875, "CSR": 0.5205853174603174, "EFR": 0.9655172413793104, "Overall": 0.7087048867679255}, {"timecode": 63, "before_eval_results": {"predictions": ["Araceli Valencia,", "\"He knows what happened at the pool that day,\"", "President Obama and Britain's Prince Charles", "the insurgency,", "Human Rights Watch.", "Genocide Prevention Task Force", "Britain's", "five female pastors", "Bob Bogle,", "Sri Lanka", "82", "Fullerton, California,", "$249", "Zulfikar Ali Bhutto,", "eight", "ClimateCare,", "Best Picture winner", "President Obama and Britain's Prince Charles", "public opinion in Turkey.\"", "high-ranking drug cartel member Arnoldo Rueda Medina.", "left his indelible fingerprints on the entertainment industry.", "murder in the beating death of a company boss who fired them.", "\"It feels good for me to talk about her,\"", "\"political and religious\"", "Siri", "Derek Mears", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Heshmat Tehran Attarzadeh", "the mammoth's skull,", "because the Indians were gathering information about the rebels to give to the Colombian military.", "--the Louvre.", "Mubarak,", "her boyfriend, Dodi Fayed, and their driver, Henri Paul.", "revelry", "Human Rights Watch", "45 minutes, five days a week.", "February 12", "1959,", "London", "\"illegitimate.\"", "the BBC's central London offices", "Russia", "industrialized nations", "Zuma --", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "to sniff out cell phones.", "shock, quickly followed by speculation about what was going to happen next,\"", "U.S. Food and Drug Administration", "$1.5 million.", "former Pakistani Prime Minister Nawaz Sharif", "raping and killing a 14-year-old Iraqi girl.", "in 1837", "the raconteur ( Austin Winkler ) and his former lover ( Emmanuelle Chriqui )", "the following year", "Reel Life:", "bluebells", "FIFA World Cup 2010", "General Allenby", "turns out to be a terrible", "The Treaty of Gandamak", "England", "juliuss efface stanze vasaris", "beak", "Candide"], "metric_results": {"EM": 0.5, "QA-F1": 0.6009392067477097}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 0.26666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.08, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.125, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5581395348837209, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.25, 0.8333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-1462", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3401", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-1098", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-3482", "mrqa_naturalquestions-validation-1912", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-3824", "mrqa_hotpotqa-validation-5720", "mrqa_hotpotqa-validation-4086", "mrqa_searchqa-validation-1971"], "SR": 0.5, "CSR": 0.520263671875, "retrieved_ids": ["mrqa_squad-train-8311", "mrqa_squad-train-80261", "mrqa_squad-train-83943", "mrqa_squad-train-54627", "mrqa_squad-train-46555", "mrqa_squad-train-5448", "mrqa_squad-train-5052", "mrqa_squad-train-70195", "mrqa_squad-train-67460", "mrqa_squad-train-69270", "mrqa_squad-train-2622", "mrqa_squad-train-51417", "mrqa_squad-train-54040", "mrqa_squad-train-6210", "mrqa_squad-train-39209", "mrqa_squad-train-81593", "mrqa_hotpotqa-validation-2728", "mrqa_triviaqa-validation-4137", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-3352", "mrqa_triviaqa-validation-1921", "mrqa_newsqa-validation-3574", "mrqa_naturalquestions-validation-3052", "mrqa_triviaqa-validation-1922", "mrqa_hotpotqa-validation-2770", "mrqa_naturalquestions-validation-4432", "mrqa_hotpotqa-validation-2785", "mrqa_newsqa-validation-1128", "mrqa_searchqa-validation-16258", "mrqa_newsqa-validation-3022", "mrqa_triviaqa-validation-6659", "mrqa_newsqa-validation-2100"], "EFR": 0.96875, "Overall": 0.7092871093750001}, {"timecode": 64, "before_eval_results": {"predictions": ["Wolfgang Amadeus Mozart", "Denmark\u2013Norway, Brandenburg and Sweden", "John Schlesinger", "business", "\"The Suite Life of Zach & Cody\"", "Two Is Better Than One", "Karl-Anthony Towns", "Omega SA", "9 November 1967", "the designated hitter rule", "Jay Park", "Wayne County, Michigan", "Hong Kong, New York City, London, Taipei, China, Bangkok and Singapore", "Cleopatra VII Philopator", "8,211", "The Allies of World War I, or Entente Powers", "August Heckscher", "Orange County, Florida, United States", "Gareth Barry", "capital crimes or capital offences", "Ned Flanders", "Westley Sissel Unseld", "Ken Howard", "Fat Albert", "Thomas Joseph \"T. J.\" Lavin", "15", "Germany", "I-League club Salgaocar", "fudge", "1887", "Saturday", "January 2001", "Sony Computer Entertainment", "England", "Ryukyuan people", "Charles White Whittlesey", "fennec fox", "from 1993 to 1996", "Yunnan-Fu", "Port Moresby, Papua New Guinea", "Macau Peninsula, Macau", "1993", "The Bangor Daily News is an American newspaper covering a large portion of rural Maine, published six days per week in Bangor, Maine", "New Mexico's", "Territory of Hawaii", "William Finn", "ZZ Top", "the group focuses on homosexuality, gay sex, and the gay bear subculture", "The Tales of Hoffmann", "Shakespeare criticism", "2017", "1984", "Afghanistan", "Michael Crawford", "parallelogram", "Operation Frequent Wind", "pakistan", "toxic smoke from burn pits", "15,000", "breast cancer.", "Troy", "Casablanca", "Vilna", "Asaph Hall"], "metric_results": {"EM": 0.5, "QA-F1": 0.621710451007326}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8000000000000002, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.39999999999999997, 0.0, 0.4, 0.7692307692307693, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.08333333333333334, 0.0, 0.5, 1.0, 0.0, 0.125, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-4702", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2383", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-589", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-1776", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-1394", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5183", "mrqa_naturalquestions-validation-75", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-305", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-4147", "mrqa_searchqa-validation-7441", "mrqa_searchqa-validation-4036"], "SR": 0.5, "CSR": 0.5199519230769231, "EFR": 1.0, "Overall": 0.7154747596153846}, {"timecode": 65, "before_eval_results": {"predictions": ["10 October 2010", "45th Vice President of the United States", "Claudio Javier L\u00f3pez", "Flamingo Hotel in Las Vegas, Nevada", "Swiss Confederation", "Double Crossed", "John McClane", "Dan Brandon Bilzerian", "Philadelphia Naval Shipyard", "The Pentagon", "1958", "our greatest comedienne - Australia's Lucille Ball", "Canadian", "Seventeen", "spot-fixing", "The Marshall Mathers LP 2", "Easter Rising of 1916", "Spain, Mexico and France", "Juilliard School", "Mark \"Chopper\" Read", "the small forward position", "CSU", "Seminole and Miccosukee", "Erreway", "Eric Liddell", "Sam Kinison", "Spring city", "Bob Hurley", "#24th", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "9,984", "Southern Progress Corporation", "the Merrimack people", "1966", "Flamingo Las Vegas", "The Soloist", "1st Earl Grosvenor", "Labour Party", "Walt Disney", "1983", "A stolperstein", "heavy metal fan", "The BMW E70", "Cleveland Cavaliers", "Larnelle Steward Harris", "May 5, 2015", "Bank of China Tower", "general secretary of the Norwegian Anthroposophical Society", "Skipton", "New Orleans, Louisiana", "eight", "IB Middle Years Program", "Mark Jackson", "1991", "Antigua and Barbuda", "cutthroat", "Montpelier", "severe", "African National Congress Deputy President Kgalema Motlanthe,", "seven", "Portugal", "Splice", "David E. Kelley", "the 2009 model year"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6472826479076479}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.5, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.15999999999999998, 1.0, 1.0, 0.5, 1.0, 0.4444444444444444, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5, 1.0, 0.8, 0.0, 0.0, 0.5714285714285715, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1135", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-776", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-3536", "mrqa_hotpotqa-validation-5258", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-5718", "mrqa_hotpotqa-validation-1123", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-695", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4057", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-16601"], "SR": 0.515625, "CSR": 0.5198863636363636, "EFR": 1.0, "Overall": 0.7154616477272728}, {"timecode": 66, "before_eval_results": {"predictions": ["October 25, 1881", "Harold Holt", "January 31, 1993", "Despicable Me 3", "Bill Clinton", "1,800", "Frederick I", "Ben Elton", "Juventus Football Club", "November 6, 2018", "four", "the fifth level", "five", "Macau, China", "American black bear", "from 1345 to 1377", "sandstone", "May 4, 2004", "The Walt Disney Company", "rickyard", "Harlem neighborhood of New York City", "1999", "Parapsychologist", "2014", "Greg Gorman and Helmut Newton", "Prince Ioann Konstantinovich", "2015", "The Winecoff Hotel fire", "B-17 Flying Fortress bomber", "Bit Instant", "Hawaii", "The Emperor of Japan", "the Ruul", "Dana Fox", "remix", "Issaquah", "Ghana's", "Scott Carson", "John Snow", "Manchester, England", "The Tonight Show", "Estado Libre y Soberano de Tamaulipas", "boundary river", "\"Naked\"", "member of the characters in Jordan Mechner's game \" Prince of Persia\"", "Queen Victoria", "Las Vegas", "Cleveland, Ohio", "Zachary Levi", "AVN Adult Entertainment Expo", "Boyd Gaming", "Hirschman", "The Romantics", "the brain, muscles, and liver", "Neville Chamberlain", "sixth Wimbledon", "Czech Republic", "18", "dual nationality", "Cpl. Richard Findley,", "rain", "46,000 tons", "Neon", "customers are lining up for vitamin injections that promise"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6071924603174603}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.8, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-3578", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-5002", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-1676", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-3126", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-780", "mrqa_hotpotqa-validation-4117", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-3419", "mrqa_hotpotqa-validation-2306", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-95", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-2698"], "SR": 0.53125, "CSR": 0.5200559701492538, "retrieved_ids": ["mrqa_squad-train-52798", "mrqa_squad-train-68258", "mrqa_squad-train-69359", "mrqa_squad-train-52622", "mrqa_squad-train-27520", "mrqa_squad-train-50501", "mrqa_squad-train-74019", "mrqa_squad-train-52701", "mrqa_squad-train-8028", "mrqa_squad-train-52730", "mrqa_squad-train-25135", "mrqa_squad-train-48031", "mrqa_squad-train-39443", "mrqa_squad-train-25948", "mrqa_squad-train-26475", "mrqa_squad-train-13243", "mrqa_newsqa-validation-2980", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-4798", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-3039", "mrqa_searchqa-validation-9558", "mrqa_newsqa-validation-2705", "mrqa_squad-validation-1891", "mrqa_triviaqa-validation-6439", "mrqa_searchqa-validation-10787", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-4064", "mrqa_hotpotqa-validation-2210", "mrqa_newsqa-validation-2809", "mrqa_hotpotqa-validation-742", "mrqa_newsqa-validation-1436"], "EFR": 0.9666666666666667, "Overall": 0.7088289023631841}, {"timecode": 67, "before_eval_results": {"predictions": ["her brother, Brian", "1947", "Portugal. The Man", "North Atlantic Ocean", "eleven", "Anthony Hopkins", "Andrew Gold", "$19.8 trillion", "a hyper - active kinase", "January 15, 2007", "1982", "Super Bowl LII", "Brittany Paige Bouck", "31", "Louis XV", "White Sox", "Korean Republic Won", "`` Judas the son of James ''", "Executive Chef Danny Veltri", "Narendra Modi", "Ossie Schectman", "Max Martin", "Marty Robbins", "Percy Jackson", "state legislators of Assam", "after saving him from Djinn poisoning", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "Chris Martin", "`` The person who has existence in two parallel worlds", "Spanish / Basque origin", "Johannes Gutenberg", "Jason Momoa", "Terrence Howard", "General Carlos Roloff", "Elena Anaya", "`` there is one body and one Spirit", "2014", "Pangaea or Pangea", "from Fort Kent, Maine, at the Canada -- US border, south to Key West, Florida", "Robin", "Mount Sinai", "Vincent Price", "Nepal", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "December 12, 2017", "April 8, 2018", "Manhattan", "1969", "on location", "Neil Young", "provinces along the Yangtze River and in provinces in the south", "Thomas Jefferson", "\"Land of the Rising Sun\"", "Strictly Come Dancing", "Adam Dawes", "Robert Paul \"Robbie\" Gould III", "Musicology", "two years,", "Lashkar-e-Tayyiba (LeT)", "HPV (human papillomavirus)", "ballet dancer", "necropolis", "Spider-Man", "Fidel Castro"], "metric_results": {"EM": 0.625, "QA-F1": 0.7228498644806913}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.35294117647058826, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.15384615384615383, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.09090909090909091, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5283018867924527, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-5775", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-9107", "mrqa_triviaqa-validation-1975", "mrqa_hotpotqa-validation-1629", "mrqa_searchqa-validation-13420"], "SR": 0.625, "CSR": 0.5215992647058824, "EFR": 0.9166666666666666, "Overall": 0.6991375612745099}, {"timecode": 68, "before_eval_results": {"predictions": ["Jeff Barry and Andy Kim", "Tzeitel", "2018", "President James Madison", "one season", "September 29", "c. 1000 AD", "Samantha Jo `` Mandy '' Moore", "March 1995", "1957", "Andrew McCarthy", "an investor couple", "Erica Rivera", "the epidermis", "the cavities and surfaces of blood vessels and organs throughout the body", "the Norman given name Robert, meaning `` bright renown '' -- from the Germanic elements `` hrod '' meaning renown and `` beraht '' meaning bright", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "11 : 15 p.m.", "Sylvester Stallone", "Patricia Field", "the Director of National Intelligence", "November 25, 2002", "S - shaped", "Marley & Me", "State Bar of Arizona", "19th - century India", "wintertime", "The Romantics", "W. Edwards Deming", "Danny and Pam", "N\u0289m\u0289n\u0289", "1992", "presidential representative democratic republic", "Randy Watson", "Vincent Price", "Andy Serkis", "Jehnna ( Olivia d'Abo )", "O'Meara", "Kimberlin Brown", "by one of the United States courts of appeals", "financial inflows to developing countries", "Tom Waits", "The Star Spangled Banner", "a Concurrent List", "StubHub Center", "Johannes Gutenberg", "plate tectonics", "Himadri Station", "Rajendra Prasad", "1916", "March 11, 2016", "a psychologist", "the Irish Setter", "Hattie McDaniel", "Tian Tan Buddha", "Bonkyll Castle", "Alf Heiberg Clausen", "The Iraqi officials said the area was sealed off, so they did not know casualty figures.", "Australian officials", "Turkey from inside northern Iraq.", "piracy", "lifejackets", "the serve", "Josh"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5700239718544047}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.10526315789473684, 0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.06451612903225808, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-1418", "mrqa_naturalquestions-validation-9521", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-8950", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-4605", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1340", "mrqa_triviaqa-validation-5724", "mrqa_hotpotqa-validation-3965", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1504", "mrqa_searchqa-validation-1397"], "SR": 0.484375, "CSR": 0.5210597826086957, "EFR": 0.9696969696969697, "Overall": 0.7096357254611332}, {"timecode": 69, "before_eval_results": {"predictions": ["Syracuse University", "The Fault in Our Stars", "Stadio Olimpico in Rome, Italy", "traditional music", "water", "May 4, 2004", "Northumbrian", "1987", "American Horror Story", "Salisbury", "University of Nevada, Las Vegas (UNLV)", "The third single from the album, \"Scars to Your Beautiful\"", "November of that year", "multi-purpose stadium", "small family car", "Jos\u00e9 Bispo Clementino dos Santos", "Laura Elizabeth \"Laurie\" Metcalf", "the \"Cisleithanian\" half of Austria-Hungary", "Russell T Davies", "George Balanchine", "Oliver Parker", "he turned 51, he died of cancer", "Snowball II", "Oakland", "FCI Danbury", "Sir Edmund Barton", "Bob Day", "The Blue Ridge Parkway", "1875", "London", "Shut Up", "James David Lofton", "Big Bad Wolf", "Dennis Hull", "books, films and other media", "Iranian-American", "The Guest", "Bay of Fundy", "2009", "Umberto II", "2016 United States elections", "War Is the Answer", "1891", "Supergirl", "DI Humphrey Goodman", "Cinderella", "1,382", "left-arm", "Syracuse", "Buckingham Palace", "Rhode Island", "Anglican", "the Soviet Union", "the Fourth Republic ( 1981 -- 86 )", "a person whose occupation is mainly to cut, dress, groom, style and shave men`s and boys` hair", "Rodgers and Hammerstein", "Ramadan", "the Dalai Lama's", "70,000 or so", "December Monday", "Lord Peter Wimsey", "carbonic acid", "neon", "Sean Maddox"], "metric_results": {"EM": 0.625, "QA-F1": 0.6968434343434343}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 0.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5619", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-3703", "mrqa_hotpotqa-validation-4352", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-278", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-5942", "mrqa_triviaqa-validation-2405", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-80", "mrqa_searchqa-validation-4425", "mrqa_searchqa-validation-10452", "mrqa_triviaqa-validation-7105"], "SR": 0.625, "CSR": 0.5225446428571429, "retrieved_ids": ["mrqa_squad-train-51346", "mrqa_squad-train-323", "mrqa_squad-train-53730", "mrqa_squad-train-44837", "mrqa_squad-train-55882", "mrqa_squad-train-7429", "mrqa_squad-train-25097", "mrqa_squad-train-75289", "mrqa_squad-train-55845", "mrqa_squad-train-80453", "mrqa_squad-train-17330", "mrqa_squad-train-51877", "mrqa_squad-train-23735", "mrqa_squad-train-43713", "mrqa_squad-train-22030", "mrqa_squad-train-11082", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-821", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-3819", "mrqa_triviaqa-validation-4784", "mrqa_hotpotqa-validation-4945", "mrqa_triviaqa-validation-4112", "mrqa_searchqa-validation-10210", "mrqa_naturalquestions-validation-3770", "mrqa_hotpotqa-validation-3707", "mrqa_naturalquestions-validation-8961", "mrqa_searchqa-validation-16274", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-123", "mrqa_hotpotqa-validation-1952"], "EFR": 1.0, "Overall": 0.7159933035714287}, {"timecode": 70, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-139", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5611", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-875", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11005", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13441", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16534", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7326", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8807", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-10284", "mrqa_squad-validation-10352", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1498", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2123", "mrqa_squad-validation-215", "mrqa_squad-validation-2197", "mrqa_squad-validation-2248", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3464", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3750", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-3904", "mrqa_squad-validation-4096", "mrqa_squad-validation-4469", "mrqa_squad-validation-457", "mrqa_squad-validation-4669", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-5869", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-6636", "mrqa_squad-validation-682", "mrqa_squad-validation-6838", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8028", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8500", "mrqa_squad-validation-8529", "mrqa_squad-validation-8719", "mrqa_squad-validation-8754", "mrqa_squad-validation-8966", "mrqa_squad-validation-9165", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4953", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6879", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-934"], "OKR": 0.830078125, "KG": 0.49375, "before_eval_results": {"predictions": ["November 17, 2017", "Melissa Disney", "the Archies", "a writ of certiorari", "Edgar Lungu", "by the early - to - mid fourth century", "Marcus Atilius Regulus", "Kamal Givens", "You are a puzzle", "San Francisco", "a crust of mashed potato", "Gregor Mendel", "October 1, 2015", "New Zealand", "New Jersey", "pit road speed", "a laurel wreath", "pressure", "1923", "Renhe Sports Management Ltd", "Michael Schumacher", "dispense summary justice", "John Quincy Adams", "Ray Charles", "NBC's", "2013 ( XLVIII )", "Internal epithelia", "Upstate New York", "Henry Purcell", "5,534", "Lord Irwin", "three levels", "Roman Reigns", "Efren Manalang Reyes", "the second Persian invasion of Greece", "provide bridging funding for existing federal programs at current, reduced, or expanded levels", "2004", "`` Fix You ''", "Ed Roland", "Jos\u00e9 Mart\u00ed", "16 August 1975", "when the forward reaction proceeds at the same rate as the reverse reaction", "Sumitra", "The management team", "Ravi River", "Sir Rowland Hill", "Pangaea", "Sally Field", "a cylinder of glass or plastic that runs along the fiber's length", "Dr. Sachchidananda Sinha", "Barbara Windsor", "shale", "The Hague", "The World is Not Enough", "held as Chairperson of the Organisation of African Unity from 25 May 1963 to 17 July 1964", "Leona Louise Lewis", "Hong Kong International Theme Parks", "'We want to reset our relationship and so we will do it together.'\"", "an average of 25 percent", "Stuart Gaffney, media director for Marriage Equality USA,", "Roget", "Galileo Galilei", "Aristophanes", "Amy Bishop Anderson,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7098518668831169}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7142857142857143, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428572, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.5, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8470", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-8326", "mrqa_triviaqa-validation-4467", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-5085", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-260", "mrqa_searchqa-validation-10531", "mrqa_newsqa-validation-2288"], "SR": 0.59375, "CSR": 0.5235475352112676, "EFR": 0.9615384615384616, "Overall": 0.7129546993499458}, {"timecode": 71, "before_eval_results": {"predictions": ["Jack Ridley", "Chester", "a pinball machine", "76,416", "Rawhide", "Chris Weidman", "Jacksonville Jacksonville", "Orpheus", "Martin Scorsese", "Citgo Petroleum Corporation", "Kagoshima Airport", "Southern Progress Corporation", "Laura Dern", "British Conservative Party", "Mickey's PhilharMagic", "45", "1241", "Hamburger Sport-Verein", "Kolkata", "Liga MX", "\"Queen City\"", "Lithuanian Basketball League", "Texas", "six", "Rob Reiner", "The R-8 Human Rhythm Composer", "WAMC", "126,202", "Claude Mak\u00e9l\u00e9l\u00e9 Sinda", "polypeptide chain", "Sacramento Kings", "1945", "Fairfax", "Syracuse", "Sir Thomas Daniel Courtenay", "Martin \"Marty\" McCann", "Green Mountain", "Maldives", "Uzbekistan", "Ella Jane", "2000", "0.500", "William Bradley", "Coronation Street", "Duke", "Richard Masur", "Anabolic steroids", "diving duck", "House of Hohenstaufen", "local South Australian and Australian produced content", "EBSCO Information Services", "Karen Taylor", "as a pH indicator, a color marker, and a dye", "Thomas Edison", "The town is served by Woodbridge railway station on the Ipswich-Lowestoft East Suffolk Line", "Runic", "cycling", "Brett Cummins", "58 people", "Security officer Stephen Johns reportedly opened the door for the man police say was", "osprey", "Fyodor Dostoevsky", "Turtle Wax", "Hyundai Steel"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6378844246031746}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.25, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1395", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-1220", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-5766", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2938", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-1000", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-82", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4357", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-4244", "mrqa_newsqa-validation-2439", "mrqa_searchqa-validation-14348", "mrqa_newsqa-validation-1048"], "SR": 0.515625, "CSR": 0.5234375, "EFR": 1.0, "Overall": 0.720625}, {"timecode": 72, "before_eval_results": {"predictions": ["direct scattering and inverse scattering", "International Society for the Study of the Origin of Life", "Australian Supercars Championship", "The Jacksonville Jaguars", "\"Crossed: Dead or Alive\"", "The Philadelphia Experiment", "Gemeinn\u00fctzige", "Abbey Road", "Laurie Metcalf", "Nine-card Brag", "\"Godspell\" (1973)", "freshman", "Free Range Films", "Mondays", "Beauty and the Beast", "Jehovah", "fourth President of Pakistan", "Tamara Ecclestone Rutland", "Newport", "Tomorrowland", "1985", "David Michael Bautista Jr.", "1002", "20th", "John Meston", "late 19th and early 20th centuries", "Terrence Jones", "1967", "Casey Bond", "actress", "Chancellor of Austria", "Naomi Campbell", "The dyers of Lincoln", "2013\u201314", "Ghostbusters Spooktacular", "an American financier", "Maldives", "Them", "its air-cushioned sole", "4,972", "Alexis Knapp", "Perth, Western Australia", "Harry F. Sinclair", "Interstate 22", "William McKinley", "Denmark", "Tunisian", "lieutenant general", "Indian state of Gujarat", "Anthony Hopkins", "Eric Bana", "8.7 %", "1980", "Isaiah Amir Mustafa", "Michigan", "Jon Stewart", "Australia", "seven", "The National Association of Broadcasters", "July", "Michelangelo", "airplanes", "Stone Mountain", "shopping"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6263838158369408}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, false, true, true, true, false], "QA-F1": [0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.2222222222222222, 1.0, 0.1818181818181818, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.375, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-294", "mrqa_hotpotqa-validation-1612", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-3500", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-5582", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-5857", "mrqa_naturalquestions-validation-9824", "mrqa_triviaqa-validation-982", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-270", "mrqa_searchqa-validation-5340"], "SR": 0.515625, "CSR": 0.5233304794520548, "retrieved_ids": ["mrqa_squad-train-44873", "mrqa_squad-train-23568", "mrqa_squad-train-61834", "mrqa_squad-train-9665", "mrqa_squad-train-25015", "mrqa_squad-train-54342", "mrqa_squad-train-12424", "mrqa_squad-train-58685", "mrqa_squad-train-37693", "mrqa_squad-train-46097", "mrqa_squad-train-49004", "mrqa_squad-train-85039", "mrqa_squad-train-70158", "mrqa_squad-train-54707", "mrqa_squad-train-61152", "mrqa_squad-train-15065", "mrqa_naturalquestions-validation-4190", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-397", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-16016", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-5212", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5348", "mrqa_hotpotqa-validation-5807", "mrqa_searchqa-validation-7332", "mrqa_naturalquestions-validation-1173", "mrqa_newsqa-validation-1856", "mrqa_hotpotqa-validation-1980"], "EFR": 1.0, "Overall": 0.7206035958904109}, {"timecode": 73, "before_eval_results": {"predictions": ["SAS Fr\u00f6sundavik", "1972", "Indianapolis, Indiana.", "a case against organized religion", "1970", "the 1745 rebellion", "96", "his bomber crash landed in the ocean during World War II", "West African descendants.", "Rigoletto", "James Stenbeck", "December 19, 1998", "Philip Quast", "November 20, 1942", "Argentine cuisine", "22 September 2015", "Matt Lucas", "Waylon J. Smithers", "2006", "Scotland", "Lakshmibai", "Umberto II", "England", "beer and soft drinks", "five", "C. H. Greenblatt", "mermaid", "1535", "There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "England", "Lord Gort", "Laurel, Mississippi", "the pronghorn", "Wayne Conley", "the Philippines and Ibero-America", "1826", "Havana", "Pablo Escobar", "David Michael Bautista Jr.", "Iftikhar Ali Khan", "October 25, 1881", "Bob Dylan", "a billion-dollar global fashion company", "Harrison Ford", "PBS", "Law Adam", "1943", "wineries", "the Animorphs", "Jeff Meldrum", "quantum mechanics", "early - to - mid fourth century", "Pebble Beach", "$2.187 billion", "Mull", "Jeffrey Archer", "Oliver Harmon Jones", "in Galveston, Texas,", "celebrities", "are concerned that the legislation will foster racial profiling, arguing that most police officers don't have enough training to look past race while investigating a person's legal status.", "Ohio", "the Aplodintidae family", "William Somerset Maugham", "bullfighting"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6776413690476191}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-1423", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-5747", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-3363", "mrqa_triviaqa-validation-7264", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-3584", "mrqa_searchqa-validation-11034", "mrqa_searchqa-validation-3745"], "SR": 0.609375, "CSR": 0.5244932432432432, "EFR": 0.96, "Overall": 0.7128361486486485}, {"timecode": 74, "before_eval_results": {"predictions": ["1947", "Bury Football Club is a professional association football club based in Bury, Greater Manchester, England.", "Nye County", "1930s and 1940s", "two-time", "a governor", "40 Acres and a Mule Filmworks", "The Panther", "the Taliban's Islamic Emirate of Afghanistan", "China", "Tamil Nadu", "Skyscraper", "17 October 2006", "April 1, 1949", "Big Machine Records", "Brady Haran (, born 18 June 1976) is an Australian-born British independent filmmaker and video journalist", "Jesus", "Heywood \"Woody\" Allen (born Allan Stewart Konigsberg; December 1, 1935) is an American filmmaker, writer, actor, comedian, and musician", "moth", "Elliot Fletcher (born June 30, 1996) is an American transgender actor known for his work on the MTV comedy series \"Faking It\" and Freeform's \"The Fosters\".", "husband and wife Kansas Joe McCoy and Memphis Minnie", "Martha Wainwright", "Christopher McCulloch (also known as \"Jackson Publick\")", "William Clark Gable (February 1, 1901 \u2013 November 16, 1960) was an American film actor and military officer, often referred to as \"The King of Hollywood\"", "1989 until 1994", "Tallahassee City Commission", "the Royal Air Force", "Mika Pauli H\u00e4kkinen ( nicknamed \"\"the Flying Finn\"\", is a Finnish former professional racing driver.", "World Music Awards", "1998", "Peter 'Drago' Sell, (born August 5, 1982) is an American mixed martial artist specializing in Brazilian Jiu Jitsu.", "\"Barney Miller\"", "\"Coyote Ugly\"", "smell what they saw on screen through scratch and sniff cards", "Cersei Lannister", "Andrew Stephen Roddick (born August 30, 1982) is an American former professional tennis player", "Bigfoot (also known as Sasquatch) is a cryptid which supposedly is a simian-like creature", "Kim Jong-hyun (born April 8, 1990), better known by the mononym Jonghyun, released a compilation album, \" Story Op.1\"", "The game telecast airs every Friday night at 7:45pm ET during the college football regular season.", "A Boltzmann machine", "January 1930", "12", "\"Pour le M\u00e9rite\"", "Franklin, Indiana", "zoonotic", "October 6, 1931", "New Boston Air Force Station", "the United States men's hockey team, led by head coach Herb Brooks, portrayed by Kurt Russell, that won the gold medal in the 1980 Winter Olympics.", "Elizabeth River", "Metrolink", "\"Complex\" magazine", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "Covington, Kentucky", "Elizabeth Dean Lail", "the badminton tournament at the 2012 Summer Olympics was held at [Wembley Arena] in London,", "Newcastle Falcons", "Rihanna", "British Prime Minister", "order after demonstrators rose up across Greece Monday in a third day of rioting over Saturday's killing of a 15-year-old boy that has left dozens injured and scores of properties destroyed.", "a nuclear weapon", "the early 18th century (circa 1720) along a highly-contested collection", "the City of Bridgeport", "a gag", "the University of Wisconsin-Madison"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5715501721521942}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.4444444444444445, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.2857142857142857, 1.0, 0.0, 0.7142857142857143, 0.0, 0.4444444444444445, 0.0, 0.8571428571428571, 1.0, 0.4, 0.2857142857142857, 0.0, 1.0, 0.3, 0.6666666666666666, 1.0, 0.0, 0.0, 0.375, 0.0, 0.11764705882352941, 0.23529411764705882, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.16, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.06666666666666667, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-3848", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-3340", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-1984", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-4499", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-958", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3336", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-5475", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2579", "mrqa_triviaqa-validation-2322", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-122", "mrqa_searchqa-validation-5810", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-12635"], "SR": 0.453125, "CSR": 0.5235416666666667, "EFR": 1.0, "Overall": 0.7206458333333333}, {"timecode": 75, "before_eval_results": {"predictions": ["France", "Jiujiang", "Paul Bunyan", "Peter Davison", "Pandora", "a large soggy pasture", "Alaska", "the village of Padworth", "The DMC-12", "VH-71 Kestrel", "lundy", "Hertz", "David Hockney", "spark-ignition", "Janis Joplin", "a nest", "Humphrey Bogart", "Piglet", "Antoine Lavoisier", "1960", "the United Provinces", "King County Executive", "Deacon Blues", "Secretary of State", "Jane Austen", "the New Economic Policy", "lime", "The Rocketeer", "littoral", "Venus", "Declaration of Independence", "decorat", "Hot Chocolate", "Jim Peters", "Armageddon", "Victoria", "Duck Soup", "Berlin", "Pangaea", "salmon", "the Old West", "the Island", "Project Gutenberg", "cesman", "California", "Nissan", "Isar", "Live and Let Die", "Salvador Allende", "The Green Mile", "Poland", "around 1600 BC", "Sean O' Neal", "May 18, 2018", "Dissection", "Baden-W\u00fcrttemberg", "March", "America's infrastructure.", "Mary Phagan,", "$500,000", "vitamin A", "Azkaban", "conga drums", "Vibe"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5813701923076923}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-5466", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-444", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-2721", "mrqa_triviaqa-validation-191", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-225", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-1493", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-3556", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-65"], "SR": 0.5625, "CSR": 0.5240542763157895, "retrieved_ids": ["mrqa_squad-train-77456", "mrqa_squad-train-20802", "mrqa_squad-train-71470", "mrqa_squad-train-54962", "mrqa_squad-train-17247", "mrqa_squad-train-71143", "mrqa_squad-train-74066", "mrqa_squad-train-76678", "mrqa_squad-train-12047", "mrqa_squad-train-51978", "mrqa_squad-train-53965", "mrqa_squad-train-82923", "mrqa_squad-train-7790", "mrqa_squad-train-54713", "mrqa_squad-train-81334", "mrqa_squad-train-8492", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-2891", "mrqa_hotpotqa-validation-2619", "mrqa_naturalquestions-validation-2170", "mrqa_hotpotqa-validation-722", "mrqa_naturalquestions-validation-2196", "mrqa_newsqa-validation-1241", "mrqa_hotpotqa-validation-2492", "mrqa_naturalquestions-validation-4074", "mrqa_newsqa-validation-3561", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-4968", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-2113", "mrqa_squad-validation-3497", "mrqa_hotpotqa-validation-2421"], "EFR": 1.0, "Overall": 0.7207483552631578}, {"timecode": 76, "before_eval_results": {"predictions": ["vicious brutality", "a \"happy ending\" to the case.", "head injury.", "Bryant Purvis", "1983", "Chaffetz", "Bryant Purvis", "jazz", "$30 million,", "1994,", "his album \"Tha Carter III\" was the top-selling disc of 2008.", "peanuts, nuts, shellfish and fish", "\"fusion teams,\"", "April 2010.", "of", "October 19,", "Addis Ababa,", "a president who understands the world today, the future we seek and the change we need", "five", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "if Gadhafi suffered the wound in crossfire or at close-range", "off Somalia's coast.", "the Indian army and separatist militants in Indian-administered", "The Charlie Daniels Band,", "Egypt", "planning processes are urgently needed", "Saturday.", "Obama", "\"the most important discovery\"", "return to the Mediterranean", "insect stings,", "Kenneth Cole", "a one-shot victory in the Bob Hope Classic on the final hole", "Natalie Cole's", "\"The people kill him with the blocks,", "Hundreds", "Current TV", "A Colorado prosecutor", "100% of its byproducts", "shark River Park", "Galveston, Texas, to Veracruz, Mexico,", "nine-wicket", "70,000", "the Nazi war crimes suspect", "his former caddy,", "three", "began to lose it all.", "named his company Polo", "overhaul domestic policies,", "President Barack Obama,", "Patrick McGoohan,", "Hathi Jr", "energy", "Bacon", "raw leather", "smith", "smith", "1,521", "Australian", "Edward R. Murrow", "Dizzy Gillespie", "smith", "Dwight D. Eisenhower", "neo-Nazi"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6725159719684974}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true], "QA-F1": [0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.23529411764705882, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.1, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2382", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2385", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2061", "mrqa_naturalquestions-validation-3184", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-6091", "mrqa_hotpotqa-validation-4863", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-7657"], "SR": 0.59375, "CSR": 0.5249594155844156, "EFR": 1.0, "Overall": 0.7209293831168831}, {"timecode": 77, "before_eval_results": {"predictions": ["Terry the Tomboy", "Gatwick", "Abdul Razzak Yaqoob", "Robert Matthew Hurley", "William Shakespeare", "Syracuse University", "American", "\"Peshwa\" (Prime Minister)", "Tennessee", "Lowe's Companies, Inc.", "Arthur Schnitzler's 1926 novella \"Traumnovelle\" (\"Dream Story\")", "Hindi", "\"Big Mamie\"", "first", "Katherine Murray Millett (September 14, 1934 \u2013 September 6, 2017) was an American feminist writer, educators, artist, and activist", "The 2000 Summer Olympics", "Leucippus ( ; Greek: \u039b\u03b5\u03cd\u03ba\u03b9\u03c0\u03c0\u03bf\u03c2, \"Le\u00fakippos\"", "Centre of Excellence", "Columbine", "381.6 days", "Richard Strauss", "South West Peninsula League", "in 1994 was convicted of assassinating civil rights leader Medgar Wiley Evers", "Yorgos Lanthimos", "Roscoe Lee Browne", "\"Black Abbots\"", "chocolate-colored", "more than 265 million", "\"Oedipus Rex\"", "Netrobalane canopus", "Telugu and Tamil", "Australian-American", "Albert Park", "\"Two Pi\u00f1a Coladas\"", "Brenton Thwaites", "Charles and Thomas Guard", "1919", "Elise Marie Stefanik", "Tribeca", "King of France", "pop music and popular culture", "Almeda Mall", "Ronald Joseph Ryan", "robot Overlords", "Doctor of Philosophy", "McClelland and Stewart", "Madeleine L' Engle", "February 5, 2015", "Nathan Bedford Forrest", "Operation Overlord", "capital crimes or capital offences", "Tigris and Euphrates rivers", "between 1765 and 1783", "September 2000", "(Rita) Hayworth", "Heisenberg", "(John) Tyndall", "Tsvangirai", "change course", "Johannesburg", "a Lotus", "a poke", "October", "Mandi Hamlin"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7024286477411478}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6, 0.0, 1.0, 1.0, 0.09999999999999999, 0.5714285714285715, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.28571428571428575, 1.0, 0.5, 0.8, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-2340", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-2550", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-5420", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-3130", "mrqa_hotpotqa-validation-5569", "mrqa_hotpotqa-validation-4061", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-2009", "mrqa_newsqa-validation-2651", "mrqa_searchqa-validation-2751", "mrqa_newsqa-validation-391"], "SR": 0.5625, "CSR": 0.5254407051282051, "EFR": 1.0, "Overall": 0.721025641025641}, {"timecode": 78, "before_eval_results": {"predictions": ["a albatross", "Gian Lorenzo Bernini", "the Land of the Hummingbird", "Denzel Washington", "a prologue", "Ben- Hur", "(Al) Capone", "a prism schism", "Bucharest", "Tennessee", "Dick Wolf", "a Mace", "Helena Bonham Carter", "Cincinnati", "Friday", "Joanne Dicken", "the Thames", "at the top", "a 4-letter adjective", "New Jersey", "Tarsus", "a gold rush", "grain", "Whitney", "Breckenridge", "satellite", "*bishops*", "Esperanto", "the Hundred Years' War", "Mending Wall", "William Shakespeare", "Kurt Kelly", "the Odyssey", "Special Boat Teams", "tamales", "the Today Show", "Sally Field", "earmarks", "Turin", "a collective noun", "a nasal septum", "All in the Family", "polo", "Wikipedia", "the Maritimes", "1773", "Richard Daley", "silicon", "\"Bee\"", "the best man", "the Missouri Waltz", "The Romantics", "Danny Veltri", "`` hero of Tippecanoe ''", "sotto voce", "the Tower of London", "Wildcats", "Knowlton School", "Ronald Wilson Reagan", "The interview", "U.S.", "137", "Jacob Zuma,", "1994"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6733630952380951}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-14670", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-5482", "mrqa_searchqa-validation-5575", "mrqa_searchqa-validation-7541", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-14621", "mrqa_searchqa-validation-11929", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-7331", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-7961", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-15622", "mrqa_searchqa-validation-15448", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-3743", "mrqa_searchqa-validation-5831", "mrqa_naturalquestions-validation-4552", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-5573", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-780", "mrqa_triviaqa-validation-3363"], "SR": 0.609375, "CSR": 0.526503164556962, "retrieved_ids": ["mrqa_squad-train-26720", "mrqa_squad-train-49341", "mrqa_squad-train-82799", "mrqa_squad-train-85724", "mrqa_squad-train-9405", "mrqa_squad-train-30647", "mrqa_squad-train-52888", "mrqa_squad-train-41458", "mrqa_squad-train-21180", "mrqa_squad-train-75496", "mrqa_squad-train-54389", "mrqa_squad-train-67671", "mrqa_squad-train-26056", "mrqa_squad-train-72142", "mrqa_squad-train-24372", "mrqa_squad-train-59996", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-499", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-5775", "mrqa_triviaqa-validation-5486", "mrqa_newsqa-validation-2495", "mrqa_squad-validation-3319", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-2025", "mrqa_hotpotqa-validation-1776", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-933", "mrqa_naturalquestions-validation-627", "mrqa_naturalquestions-validation-129"], "EFR": 1.0, "Overall": 0.7212381329113924}, {"timecode": 79, "before_eval_results": {"predictions": ["minnie", "pale yellow to golden", "Tuscaloosa", "fish", "six", "horse", "david seville", "whiskas", "John Baker", "a spear", "kitchen collectibles", "stailles", "Mujib", "a scarlet tanager", "Jack Lemmon", "high jump", "kipps: The Story of a Simple Soul", "werner", "venezuela", "goytre", "George H. W. Bush", "almanack", "golf", "2.2046", "Hugh Hefner", "florentia", "Kofi Annan", "marin Piccola (the little harbour) the Belvedere of Tragara (a high panoramic promenade lined with villas)", "right", "George Eliot", "wenceslaus", "national space Centre", "mountain of light", "contactless communication between devices like smartphones or tablets", "a wharf", "Israelites", "South Africa", "Topeka", "Brazil", "George Osborne", "The Good Life", "Florence", "Sicily", "nirvana", "Space Oddity", "the Smiths", "Seth", "Jeffery Deaver", "\u00c9dith Piaf", "cuban cigars", "l'homme qui inventa le droit humanitaire", "Asuka", "Canada south of the Arctic", "the Beldam / Other Mother", "Ballarat Bitter", "Mark Dacascos", "\"the backside.\"", "110 mph,", "\"We have several hundred people working for us in Indianapolis [alone].\"", "work together to stabilize Somalia and cooperate in security and military operations.", "Mike Rowe", "an Enigma", "Edinburgh", "free expression"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5142342032967033}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false], "QA-F1": [0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-2346", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-1090", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-7071", "mrqa_triviaqa-validation-1601", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-7729", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-6818", "mrqa_naturalquestions-validation-1872", "mrqa_naturalquestions-validation-2851", "mrqa_hotpotqa-validation-507", "mrqa_hotpotqa-validation-5375", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2234", "mrqa_searchqa-validation-328"], "SR": 0.46875, "CSR": 0.52578125, "EFR": 0.9411764705882353, "Overall": 0.709329044117647}, {"timecode": 80, "UKR": 0.75, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3967", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10493", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16534", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16891", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8807", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2248", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3750", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4669", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-5869", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8500", "mrqa_squad-validation-8529", "mrqa_squad-validation-8719", "mrqa_squad-validation-8754", "mrqa_squad-validation-8966", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7717", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-934"], "OKR": 0.8203125, "KG": 0.49453125, "before_eval_results": {"predictions": ["eight", "habsburg monarchy", "amram", "Lucy", "hedgehog", "liqueur", "grotto", "Hungary", "Greek", "jimmy nixon", "USS Constitution", "collage", "To Kill a Mockingbird", "fish", "cleveland", "1966", "monasteries", "Billy Fury", "Tuesday", "giuseppe meisterbeer", "whist", "George Clooney", "Fan Graphs", "A4", "Mussolini", "george osborne", "Margaret Thatcher", "phylvester McCoy", "cattle", "Jacob", "Something In The Air", "Chicago", "Hague", "sub-Saharan", "The Princess bride", "President Nixon", "Temple of the Dog", "aleister Crowley", "richard d'H\u00e9rouville", "\"Great and Most Fortunate Navy\"", "setts", "batsman", "mycenae", "mountain", "Saddam Hussein", "Tombstone", "liriope", "Indonesia", "dwain chambers", "Swiss", "daedalus", "in the books of Exodus and Deuteronomy", "triglycerides ( lipids )", "David Ben - Gurion", "DTM", "Tetrahydrogestrinone (THG)", "7pm", "Ma Khin Khin Leh", "prisoners at the South Dakota State Penitentiary", "Beijing", "a catheter", "George Glenn Jones", "Think Big", "the disk"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5064174107142857}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-7274", "mrqa_triviaqa-validation-5921", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-1210", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-6871", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-5378", "mrqa_triviaqa-validation-4563", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-2903", "mrqa_triviaqa-validation-5332", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-610", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9726", "mrqa_hotpotqa-validation-3117", "mrqa_newsqa-validation-2488", "mrqa_searchqa-validation-4839", "mrqa_searchqa-validation-4136", "mrqa_naturalquestions-validation-808"], "SR": 0.46875, "CSR": 0.5250771604938271, "EFR": 1.0, "Overall": 0.7179841820987655}, {"timecode": 81, "before_eval_results": {"predictions": ["cleckheaton", "settler", "pommel horse", "three", "india", "will", "people", "steel", "Columba", "jimmy harper", "power surge", "snapdragons", "weather", "for gallantry", "Zachary Taylor", "1951", "blue danube", "venice", "Cambridge", "meerkat", "Saturn", "Aintree", "bugeye", "jimmy nixon", "bexhill", "Laos", "india", "bacardi", "vanilla", "india", "Apocalypse Now", "nelson", "let It Snow", "elkie Brooks", "cruisin", "edwina currie", "meyer tchaikovsky", "turkish turkish", "india", "Charlotte Corday", "algiers", "king m2", "golf", "yellow", "india", "Trenton", "the hose", "sea otter", "Saffron", "Ghana", "jon pert", "243 days", "the Sunni Muslim family", "Terry Reid", "leg injury", "1960", "David Yates", "Mohamed Mohamud Qeyre", "15-year-old's", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "vote", "a bear", "bison", "Reform"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5739583333333333}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-5905", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-6689", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-2332", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-4943", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-7747", "mrqa_triviaqa-validation-3108", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-7053", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-5145", "mrqa_triviaqa-validation-704", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-522", "mrqa_hotpotqa-validation-3778", "mrqa_newsqa-validation-541", "mrqa_searchqa-validation-8180", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-16755", "mrqa_searchqa-validation-14428"], "SR": 0.53125, "CSR": 0.5251524390243902, "retrieved_ids": ["mrqa_squad-train-35857", "mrqa_squad-train-62914", "mrqa_squad-train-75068", "mrqa_squad-train-78850", "mrqa_squad-train-32340", "mrqa_squad-train-30359", "mrqa_squad-train-27467", "mrqa_squad-train-8692", "mrqa_squad-train-8882", "mrqa_squad-train-74309", "mrqa_squad-train-67773", "mrqa_squad-train-7466", "mrqa_squad-train-77425", "mrqa_squad-train-62238", "mrqa_squad-train-40219", "mrqa_squad-train-64314", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-780", "mrqa_newsqa-validation-983", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-390", "mrqa_hotpotqa-validation-5239", "mrqa_newsqa-validation-3159", "mrqa_searchqa-validation-11292", "mrqa_hotpotqa-validation-5562", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-16353", "mrqa_triviaqa-validation-4355", "mrqa_hotpotqa-validation-334", "mrqa_newsqa-validation-1785", "mrqa_naturalquestions-validation-6849", "mrqa_triviaqa-validation-1268"], "EFR": 0.9333333333333333, "Overall": 0.7046659044715448}, {"timecode": 82, "before_eval_results": {"predictions": ["ilfracombe", "the chord", "thailand", "fibrocartilage", "Yardbirds", "Brazil", "1123", "queen Elizabeth II", "Bruce Alexander", "granddaughter", "manhunt 2", "Carson City", "spirit-lifting jingle", "table silver", "prawns", "althea Gibson", "house sparrow", "University of Nebraska\u2013Lincoln", "Love Never Dies", "lunar new year", "island", "Jimmy Robertson", "steam locomotive", "David Davis", "Eric Coates", "\u00e9dith piaf", "king henry ii", "anteros", "football", "Arizona", "antelope", "Breakfast Club", "heavy horse", "i second son of Richard, Duke of York", "john Donne", "Loch lomond", "Salt Lake City", "red", "a toy modeled after a character on the fictional television show Woody's Roundup", "the Battle of Austerlitz", "treen", "clugaboom", "tintin", "Mercury", "guitar", "Celsius", "Phil Spector", "germany", "hans lippershey", "come on down", "yann martel", "foreign investors", "the Gilbert building", "Panzerkampfwagen VIII Maus", "two", "The Sleaford and North Hykeham by-election", "Four Weddings and a Funeral", "his mother, Katherine Jackson, his three children and undisclosed charities.", "97-year-old Millvina Dean,", "1979", "Israel", "Phil Mickelson", "the Crimean War", "Larry Ellison"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5887784090909091}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-2425", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-4127", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-4242", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-5389", "mrqa_naturalquestions-validation-4326", "mrqa_hotpotqa-validation-3037", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-1594", "mrqa_searchqa-validation-3536"], "SR": 0.5625, "CSR": 0.5256024096385542, "EFR": 0.9285714285714286, "Overall": 0.7038035176419966}, {"timecode": 83, "before_eval_results": {"predictions": ["arm", "Swedish", "wagon", "March 19", "Julie Andrews", "rage", "fungi", "verona", "phil Spector", "fidelio", "w/e 3rd Jan 1981", "minder", "Pisces", "peppers", "yMCA", "the Sahara Desert", "venus", "a crossword puzzle", "Diana Ross", "burt Reinhardt", "nettle leaves", "cartoons", "an astronaut", "riyal", "paul patrick", "boris becker", "one Thousand and one", "john of gaunt", "sewing machine", "Bristol Aeroplane Company", "tenor saxophone", "Tutankhamun", "colony", "fedora", "the Indus Valley Civilization", "helen germanley brown", "werner heath", "krakatoa", "pinocchio", "westerly state", "bertrand bar\u00e8re de Vieuzac", "belshazzar", "position", "biluim", "rue", "Ken Platt", "handley Page", "jimmy hargreaves", "venus karl suessdorf", "\"Mr Loophole\"", "1", "the Shrek franchise", "Watson and Crick", "the appendicular skeleton", "25 November 2015", "Jean Erdman", "the Netherlands", "9 a.m.", "U.S. Holocaust Memorial Museum", "a thorough understanding of the dogs' needs,", "Robert Langdon", "Parris Island", "watts", "sunflower"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6036458333333332}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4062", "mrqa_triviaqa-validation-4800", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-6348", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-2247", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-987", "mrqa_triviaqa-validation-4471", "mrqa_triviaqa-validation-4917", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-5862", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-6094", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-971", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-6516", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3468", "mrqa_hotpotqa-validation-413", "mrqa_searchqa-validation-9567"], "SR": 0.53125, "CSR": 0.5256696428571428, "EFR": 0.8333333333333334, "Overall": 0.6847693452380953}, {"timecode": 84, "before_eval_results": {"predictions": ["David Ben - Gurion", "is said to be unattainable", "Karina Smirnoff", "the public", "the eighth episode of Arrow's second season", "Major General Clarence L. Tinker", "2002", "1988", "reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time", "John Roberts", "Walter Farley", "butane", "Parker's pregnancy at the time of filming", "2004", "capital and financial markets", "David", "The Enchantress", "the true horrors of human history derive not from orc and Dark Lords, but from ourselves", "Leon Huff", "the mouth", "toodee", "March of Dimes", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "The management team", "Paris", "The Parlement de Bretagne", "Matt Flinders", "during prenatal development", "Geothermal gradient", "benzodiazepines", "Ferm\u00edn Francisco", "three", "Coriolis force", "94 by 50", "The enthalpy of fusion of a substance, also known as ( latent ) heat of fusion", "Washington metropolitan area", "Zoe McLellan", "Andy", "ice giants", "2002", "1920s", "in different parts of the globe", "the Empire of Japan", "cylinder of glass or plastic that runs along the fiber's length", "The Intolerable Acts", "Lily Travers", "Internal epithelia", "Bob Dylan", "Latitude", "Indian monks", "`` house edge ''", "Sikh", "smith smith", "smith", "Cocoa, Florida", "niece", "the second", "Kitty Kelley", "Strategic Arms Reduction Treaty", "\"We need a president who understands the world today, the future we seek and the change we need. We need Barack Obama as the next president of the United States.\"", "Cecil Calvert", "governor of yore", "Ontario", "Medellin"], "metric_results": {"EM": 0.34375, "QA-F1": 0.5118218280902105}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.33333333333333337, 1.0, 1.0, 0.7692307692307692, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666667, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.4, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615385, 1.0, 0.8571428571428572, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.11764705882352941, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-4145", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-5555", "mrqa_triviaqa-validation-1031", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-1551", "mrqa_hotpotqa-validation-4375", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-430", "mrqa_searchqa-validation-13409", "mrqa_searchqa-validation-1600"], "SR": 0.34375, "CSR": 0.5235294117647059, "retrieved_ids": ["mrqa_squad-train-77750", "mrqa_squad-train-47047", "mrqa_squad-train-18225", "mrqa_squad-train-56339", "mrqa_squad-train-43788", "mrqa_squad-train-63202", "mrqa_squad-train-54562", "mrqa_squad-train-42392", "mrqa_squad-train-74900", "mrqa_squad-train-20506", "mrqa_squad-train-59966", "mrqa_squad-train-16796", "mrqa_squad-train-13100", "mrqa_squad-train-72475", "mrqa_squad-train-57989", "mrqa_squad-train-34729", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-4863", "mrqa_squad-validation-10321", "mrqa_hotpotqa-validation-693", "mrqa_naturalquestions-validation-2387", "mrqa_hotpotqa-validation-1175", "mrqa_newsqa-validation-2497", "mrqa_searchqa-validation-14290", "mrqa_hotpotqa-validation-1372", "mrqa_triviaqa-validation-4612", "mrqa_naturalquestions-validation-1357", "mrqa_searchqa-validation-5340", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-8685", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4752"], "EFR": 0.9047619047619048, "Overall": 0.6986270133053222}, {"timecode": 85, "before_eval_results": {"predictions": ["July 14, 2017", "21 December 2017", "increased productivity, trade, and secular economic trends", "North Dakota", "Harrison Ford", "Nicolas Anelka", "18 - season", "flour and water", "Mahatma Gandhi", "J. Presper Eckert", "Missouri River", "Fa Ze Banks", "Orangeville, Ontario, Canada", "May 2016", "up to 100,000", "Kiss", "one", "30 months", "October 2012", "winter solstice", "gesture in which the head is tilted in alternating up and down arcs along the sagittal plane", "July 2, 1928", "push the food down the esophagus", "Edward G. Robinson", "The Lightning Thief", "TLC", "687 ( Earth ) days", "September 8, 2017", "William the Conqueror", "Abid Ali Neemuchwala", "Podujana Peramuna", "Chicago", "The User State Migration Tool ( USMT )", "November 17, 1800", "the season - five premiere episode `` Second Opinion ''", "Ming", "Saint Peter", "1773", "September 25, 1987", "20 years", "a homodimer of 37 - kDa subunits", "Billie Jean King", "Internal epithelia", "March 2, 2016", "detritus", "Ravi River", "Dennis C. Stewart", "12 to 36 months old", "Lulu", "in and around an unnamed village", "on a bronze plaque", "boxelder bug", "acetone", "james dukenfield", "21 August 1986", "Taylor Swift", "2006", "8 to 10 inches", "Robert Mugabe", "South Africa", "David Cameron", "Charles Boyer", "The Last of the Mohicans", "a bomb"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6076301685446907}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.1818181818181818, 1.0, 1.0, 0.125, 0.5, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-622", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-1359", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-3056", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-1368", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1705", "mrqa_triviaqa-validation-5074", "mrqa_hotpotqa-validation-3641", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-656", "mrqa_searchqa-validation-5288", "mrqa_searchqa-validation-13973"], "SR": 0.515625, "CSR": 0.5234375, "EFR": 0.9032258064516129, "Overall": 0.6983014112903226}, {"timecode": 86, "before_eval_results": {"predictions": ["Georgia Groome", "the Redenbacher family", "Louis Hynes", "Pure Java driver", "Geoffrey Jackson ( 2005 )", "Russia", "the Mongol Empire", "July 4, 1776", "Egypt", "a blighted ovum or anembryonic gestation ( an embryonic pregnancy )", "left hand ring finger", "the east coast of Queensland", "`` weight Loss ''", "3D modeling", "statute or the Constitution itself", "2012", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "Lewis Hamilton", "Ace", "1998", "Brazil", "to form a higher alkane", "1947", "the Ramones", "Arizona", "Jurchen Aisin Gioro clan", "China in American colonies", "Jacques Cousteau", "New Mexico", "Nepal", "538", "prayer, concern for the needy, self - purification, and the pilgrimage, if one is able", "President", "R / T is the performance marker", "2018", "Gary Player", "1997", "Tavish Crowe", "pathology", "332", "if the realization of one does not affect the probability distribution of the other", "the central sulcus", "2018", "mitosis", "The Maidstone Studios in Maidstone, Kent", "the Outfield", "International Border ( IB )", "Samara Cook", "a photodiode", "1858", "2010", "george ii", "jennifer kewferry", "Donington Park", "Kolkata", "The Handmaid's Tale", "fourth-ranking", "Takashi Saito, a 17-year-old sumo trainee, died in brutal circumstances, allegedly beaten to death", "Herman Cain,", "billboards with an image of the burning World Trade Center", "ethylene gas", "Dan Aykroyd", "the House of Lords", "Tom Ellis"], "metric_results": {"EM": 0.3125, "QA-F1": 0.5057062391821023}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false], "QA-F1": [0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.3333333333333333, 0.5714285714285715, 0.5, 0.4, 0.5, 0.3636363636363636, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.7826086956521738, 0.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.9411764705882353, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2420", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-8493", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-5546", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-3150", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-15707", "mrqa_hotpotqa-validation-4281"], "SR": 0.3125, "CSR": 0.5210129310344828, "EFR": 0.9772727272727273, "Overall": 0.712625881661442}, {"timecode": 87, "before_eval_results": {"predictions": ["Norway", "le belle femme skunk", "Romeo and Juliet", "The Golden Girls", "Niger", "Artemis", "by increasing the number of arcs", "Spanish", "driving Miss Daisy", "malta", "\"appealing\"", "Dubai", "Bristol", "double trouble", "s\u00e8vres", "Barack Obama", "the Observer", "Yakutat, Alaska", "Captain Mark Phillips", "william clough", "Rubenshuis Museum", "Pembrokeshire Coast National Park", "blood left at crime scenes", "willy", "javelin throw", "Port Moresby", "you'll Never Walk", "will be implemented after a two-month consultation with staff.", "non-Orthodox synagogues", "Cambridge University", "airplane", "Tim Bevan", "Keswick", "Louis Le Vau", "horseshoes", "david copperfield", "the Monkees", "'Hansel and Gretel' cottage", "pakistan", "Charlotte Marie Pomeline Casiraghi", "Michael Phelps", "Vietnam", "Mumbai", "mumps", "stop motion effects", "Sebastian Flyte", "The Eagle", "sea spray", "Hindi", "Madrid", "Paul Bunyan", "Reverse - Flash", "the primary palate", "summer of 1990 and continued until 1992", "1986", "Cameroon", "1967", "1,500 Marines", "1,500", "came forward Monday \"for the other women who couldn't or wouldn't.\"", "cholesterol", "a volcano", "Twas the Night Before Christmas", "Baldwin is a hamlet and census-designated place (CDP) located in the town of Hempstead"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5602163461538462}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_triviaqa-validation-3998", "mrqa_triviaqa-validation-994", "mrqa_triviaqa-validation-5324", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-1508", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-7596", "mrqa_triviaqa-validation-7209", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-6967", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-6617", "mrqa_triviaqa-validation-944", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-4680", "mrqa_triviaqa-validation-7042", "mrqa_triviaqa-validation-225", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-8386", "mrqa_hotpotqa-validation-3558", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-1657", "mrqa_hotpotqa-validation-1557"], "SR": 0.515625, "CSR": 0.5209517045454546, "retrieved_ids": ["mrqa_squad-train-4459", "mrqa_squad-train-1032", "mrqa_squad-train-78966", "mrqa_squad-train-79056", "mrqa_squad-train-70519", "mrqa_squad-train-39420", "mrqa_squad-train-66270", "mrqa_squad-train-79455", "mrqa_squad-train-42313", "mrqa_squad-train-3084", "mrqa_squad-train-59081", "mrqa_squad-train-23661", "mrqa_squad-train-53398", "mrqa_squad-train-32882", "mrqa_squad-train-8889", "mrqa_squad-train-57045", "mrqa_naturalquestions-validation-8186", "mrqa_searchqa-validation-9617", "mrqa_searchqa-validation-15063", "mrqa_naturalquestions-validation-1279", "mrqa_hotpotqa-validation-1349", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-971", "mrqa_naturalquestions-validation-7827", "mrqa_hotpotqa-validation-2112", "mrqa_newsqa-validation-4207", "mrqa_hotpotqa-validation-2366", "mrqa_newsqa-validation-2657", "mrqa_hotpotqa-validation-1545", "mrqa_newsqa-validation-1288", "mrqa_hotpotqa-validation-1874", "mrqa_naturalquestions-validation-5932"], "EFR": 1.0, "Overall": 0.7171590909090909}, {"timecode": 88, "before_eval_results": {"predictions": ["Cool Runnings", "five times", "an explosion and a fire", "\"geek.\"", "elderships", "Andrew Davis", "Soil", "Lombardy", "7", "\"Realty Bites\"", "2007", "The Florida Panthers", "capital crimes or capital offences", "the 10-metre platform event", "Saturday Night Live", "3,000", "beer and soft drinks", "Shohola Falls", "The Kennedy Center", "Saoirse Ronan", "Rose Mary Woods", "Netherlands", "Miss Universe 2010", "John R. Leonetti", "Fat Albert", "Oklahoma State", "stoneware", "Prudence Jane Goward", "\"Ra.One\"", "Walcha", "U.S. Marshals", "Manhattan Project", "Apatosaurus", "Currer Bell", "Anne Perry", "Bergen County", "Santiago del Estero Province", "Province of Syracuse", "Deborah Lipstadt", "country", "219", "Charice", "Harlem", "Francis Schaeffer", "Eric Edward Whitacre", "George Balanchine", "Laura Dern", "Liberty", "Axl Rose", "the second", "notable for being one of the youngest publicly documented people to be identified as transgender", "Craig T. Nelson", "1928", "October 2, 2017", "Spanish", "warblers", "oregon", "a book.", "South Africa", "Russia", "Dalai Lama", "a tooth", "Edward VI", "Yemen"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6799851190476192}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-1405", "mrqa_hotpotqa-validation-1144", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-568", "mrqa_hotpotqa-validation-2211", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-5162", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-1870", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-5133", "mrqa_triviaqa-validation-3584", "mrqa_newsqa-validation-1389", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-7264"], "SR": 0.59375, "CSR": 0.5217696629213483, "EFR": 0.9615384615384616, "Overall": 0.7096303748919619}, {"timecode": 89, "before_eval_results": {"predictions": ["2012 NBA draft", "848", "Skipton Castle", "Consigliere", "midtempo hip hop ballad", "the Beatles", "first-round", "Francis Egerton", "Tamworth", "Girls Del Rey's \"Born to Die\"", "Eric Whitacre", "psychological horror", "Nicole Kidman", "Karl Kraus", "water", "\"Orchard County\"", "he turned 51, he died of cancer", "1947", "2004", "1903", "170", "2013", "Boulder High School in Boulder, Colorado", "Tian Tan Buddha", "Rocky Mountain Institute", "Macau, China", "La Nouba", "Stephanie Plum", "Memphis", "Big 12 Conference", "Figaro", "Eve Hewson", "Leon Marcus Uris", "U2 360\u00b0 Tour", "Eielson Air Force Base", "1,462", "Commerce", "the God of Israel", "three times", "Budget Rent a Car", "Dallas", "Terrence Jones", "Republican", "2015", "\u00c6thelstan", "black", "half a million acres", "Vikram, Jyothika and Reemma Sen", "\"The Bob Edwards Show\"", "Bill Miner", "50JJB Sports Fitness Clubs and the attached retail stores", "Tigris and Euphrates rivers", "$2.187 billion", "the egg", "Goldie Hawn", "gregory", "spain haddock", "North Korea", "$40 and a bread.", "laid 11 healthy eggs", "Myron Leon \" Mike\" Wallace", "Mount Erebus", "hibernators", "Capitol Hill,"], "metric_results": {"EM": 0.625, "QA-F1": 0.7302150106837606}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-3350", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-4715", "mrqa_hotpotqa-validation-3763", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2833", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-5603", "mrqa_naturalquestions-validation-6931", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-5948", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-339", "mrqa_searchqa-validation-16801", "mrqa_searchqa-validation-4758"], "SR": 0.625, "CSR": 0.5229166666666667, "EFR": 1.0, "Overall": 0.7175520833333333}, {"timecode": 90, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1547", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3812", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8546", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13830", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8529", "mrqa_squad-validation-8754", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-2575", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4585", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-7717", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-920"], "OKR": 0.80859375, "KG": 0.4765625, "before_eval_results": {"predictions": ["Delaware River", "The Shirehorses", "Detroit", "The Expendables 2", "Bob Hill", "Pennsylvania State University", "Princess Aisha bint Hussein", "1951", "Muskogean", "River Clyde", "aging issues", "Indooroopilly Shoppingtown", "Chow Tai Fook Enterprises", "Rhode Island School of Design", "Bill Walton", "Vanessa Hudgens", "1983", "Theodore Haynes", "NBA Rookie of the Year", "Rhodesia", "Edward Leonard \"Ed\" O'Neill", "My Backyard", "Lommel", "Bharat Ratna", "Aberdeenshire", "three", "1969", "Stalybridge Celtic", "Property management", "Vincent Anthony Guaraldi", "Pulitzer Prize for drama", "one live album", "2005", "Neon City", "Province of New York", "1932 and 1934", "Hopi", "American", "pop music and popular culture", "Australian", "SKUM", "Rain Man", "Wake Atoll", "Carlos Santana", "1942", "2027", "historic buildings, arts, and published works", "Magnate", "The Indianapolis Times", "143,007", "James I", "committed suicide", "1970", "nucleus", "lung", "discus thrower", "rachmaninoff", "three,", "police dogs", "aggression, from aboard the USNS Impeccable,\"", "Sierra Leone", "Bela Lugosi", "Jumpmaster", "11th year in a row."], "metric_results": {"EM": 0.65625, "QA-F1": 0.7633928571428572}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-4514", "mrqa_hotpotqa-validation-946", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-1048", "mrqa_hotpotqa-validation-5498", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2599", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-5537", "mrqa_hotpotqa-validation-744", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4864", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5991", "mrqa_newsqa-validation-2935", "mrqa_newsqa-validation-414", "mrqa_newsqa-validation-3310"], "SR": 0.65625, "CSR": 0.5243818681318682, "retrieved_ids": ["mrqa_squad-train-33042", "mrqa_squad-train-51430", "mrqa_squad-train-51085", "mrqa_squad-train-46449", "mrqa_squad-train-47446", "mrqa_squad-train-69185", "mrqa_squad-train-24516", "mrqa_squad-train-2804", "mrqa_squad-train-29569", "mrqa_squad-train-53201", "mrqa_squad-train-6057", "mrqa_squad-train-86152", "mrqa_squad-train-72252", "mrqa_squad-train-81502", "mrqa_squad-train-80924", "mrqa_squad-train-67095", "mrqa_hotpotqa-validation-4284", "mrqa_naturalquestions-validation-6770", "mrqa_squad-validation-5351", "mrqa_triviaqa-validation-2009", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-392", "mrqa_squad-validation-1649", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-851", "mrqa_hotpotqa-validation-1330", "mrqa_naturalquestions-validation-4633", "mrqa_naturalquestions-validation-8441", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-3320", "mrqa_triviaqa-validation-4859", "mrqa_hotpotqa-validation-5603"], "EFR": 1.0, "Overall": 0.7115169986263736}, {"timecode": 91, "before_eval_results": {"predictions": ["Milwaukee", "Chicago", "E.M. Forster", "James Joyce", "Jamestown", "Helen Hayes", "a dollar", "Philip", "Dobermann", "Vermont", "Moses", "the Moors", "Margaret Mitchell", "Henrik Ibsen", "a cruller", "Iina Menzel", "Hungarian", "Sugar Smacks", "Groundhog Day", "New Balance", "Mahlemuts", "Animal Crackers", "nitrogen and oxygen", "a platypus", "Nixon", "a bicycle", "The Magic Mountain", "Fiji", "Manassas", "Jacqueline Kennedy Onassis", "Forbes", "Death Row", "Anthony 'Tony Jack' Giacalone", "a DD clue", "a rabbit", "Bait-and-switch", "the Wiener Journal", "apartheid", "Robert Browning", "compound", "Molly Ringwald", "home security", "the Marine Corps", "Ho Chi Minh", "Ghost", "Arkansas", "the Stone of Destiny", "remoulade", "Florida", "Night Fever", "Latin", "Jane Addams", "Thomas Chisholm", "Norman Pritchard", "Ron Howard", "spider", "Istanbul", "Battle of the Rosebud", "40 Acres", "1851", "26", "$22 million", "Sixteen", "Ricky Nelson"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7578395562770562}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-8297", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-15957", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-524", "mrqa_searchqa-validation-13732", "mrqa_searchqa-validation-392", "mrqa_searchqa-validation-2402", "mrqa_searchqa-validation-8137", "mrqa_searchqa-validation-6520", "mrqa_searchqa-validation-15184", "mrqa_searchqa-validation-12969", "mrqa_searchqa-validation-13793", "mrqa_searchqa-validation-3990", "mrqa_searchqa-validation-13167", "mrqa_naturalquestions-validation-2176", "mrqa_hotpotqa-validation-2520"], "SR": 0.65625, "CSR": 0.5258152173913043, "EFR": 0.9545454545454546, "Overall": 0.7027127593873518}, {"timecode": 92, "before_eval_results": {"predictions": ["a bust", "A Chorus Line", "Pamplona", "French Toast", "Pop-Tarts", "Aesop", "the Shawnee", "postscript", "Babe Ruth", "the brothels", "the Orinoco", "(John) Bunyan", "Sicilian pizza", "Punjabi", "insulin", "(Jimmy) Hoffa", "Utah", "Newton", "Cincinnati", "George Washington", "Boots", "bronchodilator", "the Perseid meteor shower", "Phil Cavilleri", "the wall", "a person who carries luggage at a railway station", "Fess Parker", "Michelangelo", "I Wanna Be Your Man", "grease", "Kathleen Kennedy Townsend", "Henry Cavendish", "Israel", "euphoria", "Don Quixote", "Charlie and the Chocolate Factory", "baboon", "Last Summer", "shalom", "genes", "the Wild Thornberrys", "the Bionic Woman", "Selma", "Wynona Judd", "paradise", "knightly club", "HIV/AIDS", "the Smothers Brothers", "Henry Hudson", "the Sidecar", "Ford", "The management team", "the United States, the United Kingdom, and their respective allies", "2017", "table salt", "Toonami", "Caitlin", "AMC Entertainment Holdings, Inc.", "CBS' \"The Danny Kaye Show\"", "Montreal, Quebec", "Matthew Fisher,", "$150 billion over 10 years in clean energy.", "Tennessee.", "Johannes Gutenberg"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7639136904761905}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.8571428571428571, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-9251", "mrqa_searchqa-validation-6725", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-16819", "mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-9414", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-9483", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-8750", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-1759", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-3758", "mrqa_hotpotqa-validation-582", "mrqa_newsqa-validation-675"], "SR": 0.6875, "CSR": 0.5275537634408602, "EFR": 1.0, "Overall": 0.712151377688172}, {"timecode": 93, "before_eval_results": {"predictions": ["the three-Cornered Hat", "7-11", "Memphis", "Australia", "Anna", "the Hippopotamus", "Grant & Sherman", "carbon", "Victory", "the fingers", "a carriage", "the Catacombs", "Bahrain", "the Cohans", "Jean Harlow", "the yardarm", "Iesous", "29", "Mistletoe", "the council", "Islam", "Tijuana", "Cleopatra", "the Irish The Diffy", "the Capitol", "the Bauhaus", "Homeland Security", "JUMP", "the batrachus", "paradise", "the Baltic Sea", "Blazing Saddles", "Dom u Dobreho", "While You Were Out", "3", "the mnemonic rule of thumb", "Love Story", "Ramen", "Truman", "St. Louis", "stars", "the Pulled Pork Chili Verde", "Elizabeth Edward", "Stephen I", "Sgt. Pepper\\'s Lonely Hearts Club Band", "Colby", "the Amistad", "Prince", "the universe", "Wiktionary", "Tycho Brahe", "displacement", "Office of Inspector General", "February 6, 2005", "group", "Massachusetts", "Charles Darwin", "the Peninsular War", "The Sun", "24", "ice jam", "jobs", "removal of his diamond-studded braces.", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6509114583333333}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1189", "mrqa_searchqa-validation-11754", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-2718", "mrqa_searchqa-validation-5239", "mrqa_searchqa-validation-9419", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-13462", "mrqa_searchqa-validation-5163", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-1769", "mrqa_searchqa-validation-3622", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-9980", "mrqa_searchqa-validation-12543", "mrqa_searchqa-validation-7097", "mrqa_searchqa-validation-15694", "mrqa_searchqa-validation-4568", "mrqa_searchqa-validation-6841", "mrqa_searchqa-validation-5137", "mrqa_naturalquestions-validation-6993", "mrqa_triviaqa-validation-5122", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-2759", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-4042"], "SR": 0.578125, "CSR": 0.5280917553191489, "retrieved_ids": ["mrqa_squad-train-18269", "mrqa_squad-train-77045", "mrqa_squad-train-56825", "mrqa_squad-train-76479", "mrqa_squad-train-32057", "mrqa_squad-train-66929", "mrqa_squad-train-76353", "mrqa_squad-train-18345", "mrqa_squad-train-54938", "mrqa_squad-train-27204", "mrqa_squad-train-1553", "mrqa_squad-train-42977", "mrqa_squad-train-38516", "mrqa_squad-train-8845", "mrqa_squad-train-77695", "mrqa_squad-train-69086", "mrqa_newsqa-validation-3224", "mrqa_naturalquestions-validation-9181", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-2385", "mrqa_hotpotqa-validation-558", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-2063", "mrqa_hotpotqa-validation-4316", "mrqa_naturalquestions-validation-2420", "mrqa_newsqa-validation-1589", "mrqa_triviaqa-validation-7696", "mrqa_newsqa-validation-2018", "mrqa_hotpotqa-validation-5821", "mrqa_newsqa-validation-2491", "mrqa_triviaqa-validation-987"], "EFR": 1.0, "Overall": 0.7122589760638298}, {"timecode": 94, "before_eval_results": {"predictions": ["Southend Pier", "9 February 2018", "over most of the sixth century", "a party of six members of France's Legislative Assembly", "Peter Gardner Ostrum", "Jesse McCartney", "1902", "Beverly Callard", "February 27, 2007", "Toot - Toot", "Chuck Noland", "his teenage role as the title character on the Disney Channel television series The Famous Jett Jackson ( 1998 -- 2001 ) and as Chris Comer in the movie Friday Night Lights ( 2004 )", "1938", "60 by West All - Stars", "around 1872", "the plane crash", "February 2017", "the root systemic artery", "Pittsburgh", "the beginning of the American colonies", "James Ray", "Los Angeles", "The Outback", "regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Abigail Hawk", "January 2018", "The Archers", "Cody Fern", "homicidal thoughts of a troubled youth", "August 5, 1937", "San Francisco", "the most devastating stock market crash in the history of the United States", "The Magician", "The Royalettes", ". java", "`` Killer Within ''", "Juliet", "1995", "Roman Reigns", "1901", "1972", "Yondu Udonta", "1977", "Richard Stallman", "Keeley Clare Julia Hawes", "Abid Ali Neemuchwala", "Jay Baruchel", "March 26, 1973", "July 25, 2017", "1922", "Eddie Van Halen", "the Cyclades", "Some Like It Hot", "bird", "Vikram Bhatt", "Dar es Salaam", "fifth-largest", "Bob Johnson", "Iran's parliament speaker", "Sri Lanka", "love", "Mathew Brady", "the Rings", "Greenham Common"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6968867304804804}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.19999999999999998, 0.0, 0.0, 0.25, 0.4, 1.0, 0.0, 1.0, 0.972972972972973, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1911", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3557", "mrqa_searchqa-validation-9281"], "SR": 0.640625, "CSR": 0.5292763157894738, "EFR": 0.9130434782608695, "Overall": 0.6951045838100687}, {"timecode": 95, "before_eval_results": {"predictions": ["Jet Republic", "Frank Ricci,", "abusing its dominant position in the computer processing unit (CPU) market.", "drug cartels", "The students, who became known as the Little Rock Nine, were taunted and threatened by an angry mob.", "five", "Ben Roethlisberger", "Alan Graham", "Muslim", "Asashoryu", "\"Slumdog Millionaire\"", "Ma Khin Khin Leh,", "$17,000", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the pirates", "10 municipal police officers", "Bangladesh Rifle (BDR)", "Rima Fakih", "Mikkel Kessler", "Friday,", "would slow economic growth with higher taxes.", "Longo-Ciprelli, finally cracked her Olympic jinx.", "\"He tried to suppress the memories and to live as normal a life as possible; the culture of his time said that he was fortunate to have survived and that he should get on with his", "Hayden", "Muslim revolutionary named Malcolm X", "Dangjin", "\"mentally deranged person", "In the east and the north of the city, water was at waist-level in some neighborhoods.", "The patient, who prefers to be anonymous, is finally able to breathe through her nose, smell, eat solid foods and drink out of a cup,", "stole", "one", "Black History Month", "opposition parties", "via YouTube days after 35 bodies were found in two trucks during rush hour in the city of Boca del Rio.", "Cambodian territory", "2.5 million", "using injectable vitamin supplements because the quantities are not regulated.", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "President Obama has given the Muslim community around the world the message we have been waiting for.", "Abhisit Vejjajiva", "the iPods", "Yemeni port city of Aden", "228", "More than 15,000", "Rwandan genocide", "Operation Crank Call", "$5.5 billion to build.", "$106,482,500", "Long troop deployments in Iraq,", "prostate cancer,", "bartering", "William Wyler", "1608", "Philippians", "chicken", "time-distance", "fractal Geometry", "Paper", "SBS", "The Lord of the Rings", "the Deathly Hallows", "a diphthong", "Lotus", "nuclear weapons"], "metric_results": {"EM": 0.625, "QA-F1": 0.6760999111182935}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.07692307692307693, 0.0, 0.1142857142857143, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809523, 1.0, 1.0, 0.5714285714285714, 0.15384615384615385, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3913", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-1044", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-2184", "mrqa_hotpotqa-validation-3247", "mrqa_searchqa-validation-9089", "mrqa_searchqa-validation-6473"], "SR": 0.625, "CSR": 0.5302734375, "EFR": 0.9166666666666666, "Overall": 0.6960286458333333}, {"timecode": 96, "before_eval_results": {"predictions": ["\"one of the most magnificent expressions of freedom and free enterprise in history\"", "off Somalia's coast.", "2009", "club managers,", "5,600", "a construction site in the heart of Los Angeles.", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "free services.", "delivers a big speech", "Basilan", "the highest ranking former member of Saddam Hussein's regime still at large", "Fareed Zakaria", "Gov. Mark Sanford", "56", "$1.4 million,", "Climatecare,", "Australian officials", "fake his own death", "The torch for the 2010 Vancouver Olympics was lit in a ceremony at the ancient Greek site of Olympia", "Police", "one", "President Alvaro Uribe", "that the soldiers were exposed to sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium.", "The University of California San Diego", "July", "the two remaining crew members from the helicopter, which went down late Tuesday night,", "Darrel Mohler", "\"GoldenEye\"", "$1.5 million", "\"People have lost their homes, their jobs, their hope,\"", "$81,88010", "CNN's Larry King", "HPV (human papillomavirus)", "\"For the life of me I can't believe the TWU walked away from that offer,\"", "Body Tap,", "the Southern Baptist Convention", "Stephen Tyrone Johns", "Mawise Gumba", "\"The first line of law and order", "Facebook", "is saving jobs up and down the auto supply chain: from dealers to assembly workers and parts markers.", "police", "Susan Atkins,", "off the coast of Dubai", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "\"falling space debris,\"", "part", "Silicon Valley.", "63", "\"Slumdog Millionaire\"", "Lillo Brancato Jr.", "Maria works in a bridal shop with Anita", "Sylvester Stallone", "a decorative ornament", "butterflies", "vanilla", "France", "1241 until his death in 1250", "October 31, 1999", "Westfield Tea Tree Plaza", "Jacob Marley", "Johnny Weissmuller", "Attachment the terminal vertebrae to the jackass", "Mount Hood"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7155522486772488}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.19047619047619044, 1.0, 1.0, 0.0, 0.2962962962962963, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07999999999999999, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-1163", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-3800", "mrqa_newsqa-validation-2261", "mrqa_naturalquestions-validation-5305", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-5864", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-8576"], "SR": 0.640625, "CSR": 0.5314110824742269, "retrieved_ids": ["mrqa_squad-train-51718", "mrqa_squad-train-8071", "mrqa_squad-train-77788", "mrqa_squad-train-5997", "mrqa_squad-train-12031", "mrqa_squad-train-47676", "mrqa_squad-train-66831", "mrqa_squad-train-45224", "mrqa_squad-train-26319", "mrqa_squad-train-20693", "mrqa_squad-train-54495", "mrqa_squad-train-84769", "mrqa_squad-train-66803", "mrqa_squad-train-61108", "mrqa_squad-train-74569", "mrqa_squad-train-255", "mrqa_naturalquestions-validation-4387", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-2520", "mrqa_hotpotqa-validation-305", "mrqa_naturalquestions-validation-156", "mrqa_hotpotqa-validation-4501", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2941", "mrqa_hotpotqa-validation-2156", "mrqa_triviaqa-validation-1759", "mrqa_squad-validation-7131", "mrqa_hotpotqa-validation-1175", "mrqa_newsqa-validation-3160", "mrqa_naturalquestions-validation-5942", "mrqa_searchqa-validation-7435", "mrqa_searchqa-validation-7961"], "EFR": 0.9565217391304348, "Overall": 0.7042271893209323}, {"timecode": 97, "before_eval_results": {"predictions": ["Fred Astaire", "winnie mae", "Manchester United", "Pocahontas", "Geoffrey Chaucer", "Travis", "Jordan", "John Donne", "West Virginia", "dogs", "watchmaking", "Central African Republic", "ballet", "South Bank", "Cornwall", "the Severn", "green", "Fred Trueman", "Chapters I\u2013II", "Athens", "Yemen", "Loch Morar", "leprosy", "Manhunt 2", "your phone", "piano", "a gun", "collies", "Chongqing", "the best month", "2 Samuel", "George Fox", "bat", "person employed to write or type what another dictates or to copy what has been written", "France", "Melissa Duck", "haddock", "a single groove", "Ross MacManus", "to be performed", "dry rot", "a cuckoo", "Northumberland", "6", "Midnight Cowboy", "three", "1911", "a plump", "Ireland", "Lorenzo", "Pat Houston", "Robin Williams", "1260 cubic centimeters ( cm )", "John von Neumann", "Florida Panthers", "Fortunino Francesco Verdi", "67,575", "in July", "speed sailing", "Mary Procidano,", "Hedy Lamarr", "Quinn", "the Komodo Dragon", "goalkeeper"], "metric_results": {"EM": 0.625, "QA-F1": 0.6885416666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-5438", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-3406", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-7539", "mrqa_triviaqa-validation-7413", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-3485", "mrqa_triviaqa-validation-1585", "mrqa_naturalquestions-validation-7458", "mrqa_hotpotqa-validation-3953", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-1446"], "SR": 0.625, "CSR": 0.5323660714285714, "EFR": 0.875, "Overall": 0.6881138392857142}, {"timecode": 98, "before_eval_results": {"predictions": ["The Kite Runner", "Yves Saint Laurent", "Emily Post", "a shepherd", "Chopin", "curly", "glass", "Bolshoi Ballet", "\"Mending Wall\"", "Nathan Lane", "Cheaper by the Dozen", "Ferdinand", "Marlon Brando", "Sagamore Hill", "Peru", "Copenhagen", "the Arctic Ocean", "Henry Hudson", "Blofeld", "Dudevant", "Richard Cory", "Truffaut", "Barbie Doll", "one", "Chlorine", "David Letterman", "Hanoi", "the Byzantine Empire", "Cameroon", "Flav", "Dairy Queen", "Balkan", "Hawaii", "( Jimmy) Hoffa", "a frothy 'latte'", "Cincinnati", "bulldog", "gin", "John Paul Jones", "walk the plank", "Three Amigos!", "Haunted mansion halloween", "King George II", "Stonehenge", "Grease", "Nevada", "a tick", "pH", "Halsey", "Bangkok", "the lungs", "DeWayne Warren", "Doug Diemoz", "New England Patriots", "Philippines", "micky dolenz", "brashy", "Iynx", "400 MW", "high court of Admiralty", "held to a 1-1 draw at Stoke City.", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "the end of TV's rabbit-ears era.", "uncle"], "metric_results": {"EM": 0.625, "QA-F1": 0.7298611111111111}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-2008", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-15451", "mrqa_searchqa-validation-9719", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-13423", "mrqa_searchqa-validation-14990", "mrqa_searchqa-validation-9525", "mrqa_searchqa-validation-6036", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-11126", "mrqa_searchqa-validation-7874", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-9594", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-7141", "mrqa_searchqa-validation-2565", "mrqa_searchqa-validation-12782", "mrqa_naturalquestions-validation-8903", "mrqa_triviaqa-validation-6849", "mrqa_hotpotqa-validation-3975", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-509"], "SR": 0.625, "CSR": 0.5333017676767677, "EFR": 1.0, "Overall": 0.7133009785353536}, {"timecode": 99, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1547", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3812", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-1021", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11412", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13167", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-13374", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13830", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14826", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15694", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-2620", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-5423", "mrqa_searchqa-validation-5635", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7267", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8297", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9807", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8529", "mrqa_squad-validation-8754", "mrqa_squad-validation-9306", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-218", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2391", "mrqa_triviaqa-validation-2575", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3173", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4585", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-920"], "OKR": 0.8203125, "KG": 0.48671875, "before_eval_results": {"predictions": ["Gunpei Yokoi", "2022", "Thomas Mundy Peterson", "$2 million", "an instant messaging client that was first developed and popularized by the Israeli company Mirabilis in 1996", "first Sunday after Easter", "Acid rain", "The kid then blabs it out when Miley is around, leading Oliver to show her the picture of Jake with another girl", "1943", "Coriolis effect", "ranking used in combat sports", "a 2008 American comedy - drama film about the titular dog, Marley", "1966", "transmission", "an arm", "England and Wales", "Nicole Gale Anderson", "Massachusetts", "Dr Thomas Chamberlain ( Richard Lintern ), is introduced at the start of the seventeenth series", "George Ratzenberger", "In the episode `` Kobol's Last Gleaming ''", "March 1602", "Ethiopia", "third season", "the Hongwu Emperor of the Ming Dynasty", "lead vocalist Bart Millard", "Victory gardens", "The management team", "Nick Kroll", "Longliners commonly target swordfish, tuna, halibut, sablefish and many other species", "Valens", "FUE harvesting method", "a minority report", "Massachusetts", "the conclusion of a syllogism", "states", "2017 / 18", "2001", "cadmium", "Jason Momoa", "Phillip Paley", "Ku - Klip", "July 21, 1861", "Bonnie Aarons", "Andrew Michael Harrison", "in the cell nucleus", "drizzle, rain, sleet, snow, graupel and hail", "Internal epithelia", "the American League ( AL ) champion Cleveland Indians", "a multilayer", "Graub\u00fcnden", "jennel", "George Washington", "Lily Allen", "Tatton Park", "blind girl named Selina D'Arcy", "1964", "Ashley \"A.J.\" Jewell,", "42", "step up.\"", "Argentina", "ventriloquist", "Anne Hathaway", "HBO World Championship Boxing"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6210835987949959}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 0.23076923076923075, 1.0, 1.0, 0.3846153846153846, 0.1818181818181818, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-7963", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-402", "mrqa_triviaqa-validation-1348", "mrqa_hotpotqa-validation-3689", "mrqa_newsqa-validation-495", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-792"], "SR": 0.546875, "CSR": 0.5334375, "retrieved_ids": ["mrqa_squad-train-34132", "mrqa_squad-train-52625", "mrqa_squad-train-17867", "mrqa_squad-train-44452", "mrqa_squad-train-78985", "mrqa_squad-train-36563", "mrqa_squad-train-27519", "mrqa_squad-train-41597", "mrqa_squad-train-83231", "mrqa_squad-train-9479", "mrqa_squad-train-59734", "mrqa_squad-train-13673", "mrqa_squad-train-35059", "mrqa_squad-train-46084", "mrqa_squad-train-32601", "mrqa_squad-train-71111", "mrqa_hotpotqa-validation-4257", "mrqa_triviaqa-validation-3634", "mrqa_hotpotqa-validation-4422", "mrqa_triviaqa-validation-5724", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3435", "mrqa_naturalquestions-validation-9141", "mrqa_newsqa-validation-4012", "mrqa_searchqa-validation-13420", "mrqa_naturalquestions-validation-1378", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-5226", "mrqa_triviaqa-validation-5566", "mrqa_searchqa-validation-875", "mrqa_naturalquestions-validation-7443", "mrqa_squad-validation-6986"], "EFR": 0.9655172413793104, "Overall": 0.705337823275862}]}