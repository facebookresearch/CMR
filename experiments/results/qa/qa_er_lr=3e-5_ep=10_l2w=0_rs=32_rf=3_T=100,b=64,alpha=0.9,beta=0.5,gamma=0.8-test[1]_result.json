{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5400, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["April 20", "Paul Whiteman", "2005", "Islamism", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "the center of the curving path", "eight", "28", "global", "Fresno", "Amazonia: Man and Culture in a Counterfeit Paradise", "88%", "broken wing and leg", "Emmy Awards", "silt up the lake", "The TEU specifically excludes certain regions, for example the Faroe Islands, from the jurisdiction of European Union law", "legitimate medical purpose", "week 7", "Kromme Rijn", "Robert Boyle", "five", "Paul Samuelson", "The First Doctor encounters himself in the story The Space Museum", "until the age of 16", "80%", "Five", "threatened \"Old Briton\" with severe consequences", "less than 200,000", "Anheuser-Busch InBev", "Charles River", "Veni redemptor gentium", "northwestern Canada", "intracellular pathogenesis", "1998", "seven months old", "The Service Module was discarded", "July", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "Article 17(3)", "Doctor Who \u2013 The Ultimate Adventure", "Eric Roberts", "electricity could be used to locate submarines", "1910\u20131940", "the owner", "2.666 million", "September 1969", "Apollo Program Director", "dreams", "Charles I", "more than 1,100 tree species", "complete the modules to earn Chartered Teacher Status", "true larvae", "apicomplexan-related diseases", "Rev. Paul T. Stallsworth", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "2009", "Daniel Diermeier", "PNU and ODM camps", "136", "12 to 15 million", "flax", "February 1, 2016", "2001", "lipophilic alkaloid toxins"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8867121848739495}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6614", "mrqa_squad-validation-4170", "mrqa_squad-validation-6426", "mrqa_squad-validation-8037", "mrqa_squad-validation-7774", "mrqa_squad-validation-3885", "mrqa_squad-validation-1492", "mrqa_squad-validation-5788", "mrqa_squad-validation-4343"], "SR": 0.859375, "CSR": 0.859375, "EFR": 1.0, "Overall": 0.9296875}, {"timecode": 1, "before_eval_results": {"predictions": ["Carolina Panthers", "1530", "Gallifrey", "ca. 2 million", "intractable problems", "effective planning", "a nucleomorph", "Canterbury", "1873", "A bridge", "clergyman", "Commission v Italy", "sports tourism", "G", "eating both fish larvae and small crustaceans", "slow to complete division", "Astra's satellites", "T. T. Tsui Gallery", "1905", "theatres", "those who proceed to secondary school or vocational training", "1521", "Search the Collections", "CD8", "3 January 1521", "a bill", "the University of Aberdeen", "2014", "Missy", "US$10 a week", "Super Bowl City", "Dudley Simpson", "esoteric", "temperature and light", "4000 years", "a new entrance building", "Levi's Stadium", "tutor", "the application of electricity", "2007", "Los Angeles", "zeta function", "adviser to churches in new territories", "over $40 million", "Sunday Service of the Methodists in North America", "Lead fusible plugs", "occupational stress", "Synthetic aperture radar", "European Parliament and the Council of the European Union", "the coronation of Queen Elizabeth II", "time O(n2)", "the Main Quadrangles", "35", "quadratic time", "antisemitic", "over-fishing and long-term environmental changes", "the Onon River and the Burkhan Khaldun mountain", "Hassan al-Turabi", "robbery", "Arlen Specter", "it is Oprah's daughters", "Venus Williams", "prisoners at the South Dakota State Penitentiary", "a globose pome"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7407606792717087}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 0.2857142857142857, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3958", "mrqa_squad-validation-4648", "mrqa_squad-validation-8864", "mrqa_squad-validation-9454", "mrqa_squad-validation-7818", "mrqa_squad-validation-1272", "mrqa_squad-validation-2122", "mrqa_squad-validation-2481", "mrqa_squad-validation-9977", "mrqa_squad-validation-1719", "mrqa_squad-validation-1818", "mrqa_squad-validation-2525", "mrqa_squad-validation-6073", "mrqa_newsqa-validation-2834", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-1148", "mrqa_triviaqa-validation-5325"], "SR": 0.703125, "CSR": 0.78125, "EFR": 1.0, "Overall": 0.890625}, {"timecode": 2, "before_eval_results": {"predictions": ["advanced research and education networking in the United States", "best-known legend", "Egyptians", "quantity surveyor", "\"missile gap\"", "Tanaghrisson", "1852", "1564", "Concentrated O2 will allow combustion to proceed rapidly and energetically", "adenosine triphosphate", "300", "11.5 inches (292.1 mm)", "1964", "infected corpses", "CD4", "Lutheran and Reformed", "Chinggis Khaan International Airport", "modern buildings", "warmest months from May through September, while the driest months are from November through April", "NBC", "Three", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Gymnosperms", "to punish Christians by God", "Word and Image department", "Nafzger", "MPEG-4", "British Sky Broadcasting Group plc", "chromalveolates", "embroidery", "three", "26", "Duran Duran", "warmest regions with hot winds blowing from nearby semi-deserts", "long distance services", "in the chloroplasts of C4 plants", "the violence that subsequently engulfed the country", "primality", "divergent boundaries", "Chancel Chapel", "Kurt H. Debus", "the construction of military roads to the area by Braddock and Forbes", "in bays where they occur in very high numbers", "Dallas", "Ted Heath", "late 14th-century", "high-voltage", "contract", "Arabic numerals", "pamphlets on Islam", "draftsman", "1993\u201394", "$10 billion", "France", "20", "Iran", "the Koreans edge into second place in Asian qualifying Group 2 to finish ahead of Saudi Arabia on goal difference and seal their place in the finals.", "the results by a chaplain about 1:45 p.m., per jail policy", "56", "school", "English Premier League Fulham produced a superb performance in Switzerland on Wednesday to eliminate opponents Basel from the Europa League with a 3-2 victory.", "AbdulMutallab was in the bathroom for about 15 to 20 minutes, which seemed long to the passenger, Tukel said.", "coca wine", "Dissection"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7813322368421052}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0909090909090909, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3088", "mrqa_squad-validation-3480", "mrqa_squad-validation-8905", "mrqa_squad-validation-7211", "mrqa_squad-validation-1909", "mrqa_squad-validation-2565", "mrqa_squad-validation-2772", "mrqa_squad-validation-9379", "mrqa_squad-validation-2906", "mrqa_squad-validation-2899", "mrqa_squad-validation-4322", "mrqa_squad-validation-2291", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-1201"], "SR": 0.71875, "CSR": 0.7604166666666666, "EFR": 0.9444444444444444, "Overall": 0.8524305555555556}, {"timecode": 3, "before_eval_results": {"predictions": ["\"vanguard of change and Islamic reform\"", "less than a year", "dampening the fire", "187 feet (57 m)", "Zagreus", "Swynnerton Plan", "extra-legal", "Harvey Martin", "December 1895", "lion, leopard, buffalo, rhinoceros, and elephant", "Establishing \"natural borders\"", "drama series", "17", "sold", "income inequality", "pastor", "1534", "mesoglea", "20%", "Isaac Newton", "Miocene", "\"citizenship\"", "13", "friction", "Super Bowl City", "three", "Georgia", "Fort Caroline", "Horace Walpole", "Afrikaans", "adaptive immune system", "24", "applications such as on-line betting, financial applications", "United Kingdom", "issues related to the substance of the statement", "2007", "Luther's anti-Jewish works", "short-tempered and even harsher", "the First Minister", "two catechisms", "orientalism and tropicality", "Robert Stephenson", "Super Bowl 50", "Beryl", "prima scriptura", "the Mongol Empire", "\"LOVE Radio", "\u201cLady\u201d or a \u201cWoman\u201d", "The Armoury Inn", "\"The Mullen \u00ef\u00bf\u00bd\"", "cutis anserina", "Bogota", "Artemis", "Malaxis paludosa", "Chile's second-largest city", "airline pilot, author and broadcaster", "\"Cup of tea!\"", "South Africa", "9", "Dorset", "The Daily Mirror", "Kat ( Jessie Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Paul Lynde", "adenosine diphosphate ( ADP )"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7543612637362638}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-3139", "mrqa_squad-validation-2486", "mrqa_squad-validation-9540", "mrqa_squad-validation-433", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_naturalquestions-validation-2472"], "SR": 0.734375, "CSR": 0.75390625, "retrieved_ids": ["mrqa_squad-train-12030", "mrqa_squad-train-16867", "mrqa_squad-train-6294", "mrqa_squad-train-67415", "mrqa_squad-train-8877", "mrqa_squad-train-11743", "mrqa_squad-train-9429", "mrqa_squad-train-85549", "mrqa_squad-train-31161", "mrqa_squad-train-71122", "mrqa_squad-train-77395", "mrqa_squad-train-39045", "mrqa_squad-train-10859", "mrqa_squad-train-50508", "mrqa_squad-train-44361", "mrqa_squad-train-65325", "mrqa_squad-validation-7211", "mrqa_squad-validation-2291", "mrqa_triviaqa-validation-5325", "mrqa_squad-validation-7818", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-808", "mrqa_squad-validation-2525", "mrqa_squad-validation-7774", "mrqa_squad-validation-4648", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2834", "mrqa_squad-validation-6073", "mrqa_squad-validation-3885", "mrqa_squad-validation-4170", "mrqa_squad-validation-1909", "mrqa_squad-validation-8905"], "EFR": 1.0, "Overall": 0.876953125}, {"timecode": 4, "before_eval_results": {"predictions": ["four days", "the International Fr\u00e9d\u00e9ric Chopin Piano Competition", "Nuda", "Erg\u00e4nzungsschulen", "21", "The Christmas Invasion", "early vertebrates", "Miocene", "Robert Underwood Johnson", "Cuba", "the Horniman Museum", "the ability to pursue valued goals", "University of Erfurt", "curved", "Johann Sebastian Bach", "1524\u201325", "one of the most significant developments in a century of Anglo-French conflict", "the basis of the methodology used", "2009", "British", "2005", "Germany and Austria", "simple self-starting design", "two tumen", "international metropolitan region", "Henry Laurens", "planned projects sponsored by the National Science Foundation (NSF) beginning in 1985", "Arriva", "international football", "second-largest", "biologist", "upper sixth", "arrested", "Pakistan", "1185", "Central business districts", "Hisao Yamada", "the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "cornwall", "fossils", "co-NP", "between 1.4 and 5.8 \u00b0C above 1990 levels", "in 2007 Charlie Crist took over for Jeb Bush", "in 1986 American coming-of-age comedy-drama adventure film directed by Rob Reiner", "ltd", "Rush H. Limbaugh III", "Henry Paulson", "ltd", "Marie Antoinette", "oak leaf", "in the car with her mother, Mrs. Har-... crease in milk sales in the college.", "the three kingdoms of Denmark, Sweden", "Milton Glaser", "in 1908, 27 years after killing Billy the Kid, he was shot to death by a New Mexico rancher", "ltd", "the Killers", "ltd", "Russell Crowe", "The Love Boat", "the Holy Grail", "Skat", "in 2011, one of the most lucrative, and legally complicated, ways for musicians to earn money", "Christopher Nolan", "gun"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6899373973727423}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 0.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714288, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-680", "mrqa_squad-validation-10145", "mrqa_squad-validation-1360", "mrqa_squad-validation-6046", "mrqa_squad-validation-4848", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-8372", "mrqa_squad-validation-6279", "mrqa_squad-validation-5178", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-9913", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-2062", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-5041", "mrqa_triviaqa-validation-1816"], "SR": 0.640625, "CSR": 0.73125, "EFR": 0.9565217391304348, "Overall": 0.8438858695652174}, {"timecode": 5, "before_eval_results": {"predictions": ["the wedding banquet", "respiration", "a deficit", "$216,000", "City of Malindi", "Apollo Applications Program", "trial division", "1991", "An attorney", "eight", "The Book of Discipline", "Olivier Messiaen", "1759-60", "The Rankine cycle", "deadly explosives", "first half of the eighteenth century", "5,560", "Tricia Marwick", "the main contractor", "\"ctenes", "third", "adjustable spring-loaded valve", "agriculture", "metamorphosed", "David Suzuki", "in territories where a member state is responsible for external relations", "June 1979", "4.95 mL", "December 12", "second half of the 20th Century", "Ten", "the edge railed rack and pinion Middleton Railway", "John Elway", "1598", "The Eleventh Doctor", "Tugh Temur", "\"War of Currents", "dampening the fire", "eight", "the Dalai Lama", "it was part of a planned training exercise designed to help the prince learn to fly in combat situations", "a multibillion dollar arms deal", "the Dalai Lama", "lethal", "2-0", "Chesley", "Stanford", "issued his first military orders", "July", "women", "McChrystal", "the Bronx", "Beijing", "Fernando Gonzalez", "sportswear", "friends", "suspend all aid operations", "the sins of the members of the church", "humans", "60", "George Best", "June 26, 2018", "Mahler Symphonies 1 & 8", "March 19, 2017"], "metric_results": {"EM": 0.703125, "QA-F1": 0.753421160130719}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9411764705882353, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7831", "mrqa_squad-validation-4172", "mrqa_squad-validation-3555", "mrqa_squad-validation-3179", "mrqa_squad-validation-383", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-539", "mrqa_naturalquestions-validation-8633", "mrqa_hotpotqa-validation-5406", "mrqa_searchqa-validation-6817"], "SR": 0.703125, "CSR": 0.7265625, "EFR": 1.0, "Overall": 0.86328125}, {"timecode": 6, "before_eval_results": {"predictions": ["John Mayow", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "1 July 1851", "1562", "32.9%", "the architect's client and the main contractor", "2009", "Katy\u0144 Museum", "Linebacker Brandon Marshall", "silicon", "methotrexate or azathioprine", "over 100,000", "3.55 inches", "Bukhara", "Beyonc\u00e9", "its safaris", "Huntington Boulevard", "Northern Pride Festival", "eight", "Paul Revere", "Roone Arledge", "Levi's Stadium", "autoimmune", "Diarmaid MacCulloch", "permafrost", "University Athletic Association", "idolatry", "Protestant clergy to marry", "a suite of network protocols", "German creedal hymn", "21 to 11", "eight", "\"Yes, I committed the act of which you accuse me.", "1130", "at least 28", "the 50 states of the United States of America", "November 25, 2002", "the center of the Northern Hemisphere", "fovea centralis", "Malcam", "Will", "MGM Resorts International", "Bobby Darin", "Gene MacLellan", "Matt Monro", "Andhra Pradesh and Odisha", "2013", "Wisconsin", "Jonathan Breck", "Neil Young", "cutting", "the heat required to produce more of them", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "humid subtropical", "Sir Alex Ferguson", "Bachendri Pal", "April 2011", "1979", "Manet", "Justin Spitzer", "Dr. Paul Appelbaum", "Resting on or touching the ground or bottom", "a lung disease that makes it hard to breathe", "President of the United States."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6582726077685754}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.5, 1.0, 0.18181818181818185, 1.0, 0.26666666666666666, 0.16666666666666666, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.967741935483871, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1166", "mrqa_squad-validation-291", "mrqa_squad-validation-3511", "mrqa_squad-validation-8400", "mrqa_squad-validation-3119", "mrqa_squad-validation-170", "mrqa_squad-validation-6223", "mrqa_squad-validation-4673", "mrqa_squad-validation-978", "mrqa_squad-validation-6913", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-4865", "mrqa_newsqa-validation-130", "mrqa_searchqa-validation-10794", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-9370"], "SR": 0.578125, "CSR": 0.7053571428571428, "retrieved_ids": ["mrqa_squad-train-29941", "mrqa_squad-train-37992", "mrqa_squad-train-10409", "mrqa_squad-train-48740", "mrqa_squad-train-62647", "mrqa_squad-train-76105", "mrqa_squad-train-54845", "mrqa_squad-train-30963", "mrqa_squad-train-8474", "mrqa_squad-train-66286", "mrqa_squad-train-15018", "mrqa_squad-train-8819", "mrqa_squad-train-76429", "mrqa_squad-train-54075", "mrqa_squad-train-74548", "mrqa_squad-train-30549", "mrqa_searchqa-validation-3352", "mrqa_squad-validation-8864", "mrqa_squad-validation-4848", "mrqa_newsqa-validation-2281", "mrqa_triviaqa-validation-1816", "mrqa_searchqa-validation-5041", "mrqa_squad-validation-6426", "mrqa_naturalquestions-validation-8633", "mrqa_newsqa-validation-3727", "mrqa_triviaqa-validation-4928", "mrqa_squad-validation-3139", "mrqa_searchqa-validation-6902", "mrqa_squad-validation-9454", "mrqa_squad-validation-8905", "mrqa_squad-validation-3088", "mrqa_newsqa-validation-2998"], "EFR": 1.0, "Overall": 0.8526785714285714}, {"timecode": 7, "before_eval_results": {"predictions": ["The Hoppings", "Chicago", "Genghis Khan is mad to have massacred so many people and laid waste so many lands", "Inherited wealth", "\"There is a world of difference between his belief in salvation and a racial ideology.", "StubHub Center", "WMO Executive Council and UNEP Governing Council", "trial division", "1999", "Gian Lorenzo Bernini", "housing", "dendritic cells, keratinocytes and macrophages", "Newcastle Mela", "ambiguity", "1665", "The Day of the Doctor", "punts", "1.6 kilometres", "\"winds up\" the debate", "2005", "around 100,000", "thermal expansion", "John Sutcliffe", "Owen Daniels", "incitement to terrorism", "Brookhaven", "A construction project", "Roy", "17 February 1546", "if (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people)", "September 30, 1960", "Brian Steele", "the status line", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "a premium", "left coronary artery", "five", "Havana Harbor", "four", "1932", "Walter Mondale", "Mitch Murray", "Marie Fredriksson", "December 1, 2009", "virtual reality simulator", "Noah Schnapp", "the outlaw couple Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "compasses", "Kanawha River", "Julie Stichbury", "in consistency and content", "William J. Bell", "Spanish", "September 2017", "Krypton", "1990", "the Soviet Union", "meadow brown", "Murcia", "India", "Wolfgang Amadeus Mozart", "Armin Meiwes", "Mot\u00f6rhead", "Bill Clinton"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6697865774216663}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7428571428571429, 1.0, 1.0, 1.0, 0.8372093023255813, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6294", "mrqa_squad-validation-2608", "mrqa_squad-validation-8910", "mrqa_squad-validation-5189", "mrqa_squad-validation-6402", "mrqa_squad-validation-6205", "mrqa_squad-validation-7632", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-5010", "mrqa_triviaqa-validation-2674", "mrqa_searchqa-validation-5845"], "SR": 0.609375, "CSR": 0.693359375, "EFR": 1.0, "Overall": 0.8466796875}, {"timecode": 8, "before_eval_results": {"predictions": ["Prague", "high humidity", "Peyton Manning", "Sports Night", "2nd century BCE", "taxation", "punish to our emperor perpetual victory over our enemies", "collective bargaining", "the Ilkhanate", "a second Gleichschaltung or similar event in the future", "1864", "NL and NC", "fifty", "Albert Einstein", "discarded", "state or government schools", "\"ash tree\"", "Commission v France", "without destroying historical legitimacy", "1652", "adaptive immune system", "case law by the Court of Justice", "Von Miller", "mad scientist", "Dendritic cells", "Allston Science Complex", "writing a five volume book in his native Greek \u03a0\u03b5\u03c1\u03af \u03cd\u03bb\u03b7\u03c2 \u03b9\u03b1\u03c4\u03c1\u03b9\u03ba\u03ae\u03c2 in the 1st century AD", "wars", "in soils", "creating complementary economic and political units for different ethnic groups", "Hirschman", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Professor Kantorek", "four", "bohrium", "a major fall in stock prices", "Buddhism", "March 31 to April 8, 2018", "Blind carbon copy to tertiary recipients who receive the message", "start fires, hunt, and bury their dead", "Ray Charles", "while studying All My Sons by Arthur Miller", "omitted and an additional panel stating the type of hazard ahead", "Rose Stagg", "provide funding at a rate or formula based on the previous year's funding", "Arnold Schoenberg", "Tiffany Adams Coyne", "Kent Robbins", "Evermoist   Whiskey Shivers as Saddle Up", "Southampton", "July 23, 2016", "Auburn Tigers football team", "Katherine Kiernan Maria", "1943", "Rumplestiltskin", "Kryptonite", "molecular cloning, RNA sequencing, polymerase chain reaction ( PCR ), or genome analysis", "Santiago", "Japan", "Lance Cpl. Maria Lauterbach", "Johnny Depp", "californium", "(Heifetz, Perlman", "polygon with three edges and three vertices"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5824092890191064}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.19999999999999998, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 0.0, 1.0, 0.9142857142857143, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.5, 0.5, 0.6, 1.0, 0.32558139534883723, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.5, 0.0, 0.3333333333333333, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 1.0, 0.10256410256410255, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-976", "mrqa_squad-validation-2565", "mrqa_squad-validation-7409", "mrqa_squad-validation-6962", "mrqa_squad-validation-4634", "mrqa_squad-validation-3113", "mrqa_squad-validation-6678", "mrqa_squad-validation-3946", "mrqa_squad-validation-6310", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-2351", "mrqa_newsqa-validation-2518", "mrqa_searchqa-validation-10924", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-2789"], "SR": 0.453125, "CSR": 0.6666666666666667, "EFR": 0.9714285714285714, "Overall": 0.819047619047619}, {"timecode": 9, "before_eval_results": {"predictions": ["gold", "War of Currents", "113", "Antoine Lavoisier", "the United Kingdom, Australia, Canada and the United States", "well before Braddock's departure for North America.", "Philip Webb and William Morris", "forming a 'A National Gallery of British Art',", "the Middle East could become another superpower confrontation with the USSR was of more concern to the US than oil.", "the government of the United Kingdom was controlled by the Conservative Party, while Scotland itself elected relatively few Conservative MPs.", "exceeds any given number", "90.20 K (\u2212182.95 \u00b0C, \u2212297.31 \u00b0F)", "the Channel Islands", "additional warming of the Earth's surface", "they were nomads", "private southern Chinese manufacturers and merchants", "Owen Jones", "the Eleventh Doctor", "the BBC National Orchestra of Wales", "bilaterians", "the traditional Chinese autocratic-bureaucratic system", "August 1992", "NBC", "the region was superior to that of the British, since Ren\u00e9-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier.", "New South Wales", "November 17, 2017", "Daren Maxwell Kagasoff", "the Royal Air Force ( RAF )", "the Sunni Muslim family", "3,000 metres ( 9,800 ft )", "to form a higher alkane", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "James Watson and Francis Crick", "September 29, 2017", "11 p.m. to 3 a.m", "the Anglo - Norman French waleis", "204,408", "Lake Michigan", "orbit", "7000301604928199000", "the frequency f, wavelength \u03bb, or photon energy E.", "Spanish explorers", "the roofs of the choir side - aisles at Durham Cathedral", "the president", "in different parts of the globe", "epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results ( including peer review and occasional systematic review )", "in florida it is illegal to sell alcohol before 1 pm on any sunday", "4,840 community hospitals, which are defined as nonfederal, short - term general, or specialty hospitals", "Napoleon", "the level of the third lumbar vertebra, or L3, at birth", "T'Pau", "the London Symphony Orchestra and London Philharmonic", "Julia Ormond", "the term was historically associated with fighters such as Benny Leonard and Sugar Ray Robinson who were widely considered to be the most skilled fighters of their day, to distinguish them from the generally more popular ( and better compensated ) heavyweight champions", "George Strait", "Amber Riley and her partner Derek Hough", "the uterus", "study insects and their relationship to humans, other organisms, and the environment", "195029 June 1994", "Efrem Zimbalist Jr.", "Guam", "Hakeemullah Mehsud", "Bob Dylan", "a mythical half-human and half-eagle creature"], "metric_results": {"EM": 0.5, "QA-F1": 0.5832119899044126}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7647058823529412, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.8181818181818182, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.06666666666666667, 1.0, 0.11764705882352941, 1.0, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 0.16666666666666666, 0.4, 0.0, 1.0, 0.2711864406779661, 1.0, 0.0, 0.0, 0.16666666666666669, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7888", "mrqa_squad-validation-3585", "mrqa_squad-validation-9334", "mrqa_squad-validation-7771", "mrqa_squad-validation-8410", "mrqa_squad-validation-5733", "mrqa_squad-validation-10232", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-1783", "mrqa_triviaqa-validation-2579", "mrqa_triviaqa-validation-3868", "mrqa_hotpotqa-validation-940", "mrqa_hotpotqa-validation-1398", "mrqa_newsqa-validation-3354", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-16618"], "SR": 0.5, "CSR": 0.65, "retrieved_ids": ["mrqa_squad-train-43233", "mrqa_squad-train-57713", "mrqa_squad-train-85862", "mrqa_squad-train-85869", "mrqa_squad-train-46176", "mrqa_squad-train-11966", "mrqa_squad-train-61877", "mrqa_squad-train-31162", "mrqa_squad-train-82691", "mrqa_squad-train-78116", "mrqa_squad-train-58279", "mrqa_squad-train-85836", "mrqa_squad-train-79989", "mrqa_squad-train-62945", "mrqa_squad-train-4324", "mrqa_squad-train-64163", "mrqa_naturalquestions-validation-10271", "mrqa_squad-validation-3480", "mrqa_newsqa-validation-2834", "mrqa_squad-validation-4172", "mrqa_squad-validation-6678", "mrqa_newsqa-validation-1811", "mrqa_squad-validation-3113", "mrqa_newsqa-validation-2778", "mrqa_squad-validation-6223", "mrqa_triviaqa-validation-387", "mrqa_naturalquestions-validation-8474", "mrqa_squad-validation-5178", "mrqa_squad-validation-7774", "mrqa_naturalquestions-validation-1340", "mrqa_squad-validation-8372", "mrqa_squad-validation-4673"], "EFR": 1.0, "Overall": 0.825}, {"timecode": 10, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-940", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8859", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-996", "mrqa_naturalquestions-validation-9961", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1354", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-299", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3059", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-1101", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-13775", "mrqa_searchqa-validation-16618", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10035", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10145", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10301", "mrqa_squad-validation-10359", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-10415", "mrqa_squad-validation-10466", "mrqa_squad-validation-1082", "mrqa_squad-validation-1109", "mrqa_squad-validation-1131", "mrqa_squad-validation-1151", "mrqa_squad-validation-1166", "mrqa_squad-validation-1180", "mrqa_squad-validation-1180", "mrqa_squad-validation-1195", "mrqa_squad-validation-121", "mrqa_squad-validation-1217", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1395", "mrqa_squad-validation-1468", "mrqa_squad-validation-1488", "mrqa_squad-validation-1492", "mrqa_squad-validation-1621", "mrqa_squad-validation-1630", "mrqa_squad-validation-1645", "mrqa_squad-validation-170", "mrqa_squad-validation-1719", "mrqa_squad-validation-1732", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1962", "mrqa_squad-validation-1971", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2082", "mrqa_squad-validation-2122", "mrqa_squad-validation-2159", "mrqa_squad-validation-217", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2361", "mrqa_squad-validation-2412", "mrqa_squad-validation-2416", "mrqa_squad-validation-2418", "mrqa_squad-validation-2457", "mrqa_squad-validation-2486", "mrqa_squad-validation-2548", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2608", "mrqa_squad-validation-2633", "mrqa_squad-validation-2667", "mrqa_squad-validation-2678", "mrqa_squad-validation-2843", "mrqa_squad-validation-2861", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2899", "mrqa_squad-validation-291", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-2985", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3088", "mrqa_squad-validation-3104", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3119", "mrqa_squad-validation-3162", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3209", "mrqa_squad-validation-3215", "mrqa_squad-validation-3241", "mrqa_squad-validation-3307", "mrqa_squad-validation-3355", "mrqa_squad-validation-3362", "mrqa_squad-validation-3407", "mrqa_squad-validation-3411", "mrqa_squad-validation-3413", "mrqa_squad-validation-3451", "mrqa_squad-validation-348", "mrqa_squad-validation-349", "mrqa_squad-validation-3522", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3559", "mrqa_squad-validation-3569", "mrqa_squad-validation-3585", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3822", "mrqa_squad-validation-383", "mrqa_squad-validation-3830", "mrqa_squad-validation-3841", "mrqa_squad-validation-3855", "mrqa_squad-validation-3882", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-3946", "mrqa_squad-validation-4008", "mrqa_squad-validation-4028", "mrqa_squad-validation-4046", "mrqa_squad-validation-4121", "mrqa_squad-validation-4172", "mrqa_squad-validation-423", "mrqa_squad-validation-4296", "mrqa_squad-validation-433", "mrqa_squad-validation-4331", "mrqa_squad-validation-4376", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4418", "mrqa_squad-validation-4430", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-4463", "mrqa_squad-validation-4495", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4648", "mrqa_squad-validation-4673", "mrqa_squad-validation-4704", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-477", "mrqa_squad-validation-4772", "mrqa_squad-validation-4803", "mrqa_squad-validation-4807", "mrqa_squad-validation-4841", "mrqa_squad-validation-4848", "mrqa_squad-validation-4936", "mrqa_squad-validation-4983", "mrqa_squad-validation-5023", "mrqa_squad-validation-5063", "mrqa_squad-validation-5083", "mrqa_squad-validation-513", "mrqa_squad-validation-513", "mrqa_squad-validation-5136", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5247", "mrqa_squad-validation-5250", "mrqa_squad-validation-5254", "mrqa_squad-validation-5265", "mrqa_squad-validation-5295", "mrqa_squad-validation-5307", "mrqa_squad-validation-5318", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5418", "mrqa_squad-validation-5445", "mrqa_squad-validation-5485", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5563", "mrqa_squad-validation-5564", "mrqa_squad-validation-5566", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5608", "mrqa_squad-validation-5653", "mrqa_squad-validation-5664", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5766", "mrqa_squad-validation-5782", "mrqa_squad-validation-5788", "mrqa_squad-validation-5821", "mrqa_squad-validation-5843", "mrqa_squad-validation-5852", "mrqa_squad-validation-5951", "mrqa_squad-validation-6046", "mrqa_squad-validation-6049", "mrqa_squad-validation-6067", "mrqa_squad-validation-6073", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6294", "mrqa_squad-validation-6310", "mrqa_squad-validation-638", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-653", "mrqa_squad-validation-6531", "mrqa_squad-validation-6535", "mrqa_squad-validation-6548", "mrqa_squad-validation-6567", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-6678", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6840", "mrqa_squad-validation-6841", "mrqa_squad-validation-6868", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6913", "mrqa_squad-validation-6917", "mrqa_squad-validation-6959", "mrqa_squad-validation-6962", "mrqa_squad-validation-6981", "mrqa_squad-validation-703", "mrqa_squad-validation-7030", "mrqa_squad-validation-7142", "mrqa_squad-validation-7175", "mrqa_squad-validation-7211", "mrqa_squad-validation-7252", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-743", "mrqa_squad-validation-7435", "mrqa_squad-validation-7456", "mrqa_squad-validation-7554", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7632", "mrqa_squad-validation-7633", "mrqa_squad-validation-7678", "mrqa_squad-validation-7699", "mrqa_squad-validation-7704", "mrqa_squad-validation-7709", "mrqa_squad-validation-7716", "mrqa_squad-validation-7747", "mrqa_squad-validation-7766", "mrqa_squad-validation-7771", "mrqa_squad-validation-7774", "mrqa_squad-validation-7775", "mrqa_squad-validation-7800", "mrqa_squad-validation-7818", "mrqa_squad-validation-7831", "mrqa_squad-validation-7846", "mrqa_squad-validation-7863", "mrqa_squad-validation-7888", "mrqa_squad-validation-7899", "mrqa_squad-validation-795", "mrqa_squad-validation-7965", "mrqa_squad-validation-797", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8071", "mrqa_squad-validation-8163", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8365", "mrqa_squad-validation-8372", "mrqa_squad-validation-8410", "mrqa_squad-validation-8414", "mrqa_squad-validation-8441", "mrqa_squad-validation-8486", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8589", "mrqa_squad-validation-859", "mrqa_squad-validation-8598", "mrqa_squad-validation-8600", "mrqa_squad-validation-8666", "mrqa_squad-validation-8670", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8907", "mrqa_squad-validation-9020", "mrqa_squad-validation-9030", "mrqa_squad-validation-9050", "mrqa_squad-validation-9116", "mrqa_squad-validation-9121", "mrqa_squad-validation-9151", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9379", "mrqa_squad-validation-9401", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9454", "mrqa_squad-validation-9465", "mrqa_squad-validation-9484", "mrqa_squad-validation-9540", "mrqa_squad-validation-9590", "mrqa_squad-validation-9608", "mrqa_squad-validation-9689", "mrqa_squad-validation-9738", "mrqa_squad-validation-976", "mrqa_squad-validation-9760", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9954", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2579", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-6711"], "OKR": 0.931640625, "KG": 0.409375, "before_eval_results": {"predictions": ["2009", "Huntington Boulevard", "regularly since 1979", "William Hartnell and Patrick Troughton", "Distributed Adaptive Message Block Switching", "1331", "Confucianism", "a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981", "pseudo-scienceences", "a military coup d'\u00e9tat", "the Tyne Tunnel", "proteolysis", "a form of starch", "Eisleben", "break off the cathode, pass out of the tube, and physically strike him", "a special episode of The Late Late Show with James Corden", "13\u20137", "around half", "the comprehensive institutions of the Great Yuan", "in homologous recombination and replication structures similar to bacteriophage T4", "Chuck Howley", "with observations", "Annette", "Tim McGraw and Kenny Chesney", "a man who could assume the form of a great black bear", "a house edge of between 0.5 % and 1 %, placing blackjack among the cheapest casino table games", "artificial insemination, in vitro fertilization, external ejaculation without copulation, or copulation shortly after ovulation", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "a sociological perspective which developed around the middle of the twentieth century and that continues to be influential in some areas of the discipline", "6 January 793", "Colon Street", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Sam Waterston", "Robert Irsay", "Edgar Lungu", "UNESCO / ILO", "five points", "Achal Kumar Jyoti", "the President", "Homer Banks, Carl Hampton and Raymond Jackson", "The management team", "British R&B girl group Eternal", "Montreal", "When the others arrive", "six", "Yuzuru Hanyu", "a receptor or enzyme is distinct from the active site", "a geologist James Hutton", "a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "31 October 1972", "December 2, 1942", "September 8, 2017", "Hollywood Masonic Temple", "Christina Pickles", "the Gaget, Gauthier & Co. workshop", "Renhe Sports Management Ltd", "Tangled", "a form of contactless communication between devices like smartphones or tablets", "Kenneth Hood \"Buddy\" MacKay Jr.", "a hard rock/blues rock band, they have also been considered a heavy metal band, although they have always dubbed their music simply \"rock and roll\"", "Scotland", "proud", "prohibiting the expansion of slavery into a territory where slave status was favored", "Peyton Place"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5818851231045524}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.782608695652174, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.4444444444444445, 0.13333333333333333, 1.0, 0.09523809523809523, 0.0, 1.0, 0.375, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2, 0.8, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7832", "mrqa_squad-validation-8160", "mrqa_squad-validation-4849", "mrqa_squad-validation-9831", "mrqa_squad-validation-5357", "mrqa_squad-validation-1388", "mrqa_squad-validation-434", "mrqa_squad-validation-625", "mrqa_squad-validation-8747", "mrqa_squad-validation-8530", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-2208", "mrqa_triviaqa-validation-2277", "mrqa_hotpotqa-validation-1315", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3223", "mrqa_searchqa-validation-15757"], "SR": 0.46875, "CSR": 0.6335227272727273, "EFR": 0.9705882352941176, "Overall": 0.739415942513369}, {"timecode": 11, "before_eval_results": {"predictions": ["The availability of the Bible in vernacular languages", "detention", "Western Xia", "by qualified majority", "carbon related", "co-chair", "Katharina", "three", "August 10, 1948", "the holy catholic (or universal) church", "30\u201360%", "Louis Paul Cailletet", "Class II MHC molecules", "crust and lithosphere", "orange", "electrons are fermions", "stroke", "1525", "polynomial-time reductions", "a piston into the partial vacuum generated by condensing steam", "Chouseb", "1947", "Spanish explorers", "Western Australia", "Two Days Before the Day After Tomorrow", "April 10, 2018", "October 1, 2015", "number of games where the player played, in whole or in part", "cat in the hat", "f\u0254n", "Tom Brady", "zhongguo", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "contemporary finalists ( plus Emma )", "1960", "Bengal tiger named Richard Parker", "Deathly Hallows", "a barrier that runs across a river or stream to control the flow of water", "De Wayne Warren", "triacylglycerol", "1971", "autu", "Tom Brady", "Margaery Tyrell", "Mount Mannen in Norway and at the Isle of Sheppey in England", "Lake Powell", "Clarence L. Tinker", "Hotel barge   Bed and breakfast   Botel", "Trace Adkins", "Garbi\u00f1e Muguruza", "Keith Richards", "Hercules", "June 12, 2018", "Bart Howard", "111", "Leonard Bernstein", "The Hague", "historic buildings, arts, and published works", "Troy McClure", "Arnoldo Rueda Medina", "tax incentives", "Albert Brewers", "Boca Raton", "CNN"], "metric_results": {"EM": 0.453125, "QA-F1": 0.570584896618904}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.5, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5714285714285715, 0.11764705882352941, 0.4, 0.0, 0.5, 0.0, 1.0, 0.0, 0.625, 1.0, 0.4615384615384615, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4074", "mrqa_squad-validation-8581", "mrqa_squad-validation-3474", "mrqa_squad-validation-4952", "mrqa_squad-validation-10483", "mrqa_squad-validation-2274", "mrqa_squad-validation-1720", "mrqa_squad-validation-3385", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-1023", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-386", "mrqa_hotpotqa-validation-2436", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1549", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-13057"], "SR": 0.453125, "CSR": 0.6184895833333333, "EFR": 0.9714285714285714, "Overall": 0.7365773809523809}, {"timecode": 12, "before_eval_results": {"predictions": ["savanna or desert", "Downtown Riverside", "Saul Bellow", "three", "Graz, Austria", "Sky Digital", "until 1796", "Einstein", "Gary Kubiak", "soy farmers", "The Sinclair Broadcast Group", "mercuric oxide", "Guglielmo Marconi", "Luther's education", "wages and profits", "Muslim Iberia", "The judicial branch", "biostratigraphers", "1999", "the Miami Heat", "the nasal septum", "the Grey Wardens", "Bob Dylan", "1799", "interphase", "$2 million in 2011", "birch", "Marin \u010cili\u0107", "James Corden", "Pasek & Paul", "October 2", "1956", "Thomas Lennon", "Gwendoline Christie", "Orangeville, Ontario, Canada", "Guy Pemberton", "modern random - access memory ( RAM )", "Afghanistan", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Walter Brennan", "James Rodr\u00edguez", "8ft", "various submucosal membrane sites", "1985", "Austria - Hungary", "April 17, 1982", "Acid rain", "July 1, 2005", "crossbar", "Thespis", "Ra\u00fal Eduardo Esparza", "By functions", "Thebes", "a constitutional monarchy in which the power of the Emperor is limited and is relegated primarily to ceremonial duties", "the Vital Records Office of the states, capital district, territories and former territories", "false", "hindfoot", "southwestern", "Planet Terror", "Atlantic Ocean", "one", "a Turkish River", "snowflake curve", "the ruble"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7099557705026456}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.07142857142857142, 1.0, 1.0, 0.0, 0.29629629629629634, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.625, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8032", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2644", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6998", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-559", "mrqa_newsqa-validation-2782", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-10635"], "SR": 0.671875, "CSR": 0.6225961538461539, "retrieved_ids": ["mrqa_squad-train-76152", "mrqa_squad-train-86066", "mrqa_squad-train-65406", "mrqa_squad-train-42546", "mrqa_squad-train-7468", "mrqa_squad-train-6583", "mrqa_squad-train-31196", "mrqa_squad-train-47638", "mrqa_squad-train-10034", "mrqa_squad-train-80023", "mrqa_squad-train-69317", "mrqa_squad-train-61739", "mrqa_squad-train-85700", "mrqa_squad-train-13924", "mrqa_squad-train-43612", "mrqa_squad-train-7367", "mrqa_newsqa-validation-2998", "mrqa_squad-validation-3946", "mrqa_squad-validation-4172", "mrqa_naturalquestions-validation-2876", "mrqa_squad-validation-5788", "mrqa_triviaqa-validation-5325", "mrqa_naturalquestions-validation-9773", "mrqa_squad-validation-8910", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-2472", "mrqa_squad-validation-4634", "mrqa_squad-validation-1388", "mrqa_naturalquestions-validation-1745", "mrqa_squad-validation-8400", "mrqa_newsqa-validation-3418"], "EFR": 0.9523809523809523, "Overall": 0.7335891712454212}, {"timecode": 13, "before_eval_results": {"predictions": ["return home", "socialist realism", "to spearhead the regeneration of the North-East", "sleep deprivation", "July 1977", "Anderson", "quantum electrodynamics (or QED)", "provisional elder/deacon", "chainging", "kinematic measurements", "worker, capitalist/business owner, landlord", "the communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "Licensed Local Pastor", "driving them in front of the army", "Kenia", "24 March 1879", "Rob Van Winkle", "electrons", "Edie Falco", "Leonard Bernstein", "Empire of the Sun", "James K. Polk", "Nicaragua", "Afghanistan", "Cambodia", "the Aladdin", "Uncle Henry", "Lady and the Tramp", "This type of \"episode\"", "China", "solemn", "Hanoi", "unsettling", "Alexander Ulyanov", "Beatrix Potter", "Elvre, 1646", "George Washington", "Volvic", "Andrea del Sarto", "Colorado is of Spanish origin, meaning \"colored red.\"", "Christopher Columbus", "The Balfour Declaration", "a contingency fee basis", "a tortoise", "a gun manufacturer", "Shinto", "The Simpsons", "a genetic disease that prevents blood from clotting", "\"Beauty is truth, truth beauty,that is all.", "a single strand", "Bad Boys", "Philadelphia", "Robert Downey Jr.", "Fresh Prince of Bel-Air", "her abusive husband", "the Dred Scott case", "Sylvester J. Pussycat, Sr.", "Daniel Defoe", "Atlantic Ocean", "Battle of Britain and the Battle of Malta", "the U.S. Consulate in Rio de Janeiro", "the club's board has yet to make it so for all the camps", "three", "a night out in the clubs of Hollywood."], "metric_results": {"EM": 0.65625, "QA-F1": 0.6968874007936507}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-6624", "mrqa_squad-validation-8065", "mrqa_squad-validation-8258", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-4663", "mrqa_searchqa-validation-4983", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-12856", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-11170", "mrqa_naturalquestions-validation-4762", "mrqa_triviaqa-validation-4717", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-2199"], "SR": 0.65625, "CSR": 0.625, "EFR": 0.9545454545454546, "Overall": 0.7345028409090909}, {"timecode": 14, "before_eval_results": {"predictions": ["Protestantism", "1281", "Tower Theatre", "the Mi'kmaq and the Abenaki", "basic design typical of Eastern bloc countries.", "Palestine", "a pair of retractable tentacles fringed with tentilla (\"little tentacles\")", "drawn by the convenience of the railroad and worried about flooding,", "Ireland", "18 April 1521", "Melus of Bari", "Warszawa", "in the Migration period", "2012", "35", "Trump", "a landmark", "the Blue Nile", "the king", "Betsey Johnson", "Eragon", "Rawhide", "p puppy", "Garland", "Paul Hornung", "silk", "a 1979 comedy film that starred Dom DeLuise, Suzanne Pleshette, Jerry Reed and Ossie Davis.", "a cosmic traveler", "John Mahoney", "Murder By Death", "Dave Brubeck", "Dalton Gang", "Washington", "Gilda Radner", "(April 2, 1965  June 17, 2012)", "Franklin D. Roosevelt", "rice", "udon", "Sirhan Sirhan", "stars", "Winter", "a word", "Mountain Dew", "Omaha", "vocalist", "actress", "the Erie Canal", "The ____ Family", "Baltic Sea", "senators", "Earvin \"Magic\" Johnson", "a cathedral of Santa Maria dei Fiori", "hoo-hoo", "tonic sol-fa", "political nation from the Great Plains, consisted of present - day eastern New Mexico, southeastern Colorado, southwestern Kansas, western Oklahoma, and most of northwest Texas and northern Chihuahua.", "in the 1980 BBC adaptation of Pride and Prejudice starring Elizabeth Garvie and David Rintoul, adapted from the novel by Fay Weldon.", "Imola Circuit", "sheep", "provide travel agencies in Japan with booking and ticketing capabilities for a wider range of international airlines.", "Berea College", "held a nomination paper for a parliamentary seat on November 25 and appeared headed for a power showdown with Musharraf before she was assassinated Thursday.", "Bialek", "The European Council", "Antigua"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5562127976190476}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.25, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2943", "mrqa_squad-validation-4621", "mrqa_squad-validation-1062", "mrqa_squad-validation-9348", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-6488", "mrqa_searchqa-validation-6730", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-5304", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-7242", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-8575", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-15836", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-12619", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-534", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-10408", "mrqa_triviaqa-validation-1936", "mrqa_hotpotqa-validation-5184", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-2583", "mrqa_triviaqa-validation-2317"], "SR": 0.46875, "CSR": 0.6145833333333333, "EFR": 0.9705882352941176, "Overall": 0.7356280637254902}, {"timecode": 15, "before_eval_results": {"predictions": ["easier and more efficient than anywhere else", "attempted to enter the test site knowing that they faced arrest", "Paul Revere", "the death of Elisabeth Sladen", "3.6%", "42,000", "14", "the United States Census Bureau", "Asia", "19 April 1943", "actions-oriented", "rapidly evolve and adapt", "present-day Upstate New York and the Ohio Country", "the Roman Republic", "Phil Woolas", "South Africa", "William David Charles Carling", "the Chatham House Rule", "phase changes", "America Online", "Aramis", "bees", "The Firm", "The Streets", "violin", "a blazer", "a crowd", "the Titanic", "groin", "the obturators", "Mario", "Georgia", "Massachusetts", "the London theatre district, ballet and circus genre", "La Boh\u00e8me", "John Quincy Adams", "guggul", "Ethel Skinner", "Queen Victoria", "My Fair Lady", "Italy", "lignin", "Adolphe Adam", "a flatline", "Barcelona", "a coelacanth", "mono", "cats", "Love Never Dies", "Elizabeth I", "a blues band", "Velologists", "the ISS", "the Marcy Brothers", "1770 BC", "a type of party, common mainly in contemporary Western culture", "the oil platforms in the North Sea", "Sydney", "German Chancellor Angela Merkel", "Charlie Moore", "Mike Wallace", "the Professor", "Burundi", "Futurama"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6189157196969697}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true], "QA-F1": [0.7272727272727273, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1830", "mrqa_squad-validation-6702", "mrqa_squad-validation-5893", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-871", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-7024", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-3616", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8359", "mrqa_hotpotqa-validation-1537", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-14441"], "SR": 0.546875, "CSR": 0.6103515625, "retrieved_ids": ["mrqa_squad-train-54776", "mrqa_squad-train-55916", "mrqa_squad-train-31587", "mrqa_squad-train-32034", "mrqa_squad-train-7341", "mrqa_squad-train-39311", "mrqa_squad-train-10948", "mrqa_squad-train-16919", "mrqa_squad-train-59411", "mrqa_squad-train-2496", "mrqa_squad-train-54775", "mrqa_squad-train-70136", "mrqa_squad-train-81654", "mrqa_squad-train-18945", "mrqa_squad-train-49927", "mrqa_squad-train-22943", "mrqa_naturalquestions-validation-7351", "mrqa_searchqa-validation-7878", "mrqa_squad-validation-7771", "mrqa_newsqa-validation-3802", "mrqa_searchqa-validation-5304", "mrqa_squad-validation-10232", "mrqa_searchqa-validation-3105", "mrqa_squad-validation-7211", "mrqa_squad-validation-434", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-16618", "mrqa_naturalquestions-validation-8068", "mrqa_squad-validation-6402", "mrqa_naturalquestions-validation-1315", "mrqa_squad-validation-9578", "mrqa_searchqa-validation-10199"], "EFR": 0.9655172413793104, "Overall": 0.7337675107758621}, {"timecode": 16, "before_eval_results": {"predictions": ["computational complexity theory", "\"apostate\" leaders of Muslim states", "tensions over slavery and the power of bishops in the denomination.", "mannerist architecture", "489", "Grainger Town area", "the Great Fire of London", "Theory of the Earth", "Ward", "Newton", "bilaterians", "one of the daughters of former King of Thebes, Oedipus", "the forces of Andrew Moray and William Wallace", "differential erosion", "between two and 30 eggs", "2018", "Arthur Chung", "18", "9 February 2018", "Yuzuru Hanyu", "Marley & Me", "81.617 \u00b0 W \ufeff / 26.617", "Kansas", "Duck", "McKim Marriott", "Hanna Alstr\u00f6m", "31", "Austin, Texas", "the optic chiasm", "the Twelvers, and Seven pillars of Ismailism", "vasoconstriction of most blood vessels", "chromosome 21 attached to another chromosome", "Rockwell", "1 November", "Sylvester Stallone", "In Time", "Rodney Crowell", "9 or 10 national", "eukaryotic cells", "Melissa Disney", "Darren McGavin", "UNESCO / ILO", "Thaddeus Rowe Luckinbill", "ummat al - Islamiyah", "William DeVaughn", "Tavares", "Brevet Colonel Robert E. Lee", "The Annunciation", "Santiago Ram\u00f3n y Cajal", "boy", "seven", "novella", "Southwest Florida International Airport ( RSW )", "21 June 2007", "13", "California", "England", "Roc Me Out", "\"Dear John,\"", "root out terrorists within its borders.", "Rudolf Nureyev", "43", "the Kurdish militant group in southeastern Turkey.", "Mohammed Mohsen Zayed"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6397353778467909}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3478260869565218, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6640", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-5348", "mrqa_triviaqa-validation-4946", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-3649", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-4917", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-814"], "SR": 0.578125, "CSR": 0.6084558823529411, "EFR": 0.9259259259259259, "Overall": 0.7254701116557734}, {"timecode": 17, "before_eval_results": {"predictions": ["in Sydney", "Doritos", "Hans Vredeman de Vries", "$20.4 billion", "\u00dcberlingen", "Finsteraarhorn", "seven", "\"Professor Sherlockarty to the Doctor's Sherlock Holmes\"", "Lippe", "the American Revolutionary War", "25", "Chuck Noland", "Donna Mills", "March 31, 2013", "pulmonary heart disease ( cor pulmonale )", "before the first year begins", "Iraq", "the president", "UNESCO / ILO", "asexually", "the eastern jungle and the nearby city of Ba\u00f1os de Agua Santa in the Ecuadorian Andes", "2002", "The virion must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "Travis Tritt", "March 16, 2018", "Cee - Lo", "the star at the center of the Solar System", "prokaryotic cell ( or organelle )", "1966", "Charlotte Hornets", "Julie Adams", "April 29, 2009", "J. Presper Eckert and John William Mauchly's ENIAC", "201", "1983", "Thursdays at 8 : 00 pm ( ET )", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "Tom Tucker", "regulate the employment and working conditions of civil servants", "Hercules", "Angel Island ( California )", "Rufus and Chaka Khan", "moist temperate climates", "before they kill him", "7.6 mm", "The Ecology", "1955", "John B. Watson", "2020", "continues the pre-existing appropriations at the same levels as the previous fiscal year ( or with minor modifications ) for a set amount of time", "during World War II", "2004", "Orangeville, Ontario, Canada", "parabola", "Switzerland", "John Wilkes Booth", "St Augustine's Abbey", "two Russian bombers have landed at a Venezuelan airfield where they will carry out training flights for several days,", "the insurgency", "Marilyn Monroe", "oregon", "NBA 2K16", "Baltimore", "September 25, 2017"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5931852377946129}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8333333333333334, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.4, 1.0, 0.0, 0.125, 0.0, 0.72, 0.5714285714285715, 1.0, 0.0, 0.0, 0.4, 1.0, 0.4444444444444445, 1.0, 0.0, 0.18181818181818182, 0.25, 1.0, 0.0, 0.33333333333333337, 0.0, 0.5925925925925926, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.17142857142857143, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2998", "mrqa_squad-validation-9285", "mrqa_squad-validation-7700", "mrqa_squad-validation-7288", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-7409", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7857", "mrqa_hotpotqa-validation-538", "mrqa_newsqa-validation-3489", "mrqa_searchqa-validation-16626", "mrqa_searchqa-validation-10032", "mrqa_hotpotqa-validation-4735"], "SR": 0.46875, "CSR": 0.6006944444444444, "EFR": 0.9411764705882353, "Overall": 0.726967933006536}, {"timecode": 18, "before_eval_results": {"predictions": ["1904", "Cardinal Cajetan Luther stated that he did not consider the papacy part of the biblical Church because historistical interpretation of Bible prophecy concluded that the Papacy was the Antichrist", "Michael Heckenberger and colleagues of the University of Florida", "declined significantly", "quantum mechanics", "prices", "Miller", "Super Bowl XX", "gold", "p", "2017", "to collect menstrual flow", "Sets heart in mediastinum and limits its motion", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "12 to 36 months old", "the mixing of sea water and fresh water", "her abusive husband", "HTTP / 1.1", "the south coast of eastern New Guinea", "2009", "December 2, 1942", "John Cooper Clarke", "1792", "Amitabh Bachchan, Akshay Kumar, Bobby Deol, Divya Khosla Kumar, Sandali Sinha and Nagma", "Annette Strean", "the Bee Gees", "September 25, 1987", "Lord's", "the Senate", "a little girl ( Addy Miller )", "Ritchie Cordell", "9.0 -- 9.1 ( M )", "the right to peaceably assemble, or to petition for a governmental redress of grievances", "2010", "James Corden", "the development of electronic computers in the 1950s", "the uterus in females", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "Exodus 20 : 1 -- 17", "its population", "beta decay", "Rachel Sarah Bilson", "the NIRA", "March 5, 2014", "Haikou on the Hainan Island", "19 June 2018", "stromal connective tissue", "15 May 2004", "Stephen Curry of Davidson", "Susie's father", "Rufus and Chaka Khan", "Jodie Foster", "Rory McIlroy", "Aristotle", "Cubs", "1919", "Lithuania", "President Obama's race in 2008", "Lou and Wilson", "dana point bail", "oregon Medicine", "Pete Seeger", "Kiss Me, Kate", "Wigan"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5523178118839689}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.07407407407407407, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.4444444444444445, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.375, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5106382978723404, 1.0, 1.0, 0.0, 0.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.888888888888889, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.10810810810810811, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2268", "mrqa_squad-validation-10388", "mrqa_squad-validation-845", "mrqa_squad-validation-5", "mrqa_squad-validation-9213", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2904", "mrqa_triviaqa-validation-1259", "mrqa_hotpotqa-validation-4927", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3307", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-10336"], "SR": 0.453125, "CSR": 0.5929276315789473, "retrieved_ids": ["mrqa_squad-train-56732", "mrqa_squad-train-20562", "mrqa_squad-train-18148", "mrqa_squad-train-33808", "mrqa_squad-train-21643", "mrqa_squad-train-60645", "mrqa_squad-train-8524", "mrqa_squad-train-9019", "mrqa_squad-train-56201", "mrqa_squad-train-72202", "mrqa_squad-train-80016", "mrqa_squad-train-71554", "mrqa_squad-train-5388", "mrqa_squad-train-53555", "mrqa_squad-train-2599", "mrqa_squad-train-23132", "mrqa_squad-validation-9334", "mrqa_squad-validation-2565", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-2462", "mrqa_squad-validation-7818", "mrqa_naturalquestions-validation-3391", "mrqa_searchqa-validation-12619", "mrqa_naturalquestions-validation-4924", "mrqa_newsqa-validation-2778", "mrqa_squad-validation-2772", "mrqa_squad-validation-4634", "mrqa_searchqa-validation-9477", "mrqa_naturalquestions-validation-2572", "mrqa_searchqa-validation-10794", "mrqa_newsqa-validation-2820", "mrqa_naturalquestions-validation-6706"], "EFR": 0.9428571428571428, "Overall": 0.7257507048872179}, {"timecode": 19, "before_eval_results": {"predictions": ["phagosomal", "well before Braddock's departure for North America", "General Hospital", "destroyed", "night", "assertive", "antagonistic", "whether or not to plead guilty", "parallelogram", "Valley Falls", "22 July 1930", "Sam Raimi", "Saddle Rock Elementary School", "political thriller", "Timothy McVeigh", "Robert \"Bobby\" Germaine, Sr.", "1986 to 2013", "the NYPD's 83rd Precinct", "Azeroth", "October 5, 1930", "1999", "musical research", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.", "Michael Sheen", "the Seelie Court", "the Mayor of the City of New York", "841", "My Boss, My Hero", "Roy Spencer", "Bardney", "tragedy", "The Everglades", "Saint Louis", "1891", "Eielson Air Force Base in Alaska", "Serhiy Paradzhanov", "Germany", "eight", "Carson City", "Lindsey Islands", "the 2008 presidential election", "John Randle", "Eli Roth", "Paige O'Hara", "Northern Ireland", "St. Louis, Missouri", "the first hole of a sudden-death playoff", "Prince", "every aspect of public and private life wherever feasible", "Burnley", "Russian Empire", "between 11 or 13 and 18", "Alzheimer's disease", "phayanchana ), 15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least", "7 June 2005", "Imola", "Sufjan Stevens", "producing rock music with a country influence", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "tutus", "Ethel Merman", "$1.5 million", "Sen. Barack Obama", "Tutsi ethnic minority and the Hutu majority"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6739645337301587}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.8571428571428571, 0.8571428571428571, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.8750000000000001, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_squad-validation-6024", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-5392", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-904", "mrqa_hotpotqa-validation-5485", "mrqa_naturalquestions-validation-2629", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-12758", "mrqa_newsqa-validation-3659"], "SR": 0.5625, "CSR": 0.59140625, "EFR": 1.0, "Overall": 0.736875}, {"timecode": 20, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2530", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5392", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10019", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1939", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2918", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3855", "mrqa_naturalquestions-validation-386", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4867", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-5582", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-846", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15836", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-346", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-4917", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-5704", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5845", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6488", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8487", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9432", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10151", "mrqa_squad-validation-10192", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10381", "mrqa_squad-validation-10388", "mrqa_squad-validation-10399", "mrqa_squad-validation-10408", "mrqa_squad-validation-1062", "mrqa_squad-validation-1082", "mrqa_squad-validation-1131", "mrqa_squad-validation-1180", "mrqa_squad-validation-1248", "mrqa_squad-validation-1272", "mrqa_squad-validation-1334", "mrqa_squad-validation-1348", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-150", "mrqa_squad-validation-1530", "mrqa_squad-validation-1551", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1939", "mrqa_squad-validation-2003", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-217", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2361", "mrqa_squad-validation-2416", "mrqa_squad-validation-2481", "mrqa_squad-validation-2511", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2667", "mrqa_squad-validation-2772", "mrqa_squad-validation-2861", "mrqa_squad-validation-2881", "mrqa_squad-validation-2906", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3148", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3270", "mrqa_squad-validation-3355", "mrqa_squad-validation-3385", "mrqa_squad-validation-3411", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3830", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4159", "mrqa_squad-validation-4186", "mrqa_squad-validation-423", "mrqa_squad-validation-4331", "mrqa_squad-validation-434", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4430", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4681", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-501", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5198", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5265", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-552", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5640", "mrqa_squad-validation-5653", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5849", "mrqa_squad-validation-5893", "mrqa_squad-validation-5986", "mrqa_squad-validation-6024", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6294", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-6728", "mrqa_squad-validation-680", "mrqa_squad-validation-6841", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6917", "mrqa_squad-validation-7029", "mrqa_squad-validation-7164", "mrqa_squad-validation-7175", "mrqa_squad-validation-7302", "mrqa_squad-validation-7362", "mrqa_squad-validation-7366", "mrqa_squad-validation-7435", "mrqa_squad-validation-744", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7709", "mrqa_squad-validation-7740", "mrqa_squad-validation-7766", "mrqa_squad-validation-7775", "mrqa_squad-validation-7818", "mrqa_squad-validation-7821", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8144", "mrqa_squad-validation-8163", "mrqa_squad-validation-828", "mrqa_squad-validation-8336", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8372", "mrqa_squad-validation-848", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8600", "mrqa_squad-validation-8687", "mrqa_squad-validation-8747", "mrqa_squad-validation-8759", "mrqa_squad-validation-8807", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-9050", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9348", "mrqa_squad-validation-9401", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9830", "mrqa_squad-validation-9831", "mrqa_squad-validation-9893", "mrqa_squad-validation-9930", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-1816", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4825", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7542", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-871"], "OKR": 0.83984375, "KG": 0.43046875, "before_eval_results": {"predictions": ["one way", "since at least the mid-14th century", "Lady Gaga", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "Classic", "88", "826", "leishmaniasis", "Romulus, My Father", "Theodore Roosevelt Mason", "September 1903", "1964", "Kinnairdy Castle", "James Edward Franco", "(Thai: \u0e44\u0e17\u0e22\u0e41\u0e2d\u0e4c\u0e40\u0e0a\u0e35\u0e22", "December 24, 1973", "Baden-W\u00fcrttemberg", "James Dean", "Rob Reiner", "Kansas", "Westfield Tea Tree Plaza", "26,000", "Alemannic", "1992", "Darci Kistler", "the EN World web site", "(25 March 1948 \u2212 27 December 2013)", "Pigman's Bar-B- Que", "Columbus", "the Emilia-Romagna Region in Northern Italy", "Miami Gardens, Florida", "The Tower of London", "John Sullivan", "churros", "Franklin, Indiana", "Ubbe Eert \"Ub\" Iwerks", "mentalfloss.com", "Omega SA", "Flashback", "from 1942 to 1945", "Kiernan Shipka", "1993", "Zaire", "Brett Ryan Eldredge", "The pronghorn", "Walcha", "French", "FBI", "The BMW E70", "David Michael Bautista Jr.", "the Cherokee River", "15", "Taylor Swift", "for control purposes", "18 Divisional Round", "Robin", "Melbourne, Victoria, Australia", "Felipe Massa", "The Washington Post", "Constantine XI Palaiologos", "Viva Las Vegas", "a bat", "Democritus", "Tennessee"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5942978896103897}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.5714285714285715, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-582", "mrqa_squad-validation-1459", "mrqa_squad-validation-6653", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-238", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-5258", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3052", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3402", "mrqa_searchqa-validation-15983"], "SR": 0.484375, "CSR": 0.5863095238095238, "EFR": 0.9696969696969697, "Overall": 0.7137012987012987}, {"timecode": 21, "before_eval_results": {"predictions": ["between 1000 and 1900", "Imperialism", "student-teacher relationships", "Approximately one million", "Ted Ginn Jr.", "Harrods", "Coptic Cathedral", "Philadelphia", "Disco", "MMA", "Kentucky River", "Claudius", "1995", "public", "Lewis Madison Terman", "Fitzroya cupressoides", "L\u00e9a Seydoux", "Warren G. Harding", "Sada Carolyn Thompson", "T. E. Lawrence", "Nelson Mandela", "Dissection", "German", "A41", "Forbes", "Bronwyn Kathleen Bishop", "The Timekeeper", "Jena Malone", "water", "Hong Kong First Division League", "Four Weddings and a Funeral", "from 1993 to 1996", "July 16, 1971", "the music genres of electronic rock, electropop and R&B", "Theodore Anthony Nugent", "Reverend Timothy \"Tim\" Lovejoy", "Geet", "October 5, 1930", "Pamelyn Ferdin", "The Ninth Gate", "Comeng and Clyde Engineering", "Martin Noel Galgani Fitzpatrick", "Saint Motel", "About 200", "War Is the Answer", "Eleanor of Aquitaine", "Trilochanapala", "Rashida Jones", "The Division of Cook", "King George VI", "Blue Origin", "Taylor Swift", "18 December 1975", "a convergent plate boundary", "its vast territory was divided into several successor polities", "Kiri Te Kanawa", "Christchurch", "30-minute", "the helicopter went down", "John-Boy Walton", "the wind band", "how now '' is a greeting", "once every 23 hours", "Nigel Lythgoe"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5992136769480519}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.4, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.6666666666666666, 0.0, 0.0, 0.28571428571428575, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9904", "mrqa_squad-validation-1873", "mrqa_squad-validation-3060", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-4466", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-123", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-1219", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-4172", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-5447", "mrqa_searchqa-validation-8612", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-2014"], "SR": 0.515625, "CSR": 0.5830965909090908, "retrieved_ids": ["mrqa_squad-train-48948", "mrqa_squad-train-80451", "mrqa_squad-train-66851", "mrqa_squad-train-56627", "mrqa_squad-train-57858", "mrqa_squad-train-72328", "mrqa_squad-train-60403", "mrqa_squad-train-56078", "mrqa_squad-train-14687", "mrqa_squad-train-5727", "mrqa_squad-train-67578", "mrqa_squad-train-38228", "mrqa_squad-train-37083", "mrqa_squad-train-51681", "mrqa_squad-train-82505", "mrqa_squad-train-41912", "mrqa_squad-validation-1720", "mrqa_squad-validation-9540", "mrqa_naturalquestions-validation-8298", "mrqa_squad-validation-4673", "mrqa_searchqa-validation-1127", "mrqa_newsqa-validation-3354", "mrqa_naturalquestions-validation-5476", "mrqa_searchqa-validation-4753", "mrqa_newsqa-validation-808", "mrqa_naturalquestions-validation-5999", "mrqa_newsqa-validation-1465", "mrqa_naturalquestions-validation-1930", "mrqa_squad-validation-170", "mrqa_triviaqa-validation-871", "mrqa_searchqa-validation-9535", "mrqa_squad-validation-4634"], "EFR": 1.0, "Overall": 0.7191193181818182}, {"timecode": 22, "before_eval_results": {"predictions": ["Zorro", "Zagreus", "January 1985", "Computational complexity theory", "plan the physical proceedings, and to integrate those proceedings with the other parts", "post-World War I", "Esteban Ocon", "New Orleans Saints", "Clarence Nash", "The Dressmaker", "1,800", "February 14, 1859", "Urijah Faber", "Mary-Kay Wilmers", "Psilocybin, psilocin and baeocystin", "Titus Andronicus", "Sunil Grover", "Ars Nova Theater", "Monticello", "a large portion of rural Maine, published six days per week in Bangor, Maine", "\"Menace II Society\"", "16 March 1987", "the luxury Holden Calais (VF) nameplate", "Louis \"Louie\" Zamperini", "1968", "PlayStation 3", "The Andes or Andean Mountains", "the Knight Company", "Parlophone Records", "Nicolas Winding Refn", "South America", "Morocco", "Gerard Marenghi", "1958", "126,202", "1975", "orange", "Roseann O'Donnell", "The Case for Hillary Clinton", "SAVE", "29,000", "Fortean subjects in general", "Larnelle Harris", "4,613", "a morir so\u00f1ando or orange Creamsicle", "Ramzan Kadyrov", "Albert", "Tetrahydrogestrinone", "5,042", "Captain B.J. Hunnicutt", "the Duchess's daughter, the future Queen Victoria", "The Keeping Hours", "the Gilbert building", "dead plant and animal material", "Elena Anaya", "Rome", "The Archers", "the raven", "the IV cafe", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Daniel Radcliffe", "Milla Jovovich", "the bridge", "a beetle which feeds on cotton buds and flowers"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6703650210084033}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-3156", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-460", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-5396", "mrqa_triviaqa-validation-1447", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3611", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-11933"], "SR": 0.59375, "CSR": 0.5835597826086957, "EFR": 1.0, "Overall": 0.7192119565217392}, {"timecode": 23, "before_eval_results": {"predictions": ["the growth of mass production", "Edmonton, Canada", "Ollie Treiz", "Western Xia", "Richard Trevithick", "The flushing action of tears and urine", "9", "his writings about the outdoors, especially mountain-climbing", "The Backstreet Boys", "November of that year", "the US Territory of Hawaii", "Tool", "1919", "May 5, 2015", "Arthur Miller", "Edmonton, Alberta", "Brent Robert Barry", "from 1989 until 1994", "The LA Galaxy", "Nicholas Kristof", "film and short novels", "Flushed Away", "first baseman and third baseman", "the River Welland", "The Process", "200,167", "Bohemia", "\"The Brothers\"", "Armin Meiwes", "1933", "Dirk Werner Nowitzki", "Carl Michael Edwards", "In a Better World", "the 45th Infantry Division", "Iron Man 3", "Julie 2", "Conservatorio Verdi", "6'5\"", "a terrible date", "June 26, 1970", "Lawrenceburg, Indiana", "14", "\"About a Boy\"", "1982", "1978", "A black cat", "Denmark", "1999", "The Ryukyuan people (\u7409\u7403\u6c11\u65cf, Ry\u016bky\u016b minzoku, Okinawan: \"Ruuchuu minzuku\")", "Peter Thiel", "Summerlin, Nevada", "Cersei Jaimeister (portrayed by Lena Headey as an adult)", "Kida", "a diameter between 100,000 and 180,000 light - years", "Mahinda Rajapaksa", "iron", "obi", "\"Agentiresh \" Kid\" Curry, a pair of Western cousin outlaws trying to reform.", "British broadcaster Channel 4 has been criticized for creating a new television show which looks at how children as young as eight would cope without their parents for two weeks.", "he was in the bathroom for about 15 to 20 minutes, \"pushed the plunger on the bomb and prepared to die,\"", "2.5 million", "1,000,000", "Sockeye", "the Nixon scandal"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6379236095561527}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.2857142857142857, 1.0, 0.4, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.25000000000000006, 0.05555555555555555, 0.6206896551724137, 0.4, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-6437", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-1564", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-3634", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-1368", "mrqa_triviaqa-validation-4060", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-865", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-3074"], "SR": 0.546875, "CSR": 0.58203125, "EFR": 1.0, "Overall": 0.7189062500000001}, {"timecode": 24, "before_eval_results": {"predictions": ["1965", "A job where there are many workers willing to work a large amount of time (high supply)", "A tundra", "article 30", "The mermaid", "south of Kabul in the eastern Afghan province of Logar", "the council's job was not to promote the attempts but simply to oversee them in a fair and independent manner and ratify successful efforts.", "daughter Paige, 15, and \"the crew\": Isaac, 8, Hope, 7, Noah, 5, Phoebe Joy, 3, Lydia Beth, 2, Annie, also 2,", "Kurt Cobain", "seven", "two years", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.\"", "piano", "Charlotte Gainsbourg and Willem Dafoe", "The Charlie Daniels Band", "that the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "curfew", "Missouri", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics", "five victims by helicopter", "30-minute", "San Diego", "Iran", "cancer", "baseball bat", "the USS Nimitz", "$50", "the coalition seeks to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "sportswear", "co-authored by Donald T. Phillips.", "eight", "the FAA received no reports from pilots in the air of any sightings but the agency recieved \"n approximately\" calls from people on the ground from Dallas, Texas, south to Austin, Texas.", "1944", "a Bush-era Justice Department memo released by the Obama administration.", "NATO fighters", "Jaime Andrade", "Somali President Sheikh Sharif Sheikh Ahmed", "\"Sorry, I've been feeling better every single day since surgery and this weekend my doctors gave me the green light to get back to work.\"", "that school staff and security should patrol campuses, especially violence-prone areas, during and after school events.", "Orbiting Carbon Observatory", "German Chancellor Angela Merkel", "Rivers", "Ryder Russell", "Scarlett", "the crew of the Bainbridge", "Joe Jackson, the 80-year-old Jackson family patriarch", "Asashoryu", "the District of Columbia National Guard", "the Spanish government", "Anil Kapoor", "$3 billion, with further foreign direct investment exceeding $40 billion during the operations phase", "question people if there's reason to suspect they're in the United States illegally", "a young girl", "Elizabeth Dean Lail", "1926", "a squall", "Wyoming", "Kenny Everett", "November 23, 1996 in Japan", "Richa Sharma", "The Frost Place Advanced Seminar", "A game show is a type of radio, television, or internet...", "Valentina Tereshkova", "the last three cards"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5305937355406938}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.26666666666666666, 0.08695652173913042, 0.0, 0.5, 1.0, 1.0, 0.06060606060606061, 1.0, 0.0, 1.0, 0.14814814814814817, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2608695652173913, 1.0, 0.0, 0.0, 0.24390243902439027, 0.0, 0.0, 1.0, 1.0, 0.8, 0.13333333333333333, 0.125, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 0.888888888888889, 0.0, 0.0, 0.375, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7407", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3010", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6214", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-733", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-5986"], "SR": 0.4375, "CSR": 0.5762499999999999, "retrieved_ids": ["mrqa_squad-train-25262", "mrqa_squad-train-3309", "mrqa_squad-train-85107", "mrqa_squad-train-81703", "mrqa_squad-train-53890", "mrqa_squad-train-18938", "mrqa_squad-train-75780", "mrqa_squad-train-50349", "mrqa_squad-train-16524", "mrqa_squad-train-9051", "mrqa_squad-train-58031", "mrqa_squad-train-56278", "mrqa_squad-train-73888", "mrqa_squad-train-9544", "mrqa_squad-train-46769", "mrqa_squad-train-35645", "mrqa_squad-validation-2122", "mrqa_searchqa-validation-15983", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-3993", "mrqa_hotpotqa-validation-1038", "mrqa_searchqa-validation-711", "mrqa_newsqa-validation-130", "mrqa_naturalquestions-validation-3725", "mrqa_squad-validation-4634", "mrqa_naturalquestions-validation-2452", "mrqa_squad-validation-2565", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-7409", "mrqa_searchqa-validation-16618", "mrqa_squad-validation-8372"], "EFR": 1.0, "Overall": 0.71775}, {"timecode": 25, "before_eval_results": {"predictions": ["phycobilisomes", "WatchESPN", "independent prescribing", "10,000", "it infringed on democratic freedoms", "U.N. High Commissioner for Refugees", "a house party in Crandon, Wisconsin", "Paul Ryan", "in Amstetten,", "propofol", "his comments", "Microsoft", "Polo", "three empty vodka bottles,", "Nkepile M abuse", "a Royal Air Force helicopter", "Samuel Herr,", "McDonald's", "a gay soldier who asked about the repeal of \"Don't Ask, Don't Tell,\"", "1981", "Jewish", "Dodi Fayed.", "Teresa Hairston", "Dan Parris, 25, and Rob Lehr, 26,", "18", "July", "\"Gossip Girl\"", "Jason Chaffetz", "debris", "a woman", "Hansa (Malmborgsgatan 6)", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "21", "at a construction site in the heart of Los Angeles.", "they would not be making any further comments, citing the investigation.", "North Korean leader Kim Jong Il seems to be \"testing the new administration.\"", "the 3rd District of Utah", "House-passed bill that eliminates the 3% withholding requirement for government contractors", "\"perezagruzka,\" which means 'overcharged.'\"", "only one", "Luka Modric.", "1998.", "23 people", "American", "\"we take this issue seriously,\"", "Monday.", "Frank Ricci", "Old Trafford", "Guinea, Myanmar, Sudan and Venezuela.", "Carrousel du Louvre", "homicide", "Les Bleus", "Madison", "Gibraltar", "need to repent in time", "John McCarthy", "\"One true Friend,\" by Etta-May Spenser.", "narwhals", "Atlas ICBM", "Lionel Eugene Hollins", "Duchess Eleanor of Aquitaine", "John Irving", "diabetes", "Vienna"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5192418692418692}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.5, 0.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.1818181818181818, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.15384615384615385, 1.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6354", "mrqa_squad-validation-8392", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-981", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3726", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-2673", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-5238"], "SR": 0.4375, "CSR": 0.5709134615384616, "EFR": 1.0, "Overall": 0.7166826923076923}, {"timecode": 26, "before_eval_results": {"predictions": ["Eero Saarinen", "according to a multiple access scheme", "composite number", "Climate fluctuations during the last 34 million years", "Chicago Cubs", "(\"Signal Failures' written by Dr. B. Ching is the title of a column that has appeared in the magazine\")", "trumpet", "Norman Hartnell", "four Games", "Millbank in London", "Poland", "Adam Ant", "sea levels gradually rose, the waters of continental shelves were colonized for the first time by large marine reptiles and reef-building corals of modern aspect", "New Democracy", "Missouri", "six", "mushrooms", "Turkey", "1979", "(an invasive species)", "London Borough of Walford in the East End of London", "(from Middle Latin baccalaureus)", "Laurence Olivier,", "four", "Leonard Bernstein", "\"One may lean back in his chair & work it.", "(29-13)\u20142 + 1 = 16/2 + one = 9)", "touch reading", "blood", "aircraft", "Passion fruit", "jumper", "'Zulu'", "caridean shrimp", "Yemen", "Welles", "King George IV", "Barry Briggs", "Joseph Smith", "(metropolitan boroughs)", "Kievan Rus", "of being ridiculed.", "Beaujolais Nouveau", "rowing", "$5 $20 $50 $100)", "(28 July 1925 - 14 April 1988)", "ancient man", "'Lord Nelson' or a 'Nelson'", "ancient Testament", "Argentina", "a lion", "Sinclair Lewis", "Phosphorus pentoxide", "( son of Bindusara )", "Norwegian", "the County of York", "Keene, New Hampshire", "Blue", "Phillip A. Myers", "11th year in a row.", "Opryland", "Heroes", "singer", "Alzheimer's disease"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5494791666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-118", "mrqa_triviaqa-validation-1872", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-2496", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-6073", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-4905", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-459", "mrqa_naturalquestions-validation-946", "mrqa_hotpotqa-validation-3139", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-1554"], "SR": 0.515625, "CSR": 0.5688657407407407, "EFR": 0.9354838709677419, "Overall": 0.7033699223416965}, {"timecode": 27, "before_eval_results": {"predictions": ["water-cooled", "Genghis Khan Mausoleum", "the most cost efficient bidder", "February 1, 2016", "Kylie Jenner", "53", "Gimli son of Gl\u00f3in", "1994", "Elizabeth Dean Lail", "Anthony Caruso as Johnny Rivers", "Peter Finch", "Jerry Ekandjo", "Guant\u00e1namo Bay in Cuba", "19 state rooms", "north", "23 %", "vivre", "Roanoke", "flash music video", "Central Germany", "King Saud University", "Aysgarth Falls in Yorkshire", "on the southeastern coast of the Commonwealth of Virginia", "Orlando", "epidemiology", "Kyrie Irving", "brain region that in humans is located in the dorsal portion of the frontal lobe", "Paul Hogan", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "1996", "Emma Thompson", "in capillaries", "September 6,", "two", "Shenzi", "after obtaining the consent of the United Kingdom", "3000 BC", "( 8,498 mi )", "1976", "on the vaginal floor", "October 2008", "Broken Hill and Sydney", "arts liberales", "Mockingjay -- Part 1 ( 2014 )", "1773", "Escherichia coli", "5 : 7 -- 8", "radioisotope thermoelectric generator", "Carol Worthington", "when viewed from different points on Earth", "interstitial fluid in the `` interstitial compartment ''", "2014", "reed", "Dorset", "pressure", "Frederick Martin \"Fred\" Mac Murray", "Franz Ferdinand", "extreme nationalist, and nativist ideologies, as well as authoritarian tendencies", "the Democratic VP candidate", "Jennifer Aniston,", "\"Rent,\" \"Cabaret\" and \" Proof,\"", "Portuguese", "grow old", "Holstein cow"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5721297570752439}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.25, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.25, 0.4, 0.6666666666666666, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.0, 0.8235294117647058, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.06896551724137931, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.5, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1427", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-1180", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-3827", "mrqa_hotpotqa-validation-5286", "mrqa_newsqa-validation-435", "mrqa_newsqa-validation-3635", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-7864"], "SR": 0.421875, "CSR": 0.5636160714285714, "retrieved_ids": ["mrqa_squad-train-42316", "mrqa_squad-train-21108", "mrqa_squad-train-51521", "mrqa_squad-train-35914", "mrqa_squad-train-65923", "mrqa_squad-train-84575", "mrqa_squad-train-48619", "mrqa_squad-train-36909", "mrqa_squad-train-48858", "mrqa_squad-train-62782", "mrqa_squad-train-18908", "mrqa_squad-train-82641", "mrqa_squad-train-67395", "mrqa_squad-train-11614", "mrqa_squad-train-30586", "mrqa_squad-train-47510", "mrqa_newsqa-validation-1811", "mrqa_triviaqa-validation-2496", "mrqa_hotpotqa-validation-5392", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4928", "mrqa_newsqa-validation-3415", "mrqa_hotpotqa-validation-5821", "mrqa_searchqa-validation-9535", "mrqa_triviaqa-validation-1523", "mrqa_naturalquestions-validation-6998", "mrqa_hotpotqa-validation-1038", "mrqa_naturalquestions-validation-9195", "mrqa_newsqa-validation-212", "mrqa_squad-validation-7211", "mrqa_hotpotqa-validation-4112", "mrqa_searchqa-validation-5845"], "EFR": 0.9459459459459459, "Overall": 0.7044124034749035}, {"timecode": 28, "before_eval_results": {"predictions": ["the father of the house when in his home", "Welsh", "data link management", "blue", "melomel", "Vietnam", "John Robert Parker Ravenscroft", "Moscow", "insects", "Estonia", "Siberia", "malaria", "July 28, 1948", "Kent", "Arthur, Prince of Wales", "Israel", "butterflies", "New Jersey", "the Philippines", "terrorism", "the number thirteen", "BATH", "Eric Coates", "diggory Venn", "to make a furrow", "Brothers In Arms", "Mexico", "Ceredigion", "Eric Morley", "Saskatchewan", "Mickey Spillane", "Erik Aunapuu", "Frank Sinatra", "Alberto Salazar", "half", "Niger", "The Lone Gunmen", "Ivan Owen", "piano", "tritones", "Addis Ababa", "pascal", "heart", "Nova Scotia", "\"JeSuisCharlie vigils around the world\"", "Len Hutton", "Nigeria", "the Dead Sea", "40", "the isthmus", "2010", "penultima", "President Gerald Ford", "Spanish missionaries", "William Wyler", "Nardwuar the Human Serviette", "1887", "Portal", "on various military check posts in Pakistan's border with Afghanistan", "a plaque", "Phillip A. Myers", "Priscilla Beaulieu", "Cold Mountain", "killers"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5745535714285714}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false], "QA-F1": [0.6, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2318", "mrqa_squad-validation-4968", "mrqa_triviaqa-validation-7613", "mrqa_triviaqa-validation-5563", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-3536", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-5729", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-3924", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-3348", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-388", "mrqa_newsqa-validation-2885", "mrqa_searchqa-validation-1846", "mrqa_searchqa-validation-2066"], "SR": 0.515625, "CSR": 0.5619612068965517, "EFR": 0.967741935483871, "Overall": 0.7084406284760846}, {"timecode": 29, "before_eval_results": {"predictions": ["Albert C. Outler", "Jakaya Kikwete", "Drogo", "gibraltar", "st petersburg", "Australia", "chipmunk", "Amnesty International", "nixon", "Australia", "Labrador Retriever", "Rio Grande", "horror fiction", "Eastern Orthodoxy", "Poland", "colewine", "Brooklyn", "Octavian", "pverbs", "st petersburg", "PJ Harvey", "Gryffindor", "archer", "The French Connection", "Toonami's Rising Sun", "arthur", "Rapa Nui", "copenhagen", "baku", "Giuliano Bugiardini", "Oliver Twist", "Hinduism", "forefoot", "Alanis Morissette", "high sewing", "Herbert Henry Asquith", "apples", "index fingers", "Homo sapiens", "tlachtli", "short History of Tractors", "9", "steel", "shakespears Sister", "purple", "Fenn Street School", "stidwall", "Abel", "robbo", "copertina", "times", "couscousCouscous", "March 31, 2017", "1987", "reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "Dutch", "1981", "Humberside", "\"wildcat\" strikes", ", soeoth and Diouf say they had not exhibited any combative behavior.", "\"Toy Story,\"", "Jefferson the Virginian", "tap", "microwave"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5005208333333333}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7292", "mrqa_triviaqa-validation-4988", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-7583", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-1876", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-2522", "mrqa_hotpotqa-validation-2997", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-124", "mrqa_searchqa-validation-10920"], "SR": 0.4375, "CSR": 0.5578125, "EFR": 1.0, "Overall": 0.7140625}, {"timecode": 30, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1132", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-2642", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3117", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-394", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4426", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-478", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-5368", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-998", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2364", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3307", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-913", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16626", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5986", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-9022", "mrqa_searchqa-validation-9310", "mrqa_squad-validation-10141", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-1131", "mrqa_squad-validation-1272", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-1551", "mrqa_squad-validation-1639", "mrqa_squad-validation-1641", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1732", "mrqa_squad-validation-1830", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2849", "mrqa_squad-validation-2881", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3205", "mrqa_squad-validation-3270", "mrqa_squad-validation-3385", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3841", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4166", "mrqa_squad-validation-423", "mrqa_squad-validation-4385", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4655", "mrqa_squad-validation-4673", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4806", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5198", "mrqa_squad-validation-5234", "mrqa_squad-validation-5265", "mrqa_squad-validation-5331", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5590", "mrqa_squad-validation-5640", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5986", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6614", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-668", "mrqa_squad-validation-6727", "mrqa_squad-validation-6894", "mrqa_squad-validation-6956", "mrqa_squad-validation-7029", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-7435", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7740", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8144", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-839", "mrqa_squad-validation-848", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8600", "mrqa_squad-validation-8646", "mrqa_squad-validation-8656", "mrqa_squad-validation-8687", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-915", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-939", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9813", "mrqa_squad-validation-9878", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1021", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2293", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2789", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3512", "mrqa_triviaqa-validation-3595", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3887", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4717", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4858", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7104", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7613", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-884"], "OKR": 0.85546875, "KG": 0.4609375, "before_eval_results": {"predictions": ["cellular respiration", "centre-right Liberal Party of Australia", "the most cost efficient bidder", "Prussian", "1,500 ft", "stanley and carol", "media executive and the current chair of Cox Enterprises", "hanoi", "Stephen Mangan", "saloon", "power 108", "Vixen", "Adult Swim", "Venice", "Tropical Storm Ann", "1974", "tokamak", "March 17, 2015", "American", "1958", "China Airlines", "Wayne County", "Keeper of the Great Seal of Scotland", "Richard Nixon", "Greek", "Honey Irani", "2000", "Beauty and the Beast", "137th", "Nobel Prize in Physics", "Future", "1956", "Francis Nethersole", "hiphop", "47,818", "Salisbury", "Lakshmibai", "Tony Aloupis", "sarod", "Anne Arundel County", "fK Austria Wien", "midnight sun", "Robert A. Iger", "Netherlands", "Dr. Alberto Taquini", "2 March 1972", "Terry the Tomboy", "Gracie Mansion", "Parlophone Records", "R-8 Human Rhythm Composer", "obergruppenf\u00fchrer", "World War I", "84", "Linda Davis", "Sunni Muslim family", "mona Lisa", "eight", "highlands", "an open window", "staff sergeant", "\"It has never been the policy of this president or this administration to torture.\"", "trans fats", "Anna Mary Robertson", "sigmund Freud"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5631087662337663}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8333333333333334, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.8, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2885", "mrqa_hotpotqa-validation-1449", "mrqa_hotpotqa-validation-3524", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-1370", "mrqa_hotpotqa-validation-4594", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-3473", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-2529", "mrqa_hotpotqa-validation-5082", "mrqa_hotpotqa-validation-2791", "mrqa_hotpotqa-validation-686", "mrqa_naturalquestions-validation-276", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-4709", "mrqa_newsqa-validation-3819", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-15972"], "SR": 0.46875, "CSR": 0.5549395161290323, "retrieved_ids": ["mrqa_squad-train-46024", "mrqa_squad-train-86208", "mrqa_squad-train-4974", "mrqa_squad-train-59040", "mrqa_squad-train-13057", "mrqa_squad-train-16542", "mrqa_squad-train-83645", "mrqa_squad-train-43123", "mrqa_squad-train-48729", "mrqa_squad-train-65556", "mrqa_squad-train-70455", "mrqa_squad-train-68449", "mrqa_squad-train-58672", "mrqa_squad-train-65890", "mrqa_squad-train-33443", "mrqa_squad-train-49134", "mrqa_naturalquestions-validation-9602", "mrqa_newsqa-validation-1587", "mrqa_hotpotqa-validation-5513", "mrqa_naturalquestions-validation-2044", "mrqa_squad-validation-433", "mrqa_triviaqa-validation-2460", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-4983", "mrqa_squad-validation-3119", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2350", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-5660", "mrqa_triviaqa-validation-6265", "mrqa_searchqa-validation-12856"], "EFR": 1.0, "Overall": 0.7262222782258064}, {"timecode": 31, "before_eval_results": {"predictions": ["the treatment", "phagocytes", "Finland", "Gulf of Aden", "Natty Bumppo", "San Francisco", "Amsterdam", "cellulose", "moles", "Ryan MacGraw", "Sicily", "Howard Keel", "cobra Bubbles", "Charlie henderson", "Sweet Home Alabama", "hair loss", "Quin Ivy", "make Me an Offer", "William Shakespeare Biography - Poem Hunter", "Man V Food", "1780s", "Kajagoogoo", "George Fox", "Croatian", "Manchester City", "j.P. Pennington", "heineken", "Isaac", "South Africa", "fidelio", "ABBA", "Some Like It Hot", "mercury", "Cleopatra", "fumage", "Enrico Caruso", "Hitler right to invade Russia in 1941", "Hydrogen", "nitric acid", "wales", "flesh", "Mille miaro 6C 1500 GS \"testa fissa\" Zagato", "a tiger", "rhododendron", "Uranus", "Utrecht", "Toledo", "a wedge of hard cheese", "stenodattilografo", "caliper", "arts", "Adolf Hitler", "the player to the dealer's right", "March 27, 2017", "Black Mesa Research Facility", "4,613", "Swiss Super League", "Benj Pasek and Justin Paul", "mosteller", "environmental efforts", "innovative, exciting skyscrapers", "headed to Broadway in 'Chicago'", "Shakespeare Suite", "duster"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5415178571428572}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, false, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6, 0.0, 0.0, 1.0, 0.8571428571428571, 0.7499999999999999, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-7201", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-2298", "mrqa_triviaqa-validation-6667", "mrqa_triviaqa-validation-5571", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-7145", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-10606", "mrqa_hotpotqa-validation-1690", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3", "mrqa_searchqa-validation-15722", "mrqa_searchqa-validation-199", "mrqa_searchqa-validation-10166"], "SR": 0.46875, "CSR": 0.55224609375, "EFR": 0.9705882352941176, "Overall": 0.7198012408088236}, {"timecode": 32, "before_eval_results": {"predictions": ["K-9 and Company (1981)", "Thomas Sowell", "liverpool", "Ye Shiwen", "pangrams", "October 31", "Wyoming", "Leicester", "snakes", "kangaroos", "Lisieux", "1929", "hypopituitarism", "February", "piano", "Gloucestershire", "Jupiter Mining Corporation", "Harold Bierman", "Scooby snacks", "Amal Clooney", "Lake Union", "Adriatic Sea", "hitler", "Fife", "Goran Ivanisevic", "Francis Drake", "Wikipedia", "baku", "Truro", "Frasier Crane's dog", "tundras tundra", "Madness", "Barings", "Anne Boleyn", "Mrs. Peacock", "Ken Norton", "Yann Martel", "cabbage", "John Denver", "on Fleet Street", "charlie goose", "one-third", "monaco", "sheplinsky", "sorrento", "King Edward III", "Bill Bryson", "American Tobacco Company", "\"If\u2013\u201d", "Spanish", "Norman Mailer", "\"major science finding from the agency's ongoing exploration of Mars.\"", "between 1923 and 1925", "member states", "outer layer of skin", "Sarod", "Laban Movement Analysis", "Sir Matthew Arundell of Wardour", "There were no reports of ground strikes or interference with aircraft in flight,", "on the bench", "Bright Automotive", "will & Grace", "prehensile", "the Channel"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6002331349206349}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, true, false], "QA-F1": [0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.08, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-7626", "mrqa_triviaqa-validation-1725", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-1812", "mrqa_triviaqa-validation-7073", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-1770", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-10495", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-3994", "mrqa_searchqa-validation-852"], "SR": 0.515625, "CSR": 0.5511363636363636, "EFR": 1.0, "Overall": 0.7254616477272727}, {"timecode": 33, "before_eval_results": {"predictions": ["Theory of the Earth to the Royal Society of Edinburgh", "1580s", "intelligence officer", "France", "Ted Heath", "wuthering Heights", "Portugal", "williams will never play for another Premier League club", "vice-admiral", "Sweeney Todd", "sedpies", "Benfica", "six", "1984", "11", "Buzz Aldrin", "mrs hammer", "eddie", "venus williams", "The IT Crowd", "The Cream of Manchester", "wildebeest", "turtle playing cards", "Cold Comfort Farm", "Isar River", "Ruth Rendell", "wales", "robert friml", "city centre", "brouilly", "John Constable", "sheep", "willow", "carmen Miranda", "nottingham", "will make you jump jump", "1882", "jesus williams", "car accident", "sashimi", "sea otter", "dot-coms", "Tunisia", "Hugh Grant", "earache", "fibrotic tissue", "Vladimir Putin", "croquet", "low-cost carrier", "one Canada Square", "the best value diamond for your money", "wigan Warriors", "trichatrka ( Dicotyledons )", "minimum viable", "The United States Secretary of State", "Jos\u00e9 Bispo Clementino dos Santos", "1979", "Umberto II of Italy", "Spc. Megan Lynn Touma,", "suppress the memories", "40", "intelligent design", "The Tonight Show", "copper"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5166170634920635}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true], "QA-F1": [0.11111111111111112, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.6666666666666665, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-27", "mrqa_triviaqa-validation-6778", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-5508", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-7609", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-1809", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-4146", "mrqa_newsqa-validation-3534", "mrqa_searchqa-validation-11614"], "SR": 0.453125, "CSR": 0.5482536764705883, "retrieved_ids": ["mrqa_squad-train-7280", "mrqa_squad-train-22853", "mrqa_squad-train-11200", "mrqa_squad-train-51779", "mrqa_squad-train-8562", "mrqa_squad-train-38890", "mrqa_squad-train-69018", "mrqa_squad-train-11609", "mrqa_squad-train-64264", "mrqa_squad-train-15441", "mrqa_squad-train-5204", "mrqa_squad-train-48964", "mrqa_squad-train-22225", "mrqa_squad-train-281", "mrqa_squad-train-4394", "mrqa_squad-train-49825", "mrqa_searchqa-validation-902", "mrqa_triviaqa-validation-2277", "mrqa_hotpotqa-validation-550", "mrqa_triviaqa-validation-7287", "mrqa_naturalquestions-validation-1694", "mrqa_triviaqa-validation-2298", "mrqa_hotpotqa-validation-2112", "mrqa_naturalquestions-validation-4094", "mrqa_hotpotqa-validation-388", "mrqa_squad-validation-7626", "mrqa_newsqa-validation-435", "mrqa_naturalquestions-validation-8657", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-2493"], "EFR": 1.0, "Overall": 0.7248851102941176}, {"timecode": 34, "before_eval_results": {"predictions": ["Central business districts", "1973", "France", "The third Sunday", "Kenya", "September", "Japanese", "Albert Camus", "james Garner", "Dr. Martin Luther King", "petticoat", "indus", "Puerto Rico", "1080\u00b0", "Charles Taylor", "conchita wurst", "Ireland", "The Savoy", "niki lauda", "Finland", "Andaman", "Japan", "Massachusetts", "Boutros Ghali", "The Rolling Stones", "Uranus", "mole", "Crowley", "Greece", "Spain", "mumbai", "kitsunes", "frottage", "collage", "Primrose", "eriksson", "james stewart", "Richard Wilson", "Dean Martin", "Emily Davison", "Loki", "pence", "Genghis Khan", "ethel Skinner", "Peter Blake", "ghee", "neil Caesar", "rigmarole", "commitment", "Anastasia Dobromyslova", "S\u00e8vres", "Procol Harum", "Victory gardens", "artes liberales", "John", "AT&T", "1998", "Dachshunds", "cut your insurance in half", "Iran of trying to build nuclear bombs,", "\"John Branca and John McClain, were appointed temporary special administrators to run the estate until the process is completed.\"", "Tennessee", "\"beginning\",", "anemia"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6516894257703081}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7088", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-3475", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-7019", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-6010", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-3770", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-1948", "mrqa_searchqa-validation-16252"], "SR": 0.609375, "CSR": 0.55, "EFR": 0.96, "Overall": 0.717234375}, {"timecode": 35, "before_eval_results": {"predictions": ["Seven Days to the River Rhine", "the Seerhein", "2,579", "to collect menstrual flow", "New Jersey Devils of the National Hockey League ( NHL )", "in the Near East", "on Saturday evenings", "Emily Blunt", "Hook", "an alien mechanoid being that Will first encounters on the planet that his family crash lands on", "greater parallax", "four", "Leonard Bernstein", "Prince Henry", "9 February 2018", "in the 1970s", "2001", "in the United Kingdom", "her abusive husband", "The Canterbury Tales by Geoffrey Chaucer", "two - third of the total members present", "federal republic", "July 14, 1969", "Frank Langella", "Tennessee Titan", "Italian pignatta", "April 26, 2005", "Castleford", "Action Jackson", "New England Patriots", "the world's first collected descriptions of what builds nations'wealth", "Patrick Warburton", "when the cell is undergoing the metaphase of cell division", "the sixth season", "Mara Jade", "revenge and karma", "Kyla Coleman", "Nathan Hale", "Rachel Kelly Tucker", "far lesser degree by blood capillaries extending to the outer layers of the dermis", "during World War II", "Joe Pizzulo and Leeza Miller", "1.5 times the Schwarzschild radius", "David Tennant", "Kelly Reno", "Brooke Wexler", "over $1.84 billion", "England and Wales", "a `` double - fingered version of Churchill's victory sign ''", "Billy Hill", "2005", "Pangaea", "Kent", "petula Clark", "arm", "Tufts University", "2002", "May 4, 2004", "work together to stabilize Somalia and cooperate in security and military operations.", "Communist Party of Nepal (Unified Marxist-Leninist)", "Robert Barnett", "macGyver", "Daniel Boone", "chicago"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7261783355533356}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.8, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2727272727272727, 1.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.2222222222222222, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-8338", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-10614", "mrqa_triviaqa-validation-702", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2030"], "SR": 0.59375, "CSR": 0.5512152777777778, "EFR": 0.9230769230769231, "Overall": 0.7100928151709403}, {"timecode": 36, "before_eval_results": {"predictions": ["$5,000,000", "The Handmaid's Tale", "Austrian", "Archbishop of Canterbury", "1945", "102,984", "Emmanuel Ofosu Yeboah", "Clarence Nash", "Bulgarian-Canadian", "Macomb County", "Dusty Dvoracek", "45th Vice President of the United States from 1993 to 2001", "1972", "Disney California Adventure", "Indiana", "Travis County, Texas", "The 1996 PGA Championship", "orange", "Regional League North", "a racer came second", "ragby", "life insurance", "Ukrainian", "Cape Cod", "actress and model", "Immediate Media Company", "George Clooney", "Joe Scarborough", "Tottenham Hotspur Football Club, commonly referred to simply as Tottenham ( ) or Spurs", "the attack on Pearl Harbor", "Alemannic", "Vienna", "Jesper Myrfors", "Paper", "Amway", "Ogallala Aquifer", "What's Up", "News Corp and 21st Century Fox", "taking a bet from a gambler", "12", "Rockbridge County", "jurisdiction", "Italian", "Gatwick", "Santana", "two Nobel Peace Prizes", "Dutch", "a role-playing game or wargame campaign", "2013 Cannes Film Festival", "NXT Tag Team Championship", "DJ", "Bill Belichick", "Achal Kumar Jyoti", "a set of connected behaviors, rights, obligations, beliefs, and norms as conceptualized by people in a social situation", "guitar", "17", "plac\u0113b\u014d", "a federal judge in Mississippi", "when the economy turns unfriendly,", "\"It was perfect work, ready to go for the stimulus package,\"", "blow larvae", "walruses", "achilles", "Harry S Truman"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6239154075091575}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.3076923076923077, 0.0, 0.15384615384615385, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.25, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-5164", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3109", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3145", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-2776", "mrqa_newsqa-validation-2449", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-13135"], "SR": 0.515625, "CSR": 0.5502533783783784, "retrieved_ids": ["mrqa_squad-train-33824", "mrqa_squad-train-25990", "mrqa_squad-train-66046", "mrqa_squad-train-18683", "mrqa_squad-train-57394", "mrqa_squad-train-32304", "mrqa_squad-train-64986", "mrqa_squad-train-50117", "mrqa_squad-train-55022", "mrqa_squad-train-15891", "mrqa_squad-train-37501", "mrqa_squad-train-85487", "mrqa_squad-train-26662", "mrqa_squad-train-44167", "mrqa_squad-train-80749", "mrqa_squad-train-57068", "mrqa_hotpotqa-validation-4280", "mrqa_naturalquestions-validation-8659", "mrqa_hotpotqa-validation-989", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-2317", "mrqa_naturalquestions-validation-1315", "mrqa_squad-validation-845", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-1450", "mrqa_hotpotqa-validation-1643", "mrqa_squad-validation-680", "mrqa_triviaqa-validation-5590", "mrqa_naturalquestions-validation-1284"], "EFR": 1.0, "Overall": 0.7252850506756757}, {"timecode": 37, "before_eval_results": {"predictions": ["Robert Lane and Benjamin Vail", "pjaro carpintero", "20", "cannibalism", "College of William and Mary", "China", "diamonds", "Biggie Smalls", "Adam", "chinook", "Feodor Dostoyevsky", "Emily Hughes", "Sonnets", "Caesar salad", "a case", "David Berkowitz", "Catch Me if You Can", "Taos Pueblo", "a 3500 lb. sculpture", "licorice stick", "A Moon for the Misbegotten", "Donovan", "a place name", "Dublin", "mathematical", "George II", "Suzuki Grand Vitara", "Yogi Bear", "Lebanon", "Judas Iscariot", "Christopher Reeve", "stripes", "Little Red Riding Hood", "550 nm", "Daryl Hall", "Cherokee", "the French people", "Gettysburg", "Africa", "Jackie Kennedy", "Grover Cleveland", "Orange County", "Basil", "Arkansas", "Helen of Troy", "the Board", "Faust", "Aaron Burr", "violins", "Ethanol", "Adam Smith", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "Paul", "Due to the close quarters and poor hygiene exhibited at that time", "red", "Cole Porter", "a horse", "New York City", "Westminster system", "Phil Collins", "nuclear weapon", "Egypt", "Jackson sitting in Renaissance-era clothes and holding a book.", "boxing"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5462510146103896}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.625, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.06666666666666667]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_searchqa-validation-12628", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-1092", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-12401", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-11435", "mrqa_searchqa-validation-2181", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-3190", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-8404", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-10312", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-14370", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-2068", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-822", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-10156", "mrqa_hotpotqa-validation-4081", "mrqa_newsqa-validation-48", "mrqa_naturalquestions-validation-9459"], "SR": 0.453125, "CSR": 0.5476973684210527, "EFR": 1.0, "Overall": 0.7247738486842106}, {"timecode": 38, "before_eval_results": {"predictions": ["1954", "Zion", "cloves", "Melanie Griffith", "Abraham Lincoln", "Allston", "a physician or surgeon", "Greenpeace", "a cantile", "pumpkin pie", "a canton", "a Palestinian city", "Caracas", "Giza", "Breakfast at Tiffany's", "Anyone", "Faneuil Hall", "Babe", "shrimp", "balsa", "Awar el-Sadat", "prostitutes", "a moth", "New Orleans Saints", "Princess Diana", "Tasmania", "Lakshmi Mittal", "cobalt", "New England", "The Apartment", "Spider-Man 3", "grease", "glucose", "Germany", "Nintendo", "Van Helsing", "Hugh Grant", "the Great Wall", "hand", "Hormel Foods", "Canada", "Clara Barton", "Kauai, Hawaii", "esophagus", "Napoleon", "Otsego County", "Fred Sanford", "Colombia", "Robert Livingston", "Venezuela", "canticle", "Donna", "before November 1", "first stand - alone instant messenger", "Apollon", "Chicago", "Andrew Lloyd Webber", "1851", "Tim Howard", "Princess Jessica", "Hutus and Tutsis", "Cinder block propping open a back door.", "Monday and Tuesday", "foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5205639367816092}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4827586206896552]}}, "before_error_ids": ["mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-7187", "mrqa_searchqa-validation-8722", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-6833", "mrqa_searchqa-validation-1238", "mrqa_searchqa-validation-9939", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-2019", "mrqa_searchqa-validation-3861", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11344", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-6570", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-1716", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-1534", "mrqa_searchqa-validation-16849", "mrqa_naturalquestions-validation-215", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-4264", "mrqa_hotpotqa-validation-3786", "mrqa_hotpotqa-validation-5676", "mrqa_newsqa-validation-3774", "mrqa_naturalquestions-validation-4021"], "SR": 0.453125, "CSR": 0.5452724358974359, "EFR": 0.9714285714285714, "Overall": 0.7185745764652014}, {"timecode": 39, "before_eval_results": {"predictions": ["movements of nature", "retirement", "Hill Street Blues", "bacteria", "Ross Perot", "murdered", "fuchsia", "bone", "Lance Armstrong", "Kung Fu", "tennis elbow", "sienna", "Hindu", "the Voice", "Nacho Libre", "\"Strawberry Fields Forever\"", "Cygnus", "lowlands", "nougat", "a Scotch egg", "Manhattan Project", "the Eiffel Tower", "Roger Federer", "sculpere", "the yolk", "a cheddar", "Iran", "Florida", "The Virgin Spring", "the three sons", "the Nome", "Queen Victoria", "Winnipeg", "mayonnaise", "Zorro", "(assume)", "Jack Sprat", "offbeat", "Uranium", "Ismene", "life's Refinements", "Gannett", "Tranio", "George Sand", "a glacier", "Dick Gephardt", "a Harvard Alum", "Lord Louis Mountbatten", "Master of Fine Arts", "American Idol", "Robert Peary", "down to the ground", "Hellenism", "James W. Marshall", "Ares", "linseed", "The Sixth Sense", "Wojeta", "Montreal, Quebec, Canada", "Broad Sands Bay", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "58 minutes", "\"What she's doing is putting a personal and human face on the issue... there's nothing more crucial,\"", "$31,000"], "metric_results": {"EM": 0.5, "QA-F1": 0.5567708333333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-7265", "mrqa_searchqa-validation-2577", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-12707", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-12788", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-15272", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-9039", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-262", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-4826", "mrqa_naturalquestions-validation-6856", "mrqa_triviaqa-validation-6258", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-31", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-1152"], "SR": 0.5, "CSR": 0.544140625, "retrieved_ids": ["mrqa_squad-train-62184", "mrqa_squad-train-18870", "mrqa_squad-train-73889", "mrqa_squad-train-49235", "mrqa_squad-train-36562", "mrqa_squad-train-27168", "mrqa_squad-train-39571", "mrqa_squad-train-46488", "mrqa_squad-train-43908", "mrqa_squad-train-50810", "mrqa_squad-train-25816", "mrqa_squad-train-63146", "mrqa_squad-train-8194", "mrqa_squad-train-29668", "mrqa_squad-train-3965", "mrqa_squad-train-388", "mrqa_squad-validation-7632", "mrqa_triviaqa-validation-2317", "mrqa_naturalquestions-validation-114", "mrqa_triviaqa-validation-7376", "mrqa_naturalquestions-validation-5608", "mrqa_newsqa-validation-608", "mrqa_naturalquestions-validation-5782", "mrqa_squad-validation-10388", "mrqa_hotpotqa-validation-4466", "mrqa_searchqa-validation-1127", "mrqa_naturalquestions-validation-8338", "mrqa_naturalquestions-validation-1783", "mrqa_triviaqa-validation-5563", "mrqa_hotpotqa-validation-5676", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-10166"], "EFR": 0.96875, "Overall": 0.7178125}, {"timecode": 40, "UKR": 0.763671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2621", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-2920", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3375", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3586", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3680", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-514", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1316", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12707", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-13508", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-2066", "mrqa_searchqa-validation-222", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4829", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9908", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10088", "mrqa_squad-validation-10388", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1248", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2608", "mrqa_squad-validation-2831", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3331", "mrqa_squad-validation-349", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4046", "mrqa_squad-validation-4155", "mrqa_squad-validation-4331", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4634", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5248", "mrqa_squad-validation-5307", "mrqa_squad-validation-5389", "mrqa_squad-validation-5469", "mrqa_squad-validation-5506", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5728", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6024", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6224", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6702", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7211", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-801", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-9140", "mrqa_squad-validation-915", "mrqa_squad-validation-9285", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9525", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-1425", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5729", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.830078125, "KG": 0.49921875, "before_eval_results": {"predictions": ["toward the center of the curving path", "slot machines", "Hinduism", "a turtle", "Donkey", "Jane Eyre", "Aiden", "William Shakespeare", "Louis le Grand", "Montmartre", "The Dying Swan", "Jailhouse Rock", "a protractor", "voter registration", "The Kite Runner", "white granite", "Islamabad", "horseshoe crabs", "Stephen Crane", "trespassing", "Jack Dempsey", "beheading", "Val Kilmer", "Pakistan", "Milwaukee", "a kiwi", "Pop-Tarts", "sugar", "Enrico Fermi", "Tiger Woods", "the Madding Crowd", "local broadcasters", "Grace Kelly", "a monkey's tail", "Bilbo", "Oliver Wendell Holmes", "the Constitution of the United States", "proverbs", "the fish hawk", "Maria Montessori", "an orchid", "a pine", "the Sun", "Michelangelo", "Spain", "ale", "Superman", "each phrase", "Brazil", "Puget Sound", "phylum", "Yahya Khan", "Qianlong", "1038", "Bubba", "Easter Parade", "Thom Yorke", "beer", "Keeper of the Privy Seal of Scotland", "Martin O'Malley", "1983", "The cause of the child's death will be listed as homicide by undetermined means", "six", "Oxfordshire"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7308779761904762}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-12159", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-7222", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-4248", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-9569", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-2627", "mrqa_triviaqa-validation-5693"], "SR": 0.671875, "CSR": 0.5472560975609756, "EFR": 1.0, "Overall": 0.7280449695121951}, {"timecode": 41, "before_eval_results": {"predictions": ["metals", "2004", "to capitalize on her publicity", "Karen Gillan", "Sally Dworsky", "Nick Kroll", "Albert Einstein", "Miami Heat of the National Basketball Association ( NBA )", "Poems : Series 1", "MFSK", "the east African coast", "200 to 500 mg up to 7 mL", "James Madison", "Charles Haley", "asphyxia", "1947", "Thomas Edison", "in the genome", "the work function ( sometimes spelled workfunction ) is the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Flag Day in 1954", "Erica Rivera", "Afghanistan", "Gettysburg College", "the Great Plains", "thia Weil", "directly into the bloodstream", "an oxidant, usually atmospheric oxygen", "1 mile ( 1.6 km )", "1871", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "1970", "sound travels most slowly in gases", "Presley Smith", "a combination of genetics and the male hormone dihydrotestosterone", "Dylan Massett", "qualitative data, quantitative data", "Tami Lynn", "Christianity", "radians", "1931", "Help!", "July 31, 2010", "24", "August 8, 1945", "Bibi and Bichu -- The Jugglinies", "16 December 1908", "the Comprehensive Crime Control Act of 1984", "the meridian", "Buffalo Springfield", "the times sign", "within the chapters as well", "Angevin", "asphalt", "Mar del Sur", "Julie Taymor", "an organ", "Tony Aloupis", "the American Civil Liberties Union", "Chevron", "attempted burglary", "Pancho Gonzales", "William Henry Harrison", "an orchids", "Britain"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6251312229437229}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.24000000000000002, 1.0, 0.16666666666666669, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.8636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-9972", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-9237", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-7595", "mrqa_newsqa-validation-3427", "mrqa_newsqa-validation-2835", "mrqa_searchqa-validation-2827"], "SR": 0.53125, "CSR": 0.546875, "EFR": 0.9333333333333333, "Overall": 0.7146354166666666}, {"timecode": 42, "before_eval_results": {"predictions": ["high voltage", "a buffalo", "Six Flags over Texas", "Song of Solomon", "Israel", "Denmark", "Battlestar Galactica", "Sainte-Marie", "sheep", "Mary Tudor", "Lucia di Lammermoor", "a blackbird", "Patty Duke", "Superman", "Judas Iscariot", "3,000th", "grow old", "a savanna mosaic", "Luciano", "a short U.S. ton", "Kellogg's", "Fall Guy", "a wolf", "depth and breadth and height", "Judy Garland", "crocodiles", "Greece", "Bosnia and Herzegovina, FYR", "a baboon-heart", "Morris West", "outta nowhere", "El burlador de Sevilla", "Hawaii", "Empire State Building", "the League of Nations", "Sally Ride", "bullets", "Queen Elizabeth I's Speech to the Troops at Tilbury", "a C average", "Juicy Couture Eau de Parfum Spray, 3.4 fl. oz.", "the Civil War", "gravity", "Greek Meatballs", "Free catcher", "the Caucasus mountains, southwestern", "Firebird", "14", "Lecompton, Kansas", "Midnight Cowboy", "Rosetta Stone", "Louisiana", "Malayalam Odakkuzhal", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "destroyed the only boats on the island", "Arabian Gulf", "Ross Kemp", "Marine One", "Juan Manuel Mata Garc\u00eda (] ; born 28 April 1988) is a Spanish professional footballer who plays as a midfielder for English club Manchester United and the Spain national team", "Vietnam War", "Taylor Swift", "Cain", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "on-loan David Beckham", "a 15-year-old boy that has left dozens injured and scores of properties destroyed."], "metric_results": {"EM": 0.46875, "QA-F1": 0.6145876259157509}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615385, 0.8, 0.375]}}, "before_error_ids": ["mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-9124", "mrqa_searchqa-validation-15998", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-8429", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-6562", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-11902", "mrqa_searchqa-validation-1970", "mrqa_searchqa-validation-9663", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-448", "mrqa_searchqa-validation-10127", "mrqa_searchqa-validation-7377", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-1406", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-894", "mrqa_triviaqa-validation-1411", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-115"], "SR": 0.46875, "CSR": 0.5450581395348837, "retrieved_ids": ["mrqa_squad-train-68073", "mrqa_squad-train-32659", "mrqa_squad-train-42822", "mrqa_squad-train-64462", "mrqa_squad-train-43489", "mrqa_squad-train-69584", "mrqa_squad-train-35684", "mrqa_squad-train-24649", "mrqa_squad-train-56763", "mrqa_squad-train-18304", "mrqa_squad-train-28628", "mrqa_squad-train-57404", "mrqa_squad-train-11911", "mrqa_squad-train-62217", "mrqa_squad-train-4802", "mrqa_squad-train-84703", "mrqa_hotpotqa-validation-4964", "mrqa_searchqa-validation-16618", "mrqa_squad-validation-4849", "mrqa_naturalquestions-validation-8005", "mrqa_triviaqa-validation-6488", "mrqa_newsqa-validation-2778", "mrqa_hotpotqa-validation-904", "mrqa_triviaqa-validation-729", "mrqa_searchqa-validation-2783", "mrqa_hotpotqa-validation-2351", "mrqa_squad-validation-5733", "mrqa_hotpotqa-validation-1537", "mrqa_newsqa-validation-3327", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-2460", "mrqa_naturalquestions-validation-9837"], "EFR": 1.0, "Overall": 0.7276053779069767}, {"timecode": 43, "before_eval_results": {"predictions": ["Parris Island, South Carolina", "gin", "Leicester", "a mile", "Jets", "scurvy", "Japanese", "ruddock", "falconry", "Niger", "Norman Brookes", "Billy Crystal", "Jaipur", "Goran Ivanisevic", "14", "Beninald Bosanquet", "Spain", "Henry Hudson", "Bridge", "a raven", "Much Ado About Nothing", "Felix", "Louis XV", "Mrs Mainwaring", "Australia", "Robert A. Heinlein", "Old Ironsides", "Aug. 24, 1572", "fertilization", "decoupage", "Massachusetts", "Sherlock Holmes", "Delaware", "Olivia Smith", "Costa Concordia", "spiral (S), elliptical (E), and lenticular (S0)", "orange juice", "Jim Morrison", "Mona Lisa", "clay", "Bash Street", "Mercury", "Ireland", "Gandalf", "Andile Smith", "Bogart", "Minder", "a star", "a turkey", "Eva Marie", "Tigger", "Saint Alphonsa", "Ku - Klip", "March 15, 1945", "Portsea", "1861", "Team Penske", "\" Maria\"", "Diego Milito", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Zyrtec", "Cockney lids", "1", "Leo Frank"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5703125}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3139", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-294", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-3871", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-2978", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7617", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-6266", "mrqa_hotpotqa-validation-5700", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-2751", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-11984"], "SR": 0.515625, "CSR": 0.5443892045454546, "EFR": 1.0, "Overall": 0.7274715909090909}, {"timecode": 44, "before_eval_results": {"predictions": ["Hoek van Holland", "Lori McKenna", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "Marcus Atilius Regulus", "the forward reaction proceeds at the same rate as the reverse reaction", "Lucknow", "Kristy Swanson", "Hendersonville, North Carolina", "aqueous solution", "Abraham", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "1773", "the final episode of the series", "a god of the Ammonites", "the Ming", "just after the Super Bowl", "1980s", "31 October 1972", "The Italian Agostino Bassi", "following graduation with a Bachelor of Medicine, Bachelor of surgery degree", "Germany", "Real Madrid", "LED illuminated", "BC Jean", "about 24 hours", "cognitive bias", "2015", "No. 1 seed Virginia", "Tommy Shaw", "late - 2011", "Thomas Alva Edison", "B.R. Ambedkar", "the Indian Hockey Federation ( IHF )", "Niles", "Judy Collins", "Oklahoma", "Grand Inquisition", "British pop band T'Pau", "Angel Island Immigration Station", "Johannes Gutenberg", "Terry Reid", "masons'marks", "Johannes Gutenberg", "Domhnall Gleeson", "host nation Russia", "the Pandavas", "2020 National Football League ( NFL ) season", "1994 season", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "the church at Philippi", "Chuck Noland", "The Green Mile", "Cal Ripken, Jr.", "1905", "Workers' Party", "Revolver", "140 million", "cell phones", "the International Space Station", "estimated 750", "lew springsteen", "nantucket", "Microsoft", "Love Never Dies"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6592322151532677}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.2105263157894737, 0.0, 0.9473684210526316, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.5, 1.0, 0.19999999999999998, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-4592", "mrqa_triviaqa-validation-7676", "mrqa_newsqa-validation-1345", "mrqa_newsqa-validation-755", "mrqa_searchqa-validation-7952", "mrqa_searchqa-validation-3760"], "SR": 0.515625, "CSR": 0.54375, "EFR": 0.8709677419354839, "Overall": 0.7015372983870967}, {"timecode": 45, "before_eval_results": {"predictions": ["generally westward", "off the coast of Somalia", "Felipe Massa", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Ronaldinho leaves Gava magistrates court in Spain after being granted dual nationality", "the Dalai Lama", "\"handful\" of domestic disturbance calls to police since 2000", "President Bush", "prisoners", "Sunday", "Vice President Joe Biden", "in the Mediterranean Sea", "one", "raping and murdering", "Ryder Russell", "four months", "father, Osama", "the club's board", "insect stings", "it is provocative action", "carol", "Fullerton, California", "Chinese and international laws", "$2.6 million", "Rev. Alberto Cutie", "its", "Chris Robinson", "269,000", "surrounding areas of the bustling capital", "New York Philharmonic Orchestra in North Korea", "the state's attorney", "delivered three machine guns and two silencers to the hip-hop star", "need for reconciliation in a country that endured a brutal civil war lasting nearly three decades.\"", "two", "in Melbourne", "Steve Williams", "theft in Switzerland", "Oxbow, Minnesota", "braving elements ranging from rain to wind", "Joan Rivers", "2,800", "bargyle", "the FDA", "Diversity", "650", "Yemen", "South Africa", "Daniel Radcliffe", "President Sheikh Sharif Sheikh Ahmed", "cities throughout Canada", "Florida's Everglades", "Virgil Ogletree", "Tyrion Lannister", "June 8, 2009", "'Q'", "wolf", "polio", "quantum theory", "the outdoors", "Mel Blanc", "word \"hoover\"", "Colorado", "CIA", "Billy Bob Thornton"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4926855211781682}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.09523809523809525, 0.0, 0.2857142857142857, 0.0, 0.3333333333333333, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.11764705882352941, 0.05555555555555555, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9261", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-5370", "mrqa_triviaqa-validation-3086", "mrqa_hotpotqa-validation-4796", "mrqa_searchqa-validation-6261"], "SR": 0.40625, "CSR": 0.5407608695652174, "retrieved_ids": ["mrqa_squad-train-30640", "mrqa_squad-train-7687", "mrqa_squad-train-10926", "mrqa_squad-train-4634", "mrqa_squad-train-26842", "mrqa_squad-train-11226", "mrqa_squad-train-32382", "mrqa_squad-train-74739", "mrqa_squad-train-36065", "mrqa_squad-train-78547", "mrqa_squad-train-73399", "mrqa_squad-train-50878", "mrqa_squad-train-20301", "mrqa_squad-train-75789", "mrqa_squad-train-3442", "mrqa_squad-train-25645", "mrqa_newsqa-validation-1466", "mrqa_naturalquestions-validation-1427", "mrqa_naturalquestions-validation-10408", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-5907", "mrqa_newsqa-validation-1507", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-9039", "mrqa_searchqa-validation-711", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-4837", "mrqa_searchqa-validation-5261", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-1083"], "EFR": 1.0, "Overall": 0.7267459239130434}, {"timecode": 46, "before_eval_results": {"predictions": ["every four years", "a stray wandering the streets of Moscow", "presbyters", "enterocytes of the duodenal lining", "warmth", "a limited period of time", "during meiosis", "federal government", "The alveolar process", "Katharine Hepburn", "Thorleif Haug", "multiple", "each country provides public healthcare to all UK permanent residents that is free at the point of use, being paid for from general taxation", "Gladys Knight & the Pips", "solve its problem of lack of food self - sufficiency", "January 2004", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "291", "the French CYCLADES project directed by Louis Pouzin", "when each of the variables is a perfect monotone function of the other", "the Kansas City Chiefs", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "especially in Western cultures", "Husrev Pasha", "2001", "Squamish, British Columbia, Canada", "Mankombu Sambasivan Swaminathan", "annuity", "1962", "prophets and beloved religious leaders", "Best Picture, Best Director for Fincher, Best Actor for Pitt and Best Supporting Actress for Taraji P. Henson", "The Federal Communications Commission ( FCC )", "Patris et Filii et Spiritus Sancti", "William Chatterton Dix", "1987", "Stefanie Scott", "Karen Gillan", "in the fovea centralis", "Ephesus", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "George H.W. Bush", "seven", "Uralic", "whenever concrete data is generalized or influences ambiguous information", "manga", "British and French Canadian fur traders", "Lori McKenna", "2017", "October 27, 1904", "in the mid - to late 1920s", "Major Robert Smith", "Runic", "backgammon", "Malcolm Bradbury", "Boston Red Sox", "Samuel James Cassell Sr.", "Ludvig Holberg", "Pakistan's intelligence agency", "Maj. Nidal Malik Hasan,", "February 12", "Charles Dickens", "the Capulets & the Montagues", "plutonium", "24"], "metric_results": {"EM": 0.5, "QA-F1": 0.6062165618229609}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.2222222222222222, 0.7272727272727273, 0.6666666666666666, 0.30769230769230765, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 0.16, 1.0, 0.08695652173913043, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.7499999999999999, 0.09999999999999999, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 0.6666666666666666, 0.04651162790697674, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2509", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5454", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-1692", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-4464"], "SR": 0.5, "CSR": 0.5398936170212766, "EFR": 0.875, "Overall": 0.7015724734042552}, {"timecode": 47, "before_eval_results": {"predictions": ["Max Martin", "Mitsubishi Lancer OZ Rally", "21 June 2007", "The Queen of Hearts", "the government - owned corporation of Puerto Rico responsible for electricity generation, power distribution, and power transmission on the island", "Austria - Hungary", "Mace Coronel", "libretto", "Jane Addams", "Harry", "1800", "dress shop", "As of January 17, 2018, 201", "Millerlite", "Instagram's own account", "Experimental neuropsychology", "30 years after Return of the Jedi", "Selena Gomez", "14 \u00b0 41 \u2032 34 '' N 17 \u00b0 26 \u2032 48 '' W \ufeff / \ufffdrous 14.69278 \u00b0 N 17.44667 \u00b0 W", "Christina Aguilera", "during initial entry training", "Eriq La Salle", "1997", "Bonnie Aarons", "1960", "during the summer of 1979", "Part XI of the Indian constitution", "the Constitution of India came into effect on 26 January 1950", "The Risin'Sun '' by Texas Alexander ( 1928 )", "halogenated paraffin hydrocarbons that contain only carbon, chlorine, and fluorine, produced as volatile derivative of methane, ethane, and propane", "the 1980s", "MacFarlane", "Despacito '' by Luis Fonsi and Daddy Yankee featuring Justin Bieber in 2017", "1898", "a set of components that included charting, advanced UI, and data services ( Flex Data Services )", "Matt Jones", "members of the actual club with the parading permit as well as the brass band", "The Confederate States Army ( C.S.A. )", "three", "a pair of transceivers, each with an antenna, and a source of energy ( such as a battery, a portable generator, or the electrical grid )", "Zachary John Quinto", "Sanchez Navarro", "Have I Told You Lately", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "John Travolta", "Scheria", "1 October 2006", "2017 Georgia Bulldogs", "Eda Reiss Merin", "Michael Clarke Duncan", "states that the `` Senate may propose or concur with Amendments as on other Bills", "Gestapo", "Belgium", "heartburn", "Baltimore and Ohio Railroad", "Vanessa Hudgens", "two", "at a relative's house,", "voice-assistant software", "hardship for terminally ill patients and their caregivers", "The Odyssey", "The Fly", "prego", "Marillion"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5593851605570356}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.888888888888889, 0.0, 0.0, 0.2727272727272727, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.0, 0.9500000000000001, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.08333333333333333, 0.0, 1.0, 1.0, 0.0, 0.5, 0.07407407407407408, 0.0, 0.1, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-10381", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-8182", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-89", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-2606", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-886", "mrqa_searchqa-validation-3520"], "SR": 0.4375, "CSR": 0.5377604166666667, "EFR": 0.8888888888888888, "Overall": 0.703923611111111}, {"timecode": 48, "before_eval_results": {"predictions": ["Daylight Saving Time", "Cygnus", "Constantine", "Henri de Toulouse-Lautrec", "Nigeria", "Hawaii", "Jeremiah", "a horse with a \"smooth mouth,\"", "Don Knotts", "cinnamon Life", "Copenhagen", "Porgy and Bess", "Dutchman", "Mars", "Frasier: Head Game", "Robert Frost", "geena Davis", "barement", "transtormation", "Laila Ali", "New Zealand", "Epiphany", "bordercity idee fixe", "The Hustler", "to settle, shelter, or house in or as if in a nest <the children were nestled all snug in their beds", "Led Zeppelin", "The Book of Kells", "grandmother", "nuclear weapons", "a Heisman", "Gulf of Tonkin", "Stephen Vincent Bent", "the West Indian Ocean coelacanth", "Prague", "the Federal Reserve", "gbage", "Afghanistan", "Cheetah", "Ambrose Bierce", "the American Lung Association", "croquet", "Aphrodite", "New age", "Freddie James Prinze Jr.", "Budapest", "John Mahoney", "pythons", "Nit-A-Nee", "Charles de Gaulle", "Beverly Cleary", "Afghanistan", "Christmas 2009", "a recognized group of people who jointly oversee the activities of an organization", "since 3, 1, and 4 are the first three significant digits of \u03c0", "10", "the Young Men's Christian Association", "Alice Pleasance Liddell", "the theory of direct scattering and inverse scattering", "43rd", "South America", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "$4 a gallon.", "fill a million sandbags and place 700,000 around our city.", "Michael Weir"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6249255952380952}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.42857142857142855, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-4404", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-15874", "mrqa_searchqa-validation-10346", "mrqa_searchqa-validation-7598", "mrqa_searchqa-validation-8359", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-15307", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-12165", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-3066", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-6317", "mrqa_triviaqa-validation-4738", "mrqa_hotpotqa-validation-1316", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-675", "mrqa_triviaqa-validation-336"], "SR": 0.53125, "CSR": 0.5376275510204082, "retrieved_ids": ["mrqa_squad-train-35414", "mrqa_squad-train-77470", "mrqa_squad-train-77758", "mrqa_squad-train-10153", "mrqa_squad-train-20667", "mrqa_squad-train-67602", "mrqa_squad-train-20476", "mrqa_squad-train-14356", "mrqa_squad-train-44485", "mrqa_squad-train-4536", "mrqa_squad-train-53546", "mrqa_squad-train-56366", "mrqa_squad-train-10551", "mrqa_squad-train-854", "mrqa_squad-train-45104", "mrqa_squad-train-41615", "mrqa_naturalquestions-validation-1171", "mrqa_hotpotqa-validation-5392", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-7623", "mrqa_newsqa-validation-1218", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-7864", "mrqa_triviaqa-validation-741", "mrqa_naturalquestions-validation-384", "mrqa_triviaqa-validation-6392", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-1127", "mrqa_naturalquestions-validation-5476", "mrqa_searchqa-validation-3520", "mrqa_naturalquestions-validation-5900", "mrqa_hotpotqa-validation-520"], "EFR": 1.0, "Overall": 0.7261192602040816}, {"timecode": 49, "before_eval_results": {"predictions": ["William Wyler", "Redford's adopted home state of Utah", "pelvic floor", "Kanawha Rivers", "phosphorylated by creatine kinase to form phosphocreatine", "Brian Steele", "1937", "1920s", "832 BCE", "2005", "New York City", "Beorn", "Jonathan Cheban", "1992", "Montreal", "18", "Kristy Swanson", "230 million kilometres", "once again be hosted by Camping World Stadium in Orlando", "the Bactrian", "sorrow regarding the environment", "the New York Yankees", "every president since Woodrow Wilson", "Phosphorus pentoxide", "Brooklyn, New York", "Governor Al Smith", "the Mishnah", "deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "Tom Brady", "Around 1200", "Walter Egan", "In `` The Crossing ''", "The person who has existence in two parallel worlds", "Australia", "origins of replication", "people raced modified cars on dry lake beds northeast of Los Angeles under the rules of the Southern California Timing Association ( SCTA )", "from 13 to 22 June 2012", "Jackie Robinson", "Coton in the Elms", "disagreements involving slavery and states'rights", "Pradyumna", "1979", "James Madison", "UMBC", "Roger Federer", "17 - year - old", "Kylie Minogue", "5,534", "pumped through the semilunar pulmonary valve into the left and right main pulmonary arteries ( one for each lung ), which branch into smaller pulmonary arteries that spread throughout the lungs", "electron donors", "A rear - view mirror", "George W Bush's", "USS Constitution", "Lucas McCain", "Martin Scorsese", "2014", "June 6, 1959", "gang rape", "the Listeria monocytogenes bacteria has risen to 28,", "United States, NATO member states, Russia and India", "Sagamore Hill", "hyperthyroidism", "an arsenic compound, cacodyl cyanide", "sun protection factor"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7341090823221466}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.25, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.6976744186046512, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.18181818181818182, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-616", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-2074", "mrqa_searchqa-validation-14922", "mrqa_searchqa-validation-10011"], "SR": 0.609375, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.7264062499999999}, {"timecode": 50, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4245", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-561", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9724", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3331", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-233", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.826171875, "KG": 0.47109375, "before_eval_results": {"predictions": ["a cappella", "\"leno\" fabric and then cutting the fabric into strips to make the chenille yarn", "dia", "8", "Lady Gaga", "John Alec Entwistle", "kray Brothers", "mwestwood", "monaco", "December 18, 1958", "Cumberland sausage", "Gloucestershire", "Queen Victoria and Prince Albert", "a modern Townsend Thoresen car and passenger ferry", "george w", "monaco (aboo dub-bee)", "Madagascar", "persian gulf", "Miss Havisham", "oxygen", "Macbeth", "Gentlemen Prefer Blondes", "Khrushchev", "Netherlands", "the hose", "the Hudson River", "(John) Key", "Subway's", "geodesy(Noun)", "Dylan Thomas", "Jeff Bridges", "the Tower of London", "Bridge", "quarter", "cirrus uncinus", "haiti", "the Great War", "cheese", "Klaus dolls", "Argentina", "faldo", "george b", "Count Basie Orchestra", "the Be beast", "(John) Houblon,", "maverick", "Elbow", "General Paulus", "isohyet", "neapolitan", "monaco", "July 14, 1969", "1923", "Massillon, Ohio", "1937", "LA Galaxy", "(MCG)", "The BBC,", "Frank Ricci,", "\"Three Little Beers,\"", "the sweats sweater", "Susan B. Anthony dollar", "ivory", "in the north and west of the country,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5084908963585434}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.3529411764705882]}}, "before_error_ids": ["mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6534", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-4700", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-5462", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5001", "mrqa_triviaqa-validation-7490", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1896", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-4693", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-3522", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-2793", "mrqa_searchqa-validation-1070", "mrqa_searchqa-validation-2495", "mrqa_newsqa-validation-2191"], "SR": 0.453125, "CSR": 0.5373774509803921, "EFR": 1.0, "Overall": 0.7138036151960784}, {"timecode": 51, "before_eval_results": {"predictions": ["Samson", "smith", "Carolina Parakeet", "catalyst", "Sir Edwin Landseer", "Hitler", "Albert Einstein", "smith", "piano", "gumm", "dave dENTIST", "tepuis", "inhumation", "lyle", "haiti", "hypopituitarism", "Delaware", "bees", "the Treaty of Utrecht", "limestone", "spring", "stanley", "Brittany", "mantle", "Baluch", "algae", "shine", "al-Qa'ida", "Churchill Downs", "Norway", "Leonard Bernstein", "Vladimir Putin", "gansbaai", "nahuatl", "20", "Ken Platt", "smith", "adios", "Norway", "smith", "muppet Christmas Carol", "haiti", "jump", "anglaise", "convict", "smith", "27", "haiti", "dublin", "Babylon", "the Continental Marines", "Joe Spano", "Gary Grimes", "in skeletal muscle, while the remainder is distributed in the blood, brain, and other tissues", "CBS News", "Herman's Hermits", "the United States House of Representatives", "\"Den of Spies\"", "clothing", "South Africa won the first Test in Perth from an unlikely position, chasing 920 for the loss of only four wickets.", "Brunei", "a Pringles can", "a peanut butter cup", "1983"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4712239583333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-1635", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6409", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-805", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-3374", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-5922", "mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-4652", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-7762", "mrqa_naturalquestions-validation-692", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-4091", "mrqa_searchqa-validation-3853"], "SR": 0.421875, "CSR": 0.53515625, "retrieved_ids": ["mrqa_squad-train-26411", "mrqa_squad-train-70041", "mrqa_squad-train-3233", "mrqa_squad-train-45525", "mrqa_squad-train-12558", "mrqa_squad-train-72916", "mrqa_squad-train-70038", "mrqa_squad-train-67007", "mrqa_squad-train-53777", "mrqa_squad-train-4306", "mrqa_squad-train-21928", "mrqa_squad-train-4158", "mrqa_squad-train-10234", "mrqa_squad-train-43474", "mrqa_squad-train-54035", "mrqa_squad-train-71187", "mrqa_triviaqa-validation-2317", "mrqa_newsqa-validation-2627", "mrqa_newsqa-validation-3119", "mrqa_squad-validation-9540", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-3163", "mrqa_triviaqa-validation-7201", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-215", "mrqa_searchqa-validation-7299", "mrqa_newsqa-validation-2885", "mrqa_searchqa-validation-936", "mrqa_naturalquestions-validation-2945", "mrqa_triviaqa-validation-5738", "mrqa_naturalquestions-validation-650"], "EFR": 0.972972972972973, "Overall": 0.7079539695945946}, {"timecode": 52, "before_eval_results": {"predictions": ["horse", "haiti", "sweden", "cocktail", "Tony Manero", "Snarks", "france", "silversmith", "Joshua Tree National Park", "carpathia", "Superman", "Letchworth", "velvet", "1", "crafty Hollow", "Crackerjack", "white Ferns", "Utah", "carburetors", "As You Like It", "Labrador Retriever", "what", "Delaware", "thieves", "Clara wieck", "squeeze", "christopher coppola", "bojum", "st Moritz", "david hockney", "Scafell Pike", "Edgar Allan Poe", "Chris Moyles", "Tony Blackburn", "Donna Summer", "france", "kiki", "wolf", "Titanic", "k Kenneth purdy", "trumpet", "Andrew Lloyd Webber", "Blantyre", "Zimbabwe", "david cockerman", "Mikhail Baryshnikov", "Norwich", "Ruth Rendell", "smiths", "ontario", "Ohio", "Miami Heat", "Gwen West", "20 July 2015", "1902", "FCI Danbury", "25 June 1971", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "caster Semenya", "a \"happy ending\"", "universal and equal suffrage", "Tom Cruise", "Joe Montana", "Colonel"], "metric_results": {"EM": 0.609375, "QA-F1": 0.65625}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7014", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-4558", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-2767", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-1332", "mrqa_triviaqa-validation-7627", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-3898", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-4178", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-11787"], "SR": 0.609375, "CSR": 0.5365566037735849, "EFR": 0.92, "Overall": 0.697639445754717}, {"timecode": 53, "before_eval_results": {"predictions": ["albinism", "Jack Ruby", "Google", "Hugh Dowding", "squash", "Tennessee Williams", "Jim Smith", "injecting a 7 percent solution intravenously three times a day", "David Bowie", "hemp", "canoeing", "mocambique", "Christian Louboutin", "Ironside", "Scottish Highland Games", "James Dean", "Mars", "once every two weeks", "about Eve", "chicken", "noel Orwell", "noel hemingman", "homeless", "Derbyshire", "Cubism", "malanesia", "Norfolk", "heminger healdson", "Charlie Brown", "Ruth Rendell", "nottingham", "Morticia", "hot Chocolate", "NamibiaNamibia", "1921", "heberian", "noel hemingman", "praseodymium", "Bruce Alexander", "booms", "spinal cord", "razor", "Arthur Hailey", "hart", "Madonna", "Little arrows", "a nerve cell cluster", "no why", "buttermere", "27", "orchid", "104 colonists and Discovery", "aiding the war effort", "anterograde amnesia and dissociation", "Cesar Millan", "2004 Paris Motor Show", "Jiu-Jitsu", "contraband", "surinder", "drug cartels", "Guinea-Bissau", "steel", "Horse Betting", "2012"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5703125}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-3136", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-4368", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-31", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-3119", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-6040", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4525", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4442", "mrqa_hotpotqa-validation-3655", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-3515", "mrqa_searchqa-validation-14449"], "SR": 0.53125, "CSR": 0.5364583333333333, "EFR": 0.9666666666666667, "Overall": 0.706953125}, {"timecode": 54, "before_eval_results": {"predictions": ["Big Mamie", "dancer", "Canadian", "Sunyani", "six", "basketball", "Adelaide", "Alfred Preis", "Switzerland of England", "the Pakistan Aeronautical Complex (PAC)", "Richard Price", "Switzerland", "The Keeping Hours", "Jeffrey Adam \"Duff\" Goldman", "Ang Lee", "Mineola", "67,038", "Kentucky Wildcats", "eily Atkins", "Parliamentarians (\"roundheads\") and Royalists (\"Cavaliers\")", "Harry Potter series", "Haitian Revolution", "alcoholic drinks", "Fred &quot", "John Meston", "York County", "Cleveland Cavaliers", "scotland", "Hillsborough County", "Cartoon Network Too", "San Diego County Fair", "the fictional city of Quahog, Rhode Island", "Fourier modulus", "1970", "George Raft", "As One", "1944", "the British Army", "311", "Jennifer Taylor", "Cartoon Network", "Columbia Records", "British", "6,241", "October 17, 2017", "Leslie James \"Les\" Clark", "Marilyn Martin", "Hawaii", "Gregg Popovich", "before the majority of their members quit and formed Faith No More", "Vincent Landay", "Lauren Tom", "~ 55 - 75", "Spektor", "spain", "virginia", "vanilla", "a Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "skate off box", "The Louvre", "Albert Einstein", "hemming", "not", "the she-wolf"], "metric_results": {"EM": 0.5, "QA-F1": 0.5861019736842106}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2105263157894737, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-796", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1140", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-1310", "mrqa_naturalquestions-validation-8962", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5383", "mrqa_newsqa-validation-3090", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2609", "mrqa_searchqa-validation-9619", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-9920", "mrqa_searchqa-validation-1776"], "SR": 0.5, "CSR": 0.5357954545454545, "retrieved_ids": ["mrqa_squad-train-26964", "mrqa_squad-train-34823", "mrqa_squad-train-85409", "mrqa_squad-train-10371", "mrqa_squad-train-61939", "mrqa_squad-train-21896", "mrqa_squad-train-4660", "mrqa_squad-train-72386", "mrqa_squad-train-82553", "mrqa_squad-train-28438", "mrqa_squad-train-25850", "mrqa_squad-train-26612", "mrqa_squad-train-2764", "mrqa_squad-train-65739", "mrqa_squad-train-57409", "mrqa_squad-train-77386", "mrqa_naturalquestions-validation-3725", "mrqa_squad-validation-6702", "mrqa_newsqa-validation-3415", "mrqa_squad-validation-1719", "mrqa_squad-validation-1272", "mrqa_naturalquestions-validation-1214", "mrqa_searchqa-validation-11166", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3534", "mrqa_squad-validation-10388", "mrqa_searchqa-validation-852", "mrqa_naturalquestions-validation-3351", "mrqa_newsqa-validation-2865", "mrqa_searchqa-validation-2829", "mrqa_squad-validation-8747", "mrqa_squad-validation-10232"], "EFR": 1.0, "Overall": 0.7134872159090909}, {"timecode": 55, "before_eval_results": {"predictions": ["2016 U.S. Senate election", "1952", "private", "voice-work", "400 MW", "Kerry Marie Butler", "Esp\u00edrito Santo Financial Group", "September 5, 2017", "November 23, 2011", "Javan leopard", "Salman Rushdie", "Tom Rob", "Big Bad Wolf", "Rockland County", "Personal History", "Australian-American", "The Holston River", "1943", "1996", "Tamil Nadu", "the Tallahassee City Commission", "A Boltzmann machine", "three different covers", "Hechingen in Swabia", "Floyd Mutrux and Colin Escott", "A Little Princess", "Agent Carter", "2008", "from July 2, 1967 to August 21, 1995", "Kennedy Road", "Mark Neveldine and Brian Taylor", "Quahog, Rhode Island", "Spiro Agnew", "Ralph Stanley", "three", "Noel Gallagher", "Thorgan", "1692", "9", "the twelfth title in the mainline \"Final Fantasy\" series", "Turkmenistan", "26,000", "Tallaght, South Dublin", "Bharatiya Janta Party (BJP)", "General Manager", "punk rock", "Matt Lucas", "The 1990\u201391 UNLV Runnin' Rebels basketball team", "Bill Clinton", "John D Rockefeller's Standard Oil Company", "Howler", "pigs", "In Time", "The flag of Vietnam", "pulsar", "Dogger Bank", "the first season", "\"totaled,\"", "Steven Gerrard", "ketamine", "martin", "Antarktikos", "Diebold", "Andr\u00e9s Iniesta"], "metric_results": {"EM": 0.625, "QA-F1": 0.708001893939394}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true], "QA-F1": [0.6666666666666665, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-5615", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3340", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-3512", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-2859", "mrqa_hotpotqa-validation-5228", "mrqa_triviaqa-validation-3487", "mrqa_newsqa-validation-456", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-6593"], "SR": 0.625, "CSR": 0.5373883928571428, "EFR": 1.0, "Overall": 0.7138058035714285}, {"timecode": 56, "before_eval_results": {"predictions": ["60,000", "Vishal Bhardwaj", "Macau", "Russian film industry", "no. 3", "The Government of Ireland (Irish: \"Rialtas na h\u00c9ireann\"", "Washington Street", "Eielson Air Force Base", "Atl\u00e9tico Mineiro", "Tom Shadyac", "The American relay of Michael Phelps, Ryan Lochte, Peter Vanderkaay, and Keller", "Richard Strauss", "Jamie Fraser (Sam Heughan)", "Iynx", "David Starkey", "Richard II", "Marion", "The Notorious B.I.G.", "Humphrey Goodman", "\"Darconville\u2019s Cat\"", "An invoice, bill or tab", "October 16, 2015", "Dan Brandon Bilzerian", "The Andes", "Srinagar", "STS-51-C", "Gregg Harper", "The Ones Who Walk Away from Omelas", "Minnesota's 8th congressional district", "French Revolutionary ideals of liberty, equality, and fraternity", "November 27, 2002", "Henry Albert \"Hank\" Azaria", "UFC Fight Pass", "The Sun", "Peter Yarrow", "The Frost Report", "Ready Player One", "1,925", "7 October 1978", "June 2, 2008", "Ravenna", "John Meston", "the Battelle Energy Alliance", "a co-op of grape growers", "The Spiderwick Chronicles", "goalkeeper", "Eric Arthur Blair (25 June 1903 \u2013 21 January 1950)", "Croatian", "Labour Party", "Fortunino", "Warsaw, Poland", "Texas County, Oklahoma, United States", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "September 2017", "Ken Norton", "Ethiopia", "Hastings", "the UK", "Al Nisr Al Saudi", "5 1/2-year-old", "cobalt", "the track", "the bassoon", "time in exchange for detailed public disclosure of an invention"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6286570313744226}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.14285714285714285, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.4, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4347826086956522, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-2190", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-784", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3953", "mrqa_naturalquestions-validation-8931", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-6104", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-6632"], "SR": 0.546875, "CSR": 0.5375548245614035, "EFR": 1.0, "Overall": 0.7138390899122806}, {"timecode": 57, "before_eval_results": {"predictions": ["1961", "Ballon d'Or", "Elsie Audrey Mossom", "the first trans-Pacific flight from the United States to Australia", "the House of Hohenstaufen", "BraveStarr", "the White Knights of the Ku Klux Klan", "Benjam\u00edn Arellano F\u00e9lix", "lion", "Tony Award", "\"Lebensraum\" became a geopolitical goal of Imperial Germany in World War I (1914\u20131918)", "China", "Esperanza Spalding", "1854", "Art Deco-style", "Starachowice", "Debbie Harry", "England", "Mineola", "K\u014dk\u016b Kabushiki-gaisha", "Melbourne's City Centre", "926 East McLemore Avenue", "bioelectromagnetics", "German", "casting, job opportunities, and career advice", "ten episodes", "2012", "Ford Mustang", "Igor Stravinsky", "Euripides", "second", "Clitheroe Football Club", "Peter Wooldridge Townsend", "Paper", "November 27, 2002", "Campbellsville", "1837", "2007", "al-Qaeda", "Humberside Airport", "5,000,000", "a successful racehorse breeder", "Nia Kay", "11 November 1918", "1891", "the Bank of China Tower", "Lerotholi Polytechnic Football Club", "a private liberal arts college", "932", "first", "Richard Street", "latitude 90 \u00b0 North", "a radius 1.5 times the Schwarzschild radius", "spot - type detectors", "Japan", "Apollo", "leeds", "bragging about his sex life", "people are going to look at the content.", "Fernando Torres", "Clive Davis", "Tybee Island", "Feb 19, 2014", "Yulia Lipnitskaya"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6154265873015873}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.8, 0.4, 0.5, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-4341", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4762", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4436", "mrqa_naturalquestions-validation-3499", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2325", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-6491", "mrqa_triviaqa-validation-4287"], "SR": 0.46875, "CSR": 0.5363685344827587, "retrieved_ids": ["mrqa_squad-train-63010", "mrqa_squad-train-62785", "mrqa_squad-train-29154", "mrqa_squad-train-15943", "mrqa_squad-train-25854", "mrqa_squad-train-41184", "mrqa_squad-train-6264", "mrqa_squad-train-75378", "mrqa_squad-train-27482", "mrqa_squad-train-4950", "mrqa_squad-train-76670", "mrqa_squad-train-83136", "mrqa_squad-train-52594", "mrqa_squad-train-77566", "mrqa_squad-train-55690", "mrqa_squad-train-40280", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-9576", "mrqa_searchqa-validation-8612", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-6073", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-2886", "mrqa_triviaqa-validation-5325", "mrqa_squad-validation-9454", "mrqa_triviaqa-validation-406", "mrqa_squad-validation-8905", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11170", "mrqa_naturalquestions-validation-7211"], "EFR": 1.0, "Overall": 0.7136018318965518}, {"timecode": 58, "before_eval_results": {"predictions": ["Superman", "an open window that fits neatly around him.", "King Birendra", "Susan Atkins,", "a brown coffin containing the remains of Israeli soldiers killed during the 2006 war.", "\"I really hope that what I did will enable other women to come forward in similar situations,\"", "\"The best time of your cycle to do a mammogram is going to be when your period is over,", "\"He was very swift, very tactical,\"", "EMI", "10 municipal police officers", "his past and his future", "mpire of the Sun", "Carol Browner", "\"Mammograms are known to be uncomfortable,\"", "Haeftling,", "55-year-old", "capital murder and three counts of attempted murder", "response by raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond.", "eight Indians whom the rebels accused of collaborating with the Colombian government,", "a 16th grand Slam title.", "curfew", "\"Nude, Green Leaves and Bust\"", "demolishing American third seed Venus Williams in the final of the Sony Ericsson Open in Miami on Saturday.", "Glasgow, Scotland", "an antihistamine and an epinephrine auto-injector for emergencies,", "Damon Bankston", "Afghanistan's", "aesthetic environment\" and ensure public safety", "Why do genocides and mass atrocities happen?", "warns business owners to close their shops during daily prayers, or they will be temporarily shut down,", "humans", "Vancouver, British Columbia", "an independent homeland for the country's ethnic Tamil minority", "two years ago", "is a businessman, team owner, radio-show host and author.", "Barack Obama", "inspiring people", "fake his own death", "American president toured a mosque, laid a wreath at the grave of the founder of the Turkish republic, and announced before the Turkish parliament that \"the United States is not and will never be at war with Islam.\"", "a one-of-a-kind navy dress with red lining by the American-born Lintner,", "Rev. Alberto Cutie", "citizenship", "housing, business and infrastructure repairs", "Carnival", "a mammoth", "\"She had a smile on her face, like she always does when she comes in here,\"", "the creation of an Islamic emirate in Gaza", "in the lawless southern provinces and especially in the Taliban stronghold of Helmand", "Elisabeth,", "1-1.", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "Francisco Pizarro", "Owen Vaccaro", "2003", "Renault", "cheese", "Alan Freed", "Woolsthorpe-by-Colsterworth", "War Is the Answer", "arts manager", "Greenland and Antarctica", "Ms.", "their Dreams", "freezing"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5291040026749121}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true], "QA-F1": [0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.22222222222222224, 0.6938775510204082, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.29629629629629634, 0.15384615384615383, 0.5, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.923076923076923, 0.0, 0.5833333333333334, 1.0, 0.0, 0.8333333333333333, 1.0, 1.0, 0.8, 0.0, 0.5, 0.046511627906976744, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.08, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-112", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-1055", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-4517", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-430"], "SR": 0.390625, "CSR": 0.5338983050847458, "EFR": 0.9743589743589743, "Overall": 0.707979580888744}, {"timecode": 59, "before_eval_results": {"predictions": ["a female soldier,", "Sunday", "five suspects", "Mike Griffin,", "U.N. Security Council", "Sen. Barack Obama", "Ken Choi,", "second child", "on the bench", "leftist Workers' Party.", "Robert", "Christiane Amanpour", "Narayanthi Royal", "Mexico", "five", "going somewhere very special, far away, because under the Communist regime you didn't travel that much and Prague was \"wow.\"", "a boys' learning retreat", "Keating Holland.", "Takashi Saito,", "Mutassim,", "Dr. Jennifer Arnold and husband Bill Klein,", "Lindsey oil refinery", "Sunday.", "future relations between the Middle East and Washington.", "prostate cancer", "the southern city of Naples", "at least 300", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "2002", "Spc. Megan Lynn Touma,", "how preachy and awkward cancer movies can get.\"", "Kingdom City", "strife in Somalia,", "Miami Beach, Florida,", "Charles Darwin", "New Jersey Economic Development Authority's", "President Obama.", "media", "some of the most gigantic pumpkins in the world,", "baseball bat", "Philippines", "Cedars-Sinai Medical Center", "Jaime Andrade", "Alfredo Astiz,", "ties", "The station", "The father of Haleigh Cummings,", "Amir Zaki", "$8.8 million on its third weekend", "Tuesday", "Alan Graham", "Canada south of the Arctic", "Glenn Close", "Joseph M. Scriven", "Richard Krajicek", "Manchester", "Robert", "Lonestar", "Cannes Film Festival", "Mario Winans", "3", "Rooster Cogburn", "Korea", "1936"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5709738756613757}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.07407407407407407, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.5, 0.5, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-3095", "mrqa_naturalquestions-validation-1872", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-6864", "mrqa_hotpotqa-validation-53", "mrqa_searchqa-validation-11013"], "SR": 0.4375, "CSR": 0.5322916666666666, "EFR": 1.0, "Overall": 0.7127864583333333}, {"timecode": 60, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1899", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2959", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-382", "mrqa_hotpotqa-validation-4012", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-892", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-266", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3551", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2558", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1143", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-2448", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5687", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6157", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7783", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.818359375, "KG": 0.515625, "before_eval_results": {"predictions": ["2017", "1648 - 51 war", "Fred Ott", "After realizing Stefan's desperation to keep her alive, decides to complete her transition", "Master Christopher Jones", "Juliet", "the status line", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "The Maginot Line", "1937", "Fix You", "Hirschman", "Bobby Eli", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "45 %", "Guy Berryman", "Austria - Hungary", "Phoenix Mills Limited", "Babe Ruth", "Seattle, Washington", "more than 600", "Bumping Robinson and Terrence Howard", "Pinar del R\u00edo Province ( now in Artemisa Province )", "DNA was a repeating set of identical nucleotides,", "Magnavox Odyssey", "the ball is fed into the gap between the two forward packs and they both compete for the ball to win possession", "Sunday night", "tolled ( quota ) highways", "cut off close by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "innermost in the eye while the photoreceptive cells lie beyond", "Saturday", "Glenn Close", "reservoirs at high altitudes", "aiding the war effort", "into smaller pulmonary arteries that spread throughout the lungs", "Nepal", "the symbol \u00d7", "Ace", "Phillip Paley", "10.5 %", "liver and kidneys", "the county seat and commercial center of Lee County, Florida, United States", "The onset of rigor mortis and its resolution partially determine the tenderness of meat", "Superintendent Dave Kelly", "Florida", "2010", "Wilson Pickett", "infection, irritation, or allergies", "ABC", "August 1991", "The Maidstone Studios in Maidstone, Kent", "6/36 = 1/6", "helium", "caliber", "Edward James Olmos", "Terrina Chrishell Stause", "Toyota Innova", "Osama bin Laden's sons", "a man's lifeless, naked body", "jazz", "Falkland Islands", "a cement pond", "Seventy-six trombones", "Fifty Shades of Grey"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6053603225478226}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3076923076923077, 1.0, 0.1111111111111111, 0.761904761904762, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 0.07407407407407407, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.28571428571428575, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-8998", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-6052", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-1584", "mrqa_newsqa-validation-650", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-15624", "mrqa_searchqa-validation-893"], "SR": 0.515625, "CSR": 0.5320184426229508, "retrieved_ids": ["mrqa_squad-train-54193", "mrqa_squad-train-40967", "mrqa_squad-train-18142", "mrqa_squad-train-34484", "mrqa_squad-train-61743", "mrqa_squad-train-10912", "mrqa_squad-train-62205", "mrqa_squad-train-7813", "mrqa_squad-train-22067", "mrqa_squad-train-71109", "mrqa_squad-train-43727", "mrqa_squad-train-63174", "mrqa_squad-train-23626", "mrqa_squad-train-73546", "mrqa_squad-train-5540", "mrqa_squad-train-78631", "mrqa_triviaqa-validation-559", "mrqa_hotpotqa-validation-4104", "mrqa_newsqa-validation-1385", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-7376", "mrqa_naturalquestions-validation-5001", "mrqa_triviaqa-validation-6262", "mrqa_newsqa-validation-1660", "mrqa_hotpotqa-validation-5676", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2710", "mrqa_searchqa-validation-10336", "mrqa_newsqa-validation-981", "mrqa_triviaqa-validation-272", "mrqa_naturalquestions-validation-9818", "mrqa_triviaqa-validation-1130"], "EFR": 0.9354838709677419, "Overall": 0.7063910877181385}, {"timecode": 61, "before_eval_results": {"predictions": ["17 - year - old", "Spinal fusion", "Dido", "international aid", "At of 2017, the following 15 countries or regions have reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "January 15, 2007", "Gorakhpur", "Roger Federer", "Battle of Antietam", "1933", "it `` never had any meaning other than the obvious one '' and is about the `` loss of innocence in children ''", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "John Travolta", "August 5, 1937", "Rory McIlroy", "Jonathan Breck", "the 4th century", "Smith Jerrod", "Acid rain", "2018 NCAA Division I Men's Basketball Tournament", "Mulberry Street", "Lucille Simmons", "the sinoatrial node", "1994", "Wyatt and Dylan Walters", "10 May 1940", "Phillipa Soo", "Kansas City Chiefs", "Judith Aline Keppel", "Vincent Price", "the customer's account", "along the coast of northern California", "follows a child with Treacher Collins syndrome trying to fit in", "metaphase", "Sally Field", "Eric Clapton", "E-1 through E-3", "Lagaan", "Colman", "Karen Gillan", "Easter", "Beijing, China", "4 `` 1 lakh people ''", "Havana Harbor", "1824", "1961", "Qutab Ud - Din - Aibak", "Prince Henry", "the chest showing the internal anatomy of the rib cage, lungs and heart as well as the inferior thoracic border -- made up of the diaphragm", "customary", "opinion in a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "floating ribs", "Flamour Model", "Romania", "Helensvale", "Kathryn Jean Martin \"Kathy\" Sullivan AM", "25 million", "a better environment.\"", "torture and indefinite detention", "iPods", "Chagas disease", "Jacob Marley", "John Hersey", "22 September 2015"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6908123171575759}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.5, 1.0, 1.0, 1.0, 0.09523809523809523, 0.07407407407407408, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.888888888888889, 0.9473684210526316, 1.0, 1.0, 1.0, 0.6, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2608695652173913, 0.4, 0.25806451612903225, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10172", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-8382", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6596", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-451", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-1110", "mrqa_searchqa-validation-4844", "mrqa_searchqa-validation-14622"], "SR": 0.53125, "CSR": 0.5320060483870968, "EFR": 0.9666666666666667, "Overall": 0.7126251680107527}, {"timecode": 62, "before_eval_results": {"predictions": ["Christian Dior", "cranberry", "the fort", "the fibula", "a spotted hyena", "a beached whale", "the Mississippi", "Kevin Costner", "Lil Jon", "the Boer War", "the Colosseum", "fort boyard", "fish stock", "an insect that eats and destroys wood", "Winston Churchill", "salford in Greater Manchester", "the Lincoln Tunnel", "the forta", "Louisiana", "the Kid", "Rembrandt", "Canada", "fort boyard", "Benedict XVI", "a 300 million barrel strategic petroleum reserve", "Prague", "the hanging gardens of babylon", "electricity", "the second-season premiere", "the fort boyard", "fort boyard", "the College of William and Mary", "the fox", "the oboe", "fort boyard", "Halo 4", "the pound of flesh", "Bahrain", "Best Picture", "Hamlet", "Heroes", "Russia", "beer", "the Hawks", "carnaval", "wood", "Samson", "Dustin Hoffman", "Nittany", "Sicily", "Socrates", "more than 1,000", "the interior of the cell ( s ) and the external environment", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "fort boyard", "fort boyard", "fort boyard", "Esperanza Emily Spalding", "2000", "Sargent Shriver", "Daytime Emmy Lifetime Achievement Award.", "The Impeccable", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "fort boyard"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5770933788275034}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8695652173913043, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5263157894736842, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-7337", "mrqa_searchqa-validation-13095", "mrqa_searchqa-validation-4677", "mrqa_searchqa-validation-9023", "mrqa_searchqa-validation-1906", "mrqa_searchqa-validation-6044", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-2797", "mrqa_searchqa-validation-13940", "mrqa_searchqa-validation-5976", "mrqa_searchqa-validation-1751", "mrqa_searchqa-validation-6371", "mrqa_searchqa-validation-8943", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-481", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-16438", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-1460", "mrqa_searchqa-validation-8676", "mrqa_searchqa-validation-14898", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-546", "mrqa_searchqa-validation-15398", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6028", "mrqa_triviaqa-validation-7643", "mrqa_hotpotqa-validation-2367", "mrqa_hotpotqa-validation-5895", "mrqa_newsqa-validation-3961", "mrqa_triviaqa-validation-1787"], "SR": 0.453125, "CSR": 0.5307539682539683, "EFR": 0.9714285714285714, "Overall": 0.713327132936508}, {"timecode": 63, "before_eval_results": {"predictions": ["the Voting Rights Act", "a 919mm Parabellum pistol", "mental retardation", "the Deathly Hallows", "Senator Mitchell", "a fragrance", "the Bronze Age", "People's Party", "the Imperial Palace chapel Austria", "Daniel", "the Jinx", "northern states", "California", "Cosmopolitan", "David Beckham", "Minoan", "Japan", "rodeo", "robbery", "Vietnam", "Stanford", "minor", "a prism schism", "Alaska", "Wallis Warfield Simpson", "a centipede", "Greek", "Sisters Rosensweig", "Brooklyn", "Penn State", "Easter Island", "Nasser", "the black underground river that rushes in the depths of", "Stephen Hawking", "Labour Day", "Mozambique", "landfills", "The Silence of the Lambs", "15", "Capone", "Paul McCartney", "Anne Murray", "Tennessee", "Buenos", "contagious", "Jane Austen", "wheat", "baking soda", "peanuts", "philosophy", "Byzantium", "The Paris Sisters", "Rent", "Miami Heat", "gluten", "the Battle of Hastings", "Easter", "2004", "Asia-Pacific War", "Tsavo East National Park", "Casa de Campo International Airport in the Dominican Republic", "100", "a method to reach car owners who haven't complied fully with recalls.", "great-grandfather of Miami Marlin Christian Yelich"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6705059523809525}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.16, 0.2857142857142857]}}, "before_error_ids": ["mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-9566", "mrqa_searchqa-validation-9089", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-8613", "mrqa_searchqa-validation-3380", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-2172", "mrqa_searchqa-validation-16208", "mrqa_searchqa-validation-6957", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-15049", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-14995", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-1226", "mrqa_triviaqa-validation-2320", "mrqa_triviaqa-validation-7085", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2362", "mrqa_hotpotqa-validation-798"], "SR": 0.59375, "CSR": 0.53173828125, "retrieved_ids": ["mrqa_squad-train-64284", "mrqa_squad-train-79705", "mrqa_squad-train-79260", "mrqa_squad-train-7044", "mrqa_squad-train-2513", "mrqa_squad-train-11945", "mrqa_squad-train-54831", "mrqa_squad-train-24210", "mrqa_squad-train-8964", "mrqa_squad-train-78545", "mrqa_squad-train-12920", "mrqa_squad-train-10886", "mrqa_squad-train-61410", "mrqa_squad-train-33538", "mrqa_squad-train-55889", "mrqa_squad-train-3184", "mrqa_triviaqa-validation-7643", "mrqa_hotpotqa-validation-2315", "mrqa_searchqa-validation-6371", "mrqa_searchqa-validation-15553", "mrqa_naturalquestions-validation-10147", "mrqa_searchqa-validation-5845", "mrqa_triviaqa-validation-1142", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-1537", "mrqa_naturalquestions-validation-9602", "mrqa_newsqa-validation-1308", "mrqa_hotpotqa-validation-4280", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-3374", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-6052"], "EFR": 1.0, "Overall": 0.71923828125}, {"timecode": 64, "before_eval_results": {"predictions": ["malawi", "Ethiopia", "razor", "short-beaked echidna and the duck-billed platypus", "British Airways", "Yoshi", "1929", "Blades", "Peter Carey", "Dante Alighieri", "repechage", "uranium", "Wildcats", "d'Artagnan", "Abraham Lincoln", "The Merchant of Venice", "inches", "Paul Rudd", "United Republic of Tanzania", "Julian", "Christian Louboutin", "saxophone", "Vicky Cristina Barcelona", "September", "Grantham", "Muriel Spark", "Turkey", "complaint", "Netherlands", "Lome", "Caviar", "Christian Dior", "Pat Houston", "King William IV", "Dutch", "Jack Lemmon", "Trainspotting", "Australia", "Wellington", "Diptera", "India and Pakistan", "obi", "ccoli", "heisenberg", "1976", "Cyclopes", "phrenology", "Full Metal jacket", "Tokyo", "California", "Windermere", "third", "the fictional elite conservative Vermont boarding school Welton Academy", "Norman occupational surname ( meaning tailor ) in France", "Michael Jordan", "11 or 13 and 18", "Franz Ferdinand Carl Ludwig Joseph Maria", "Chile", "the single-engine Cessna 206 went down,", "helping on the sandbags to keep the waters at bay.\"", "Rachel Carson", "disagreements", "Toni Morrison", "Copts"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6751420454545455}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3067", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-4208", "mrqa_triviaqa-validation-825", "mrqa_triviaqa-validation-1712", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-6308", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1819", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-4966", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8858", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-3410", "mrqa_newsqa-validation-1926", "mrqa_searchqa-validation-4044", "mrqa_newsqa-validation-2435"], "SR": 0.609375, "CSR": 0.5329326923076922, "EFR": 0.96, "Overall": 0.7114771634615384}, {"timecode": 65, "before_eval_results": {"predictions": ["amanda barrie", "Pearl Jam", "a wedge", "Tina Turner", "dreamgirls", "bobby stobart", "1986", "larboard", "commercial", "stanley", "Uganda", "Ash", "temperature", "satirical cartoons", "Alaska", "iron", "drogba", "c\u00e9vennes", "bagram", "mnemosyne, the Titan goddess of memory", "william williams", "The Pillow Book", "arthur", "aramian", "Denver", "smack", "Massachusetts", "air masses", "Money for Nothing", "The Apprentice", "The Altamont Speedway Free Festival", "jumpin Jack Flash", "brixham", "billy ray cyrus", "Ghana", "alla capella", "Mandela", "luigi", "Illinois", "sailing", "a rain hat", "a mark", "al-Bahrayn", "CBS", "noah beery, Jr.", "Nebraska\u2013Lincoln", "Sarajevo", "cast", "Beyonce", "jules Bernadotte", "MI5", "Games played", "Something to Sing About", "by the United States Court of Appeals for the Armed Forces", "Little Dixie", "American Way", "North African Arab historiographer and historian", "fluoroquinolones", "Alina Cho", "eight.\"", "the Penguin", "Passover", "Tales from the Vienna Woods", "John Lennon and George Harrison,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6380208333333334}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3599", "mrqa_triviaqa-validation-2184", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-7203", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-7708", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-2918", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-5878", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3265", "mrqa_naturalquestions-validation-7950", "mrqa_hotpotqa-validation-150", "mrqa_newsqa-validation-1804"], "SR": 0.5625, "CSR": 0.5333806818181819, "EFR": 0.9642857142857143, "Overall": 0.7124239042207792}, {"timecode": 66, "before_eval_results": {"predictions": ["sleepless in seattle", "Margaret Beckett", "Portugal", "typhoid fever", "sheryl crow", "melvil Dewey", "japan", "Pancho Villa", "Wild Bunch", "Mikhail Gorbachev", "South Korea", "Princess bride", "khaury", "landerstra\u00dfer", "charlesley Garrett", "Vengeance", "c\u00e9vennes", "the Eglise du Dome", "1861", "brown", "werks", "Dvorak", "arise", "aslan", "26", "the narwhal", "chicken", "photography", "Charlie Chan", "taekwondo", "phosphorus", "k Potter", "plutonium", "Mercury", "trenches", "Dorset", "j. S. Bach", "Groucho Marx", "lacrosse", "Queen Elizabeth I", "alusoji Fasuba", "1804", "Belarus", "Clydebank", "mad hatter", "Bette Davis", "chicken cooked on a charcoal powered grill, stove or hot plate", "Chrysler", "freeview", "Gambia", "karl Sagan and his wife and co-writer, Ann Druyan", "2010", "Pepsi Super Bowl LII Halftime Show", "mass of alcohol per mass of blood", "FAI Junior Cup", "the Knight Company", "Miami-Dade County", "military veterans", "January 24, 2006.", "giving birth to baby daughter Jada,", "a retronym", "Ovid", "involuntary civil", "Latin"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5975196678321679}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.1818181818181818, 0.0, 0.0, 0.6363636363636364, 1.0, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4413", "mrqa_triviaqa-validation-2153", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-2670", "mrqa_triviaqa-validation-458", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-3425", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-4907", "mrqa_triviaqa-validation-1794", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-4523", "mrqa_hotpotqa-validation-1030", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-801", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-8081"], "SR": 0.5625, "CSR": 0.5338152985074627, "retrieved_ids": ["mrqa_squad-train-2268", "mrqa_squad-train-347", "mrqa_squad-train-32277", "mrqa_squad-train-47913", "mrqa_squad-train-26085", "mrqa_squad-train-25725", "mrqa_squad-train-56043", "mrqa_squad-train-7757", "mrqa_squad-train-65136", "mrqa_squad-train-35688", "mrqa_squad-train-79802", "mrqa_squad-train-64599", "mrqa_squad-train-63051", "mrqa_squad-train-68688", "mrqa_squad-train-20492", "mrqa_squad-train-29395", "mrqa_triviaqa-validation-1184", "mrqa_naturalquestions-validation-5558", "mrqa_triviaqa-validation-2817", "mrqa_naturalquestions-validation-10428", "mrqa_triviaqa-validation-4117", "mrqa_newsqa-validation-203", "mrqa_triviaqa-validation-6258", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-3833", "mrqa_squad-validation-1909", "mrqa_newsqa-validation-3060", "mrqa_naturalquestions-validation-1745", "mrqa_searchqa-validation-13095", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-2134", "mrqa_triviaqa-validation-639"], "EFR": 0.9285714285714286, "Overall": 0.7053679704157783}, {"timecode": 67, "before_eval_results": {"predictions": ["the Rheingold", "Mahatma Gandhi", "Battlestar Galactica", "humpback whale", "South Africa", "Brett Favre", "Perilis", "Texas", "angioplasty", "anxiety", "a phaser", "Mary Pickford", "a termite", "Sayonara", "a cat", "India", "Lone Ranger", "Mars", "the Battle of Verdun", "a single death", "the Andes", "India", "Barnato", "Houston Rockets", "shank", "apartheid", "Boston", "a horse", "klatretosen", "Thomas Hobson", "pesos", "Shop", "Captain William Bligh", "Urban Outfitters", "Burt Baskin", "Andrew Wyeth", "smallpox", "jimmy", "Al Jolson", "Risk", "T", "Don Quixote", "Chesterfield, Virginia", "After midnight", "The Age of Innocence", "SDS", "Bering Sea", "the Caucasus", "Coretta Scott King", "C-E", "With a Little Help from My Friends", "the Mayflower", "After World War II", "abbreviation", "brighton", "Microsoft", "fleet street", "1966", "Juilliard School", "City of Starachowice", "producing rock music with a country influence.", "Mombasa, Kenya,", "Thaksin Shinawatra,", "peninsulas"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6369791666666667}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-2378", "mrqa_searchqa-validation-15118", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-4607", "mrqa_searchqa-validation-7736", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4010", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-12460", "mrqa_searchqa-validation-5960", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-10385", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-6311", "mrqa_searchqa-validation-1139", "mrqa_searchqa-validation-3274", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-9122", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-4308", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2674", "mrqa_naturalquestions-validation-2064"], "SR": 0.5625, "CSR": 0.5342371323529411, "EFR": 0.9642857142857143, "Overall": 0.7125951943277311}, {"timecode": 68, "before_eval_results": {"predictions": ["Davenport", "soup", "The Last Full Measure", "gonads", "the High Plains area", "the gap", "Saturday Night Fever", "South Dakota", "neurons", "John Fogerty", "the megawatt", "prince", "electrons", "the Communist Party", "Binet", "Bath", "the Atlantic Ocean", "Billboard", "James Buchanan", "Hinduism", "the Miasa French Countryside Ruby Stemware", "Wayne Brady", "Trotsky", "Arkansas", "chromosomes", "Cuba", "packet", "airplanes", "Surgeons", "Thomas Nast", "17", "freezing", "Picabo Street", "the National Security Agency", "Kiss Me\\'nin", "Honey Nut", "Hercules", "the Gallic War", "Rod Laver", "oatmeal", "Ivy Dickens", "Selma", "vote", "Herbert Hoover", "Joe Hill", "IHOP", "Generation X: Tales for an", "motor neuron", "Samuel Beckett", "Queen Elizabeth II", "Independence Day", "Daya Jethalal Gada", "Kevin Sumlin", "Charles Carson", "Albert Einstein", "Orion", "dodo", "1986", "Michael Rispoli", "Mandarin Airlines", "at least two and a half hours.", "39,", "Buenos Aires.", "Citizens for a Sound Economy"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6317708333333333}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-30", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-15032", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-5152", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-15105", "mrqa_searchqa-validation-9589", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-6721", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-15196", "mrqa_searchqa-validation-9373", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-9043", "mrqa_searchqa-validation-2684", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-14015", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_hotpotqa-validation-2003"], "SR": 0.5625, "CSR": 0.5346467391304348, "EFR": 0.9642857142857143, "Overall": 0.7126771156832298}, {"timecode": 69, "before_eval_results": {"predictions": ["Manitoba", "Santa Monica, California", "60 Minutes", "a fruitcake", "midnight", "Socrates", "Al Gore", "the Louvre", "the Kings and Queens of England", "the Desert Fox", "Baton Rouge", "Langston Hughes", "a manacle", "NCAA", "Cleveland", "a slave", "Amsterdam", "Stevie Wonder", "Typhoid Mary", "an inch", "from 700 to 2000", "Tokyo", "Tennessee Williams", "the United Arab Emirates", "Lurch", "the chancellor", "Big Ben", "Old Yeller", "a mammal", "2001: A Space Odyssey", "A Brief History of Time", "Prince", "Abraham", "James VI of Scotland", "Israel", "Brisbane", "David Webb", "the Holocaust", "a globe", "Iowa", "the Devil", "Samsonite", "the Marine Corps", "Vietnam", "the Canton of Geneva", "Punxsutawney, Pennsylvania", "Sports Illustrated", "Venus", "Shia LaBeouf", "Annapolis", "Princess Anne", "Hungary ( Hungarian : Magyarorsz\u00e1g z\u00e1szlaja )", "January 15, 2007", "Introverted Sensing ( Si )", "Aethelbert", "Trainspotting", "Stella McCartney", "1902", "Anne of Green Gables", "Daniil Shafran", "a Columbian mammoth", "Pixar", "help the convicts find calmness in a prison culture fertile with violence and chaos.", "then-Sen. Obama"], "metric_results": {"EM": 0.625, "QA-F1": 0.6796875}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.2, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-12319", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-14167", "mrqa_searchqa-validation-3164", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-4905", "mrqa_searchqa-validation-13570", "mrqa_searchqa-validation-15830", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-7011", "mrqa_searchqa-validation-6746", "mrqa_naturalquestions-validation-6020", "mrqa_triviaqa-validation-6872", "mrqa_hotpotqa-validation-3564", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3695"], "SR": 0.625, "CSR": 0.5359375, "retrieved_ids": ["mrqa_squad-train-34870", "mrqa_squad-train-64873", "mrqa_squad-train-78332", "mrqa_squad-train-29022", "mrqa_squad-train-75742", "mrqa_squad-train-62158", "mrqa_squad-train-414", "mrqa_squad-train-75694", "mrqa_squad-train-49804", "mrqa_squad-train-56266", "mrqa_squad-train-36915", "mrqa_squad-train-72428", "mrqa_squad-train-35660", "mrqa_squad-train-28971", "mrqa_squad-train-7350", "mrqa_squad-train-15395", "mrqa_naturalquestions-validation-10357", "mrqa_searchqa-validation-10385", "mrqa_hotpotqa-validation-2382", "mrqa_naturalquestions-validation-8931", "mrqa_triviaqa-validation-6064", "mrqa_searchqa-validation-15711", "mrqa_naturalquestions-validation-2721", "mrqa_newsqa-validation-1922", "mrqa_naturalquestions-validation-2182", "mrqa_triviaqa-validation-148", "mrqa_newsqa-validation-539", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-7029", "mrqa_newsqa-validation-2191", "mrqa_naturalquestions-validation-1784", "mrqa_searchqa-validation-10131"], "EFR": 1.0, "Overall": 0.720078125}, {"timecode": 70, "UKR": 0.779296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4260", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5757", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15105", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15196", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1840", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10156", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-1971", "mrqa_squad-validation-2082", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3215", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4299", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-513", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6067", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6917", "mrqa_squad-validation-6960", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9401", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.857421875, "KG": 0.534375, "before_eval_results": {"predictions": ["Doc Holliday", "Fritos", "the pale", "fowls", "fruitcake", "Alicia", "Subclue 2", "Christa McAuliffe", "Kilimanjaro", "Misbegotten", "a pumpkin", "Jumbo", "Stoke", "Mexico", "Adolf Hitler", "Portland", "imagism", "Rudolf Diesel", "a palace", "brig", "the Spanish Republic", "Ruth", "nuts", "Cheaper by the Dozen", "Hans Christian Andersen", "San Francisco", "John Cale", "Wanted", "the bugle", "BORE", "Emma Peel", "Homestead", "Liberty Island", "Max Factor", "plantain", "Marie Curie", "Middle Dutch", "Aladdin", "rain", "Peter the Great", "Toy Story", "the Tea Party", "George Sand", "Jim Jarmusch", "Afghanistan", "Minos", "salamanders", "James Fenimore Cooper", "Rosehips", "Led Zeppelin", "Cairo", "Michigan", "Texas A&M Aggies", "Alex Burrall, Jason Weaver and Wylie Draper", "Leroy", "Kentucky Derby", "Iwo Jima", "Charles L. Clifford", "Vince Staples", "Sam Bettley", "Al-Shabaab", "a national telephone survey", "Lucky Dube,", "Florida"], "metric_results": {"EM": 0.65625, "QA-F1": 0.718923611111111}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.7000000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4747", "mrqa_searchqa-validation-3582", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-2880", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-6209", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-2455", "mrqa_searchqa-validation-13954", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-8455", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-1531", "mrqa_newsqa-validation-585"], "SR": 0.65625, "CSR": 0.5376320422535211, "EFR": 1.0, "Overall": 0.7417451584507042}, {"timecode": 71, "before_eval_results": {"predictions": ["The Beatles", "The Andy Griffith Show", "Inigo Montoya", "a cedars", "a camel", "watermelon", "a race car", "a British tank", "Simon & Garfunkel", "Albert Pujols", "the Andean bear", "ordinal numbers", "nebulae", "(Scott) McClellan", "Eastwick", "The Who", "Cy Young", "Austin Powers", "a mime instructor", "conga drums", "a redwood tree", "Nellie Bly", "IBM", "Mexican Pizza", "A Lesson from Aloes", "Ricky Martin", "debts", "cloven", "Nicole Kidman", "Aristophanes", "Wimbledon", "Prince Siddhartha", "a RESTRICTIVE", "an orangutan", "a turquoise necklace", "Papua New Guinea", "Rooster Cogburn", "Halo 3", "Boz", "Sayonara", "pain", "a crescent", "The Moment of Truth", "aardvark", "Harpy", "Henry Fielding", "President Garfield", "a waterfowl", "Howie Mandel", "China", "Henry Cavendish", "three", "MFSK", "May 31, 2012", "Syria", "Otto Hahn", "(Bokm\u00e5l) or  (Nynorsk)", "World War II", "Peter Kay's Car Share", "Oneida Limited", "Matthew Perry and Leslie Mann,", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "the murders of his father and brother.\"", "mantle"], "metric_results": {"EM": 0.625, "QA-F1": 0.6871995192307692}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.23076923076923078, 0.7499999999999999, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11101", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-635", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-1282", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-2234", "mrqa_searchqa-validation-16168", "mrqa_searchqa-validation-10201", "mrqa_searchqa-validation-12152", "mrqa_searchqa-validation-1831", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-8196", "mrqa_searchqa-validation-11905", "mrqa_searchqa-validation-12971", "mrqa_searchqa-validation-11617", "mrqa_searchqa-validation-14945", "mrqa_naturalquestions-validation-7906", "mrqa_triviaqa-validation-5808", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-2382"], "SR": 0.625, "CSR": 0.5388454861111112, "EFR": 1.0, "Overall": 0.7419878472222223}, {"timecode": 72, "before_eval_results": {"predictions": ["133", "the United States", "a flock of birds during takeoff.", "the chaos and horrified reactions after the July 7, 2005, London transit bombings", "$2 billion", "\"Slumdog Millionaire\"", "17", "London's O2 arena,", "Joel \"Taz\" DiGregorio,", "second child", "her home", "forgery and flying without a valid license,", "U.S. 93", "The Maraachlis' daughter, Zeina,", "64", "Muslim festival", "figure out a way that was practical to get a drum set on a plane.", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "the immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guantanamo Bay, Cuba.", "a satellite.", "Jacob Zuma", "cross-country skiers", "Klein", "Christmas", "saying Tuesday the reality he has seen is \"terrifying.\"", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "stylish clothes", "severe flooding", "Consumer Reports", "Now Zad in Helmand province, Afghanistan.", "unprecedented rise in American politics.", "second", "through the weekend,", "Sen. Barack Obama", "African National Congress Deputy President", "the abduction of minors.", "three", "Fernando Caceres", "Donald Trump.", "\"Draquila -- Italy Trembles.\"", "David Russ,", "country directors", "Iran", "\"We must eliminate the perceived stigma, shame and dishonor of asking for help,\"", "Historically, when times get tough,", "Filippo Inzaghi", "BMW", "more than 40 people.", "the bombers", "Mom", "Asashoryu", "king", "a form of fixed - mobile convergence ( FMC )", "Ant & Dec", "asimov", "a falcon", "friends", "IT", "St. Louis Cardinals", "south-west", "popcorn", "a date", "an elephant", "Lord Salisbury"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5471378014346764}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1, 0.5, 0.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.125, 0.22222222222222224, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-1128", "mrqa_naturalquestions-validation-6294", "mrqa_triviaqa-validation-7728", "mrqa_triviaqa-validation-179", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-2653", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-3590", "mrqa_hotpotqa-validation-364"], "SR": 0.453125, "CSR": 0.5376712328767124, "retrieved_ids": ["mrqa_squad-train-14271", "mrqa_squad-train-16098", "mrqa_squad-train-20577", "mrqa_squad-train-49991", "mrqa_squad-train-59913", "mrqa_squad-train-19789", "mrqa_squad-train-22432", "mrqa_squad-train-34690", "mrqa_squad-train-38753", "mrqa_squad-train-38865", "mrqa_squad-train-68783", "mrqa_squad-train-28639", "mrqa_squad-train-62745", "mrqa_squad-train-595", "mrqa_squad-train-79229", "mrqa_squad-train-77948", "mrqa_searchqa-validation-8429", "mrqa_triviaqa-validation-7390", "mrqa_searchqa-validation-13531", "mrqa_newsqa-validation-814", "mrqa_hotpotqa-validation-3060", "mrqa_newsqa-validation-1804", "mrqa_naturalquestions-validation-5599", "mrqa_searchqa-validation-936", "mrqa_triviaqa-validation-3957", "mrqa_hotpotqa-validation-694", "mrqa_squad-validation-4848", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-1338", "mrqa_triviaqa-validation-5605"], "EFR": 1.0, "Overall": 0.7417529965753424}, {"timecode": 73, "before_eval_results": {"predictions": ["\"He made the man promise never to rob anyone again and when he agreed, Sohail gave him $40 and a bread.\"", "Justicialist Party,", "\"Vaughn,\" which is what co-workers called him,", "Dore Gold, former Israeli ambassador to the United Nations said, \"The IAEA has inspected the known nuclear sites of Iran.", "11,", "Ralph Cifaretto", "an average of 25 percent", "February 5,", "Robert Mugabe", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "OneLegacy,", "an upper respiratory infection,\"", "around 10:30 p.m. October 3,", "Arabic, French and English,", "United Nations is calling on NATO to do more to stop the Afghan opium trade", "\"Maude\" and the sardonic Dorothy on \"The Golden Girls,\"", "Bill,", "beloved and admired and dreamed about for nearly a hundred years. The dog and the orchids are both things that very disparate and sometimes unlikely people come together over.", "Jewish", "the chase", "J.G. Ballard,", "patrolling the pavement in protective shoes", "17", "2008.", "12 hours", "Now Zad in Helmand province, Afghanistan.", "Russian concerns that the defensive shield could be used for offensive aims.", "American", "Los Alamitos Joint Forces Training Base", "Al-Shabaab,", "safer surroundings.", "A McCain spokesman attacked Obama's plan, saying the Democrat's \"agenda to raise taxes and isolate America from foreign markets will not get our economy back on track or create new jobs.\"", "an upper respiratory infection,\"", "Blacks and Hispanics", "Sri Lanka's", "\"CNN Heroes: An All-Star Tribute\"", "immigration detainees", "nearly $162 billion in war funding", "scientific reasons.", "innovative, exciting skyscrapers", "in a Nazi concentration camp,", "Pakistani territory", "\"Bhutto was the daughter of Zulfikar Ali Bhutto,", "Pew Research Center", "self-styled revolutionary Symbionese Liberation Army", "38,", "a \"stressed and tired force\"", "speed attempts", "Leo Frank,", "At least 38", "Iran", "in Egypt, connecting the Mediterranean Sea to the Red Sea through the Isthmus of Suez", "Philadelphia, which is Greek for brotherly love", "September 2017", "Dr John Sentamu", "Tony Cozier", "Crosspool", "Dulwich", "New York Giants", "northeastern Argentina, southeastern Bolivia and southwestern Brazil", "Vulcan", "Beloved", "a flapjack", "Hungarian"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6059929653679654}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false], "QA-F1": [0.25, 0.3636363636363636, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.8, 0.1818181818181818, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1999", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2763", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-6111", "mrqa_triviaqa-validation-666", "mrqa_hotpotqa-validation-2170", "mrqa_searchqa-validation-1383"], "SR": 0.484375, "CSR": 0.5369510135135135, "EFR": 1.0, "Overall": 0.7416089527027028}, {"timecode": 74, "before_eval_results": {"predictions": ["South Korea's", "Dr. Jennifer Arnold and husband Bill Klein,", "16", "the program was made with the parents' full consent.", "five", "Marie-Therese Walter.", "Sixteen", "1979", "promised federal help", "consumer confidence", "$83,27014", "Climatecare,", "Haleigh Cummings,", "state senators who will decide whether to remove him from office", "August 19, 2007.", "Ghana", "6,000", "1-0", "Brian Smith.", "fight against terror will respect America's values.", "to provide security as needed.\"", "Sunday,", "city of romance, of incredible architecture and history.", "1,500 Marines", "At the weekend, the captain appealed urgently to be rescued, fearing the crew could be harmed or killed,", "American Bill Haas", "Silicon Valley.", "people switched from the very bad category to the pretty bad category,", "at least nine people", "Almost all British troops", "Cash for Clunkers", "Yemen,", "Tibet's", "800,000", "going through a metamorphosis from blobs of orange to art as night falls.", "Russia", "one", "Democratic VP candidate", "former U.S. soldier Steven Green", "Cologne, Germany,", "President George H.W. Bush", "Kurdistan Workers' Party,", "6-4 loss,", "Karen Floyd", "St Petersburg and Moscow,", "flights", "1-1 draw", "South African", "Alaska or Hawaii.", "30", "repeal of the military's \"don't ask, don't tell\" policy", "Dollree Mapp", "Robert Hooke", "the American Civil War", "giraffe", "hiking and climbing", "World Health Organization", "Idaho", "Robert Matthew", "Ashanti Region", "Michael Kors", "Katherine Heigl", "Lenin", "haiti"], "metric_results": {"EM": 0.5, "QA-F1": 0.6006913408229198}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 0.09090909090909091, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.3333333333333333, 0.2222222222222222, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-258", "mrqa_triviaqa-validation-280", "mrqa_triviaqa-validation-2282", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-5300", "mrqa_triviaqa-validation-6077"], "SR": 0.5, "CSR": 0.5364583333333333, "EFR": 0.9375, "Overall": 0.7290104166666665}, {"timecode": 75, "before_eval_results": {"predictions": ["Bear Grylls", "Reichsmark", "Adam Faith", "the Elbe", "New South Wales", "henry Sutherland", "Stockholm", "Leo", "Henry VIII", "a power outage", "jodie Foster", "Pluto", "Deep Blue", "yellow-brown colour", "1996", "squid", "the duke of Monmouth\u2019s rebellion", "Isaac", "shekel", "serbia", "The Lost Weekend", "Althorp", "the conquest of Peru", "June", "Persuasion", "Robert Taylor", "the AllStars", "the Roman legions", "Norman Mailer", "fishes", "floating", "Florence", "gold hallmarks", "pascal", "slowing", "the British pop band Go West", "11", "Israel", "football", "the Porteous Riots", "English", "Gentlemen Prefer Blondes", "puppies", "Kenya", "Conrad Murray", "rocket", "Amelia Earhart", "asparagus", "John Gorman", "Pope Benedict XVI", "cirrocumulus", "Ariana Clarice Richards", "Turner Layton", "The Parlement de Bretagne", "Campbellsville University", "Daniel Craig", "Vanilla Air", "the HPV vaccine", "\"These random events", "21 percent suggesting that", "a vacuum effect", "Kansas", "Parkinson's disease", "John Gotti"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6203869047619048}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5929", "mrqa_triviaqa-validation-3037", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-771", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-3439", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-1221", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7059", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-4986", "mrqa_naturalquestions-validation-7021", "mrqa_hotpotqa-validation-684", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-10356"], "SR": 0.578125, "CSR": 0.5370065789473684, "retrieved_ids": ["mrqa_squad-train-72842", "mrqa_squad-train-37153", "mrqa_squad-train-49608", "mrqa_squad-train-65323", "mrqa_squad-train-85274", "mrqa_squad-train-67600", "mrqa_squad-train-17027", "mrqa_squad-train-73789", "mrqa_squad-train-68524", "mrqa_squad-train-31501", "mrqa_squad-train-42149", "mrqa_squad-train-44928", "mrqa_squad-train-34339", "mrqa_squad-train-1461", "mrqa_squad-train-76454", "mrqa_squad-train-38106", "mrqa_naturalquestions-validation-8182", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-4863", "mrqa_newsqa-validation-2518", "mrqa_hotpotqa-validation-5698", "mrqa_searchqa-validation-9373", "mrqa_triviaqa-validation-6496", "mrqa_naturalquestions-validation-8245", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3313", "mrqa_squad-validation-2906", "mrqa_searchqa-validation-4802", "mrqa_newsqa-validation-560", "mrqa_naturalquestions-validation-3175", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-1926"], "EFR": 1.0, "Overall": 0.7416200657894736}, {"timecode": 76, "before_eval_results": {"predictions": ["Steve Jobs", "Les Bleus", "Islamic militants", "A prisoner killed in a Maryland county jail", "two women", "seven", "drug cartels", "suicide car bombing", "the Illinois Reform Commission", "vice-chairman of Hussein's Revolutionary Command Council.", "high tide.", "The recent violence -- which has included attacks on pipelines and hostage-taking --", "183", "80,", "the tenant", "Bob Bogle,", "back at work,\"", "Mexico", "The Rev. Alberto Cutie", "California, Texas and Florida,", "3-3 draw", "Iran of trying to build nuclear bombs,", "\"We will support efforts by the Afghan government bolstered by NATO troops \"will allow us to accelerate handing over responsibility to Afghan forces.\"", "singing", "Steve Williams", "Jenny Sanford,", "racially-tinged remark", "Sheikh Sharif Sheikh Ahmed", "8,", "our sincerity", "bipartisan rhetoric", "Haiti", "Mashhad", "10 percent", "Buenos Aires.", "U.S. Navy", "actress Natalie Wood's 1981 drowning death,", "Oaxacan countryside of southern Mexico", "Kim Clijsters", "Venus Williams", "destroyed and his business is shattered,", "\"the most dangerous precedent in this country, violating all of our due process rights.\"", "15,000", "the state's first lady,", "on vacation", "Ronald Cummings", "\"I am sick of life -- what can I say to you?\"", "E. coli", "southwestern Mexico,", "1,500", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "Jyoti Basu", "60 by West All - Stars", "Luther Ingram", "leeds", "edward I", "seattle", "stringed instrument", "Xherdan Shaqiri", "1958", "William Tecumseh Sherman", "(John) Tyler", "pressure", "tetrapods"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6178471895418627}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.18749999999999997, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727272, 0.6666666666666666, 1.0, 0.11764705882352942, 0.08695652173913043, 0.5, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.8, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-6237", "mrqa_triviaqa-validation-5828", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-1902", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-12792", "mrqa_searchqa-validation-14006", "mrqa_triviaqa-validation-1454"], "SR": 0.46875, "CSR": 0.5361201298701299, "EFR": 1.0, "Overall": 0.741442775974026}, {"timecode": 77, "before_eval_results": {"predictions": ["Bill Haas", "a French team", "more than 200.", "Stephen Worgu", "American Civil Liberties Union", "a Florida girl", "Body Works", "Ralph Lauren", "Marion Davies", "along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "Columbia,", "four", "Jenny Sanford,", "British", "hundreds", "more than 78,000 parents of children ages 3 to 17.", "St. Francis De Sales Catholic Church", "Hanin Zoabi,", "Buddhism", "Seoul,", "President Obama", "Kim Jong Il", "strangulation and asphyxiation", "more and more suspicious of the way their business books were being handled.", "13-week extension of unemployment benefits", "terrorize", "Spain", "Michael Jackson", "Brazil's", "Pixar's", "Pixar's", "1,500", "defeated Woods at the Bridgestone Invitational in Ohio in August.", "Nechirvan Barzani,", "fascinating transformation that takes place when carving a pumpkin.", "best-of-three series.", "Transport Workers Union leaders", "Amanda Knox's aunt", "in the Muslim north of Sudan", "Melbourne", "cancer", "FBI and Homeland Security said in a joint threat advisory obtained by CNN.", "the ship of violating Chinese and international laws during its patrols,", "338", "finance", "Haeftling,", "bribing other wrestlers to lose bouts,", "in the eastern Colombian province of Arauca,", "outfit from designer", "More than 100 soldiers", "\"Taz\" DiGregorio,", "active absorption", "Robin", "1965", "three Worlds", "schoolan archipelago", "sybia", "The satire", "Valhalla Highlands Historic District", "boxer", "push", "rain", "school of school", "argon"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5419034090909092}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7777777777777777, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.18181818181818182, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-2633", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-2997", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3310", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-394", "mrqa_triviaqa-validation-699", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12492", "mrqa_searchqa-validation-4358"], "SR": 0.46875, "CSR": 0.5352564102564102, "EFR": 1.0, "Overall": 0.7412700320512821}, {"timecode": 78, "before_eval_results": {"predictions": ["AS Roma beat Lecce 3-2", "Aung San Suu Kyi", "1957,", "Fayetteville, North Carolina,", "their emergency plans", "South Africa", "Al-Shabaab,", "it -- you know -- black is beautiful,\"", "the assassination of President Mohamed Anwar al-Sadat at the hands of four military officers during an annual parade celebrating the anniversary of Egypt's 1973 war with Israel.", "Diego Milito's", "11 countries,", "Robert", "More than 22 million people in sub-Saharan Africa are infected with HIV,", "a senior at Stetson University studying computer science.", "Asia and India.", "did not", "200 human bodies at various life stages", "Phillip A. Myers.", "Congress", "bipartisan", "the governor's", "1912.", "Steven Green", "The plane, an Airbus A320-214,", "Casey Anthony,", "Lifeway's 100-plus stores nationwide", "way American men and women dress", "some deaths", "park bench facing Lake Washington", "Dr. Christina Romete,", "sRI International,", "\"The Real Housewives of Atlanta\"", "independent homeland", "the situation is pretty much resolved,\"", "Hudson, Wisconsin.", "Barbara Streisand's", "the death of a pregnant soldier", "raping", "five", "Asashoryu", "Bob Johnson", "held in a trust fund", "visitors aren't allowed onto the property to view the elephants, and only a handful of media members are able to visit each year,", "natural disasters", "Joe Jackson,", "red", "the inspector-general", "anti-Israeli sentiment in Egypt in the past few months", "The United States", "following in Arizona's footsteps would take states in the wrong direction.", "seven", "a couple broken apart by the Iraq War", "antimeridian", "instructions", "The Undertones", "Johnny Mercer", "fenn Street School", "acting", "Vietnam War", "1967", "Ukraine", "capitals", "Francis Steegmuller", "The genome"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6349615036231884}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, true], "QA-F1": [0.33333333333333337, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.16, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-889", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2118", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9852", "mrqa_triviaqa-validation-6200", "mrqa_hotpotqa-validation-5128", "mrqa_searchqa-validation-6021", "mrqa_searchqa-validation-10570"], "SR": 0.546875, "CSR": 0.5354034810126582, "retrieved_ids": ["mrqa_squad-train-81586", "mrqa_squad-train-16576", "mrqa_squad-train-86234", "mrqa_squad-train-33441", "mrqa_squad-train-30931", "mrqa_squad-train-37955", "mrqa_squad-train-75686", "mrqa_squad-train-52102", "mrqa_squad-train-75194", "mrqa_squad-train-48502", "mrqa_squad-train-79802", "mrqa_squad-train-19100", "mrqa_squad-train-65967", "mrqa_squad-train-81832", "mrqa_squad-train-55848", "mrqa_squad-train-66663", "mrqa_newsqa-validation-1981", "mrqa_squad-validation-4968", "mrqa_triviaqa-validation-7059", "mrqa_hotpotqa-validation-4086", "mrqa_newsqa-validation-1656", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-6474", "mrqa_squad-validation-2318", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-1278", "mrqa_naturalquestions-validation-1171", "mrqa_newsqa-validation-2799", "mrqa_naturalquestions-validation-9564", "mrqa_searchqa-validation-11194", "mrqa_hotpotqa-validation-2606", "mrqa_naturalquestions-validation-3112"], "EFR": 0.9310344827586207, "Overall": 0.7275063427542557}, {"timecode": 79, "before_eval_results": {"predictions": ["Dick Rutan and Jeana Yeager", "the filing date subject to the payment of maintenance fees", "The eighth and final season", "Rigg", "March 2016", "king ( \u05d4\u05d5\u05d3 ) and kabod", "the name announcement of Kylie Jenner's first child", "1991", "small orange collection boxes", "seven", "February 9, 2018", "Coroebus of Elis", "Jonathan Breck", "George Harrison, his former bandmate from the Beatles", "Virgil Tibbs", "Necator americanus", "January 17, 1899", "Coordinated Universal Time ( UTC \u2212 09 : 00 )", "Thespis", "Eukarya", "Wednesday, September 21, 2016", "Robber baron", "three pilot episodes", "Tbilisi, Georgia", "off the rez", "1895", "King Harold Godwinson", "Sir Ronald Ross", "May 29, 2018", "1979", "fourth season", "ancient Athens", "Russia", "plate tectonics", "United Nations", "2018", "it culminates in a post as a Consultant, a General Practitioner ( GP ), or some other non-training post, such as a Staff grade or Associate Specialist post", "Real Madrid", "Angel Benitez", "capillary action", "1830", "Louis Mountbatten", "foreign investors", "Bhupendranath Dutt", "1932", "775 rooms", "By mid-1988", "senators", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Paul Lynde", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "the Treaty of Waitangi", "James Garfield", "The Gambia", "England", "yekaterinburg", "47,818", "seven", "and Ghana in 1974.", "to close their shops during daily prayers,", "Versailles", "Westminster College", "Bob Dylan", "Mount Pelee"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6898446120872591}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false], "QA-F1": [0.5714285714285715, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 0.0, 0.7999999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-7848", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-5144", "mrqa_newsqa-validation-1318", "mrqa_searchqa-validation-13149", "mrqa_searchqa-validation-13773"], "SR": 0.578125, "CSR": 0.5359375, "EFR": 0.8518518518518519, "Overall": 0.7117766203703704}, {"timecode": 80, "UKR": 0.779296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11088", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.875, "KG": 0.52109375, "before_eval_results": {"predictions": ["13th century", "14 December 1972", "China", "1919", "Abraham Gottlob Werner", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "September, 2016", "July 2014", "- ase", "at a given temperature", "Skat", "Claudia Grace Wells", "Bill Russell", "the Sons of Liberty in Boston, Massachusetts", "Eobard", "A lymphocytes is one of the subtypes of white blood cell in a vertebrate's immune system", "Christopher Lloyd", "four", "1923", "at the 1964 Republican National Convention in San Francisco, California", "2018", "Lilian Bellamy", "Kansas City Chiefs", "The Vamps", "1997", "William Wyler", "Ray Harroun", "one person", "NIRA", "Santa Fe, New Mexico, USA", "Kaley Christine Cuoco", "Marshall Sahlins", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) or Kozunak", "in the town of Carcassonne in Aude, France", "the Gupta Empire", "the oculus, or `` eye point ''", "James Madison", "Matt Monro", "Coton in the Elms", "Algeria", "June 1992", "prokaryotic", "hosted by Brazil", "Europeans", "Inti, the sun god of the Inca religion", "Jason Flemyng", "blue", "an expression of unknown origin", "New Zealand to New Guinea", "1960", "Kaiser Chiefs", "Donna Jo Napoli", "claire goose", "\"The Leader In Me \u2014 How Schools and Parents Around the World Are Inspiring Greatness, One Child at a Time\"", "jazz homeland section of New Orleans", "Earvin \"Magic\" Johnson Jr.", "President Sheikh Sharif Sheikh Ahmed", "snakes", "posting a $1,725 bail,", "Yitzhak Rabin", "spinal stenosis", "1936", "carbon"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6765827922077923}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [0.5, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2382", "mrqa_naturalquestions-validation-6470", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-5995", "mrqa_triviaqa-validation-4432", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2802", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-1713", "mrqa_searchqa-validation-11202"], "SR": 0.59375, "CSR": 0.5366512345679013, "EFR": 1.0, "Overall": 0.7424083719135802}, {"timecode": 81, "before_eval_results": {"predictions": ["William the Conqueror", "1962", "The time taken by Shivnarine Chanderpaul ( 18 years and 37 days )", "The President of Zambia", "Ray Charles", "in Ephesus in AD 95 -- 110", "the four - letter suffix", "in the 1970s", "Dan Stevens", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "North Dakota", "Tatsumi", "1969", "Ethiopia and Liberia", "1937", "artes liberales", "Paul Lynde", "the world's largest privately owned cruise company", "2020", "2007", "Hodel", "the coffee shop Monk's", "Siddharth Arora / Vibhav Roy as Ishaan Anirudh Sinha", "1991", "1900", "the World Trade Center Transportation Hub", "Munich, Bavaria", "spacewar", "October 30, 2017", "Paul Lynde", "regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Frank Oz", "Sam Waterston", "Sunday night", "Johnny Darrell", "the 7th century", "2018", "Ed Roland", "Ben Rosenbaum", "2013", "two", "Will Champion", "Defence Against the Dark Arts", "Pre-evaluation", "Jackie Robinson", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "Brazil, Turkey and Uzbekistan", "Times Square in New York City west to Lincoln Park in San Francisco", "two", "The pia mater", "United Nations Peacekeeping Operations", "the Kinks", "inishtrahull Island", "the Indus Valley", "Interstate 95", "Greek mythology", "an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "violating anti-trust laws.", "Lindsey Vonn", "\"Watchmen\" (No. 1)", "Ptolemy", "Dixie", "San Salvador", "Snickers candy bars"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6915193027545796}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.967741935483871, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.37499999999999994, 1.0, 1.0, 1.0, 1.0, 0.972972972972973, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.7999999999999999, 0.4, 0.631578947368421, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-10501", "mrqa_triviaqa-validation-1642", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-1861", "mrqa_newsqa-validation-3156", "mrqa_triviaqa-validation-7778"], "SR": 0.53125, "CSR": 0.5365853658536586, "retrieved_ids": ["mrqa_squad-train-46802", "mrqa_squad-train-6689", "mrqa_squad-train-56775", "mrqa_squad-train-74598", "mrqa_squad-train-55327", "mrqa_squad-train-5279", "mrqa_squad-train-72598", "mrqa_squad-train-27666", "mrqa_squad-train-20114", "mrqa_squad-train-30703", "mrqa_squad-train-72391", "mrqa_squad-train-20857", "mrqa_squad-train-74634", "mrqa_squad-train-76571", "mrqa_squad-train-62895", "mrqa_squad-train-48937", "mrqa_triviaqa-validation-2177", "mrqa_hotpotqa-validation-1473", "mrqa_searchqa-validation-11366", "mrqa_newsqa-validation-2227", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-3541", "mrqa_naturalquestions-validation-8205", "mrqa_hotpotqa-validation-4104", "mrqa_searchqa-validation-16310", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-1350", "mrqa_naturalquestions-validation-5396", "mrqa_triviaqa-validation-2670", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-5857"], "EFR": 0.9333333333333333, "Overall": 0.7290618648373983}, {"timecode": 82, "before_eval_results": {"predictions": ["the Four Seasons", "Kathleen Erin Walsh", "1603", "Stephen Graham", "in 2010", "Tulsa, Oklahoma", "beta decay", "Nickelback", "1998", "John Adams", "Ford", "NCIS Special Agent in Charge", "Linda Hamilton", "( 1839 -- 1906 )", "July 2, 1928", "Dottie West", "in northern China", "Blood is the New Black", "1977", "arm", "Professor Eobard Thawne", "Jack Nicklaus", "trunk", "St. John's, Newfoundland and Labrador", "in the Gregorian calendar", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "May 29, 2018", "The Church of England", "September 27, 2017", "April 10, 2018", "Daniel Suarez", "six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "January 15, 2007", "the Western Bloc ( the United States, its NATO allies and others )", "the optic chiasma", "Japan", "The Bangladesh -- India border", "Seven nations", "Jenny Slate", "2013", "Prince William, Duke of Cambridge", "Nicklaus", "in 1904", "Jim Carrey", "Matt Monro", "a little warmth", "ATPase couples ATP synthesis during cellular respiration to an electrochemical gradient created by the difference in proton ( H ) concentration across the mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "( born November 28, 1973 )", "when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "in Rome in 336", "September 30", "Charles Darwin", "Jon Stewart", "alfa Romeo", "five months", "Wanda", "April 24, 1934", "40", "55-year-old", "$2 billion", "Sex And The City", "coffee", "\"If Ya Wanna Be Bad Ya Gotta Be Good\"", "Oldham, in Greater Manchester, England"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6839874230419646}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9473684210526316, 1.0, 0.3636363636363636, 1.0, 0.5, 0.3333333333333333, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.25806451612903225, 0.0, 0.967741935483871, 0.4, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-9019", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-312", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9687", "mrqa_triviaqa-validation-3677", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2444", "mrqa_searchqa-validation-12962", "mrqa_triviaqa-validation-6822"], "SR": 0.515625, "CSR": 0.5363328313253012, "EFR": 0.9354838709677419, "Overall": 0.7294414654586086}, {"timecode": 83, "before_eval_results": {"predictions": ["American 3D computer-animated comedy", "1964", "Hindi", "eight", "Flula Borg", "Comanche County, Oklahoma", "Swiss", "1999", "Congo", "4,613", "George Washington Bridge", "H. R. Haldeman", "Guardians of the Galaxy Vol.  2", "2.1 million", "6'5\"", "526", "North West England", "casting, job opportunities, and career advice", "Saint Elgiva", "Scotland", "2006", "Carrefour", "infinite sum of terms", "\"The Rite of Spring\"", "Manchester United", "second half of the third season", "Johnson & Johnson", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present", "Brown Mountain", "Backstreet Boys", "45th", "Foxborough", "Wichita", "animation", "John Major", "sandstone", "Reginald Engelbach", "Dutch", "BC Dz\u016bkija", "Anne", "Kristin Scott Thomas", "Rebirth", "Big Mamie", "Albert Bridge, London", "Cartoon Network", "Prince Alexander of Denmark", "King of France", "Alemannic", "Larry Alphonso Johnson Jr.", "Marlon St\u00f6ckinger", "Taylor Swift", "October 27, 2016", "tenderness of meat", "Tigris and Euphrates rivers", "Indian Ocean", "the Astor family", "Tashkent", "Afghanistan,", "Police", "a grizzly bear", "Speed Racer", "Yogi Bear", "sakura", "to function like an endocrine organ, and dysregulation of the gut flora has been correlated with a host of inflammatory and autoimmune conditions"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6742716165413534}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4826", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-1512", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-245", "mrqa_naturalquestions-validation-1728", "mrqa_triviaqa-validation-6312", "mrqa_triviaqa-validation-5311", "mrqa_newsqa-validation-1216", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5496", "mrqa_naturalquestions-validation-7393"], "SR": 0.59375, "CSR": 0.5370163690476191, "EFR": 1.0, "Overall": 0.7424813988095238}, {"timecode": 84, "before_eval_results": {"predictions": ["As of January 17, 2018, 201 episodes", "Atlanta", "1872", "1978", "Hodel", "Central Germany", "Kida", "the Outfield", "the world", "in Christian eschatology", "Robin Williams", "artes liberales", "through the right atrium to the atrioventricular node to cause contraction of the heart muscle", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On ''", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "December 11, 2014", "A vanishing point", "Janie Crawford", "UTC \u2212 09 : 00", "mining", "Orange Juice", "interstitial fluid", "nearly 92 %", "the Missouri River", "iOS, watchOS, and tvOS", "Jason Marsden", "The Paris Sisters", "American country music artists Reba McEntire and Linda Davis", "Fats Waller", "Thespis", "Bill Belichick", "May 19, 2017", "season two", "Queenstown ( now Cobh ) in Ireland", "the status line", "Samantha Jo `` Mandy '' Moore", "six doctors from Seattle Grace Mercy West Hospital", "Nodar Kumaritashvili", "TLC - All That Theme Song", "Welch, West Virginia", "Nicki Minaj", "spacewar", "Julia Roberts", "the church at Philippi", "data", "to solve its problem of lack of food self - sufficiency", "Bed and breakfast", "The Lykan Hypersport", "five seasons", "Geothermal gradient", "soft contact lenses", "zak Starkey", "fire insurance", "Seoul, South Korea", "Friday", "Al Capone", "14 years", "ambassadors", "Russia", "a rubbish bin", "the Equator", "the Byzantine Emperor", "There's no chance"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6825515378438766}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.3333333333333333, 1.0, 1.0, 0.14285714285714285, 0.7000000000000001, 0.8387096774193548, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-1767", "mrqa_triviaqa-validation-6303", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-16205", "mrqa_newsqa-validation-2213"], "SR": 0.5625, "CSR": 0.5373161764705883, "retrieved_ids": ["mrqa_squad-train-25474", "mrqa_squad-train-84952", "mrqa_squad-train-65822", "mrqa_squad-train-5461", "mrqa_squad-train-70789", "mrqa_squad-train-15254", "mrqa_squad-train-31603", "mrqa_squad-train-57332", "mrqa_squad-train-25033", "mrqa_squad-train-80240", "mrqa_squad-train-27237", "mrqa_squad-train-19540", "mrqa_squad-train-70700", "mrqa_squad-train-39329", "mrqa_squad-train-25960", "mrqa_squad-train-3257", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-11614", "mrqa_naturalquestions-validation-6447", "mrqa_hotpotqa-validation-2653", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-2782", "mrqa_hotpotqa-validation-5082", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-4844", "mrqa_newsqa-validation-1010", "mrqa_hotpotqa-validation-2360", "mrqa_naturalquestions-validation-6474", "mrqa_newsqa-validation-2919", "mrqa_searchqa-validation-12460", "mrqa_triviaqa-validation-24", "mrqa_searchqa-validation-13954"], "EFR": 0.9642857142857143, "Overall": 0.7353985031512604}, {"timecode": 85, "before_eval_results": {"predictions": ["the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "the uvea", "25 June 1932", "Felicity Huffman", "to control gene expression and mediate the replication of DNA during the cell cycle", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Roger Nichols and Paul Williams", "peninsular", "44", "letter series", "eleven separate regions of the Old and New World", "As of January 17, 2018, 201 episodes", "Derrick Henry", "Monk's", "B.R. Ambedkar", "the septum", "Cecil Lockhart", "Executive Residence of the White House Complex", "David Motl", "invertebrates", "The sales area is primarily concentrated in the Southern United States, and has been sold as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "a hexamer in secretory vesicles", "the New Croton Reservoir in Westchester and Putnam counties", "Professor Eobard Thawne", "Freddie Highmore", "the closing scene of the final episode of the first season", "1939", "74", "during prenatal development", "Zedekiah", "A footling breech", "Robert Gillespie Adamson IV", "Waylon Jennings", "A rear - view mirror", "the fourth ventricle", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Doug Diemoz", "$19.8 trillion", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "Zappa's", "1956", "Camping World Stadium in Orlando, Florida", "Louis XV", "the final episode of the series", "Jonathan Breck", "Ludacris", "The long - hair gene", "126", "James Chadwick", "Elis", "George Strait", "Swiss", "Trade Mark Registration Act 1875", "Las Vegas Boulevard, commonly referred to as the Las Vegas strip, or the strip,", "Hindi", "Selden", "January 2004", "space shuttle Discovery", "dismissed all charges", "\"Empire of the Sun\"", "frittata", "a crab", "a meter", "Matlock"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7258455086580087}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35, 0.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6485", "mrqa_naturalquestions-validation-6066", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2863", "mrqa_naturalquestions-validation-10583", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-1422", "mrqa_hotpotqa-validation-5848", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-10904"], "SR": 0.65625, "CSR": 0.5386991279069768, "EFR": 0.9090909090909091, "Overall": 0.7246361323995771}, {"timecode": 86, "before_eval_results": {"predictions": ["Los Angeles", "a dancing twins", "Mikhail Baryshnikov", "filibustero", "Christo", "Berlin", "(Cyrus) McCormick", "the Ziegfeld Follies", "ballpoint pen", "a tabernacle", "the insect life cycle", "tequila", "the Iroquois", "Albert Einstein", "gambling", "a roller coaster", "President Bush", "Elvis Presley", "a flushing toilet", "an herbicides", "The Overcoat", "the Hudson River", "Banquo", "virgo", "Texas", "a pardon", "an alligator", "General Mills", "a comb", "( Ferdinand) Magellan", "France", "The left side of the heart pumps oxygenated blood to the body", "\"Jersey Boys\"", "the father's death", "Greyhounds", "a circadian rhythm", "(John) Molson", "a lottery", "Al Lenhardt", "Tanzania", "Colorado", "Christopher Columbus", "the Caribbean Sea", "Slovakia", "polygons", "Frederic Remington", "Reno Gazette- Journal", "Evan Almighty", "$2", "64", "a pillar", "Johannes Gutenberg", "203", "4 January 2011", "Neighbours", "curb-roof", "parsley", "Norway", "Big Machine Records", "Campbell Soup Company", "Stoke City.", "identity documents", "Los Angeles.", "Venezuela"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6069444444444445}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-10854", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-14222", "mrqa_searchqa-validation-6274", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-11180", "mrqa_searchqa-validation-14302", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-2269", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-632", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3854", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-2353", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-13536", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-971", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6649", "mrqa_newsqa-validation-1516"], "SR": 0.515625, "CSR": 0.538433908045977, "EFR": 1.0, "Overall": 0.7427649066091954}, {"timecode": 87, "before_eval_results": {"predictions": ["Shopgirl", "Easter Island", "True", "The Bridge on the River Kwai", "Edwin Starr", "Virgin Atlantic", "dura", "Indira Gandhi", "pew", "Hitchcock", "circulatory system", "the London Bridge", "Ho Chi Minh", "Vesuvius", "the Himalayas", "Kodachrome", "\"House M.D.\"", "Paul Simon", "Sri Lanka", "A Prairie Home Companion", "Concord", "a bug", "the Thames", "Taipei", "Melba", "the Caspian Sea", "Babe Ruth", "the Edo era", "Stromboli", "a pear", "chocolates", "bay leaf", "the Canterbury Tales", "Japan", "Ben & Jerry", "a Sweater girl", "Krakauer", "Nuradin", "Sweden", "John Glenn", "the Andes", "an ink", "Romney", "Goofy", "Peter Jackson", "an inch", "Neptune", "Earhart", "a ray", "Simon Cowell", "the Northern Mockingbird", "origins of replication, in the genome", "( 27 January -- 16 April 1898 )", "57 days", "Mr. Humphries", "James Mason", "Tempah", "Richard Arthur", "Ghana", "Labour", "because he was depriving his wife of the liberty to come and go with her face uncovered,", "1,500", "MDC head Morgan Tsvangirai.", "Louis XV"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7916666666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3533", "mrqa_searchqa-validation-10264", "mrqa_searchqa-validation-9282", "mrqa_searchqa-validation-13168", "mrqa_searchqa-validation-2075", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-8536", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-6947", "mrqa_naturalquestions-validation-1277", "mrqa_triviaqa-validation-6260", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-301", "mrqa_newsqa-validation-3391"], "SR": 0.703125, "CSR": 0.5403053977272727, "retrieved_ids": ["mrqa_squad-train-8163", "mrqa_squad-train-17296", "mrqa_squad-train-45013", "mrqa_squad-train-8854", "mrqa_squad-train-34346", "mrqa_squad-train-64238", "mrqa_squad-train-12625", "mrqa_squad-train-35586", "mrqa_squad-train-55537", "mrqa_squad-train-65162", "mrqa_squad-train-17029", "mrqa_squad-train-39928", "mrqa_squad-train-18660", "mrqa_squad-train-21376", "mrqa_squad-train-14465", "mrqa_squad-train-85012", "mrqa_searchqa-validation-12962", "mrqa_naturalquestions-validation-1538", "mrqa_triviaqa-validation-13", "mrqa_newsqa-validation-2513", "mrqa_searchqa-validation-4009", "mrqa_naturalquestions-validation-5366", "mrqa_searchqa-validation-11061", "mrqa_squad-validation-8905", "mrqa_naturalquestions-validation-8545", "mrqa_newsqa-validation-2784", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-1218", "mrqa_triviaqa-validation-148", "mrqa_naturalquestions-validation-8931", "mrqa_naturalquestions-validation-3898"], "EFR": 1.0, "Overall": 0.7431392045454545}, {"timecode": 88, "before_eval_results": {"predictions": ["Edgar Allan Poe", "a fish", "Sherlock", "Grant Wood", "Jeff Probst", "\"Twelfth Night\"", "the Black Sea", "Eggs Benedict", "lovebird", "Agatha Christie", "a church", "Tracy Tynan", "the Mossad", "a backstroke", "Swahili", "a defibrillator", "Katrina", "a crash sensor", "a proscenium arch", "yellowfin", "Pocahontas", "sinuses", "Africa", "Jane Eyre", "Kandahr", "Homer Bailey", "tofu", "(William) Harrison", "clay", "the Jutland Peninsula", "(NASA)", "the Fourteen Points", "Misery", "(Cora) Munro", "a crossword clue", "the Osmonds", "a guitar", "Roberta Flack", "Flemish", "SHIELD", "Chicago", "an actuary", "a latkes", "Montana", "a dulcimer", "(Candice) Bergen", "lead", "apocrypha", "a discus", "an Ayhuasca", "Top Gun", "a chimera ( a mixture of several animals )", "an electrochemical proton gradient that drives the synthesis of adenosine triphosphate ( ATP ), a molecule that stores energy chemically in the form of highly strained bonds", "Nick Sager", "horse racing", "australia", "Mauna Kea", "Deftones", "The Bridge Between Science and Theology", "Dutch", "Form Design Center.", "3-0", "a bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "Ronald Lyle \" Ron\" Goldman"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6386160714285714}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9047619047619047, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-7165", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-16778", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-13978", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-14060", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-10698", "mrqa_searchqa-validation-8423", "mrqa_searchqa-validation-10749", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-12604", "mrqa_searchqa-validation-2486", "mrqa_searchqa-validation-7622", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-13957", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5284", "mrqa_triviaqa-validation-2631", "mrqa_triviaqa-validation-2402", "mrqa_hotpotqa-validation-3258", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-692", "mrqa_hotpotqa-validation-2410"], "SR": 0.5625, "CSR": 0.5405547752808989, "EFR": 0.9642857142857143, "Overall": 0.7360462229133227}, {"timecode": 89, "before_eval_results": {"predictions": ["Four bodies", "The son of Gabon's former president", "his replacement, African National Congress Deputy President Kgalema Motlanthe,", "Both men were hospitalized and expected to survive,", "that the six imprisoned leaders of the religious minority were held for security reasons and not because of their faith.", "18th", "monarchy", "not guilty of affray", "Retailers who don't speak out against it", "near the equator,", "U.S. senators", "\"we have more work to do,\"", "a one-shot victory", "that she was lured to a dorm and assaulted in a bathroom stall.", "Citizens", "death", "a businessman", "his business dealings", "plastic surgery", "Meira Kumar", "The exact cause of IBS remains unknown,", "a nearby day care center whose children are predominantly African-American.", "Mexican military", "The FBI's Baltimore field office", "British", "a floating National Historic Landmark,", "Adidas,", "\"stressed and tired force\"", "Zac Efron", "he wants a \"happy ending\" to the case.", "3 to 17", "4.6 million", "Miss USA Rima Fakih", "the Airbus A330-200", "the Internet", "228", "London's O2 arena", "The Georgia Aquarium", "London Heathrow's Terminal 5.", "a \"extensive\" drug-trafficking ring.", "Bob Dole,", "Section 60.", "Muqtada al-Sadr", "Leaders of more than 30 Latin American and Caribbean nations", "At least 38 people", "two years", "heavy turbulence", "Alwin Landry's supply vessel Damon Bankston", "Florida", "tax", "The Human Rights Watch organization", "The genome", "2010", "Manchuria", "the Black Sea", "William WymarkJacobs", "serbia", "wineries", "Tsavo East National Park", "\"Lucky\"", "Bananas", "a magpies", "Midas", "Home Alone"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6335964458247068}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.8750000000000001, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.125, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-1996", "mrqa_naturalquestions-validation-5449", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-1968"], "SR": 0.5625, "CSR": 0.5407986111111112, "EFR": 1.0, "Overall": 0.7432378472222222}, {"timecode": 90, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-11678", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.861328125, "KG": 0.5328125, "before_eval_results": {"predictions": ["1974", "a small, hard, leather-cased ball with a rounded end wooden, plastic or metal bat", "Lynn Minmei", "Forest of Bowland", "Ford Island", "Food and Agriculture Organization", "Beno\u00eet Jacquot", "Lincoln", "World War II", "Les Temps modernes", "Agra", "1986", "King of Cool", "841", "Missouri River", "1.23 million", "32", "Clarence Nash", "2015", "James Franco", "1970", "Sam Kinison", "1918", "Bob Gibson", "3730 km", "Washington State Cougars football team", "1939", "Forbes", "Univision", "an English Wesleyan minister and biographer", "1865", "Dallas", "237", "Franklin", "Orlando", "Oklahoma City", "Dakota Johnson", "The School Boys", "Michele Marie Bachmann", "2007", "Japanese", "fixed-roof", "Singapore", "balloons Street, Manchester", "\"Ted\"", "The National League", "Anne Erin \"Annie\" Clark", "Atlantic", "Samantha Spiro", "Fort Hood, Texas", "Hong Kong", "Clarence Darrow", "Henry Haller", "Instagram's own account", "(1907)", "Adam Smith", "geese", "Bill Klein,", "last summer.", "in a fair and independent manner and ratify successful efforts.", "Andrew Wyeth", "a calico", "biddy", "to the U.S. Holocaust Memorial Museum"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6671626984126984}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.26666666666666666, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5714285714285715, 0.0, 0.4, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-3161", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4539", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-2900", "mrqa_hotpotqa-validation-2970", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-7502", "mrqa_triviaqa-validation-7442", "mrqa_triviaqa-validation-6410", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-2420"], "SR": 0.578125, "CSR": 0.5412087912087913, "retrieved_ids": ["mrqa_squad-train-31470", "mrqa_squad-train-15687", "mrqa_squad-train-2287", "mrqa_squad-train-76074", "mrqa_squad-train-52579", "mrqa_squad-train-11602", "mrqa_squad-train-82995", "mrqa_squad-train-83240", "mrqa_squad-train-59508", "mrqa_squad-train-74235", "mrqa_squad-train-53659", "mrqa_squad-train-5306", "mrqa_squad-train-59757", "mrqa_squad-train-73002", "mrqa_squad-train-80554", "mrqa_squad-train-16726", "mrqa_newsqa-validation-1961", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-10336", "mrqa_naturalquestions-validation-6078", "mrqa_hotpotqa-validation-2367", "mrqa_triviaqa-validation-4525", "mrqa_searchqa-validation-12492", "mrqa_searchqa-validation-5986", "mrqa_triviaqa-validation-4246", "mrqa_newsqa-validation-2696", "mrqa_squad-validation-291", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-4868"], "EFR": 0.9259259259259259, "Overall": 0.7234269434269434}, {"timecode": 91, "before_eval_results": {"predictions": ["Leo Tolstoy", "Montana", "Tigger", "Huguenots", "Nintendo", "Lexington", "a ray", "mint", "Roxanne", "Salutatorian", "Roald Dahl", "ER", "Buffalo Bill", "a pager", "Hawaii", "Sir Isaac Newton", "Radiohead", "Cain", "Lignite", "the Vietnam War", "Algebra", "Catherine", "McCartney", "Daisy", "Drumline", "Donnie", "a bug", "the Unabomber", "Tom Petty", "Harry Potter", "The Sixth Sense", "American new wave band Talking Heads", "Tanqueray gin", "Santa", "Prada", "Billy the Kid", "the Stone Age", "Cecil John Rhodes", "James Garner", "the Danube", "Michelle Pfeiffer", "a double take", "Jacob Webster", "Michael Phelps", "Papua New Guinea", "the Chinook wind", "a spectra", "sesame", "a quart", "hock", "Larry King", "UNICEF's global programing", "defense against rain", "Vijaya Mulay", "Peru", "Istanbul", "World War I", "Dante", "George Raft", "Grace O'Malley (c. 1530 \u2013 c. 1603; also Gr\u00e1inne N\u00ed Mh\u00e1ille\" ) was lord of the \u00d3 M\u00e1ille dynasty in the west of Ireland", "Zed", "Communist", "an empty water bottle", "The Fixx"], "metric_results": {"EM": 0.625, "QA-F1": 0.757165750915751}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_searchqa-validation-9909", "mrqa_searchqa-validation-12917", "mrqa_searchqa-validation-5080", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-14637", "mrqa_searchqa-validation-12260", "mrqa_searchqa-validation-15372", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-3604", "mrqa_searchqa-validation-2465", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-11934", "mrqa_searchqa-validation-3638", "mrqa_searchqa-validation-6925", "mrqa_searchqa-validation-13368", "mrqa_searchqa-validation-11404", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-2238", "mrqa_triviaqa-validation-3479", "mrqa_hotpotqa-validation-2012", "mrqa_naturalquestions-validation-8584"], "SR": 0.625, "CSR": 0.5421195652173914, "EFR": 0.9583333333333334, "Overall": 0.7300905797101449}, {"timecode": 92, "before_eval_results": {"predictions": ["John Foster Dulles", "aluminum", "Crest", "The New York Times", "the PATRIOT Act", "hock", "John Madden", "Ernest Hemingway", "a rubaiyat", "a federal judge", "Malaysia", "a yam", "Mariner", "the Chunnel", "President Lincoln", "Smithfield", "(John) Scorsese", "Poland", "The Mausoleum at Halicarnassus", "a pizza", "the Tabernacle", "The Indianapolis 500", "the Galapagos", "Boston", "the Nautilus", "the Phoenician", "parez", "Italy's second-oldest university", "Athens", "Calvin Klein Eternity", "Christopher Plummer", "Pennsylvania", "Cotton Mather", "the Berlin Wall", "Suzanne Valadon", "Sexuality", "elephants", "an axe", "Albert Kesselring", "France", "wheat", "Vermont", "Mending Wall", "Thomas Jefferson", "copper", "Wrigley", "rum", "a Towel", "Brian C. Wimes", "steel", "Cormac McCarthy", "Eydie Gorm\u00e9", "A turlough, or turlach", "6,259 km", "Tutankhamun", "Funchal", "Ruth Rendell", "Anthony Ray Lynn", "The 2016 United States Senate election", "Martin Scorsese", "African National Congress Deputy President Kgalema Motlanthe,", "Angela Merkel", "September,", "Somali-based"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6678819444444445}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-13482", "mrqa_searchqa-validation-11", "mrqa_searchqa-validation-5947", "mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-16916", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-15747", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-4976", "mrqa_searchqa-validation-4760", "mrqa_searchqa-validation-14570", "mrqa_searchqa-validation-1937", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-12309", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-13222", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-335", "mrqa_hotpotqa-validation-5730", "mrqa_newsqa-validation-1382"], "SR": 0.5625, "CSR": 0.5423387096774194, "EFR": 0.9642857142857143, "Overall": 0.7313248847926267}, {"timecode": 93, "before_eval_results": {"predictions": ["sleepover", "Romeo & Juliet", "doughboy", "Georgetown", "the Dalmatian", "a Cricket", "a duke", "Two and a Half Men", "a mummies", "Lot", "a hull", "aluminum", "Kevin Smith", "Jabez Stone", "lieutenant", "the Monitor", "Pasteur", "a deaneries", "the Hudson", "wheat", "Edgar Allan Poe", "Thomas Edison", "Manhattan", "(Curly) Lambeau", "T.E. Lawrence", "Cannes", "ape", "Eliot Spitzer", "licorice stick", "the union", "a capybara", "Edgar Allan Poe", "a star", "impeachment", "Santo Domingo", "a Dagger", "(N Carolina)", "Nightingale", "'where's the beef?'", "Harvey Milk", "butterflies", "a computer", "a devil's food cake", "Goodyear", "corpulent", "Edinburgh", "a crumpet", "trailgator bars", "Great Smoky Mountains", "(Robert) Scott", "Punjabi", "1986", "in the Duodenum by enterocytes of the duodenal lining", "Anthony Hopkins", "six", "Dubai", "horse racing", "Abdul Razzak Yaqoob", "England", "\"Brotherly Leader\"", "first team to overcome Australia at home since the West Indies in 1992-93.", "Haiti,", "if Gadhafi suffered the wound in crossfire or at close-range -- a key question that has prompted the United Nations and international human rights groups to call for an investigation", "Roberto Micheletti,"], "metric_results": {"EM": 0.625, "QA-F1": 0.6903172348484848}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3704", "mrqa_searchqa-validation-3150", "mrqa_searchqa-validation-14177", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-6686", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-5257", "mrqa_searchqa-validation-7782", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-221", "mrqa_hotpotqa-validation-3442", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2385"], "SR": 0.625, "CSR": 0.543218085106383, "retrieved_ids": ["mrqa_squad-train-73244", "mrqa_squad-train-64546", "mrqa_squad-train-12029", "mrqa_squad-train-39371", "mrqa_squad-train-11406", "mrqa_squad-train-60463", "mrqa_squad-train-84689", "mrqa_squad-train-16786", "mrqa_squad-train-49467", "mrqa_squad-train-81329", "mrqa_squad-train-23278", "mrqa_squad-train-8039", "mrqa_squad-train-31671", "mrqa_squad-train-46082", "mrqa_squad-train-60764", "mrqa_squad-train-966", "mrqa_searchqa-validation-10011", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-4454", "mrqa_hotpotqa-validation-4964", "mrqa_naturalquestions-validation-1838", "mrqa_hotpotqa-validation-2244", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-2365", "mrqa_hotpotqa-validation-4418", "mrqa_triviaqa-validation-4208", "mrqa_hotpotqa-validation-2777", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7613", "mrqa_hotpotqa-validation-460", "mrqa_squad-validation-9831", "mrqa_newsqa-validation-124"], "EFR": 1.0, "Overall": 0.7386436170212766}, {"timecode": 94, "before_eval_results": {"predictions": ["John Mayer", "Benjamin Franklin", "the Amstel", "Constantinople", "Georgie Porgie", "Chad", "Methuselah", "Puerto Rico", "hearth", "Quebec", "Belshazzar", "the Cincinnati Reds", "Once", "the shaft which flies", "China", "Frederick Douglass", "Sitka", "the Amazons", "Debussy", "Aziraphale", "Boojangles", "Aunt Jemima", "Frank Sinatra", "a saucer", "surfing", "KLM", "(John) McPhee", "a carriage", "a vest", "The Adventures of Sherlock Holmes", "Ned Kelly", "his wife's charity hosts them", "World of Warcraft", "Shakespeare", "the Inca", "Alaska", "Sam Kinison", "the Wii", "the high jump", "Champagne", "a new Broom", "Danica Patrick", "pancreas", "Midway Atoll", "stars", "Henry Cisneros", "the sacristy", "the Great Seal", "Rihanna", "\"24\"", "Tom Brady", "a star", "the final scene of the fourth season", "Billy Hill", "iron", "Henry Ford", "bartholomew", "1943", "Battle of Britain and the Battle of Malta", "Kaep", "not", "David Beckham", "the vicious brutality which accompanied the murders of his father and brother.\"", "Lana Clarkson"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7305584733893558}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-10187", "mrqa_searchqa-validation-9190", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-13912", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-15916", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-16622", "mrqa_searchqa-validation-15733", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-615", "mrqa_searchqa-validation-4359", "mrqa_searchqa-validation-11261", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-13716", "mrqa_naturalquestions-validation-5305", "mrqa_hotpotqa-validation-5383", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1915"], "SR": 0.671875, "CSR": 0.5445723684210526, "EFR": 0.9523809523809523, "Overall": 0.729390664160401}, {"timecode": 95, "before_eval_results": {"predictions": ["Buckwheat Boyz", "1998", "between the Eastern Ghats and the Bay of Bengal", "The Indian Ocean is the largest among the tropical oceans, and about 3 times faster than the warming observed in the Pacific", "Gatiman express its ranges 160km / hour between Delhi to Agra In 100 min its cross 180km", "Dr. Rajendra Prasad", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "no more than 4.25 inches ( 108 mm )", "the Gentiles", "art of the Persian Safavid dynasty", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "typically the player to the dealer's right", "his guilt in killing the bird", "February 6, 2005", "a global cultural icon of France", "the episode `` Kobol's Last `` ''", "Philippe Petit", "James Cowser", "anion", "four", "1952", "The User State Migration Tool ( USMT )", "2017", "restoring someone's faith in love and family relationships", "an illustration by Everest creative Maganlal Daiya", "if the concentration of a compound exceeds its solubility ( such as when mixing solvents or changing their temperature )", "solemniser", "1961", "36 months", "an integral membrane protein that builds up a proton gradient across a biological membrane", "during the period of rest ( day )", "Ethel `` Edy '' Proctor", "Randy VanWarmer", "Isle Vierge ( 48 \u00b0 38 \u2032 23 '' N 4 \u00b0 34 \u2032 13 '' W", "Kevin Spacey", "Miller Lite", "the body - centered cubic ( BCC ) lattice", "1986", "their son Jack ( short for Jack - o - Lantern )", "Spektor", "9pm ET ( UTC - 5 )", "Majo to Hyakkihei 2", "July 21, 1861", "her attractive Tatted neighbour ( Holden Nowell )", "Muhammad Yunus", "Jules Shear", "Juliet", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "Ethel Merman", "The New Croton Aqueduct", "Judith Cynthia Aline Keppel", "Kenny Everett", "Trinidad", "Ghost", "Haleiwa, Hawaii", "William Bradford", "Chuck Noll", "Scudetto", "three", "suicides", "John Henry", "compost", "Frontier", "2004 Paris Motor Show"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6542170185621129}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.15384615384615383, 0.2222222222222222, 1.0, 0.9428571428571428, 1.0, 0.0, 0.0, 1.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.823529411764706, 0.8, 0.5454545454545454, 0.2857142857142857, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.41791044776119407, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9683", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-6035", "mrqa_hotpotqa-validation-4553", "mrqa_newsqa-validation-2754", "mrqa_searchqa-validation-13621"], "SR": 0.53125, "CSR": 0.54443359375, "EFR": 0.9, "Overall": 0.71888671875}, {"timecode": 96, "before_eval_results": {"predictions": ["oregon", "painting", "gay, bisexual, and transgender", "Pisces", "Law Society", "Russell Crowe", "17", "Saudi Arabia", "Clint Eastwood", "1921", "morocco", "David Bowie", "Gone With the Wind", "dussel", "Volcanic Zone", "Porridge", "South Africa", "New Orleans", "eye", "pooh bear", "Ringo Starr", "John Mortimer", "bushfires", "Boise", "Oswald Chesterfield Cobblepot", "Sweden", "four players", "Kenny Walker and Acklee King", "rastafari", "Sydney", "800m", "Leo Tolstoy", "Lotus", "Plymouth Rock", "Rick Wakeman", "Benghazi", "Brazil", "The Mary Tyler Moore Show", "Gordon Jackson", "scotlands", "Beyonce", "West Sussex", "Laputa", "Colombia", "lurch", "mad", "Tanzania", "Darby and Joan", "Robert Boyle", "Japan", "Hugh Laurie", "Tokyo", "sometime between 124 and 800 CE", "in the United States, currently used by 31 states, the federal government, and the military", "1 January 1788", "guitar feedback", "Stephen Lee", "Arroyo and her husband", "artificial intelligence.", "North Korea is technically capable of launching a rocket in as little as two to four days,", "Vietnam War", "white granite", "the humerus", "pinch"], "metric_results": {"EM": 0.578125, "QA-F1": 0.633888888888889}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true], "QA-F1": [0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4799999999999999, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-1554", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3638", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-4141", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6139", "mrqa_triviaqa-validation-6359", "mrqa_triviaqa-validation-1431", "mrqa_triviaqa-validation-2359", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-50", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-6747", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-709", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-1433", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-1941", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-1662"], "SR": 0.578125, "CSR": 0.5447809278350515, "retrieved_ids": ["mrqa_squad-train-10348", "mrqa_squad-train-70856", "mrqa_squad-train-48087", "mrqa_squad-train-54654", "mrqa_squad-train-76479", "mrqa_squad-train-60390", "mrqa_squad-train-3013", "mrqa_squad-train-62854", "mrqa_squad-train-61406", "mrqa_squad-train-39450", "mrqa_squad-train-61606", "mrqa_squad-train-59149", "mrqa_squad-train-10131", "mrqa_squad-train-39319", "mrqa_squad-train-14425", "mrqa_squad-train-65185", "mrqa_naturalquestions-validation-312", "mrqa_searchqa-validation-936", "mrqa_naturalquestions-validation-3358", "mrqa_squad-validation-5189", "mrqa_searchqa-validation-2797", "mrqa_newsqa-validation-1004", "mrqa_naturalquestions-validation-4569", "mrqa_squad-validation-7409", "mrqa_searchqa-validation-7006", "mrqa_naturalquestions-validation-7685", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-31", "mrqa_hotpotqa-validation-4711", "mrqa_triviaqa-validation-5024", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-2220"], "EFR": 1.0, "Overall": 0.7389561855670104}, {"timecode": 97, "before_eval_results": {"predictions": ["40 militants and six Pakistan soldiers dead,", "recall communications", "Javier Hernandez", "Two United Arab Emirates based companies", "Britain.", "The monarchy's end after 239 years of rule", "Kerstin and two of her brothers,", "56,", "\"Freshman Year\" experience", "more than 200.", "Princess Diana", "1825", "maintain an \"aesthetic environment\" and ensure public safety,", "Caylee Anthony,", "The elections are slated for Saturday.", "Alexandre Caizergues, of France,", "Arroyo and her husband", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "Stratfor,", "India", "non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "Ronald Cummings", "Glenn McConnell,", "Toffelmakaren.", "was gunned down as a gang tried to steal his BMW car in the early hours of Sunday morning.", "researchers have developed technology that makes it possible", "between 1917 and 1924", "the fast-food chain is conquering one of the country's most valued cultural institutions --the Louvre.", "3,000", "10 percent", "$250,000", "April 22.", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Yusuf Saad Kamel", "Barack Obama", "flights affected", "try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Jeffrey Jamaleldine", "more than two years,", "it was unjustifiable", "up to $50,000 for her,", "Citizens", "41,280 pounds", "Adam Lambert", "President Clinton.", "allegedly involved in forged credit cards and identity theft", "Iran's nuclear program.", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "133", "\"He tried", "deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan.", "Richard Parker", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "the stable balance of attractive and repulsive forces between atoms, when they share electrons, is known as covalent bonding", "cricket", "spain", "takerx.comroman", "Hawaii", "stolperstein", "Paper", "Las Vegas", "the hypothalamus", "the orangutan", "enid blyton"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6342216083449819}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true], "QA-F1": [0.923076923076923, 0.0, 0.0, 1.0, 0.5, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0909090909090909, 0.0, 1.0, 0.1, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.20689655172413793, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 1.0, 0.19047619047619047, 0.8, 0.12121212121212123, 1.0, 1.0, 0.0, 1.0, 0.24489795918367346, 0.08695652173913045, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-1451", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-4055", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1443", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-7701", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-4975"], "SR": 0.515625, "CSR": 0.5444834183673469, "EFR": 0.9354838709677419, "Overall": 0.7259934578670177}, {"timecode": 98, "before_eval_results": {"predictions": ["does not", "Anders J\u00f3rle", "Denver, Colorado.", "Friday,", "Heshmat Tehran Attarzadeh", "Daniel Radcliffe", "Yusuf Saad Kamel", "the mine was shut down in 1882,", "Susan Atkins,", "Pakistan's High Commission in India", "\"Wicked,\"", "Rolling Stone", "Tsvangirai", "The oceans", "Asashoryu", "US Airways Flight 1549", "autonomy.", "South Africa", "\"A good vegan cupcake has the power to transform everything for the better,\"", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "between 1917 and 1924", "Les Bleus", "Too many glass shards left by beer drinkers in the city center,", "collapsed ConAgra Foods plant", "40-year-old", "Krishna Rajaram,", "The island's dining scene", "in the U.S.", "clogs", "Nirvana frontman,", "Turkey,", "Moody and sinister,", "United States, NATO member states, Russia and India", "Ryder Russell,", "spending billions to revitalize the nation's economy,", "Palestinian Islamic Army,", "Haeftling,", "off the coast of Dubai with a celebrity-studded gala and a three-day party.", "change course", "Sri Lanka,", "eight-week", "Mexican officials", "head for Italy.", "Anil Kapoor.", "\"The Closer.", "38 feet", "St. Louis, Missouri.", "64,", "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"", "fifth season", "order", "1977", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "Nasdaq", "Van Rijn", "a bird", "\"Field of Dreams\"", "hulder", "Lucille Ball", "Ziploc", "South Africa", "ivory", "Kraftwerk"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7521577380952381}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-3636", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-1683", "mrqa_hotpotqa-validation-2399"], "SR": 0.71875, "CSR": 0.5462436868686869, "EFR": 0.9444444444444444, "Overall": 0.7281376262626262}, {"timecode": 99, "UKR": 0.765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1232", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13865", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14527", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-622", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8108", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9502", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1713", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3633", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3883", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4141", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-709", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.859375, "KG": 0.5125, "before_eval_results": {"predictions": ["Number Ones", "\"It feels good for me to talk about her,\"", "$40 billion during", "can I", "10 municipal police officers", "Lifeway's 100-plus stores nationwide", "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"", "2010", "two years", "A group of college students of Pakistani background", "gasoline", "Her husband and attorney, James Whitehouse,", "North Korea,", "paintings, the 1962 \"Tete de Cheval\" (\"Horse's Head\") and the 1944 \"Verre et Pichet\" (\"Glass and Pitcher\") by Picasso.", "Omar bin Laden,", "nude beaches.", "1-1 draw", "Utah's Chaffetz", "Martin Aloysius Culhane", "President Obama", "A member of the group dubbed the \"Jena 6\"", "Animal Planet", "Minerals Management Service Director Elizabeth Birnbaum", "Mandi Hamlin", "41,280", "Arlington National Cemetery's Section 60,", "Bill Haas", "Caster Semenya", "Michael Jackson", "Elisabeth captive", "in the neighboring country of Djibouti,", "123 pounds of cocaine and 4.5 pounds of heroin,", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "Brazil, Argentina, Mexico, Colombia and Venezuela.", "second-degree aggravated battery.", "Emmy-winning Patrick McGoohan,", "Alina Cho", "The forward's lawyer", "Turkish President Abdullah Gul.", "salary to the underprivileged.", "secure more funds from the region.", "five female pastors", "14", "South Carolina Republican Party Chairwoman Karen Floyd", "African National Congress Deputy President", "1913,", "second", "Chinese President Hu Jintao", "Michael Arrington", "asylum in Britain.", "Indian army", "New York City", "2009", "Massachusetts, protesters were able to force the tea consignees to resign or to return the tea to England", "Phil Mickelson", "the Quran", "Shanghai", "UDC", "a wooden Indian", "Aldosterone", "freezing", "Power", "Maine", "November 17, 2017"], "metric_results": {"EM": 0.625, "QA-F1": 0.7412573247216219}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.08, 0.4, 1.0, 1.0, 1.0, 0.9302325581395349, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 0.5, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-1139", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3689", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-3895", "mrqa_naturalquestions-validation-644", "mrqa_hotpotqa-validation-397"], "SR": 0.625, "CSR": 0.5470312500000001, "retrieved_ids": ["mrqa_squad-train-34942", "mrqa_squad-train-49851", "mrqa_squad-train-1613", "mrqa_squad-train-333", "mrqa_squad-train-28223", "mrqa_squad-train-56263", "mrqa_squad-train-62906", "mrqa_squad-train-42965", "mrqa_squad-train-70233", "mrqa_squad-train-68338", "mrqa_squad-train-4597", "mrqa_squad-train-78273", "mrqa_squad-train-33724", "mrqa_squad-train-74746", "mrqa_squad-train-58613", "mrqa_squad-train-85853", "mrqa_searchqa-validation-2674", "mrqa_triviaqa-validation-6028", "mrqa_newsqa-validation-3638", "mrqa_triviaqa-validation-386", "mrqa_hotpotqa-validation-2653", "mrqa_newsqa-validation-1604", "mrqa_searchqa-validation-16013", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2118", "mrqa_hotpotqa-validation-2403", "mrqa_searchqa-validation-4534", "mrqa_triviaqa-validation-453", "mrqa_naturalquestions-validation-4761", "mrqa_hotpotqa-validation-4819", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-560"], "EFR": 1.0, "Overall": 0.73690625}]}