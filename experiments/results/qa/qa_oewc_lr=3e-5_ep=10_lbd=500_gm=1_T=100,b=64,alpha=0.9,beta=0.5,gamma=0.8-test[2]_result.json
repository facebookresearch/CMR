{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=500_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=500.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=500_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4080, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Jelme and Bo'orchu", "gauge bosons", "consumer prices", "Albert C. Outler", "a computational problem", "1521", "River Tyne", "Boston", "San Jose Marriott", "illegal boycotts", "Mitochondria", "bilaterians", "Alexandre Yersin", "Methodists today", "Beyonc\u00e9", "the Rhine and its downstream extension", "7\u20134\u20132\u20133", "Horniman Museum", "400 AD to 1914", "early 1526", "The individual is the final judge of right and wrong", "five", "Battle of B\u1ea1ch \u0110\u1eb1ng", "Time Lady", "oxygen-16", "The Day of the Doctor", "Sierra Sky Park", "James Clerk Maxwell", "Bill Clinton", "in areas its forces occupied in Eastern Europe", "20,000", "Queen Elizabeth II", "The Daleks", "gas turbines", "Newton", "Miasma theory", "Ealy", "several medals", "remaining in black and white", "computability theory", "autoimmune", "American Sweetgum", "Pleistocene epoch", "Feynman diagrams", "orange", "oxygen compounds", "four", "Fort Caroline", "counties or powiats", "chemical bonds", "2015", "France's claim to the region was superior to that of the British", "double", "helps many proteins bind the polypeptide", "Islamism", "lines or a punishment essay", "mercuric oxide", "released Islamists from prison and welcomed home exiles", "Washington and Thomas Gage", "The individual", "Bruno Mars", "the public", "Thomas Edison and George Westinghouse", "a freshwater lake"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8040364583333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6848", "mrqa_squad-validation-9923", "mrqa_squad-validation-100", "mrqa_squad-validation-9434", "mrqa_squad-validation-6966", "mrqa_squad-validation-7725", "mrqa_squad-validation-8538", "mrqa_squad-validation-1703", "mrqa_squad-validation-3511", "mrqa_squad-validation-3597", "mrqa_squad-validation-9567", "mrqa_squad-validation-6973", "mrqa_squad-validation-2025"], "SR": 0.796875, "CSR": 0.796875, "EFR": 1.0, "Overall": 0.8984375}, {"timecode": 1, "before_eval_results": {"predictions": ["the General Sejm", "232", "New Holland", "the \"Rhine knee\"", "the U.S. South", "the Schmalkaldic League", "January 1985", "an Executive Committee", "King Sancho VI of Navarre", "the Arizona Cardinals", "36", "Chloroplasts", "Sydney", "the Panic of 1901", "Muslim medicine", "the Silk Road", "silicon dioxide", "statocyst", "Several thousand", "the Fourth Intercolonial War and the Great War for the Empire", "medieval", "30\u201360% of Europe's total population", "the laws of physics", "the Ten Commandments", "the San Fernando Valley", "Roger NFL", "Hugh L. Dryden", "metals", "1.5 gigatons", "Denver Broncos", "\u00a31 of capital", "the 2010 series", "megaprojects", "1024-bit primes", "the portrait of Fran\u00e7ois, Duc d'Alen\u00e7on by Fran\u00e7ois Clouet, Gaspard Dughet", "Africa", "the Electorate of Saxony", "ice-sheets", "the lion, leopard, buffalo, rhinoceros, and elephant", "A plea of no contest is sometimes regarded as a compromise between the two", "seven", "Demaryius Thomas", "Napoleon's", "the Santa Clara Marriott", "kinematic measurements", "shipping", "12th", "helical thylakoid model", "the A69", "14%", "Thomas Edison", "Toshiba", "detention", "antigen presentation", "hunter's garb", "The Harvard Crimson competes in 42 intercollegiate sports in the NCAA Division I Ivy League", "British Gas plc", "Demaryius Thomas", "Homebrewing", "Joe Scarborough", "became a politician", "Gareth Jones", "Sivakumar, S. V. Subbaiah, Jayachitra, Srividya, Shubha, Kamal Haasan and Jayasudha", "Teresa Hairston"], "metric_results": {"EM": 0.75, "QA-F1": 0.7791752518315018}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25000000000000006, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3076923076923077, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9196", "mrqa_squad-validation-1117", "mrqa_squad-validation-3664", "mrqa_squad-validation-85", "mrqa_squad-validation-4482", "mrqa_squad-validation-8978", "mrqa_squad-validation-5490", "mrqa_squad-validation-8446", "mrqa_squad-validation-8278", "mrqa_squad-validation-6914", "mrqa_squad-validation-7155", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2423", "mrqa_newsqa-validation-2275"], "SR": 0.75, "CSR": 0.7734375, "EFR": 1.0, "Overall": 0.88671875}, {"timecode": 2, "before_eval_results": {"predictions": ["Ugali with vegetables, sour milk, meat, fish or any other stew", "TFEU article 294", "over $20 billion", "was a major source of water pollution", "unity of God", "all war", "1000 and 1900", "Gamal Abdul Nasser", "viniculture and tourism", "minor", "1162", "lost in the 5th Avenue laboratory fire of March 1895", "ABC Cable News", "22 May 2006", "Germany and Austria", "Golden Gate Bridge", "the Welsh", "acquiring nutrients", "Muslims in the semu class", "the Chinese", "temperature and light", "12 May 1999", "1852", "the development of safety lamps", "stabilize the rest of the chloroplast genome", "The Mongols' extensive West Asian and European contacts", "24%", "Milton Friedman Institute", "Donald Davies", "three", "student motivation and attitudes towards school", "1560", "1891", "Lutheran views", "electron", "fear of their lives", "Science", "John Pell, Lord of Pelham Manor", "Cam Newton", "Osama bin Laden", "international drug suppliers", "President", "expelled Jews", "arid and semi-arid areas with near-desert landscapes", "Yosemite Freeway", "Annan and his UN-backed panel and African Union chairman Jakaya", "The Warsaw Stock Exchange", "it becomes an Act of the Scottish Parliament", "certification by a recognized body", "a chain or screw stoking mechanism", "Battle of B\u1ea1ch \u0110\u1eb1ng (1288)", "silver", "1947", "1860", "The club will participate in the Premier League, FA Cup, EFL Cup (as holders), UEFA Champions League and UEFA Super Cup.", "Detroit, Michigan", "the Emancipation Proclamation", "26,000", "Pakistan A", "Ricky Skaggs", "Saturday Night Live", "last living pilot of the X-15 program", "Two new U.S. representatives are teaming up with CNN.com to report their \"Freshman Year\" experience through videos and commentaries", "250,000"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8239718614718615}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.9523809523809523, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.1818181818181818, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9343", "mrqa_squad-validation-9093", "mrqa_squad-validation-9388", "mrqa_squad-validation-5157", "mrqa_squad-validation-8399", "mrqa_squad-validation-4562", "mrqa_squad-validation-8383", "mrqa_squad-validation-9499", "mrqa_squad-validation-8222", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-368"], "SR": 0.765625, "CSR": 0.7708333333333334, "EFR": 1.0, "Overall": 0.8854166666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["10 February 1763", "good, clear laws, fairly and democratically", "shaping ideas about the free market", "SAP Center in San Jose", "older", "university and military academy", "Foreign Protestants Naturalization Act", "inequality", "jigg TV", "three", "permafrost", "Silas B. Cobb", "the traditional salute of a knight winning a bout", "Jane Kim", "the Presiding Officer", "lipophilic alkaloid toxins", "one", "William Rainey Harper", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "June", "Huguenot", "US President Barack Obama chose not to visit the country", "Ralph Woodward", "Susan Foreman", "clinical pharmacists", "teleforce", "British failures in North America, combined with other failures in the European theater", "300", "66 million years ago", "Hong Kong", "in commerce, schooling and government", "Krak\u00f3w", "France", "three", "power outage", "easier and more efficient than anywhere else", "Muslim and Chinese", "free trade", "15,100", "Cuba", "high pressure shock waves", "28,000", "21 to 11", "Cam Newton", "128,843", "Van Gend en Loos v Nederlandse Administratie der Belastingen", "four years", "Howard Keel", "Pangaea (or Pangea)", "Mel Gibson", "George Washington", "John Uhler Lemmon III", "six", "ccoli", "Charles, Eric Clapton, Bob Dylan and Johnny Cash", "Henry Kelly", "hedgehogs", "George IV", "The Time Machine", "the Granite City", "the natural world and mysticism", "more funds", "Brad Blauser", "$1.45 billion"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7401697261072261}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1831", "mrqa_squad-validation-4971", "mrqa_squad-validation-5975", "mrqa_squad-validation-6223", "mrqa_squad-validation-9570", "mrqa_squad-validation-3044", "mrqa_squad-validation-8326", "mrqa_squad-validation-1830", "mrqa_squad-validation-4065", "mrqa_squad-validation-973", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1517", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-1148"], "SR": 0.6875, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 4, "before_eval_results": {"predictions": ["vote clerk", "estimated $200,000", "carbohydrates", "redistributive taxation", "the League of Nations", "two", "\"The Time of the Doctor\"", "Nairobi", "Missy", "free", "7:00 to 9:00 a.m.", "Professor Richard ( Dick) Geary", "lipid monolayer", "2009", "whether a state or threat of war existed", "the European Commission", "Jin", "greater equality but not per capita income", "John Houghton", "carbohydrates", "in both houses of Congress", "America's Funniest Home Videos", "42%", "19", "specialised education and training", "layered basaltic lava flows", "October 2007", "Robert Maynard Hutchins", "Shoushi Li", "clerical marriage", "40%", "Kevin Harlan", "about 200 Troupes de la marine and 30 Indians.", "Half", "Independence Day: Resurgence", "duty", "complex silicates (in silicate minerals)", "Worldvision Enterprises", "hunter's garb", "in the Channel Islands", "\"Blue Harvest\" and \"420\"", "1225", "georgie", "george jackson", "jackson", "\"Hey there Delilah, I know... God speed your love to me\"", "Mark Antony", "Cuba", "his father", "filius vocabat Marcum", "on the right side", "george hopp", "george hanger", "billy jackson", "jackson King", "billy jackson", "preston's men stormed the shores of this Barbary state at Derna and... the first verse of the Marines' Hymn: \"From the Halls of Montezuma", "Topix", "London", "Britomart", "Casey Beane", "Islamabad", "The Jefferson Memorial", "murder"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6343487394957983}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2491", "mrqa_squad-validation-3948", "mrqa_squad-validation-3932", "mrqa_squad-validation-8234", "mrqa_squad-validation-10162", "mrqa_squad-validation-3507", "mrqa_squad-validation-1131", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-9286", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-2118", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-14197", "mrqa_naturalquestions-validation-5040", "mrqa_newsqa-validation-839"], "SR": 0.59375, "CSR": 0.71875, "EFR": 0.9615384615384616, "Overall": 0.8401442307692308}, {"timecode": 5, "before_eval_results": {"predictions": ["Justifying Grace", "neuronal dendrites", "an electrical generator", "Doctor Who", "coronary thrombosis", "his grandson", "San Francisco", "consumer prices", "2016", "as soon as they enter into force", "colonizing empires", "\"hockey stick graph\"", "OpenTV", "intuition", "1720", "around 300", "2001", "The Chase", "cortisol and catecholamines", "Economist Intelligence Unit", "the Decalogue (the Ten Commandments) and the Lord's Prayer", "Paramount Pictures", "Thomas Coke", "The Neighbors", "waldzither", "the United States", "\u20ac53,423", "the Helicosproidia", "build their own dedicated networks", "2001", "the \"Rhine knee\"", "Justin Tucker", "Colorado Springs", "2004", "Newton", "26", "University College London", "Jerricho Cotchery", "a talking horse", "the cube root of a negative number", "Willa Cather", "The third law of thermodynamics", "Lewis and Clark", "Truman", "pope", "manganese", "a Fokker", "Richardter", "(2008)", "Ian Fleming", "The Caresse D'Eole Secret Duo", "The Thing", "manganese", "Ely", "the Mensheviks", "Charlotte Russe cake", "Schiller", "The Tale of Genji", "\"The Daily Show\"", "Ant & Dec", "\"Unchampram\"", "Cherokee Nation", "said Rivers.The victory means $250,000 for Rivers' charity: God's Love We Deliver.", "UFC 50: The War of '04"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6400222173659673}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 0.7499999999999999]}}, "before_error_ids": ["mrqa_squad-validation-6267", "mrqa_squad-validation-9865", "mrqa_squad-validation-7827", "mrqa_squad-validation-2391", "mrqa_squad-validation-4874", "mrqa_squad-validation-9199", "mrqa_squad-validation-5214", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-8713", "mrqa_triviaqa-validation-3042", "mrqa_newsqa-validation-1583", "mrqa_hotpotqa-validation-1190"], "SR": 0.59375, "CSR": 0.6979166666666667, "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["ten", "", "Von Miller", "a downward pressure on wages", "Catholic", "nine", "11:28", "chest pains", "March 1896", "T cells", "economically", "private networks were often connected via gateways to the public network to reach locations not on the private network", "the college", "Yes\u00fcgei", "research", "toward the end of his life", "Bill Clinton", "Ollie Treiz", "Sufism", "San Andreas Fault", "30%\u201350%", "a double coronation", "ESPN", "p", "plantar fasciitis", "Peter Capaldi", "6000 Da", "Queen Victoria and Prince Albert", "cortisol and catecholamines", "Manakintown", "1985", "a stronger, tech-oriented economy", "stream capture", "the general number field sieve", "identity documents", "The Bronx County District Attorneys Office", "a woman", "Nothing But Love", "a man's lifeless, naked body", "the Sri Lankan cricket team", "a comprehensive detainees policy", "Richard Findley", "8 p.m. local time", "\"Jersey Shore\"", "diabetes and hypertension", "6-4", "\"wow.\"", "Silvan Shalom", "2009", "\"It has never been the policy of this president or this administration to torture.\"", "Himalayan", "Wednesday", "2,000 euros", "Siri", "88", "Charles Bukowski", "two", "Russia", "House of Representatives", "watensis", "Argentinian", "lion", "Department of Homeland Security", "Bessarabia"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6382575757575758}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10413", "mrqa_squad-validation-6008", "mrqa_squad-validation-2122", "mrqa_squad-validation-9213", "mrqa_squad-validation-6696", "mrqa_squad-validation-3193", "mrqa_squad-validation-2835", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3491", "mrqa_triviaqa-validation-2240", "mrqa_hotpotqa-validation-985", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-9754", "mrqa_searchqa-validation-8198"], "SR": 0.578125, "CSR": 0.6808035714285714, "EFR": 1.0, "Overall": 0.8404017857142857}, {"timecode": 7, "before_eval_results": {"predictions": ["Blaydon Race", "The Central Region", "ten", "viral pathogens", "100\u2013150", "1886", "from \u00a318m to \u00a334m per year", "Alvaro Martin", "more efficient solutions", "Schedule 5", "BBC 1", "victory at Fort Niagara successfully cut off the French frontier forts further to the west and south.", "cantatas", "January 30", "seven", "\u20ac25,000 per year", "St. Bartholomew's Day massacre", "9th", "principle of equivalence", "The Entertainment Channel", "when they improve society as a whole, including the poorest members", "1725", "the incentive for the democratic changes", "St. Johns River", "Life", "priest", "Jan Andrzej Menich", "Jane Kim", "the global last ice age maximum", "biomass", "1562", "9 a.m.-1 p.m.", "Empire of the Sun", "22", "four", "Ross Perot", "Baghdad", "$1.45 billion", "her home", "more than 2,800", "in a tenement in the Mumbai suburb of Chembur", "the contraband is then moved through an elaborate series of drop points", "it, a crime that triggered a nationwide manhunt and search for the girl", "July 4.", "Evan Bayh", "Hu Jintao", "April 24", "the president would legally be able to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "scientific reasons", "around 1918 and 1919", "the first or second week in April", "Pakistan", "Pakistan", "jazz", "appealed against the punishment", "the used-luxury market", "Steve Williams", "Ali", "Eid-al-Adha", "Mickey's PhilharMagic", "Sugar Ray", "Oedipus Rex", "guitar feedback", "hyperaccumulators"], "metric_results": {"EM": 0.625, "QA-F1": 0.7339839541585865}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.846153846153846, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.923076923076923, 0.0, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6561", "mrqa_squad-validation-2923", "mrqa_squad-validation-10269", "mrqa_squad-validation-2419", "mrqa_squad-validation-7564", "mrqa_squad-validation-2422", "mrqa_squad-validation-9144", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-2809", "mrqa_triviaqa-validation-955", "mrqa_hotpotqa-validation-3972"], "SR": 0.625, "CSR": 0.673828125, "EFR": 1.0, "Overall": 0.8369140625}, {"timecode": 8, "before_eval_results": {"predictions": ["\"nolo contendere\"", "5,984", "Jacksonville", "fish stocks to collapse", "applied force", "DuMont Television Network", "Amtrak San Joaquins", "teaching", "the traditional salute of a knight", "a ribosome in the cytosol", "British", "1887", "February 7, 2016", "Cargill Meat Solutions and Foster Farms", "Henry Plitt", "1978", "Von Miller", "about 0.7%", "water", "Electronic Frontier Foundation", "Tyndale Bible", "pharmacists know about the mode of action of a particular drug, and its metabolism and physiological effects on the human body", "Il milione", "Guo Shoujing", "1908", "1560", "reason", "only \"essentials\"", "30,000", "a Taliban member who had come for the talks about peace and reconciliation, and detonated the explosives", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Keating Holland.", "California-based Current TV", "Behar.", "his business dealings", "summer", "the Dalai Lama's current \"middle way approach,\"", "the U.S. Holocaust Memorial Museum,", "a body", "Brian Mabry", "iTunes, which completely changed the business of music,", "more than 4,000", "consumer confidence", "autonomy", "1996", "1831", "Russia", "a ruthless cartel", "clothes that are consistent and accessible.", "Arizona", "Muslim festival of Eid al-Adha.", "BET", "FBI recordings of his phone calls.", "torture and indefinite detention", "African President Thabo Mbeki,", "Italian and six Africans", "an impromptu memorial for the late singer", "China", "100mph,", "Prime Minister Margaret Thatcher", "Mason-Dixon Line Segment.", "1898", "11 : 40 p.m. ship's time", "18th century"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6708193542568542}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1212121212121212, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.16]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-2048", "mrqa_squad-validation-8960", "mrqa_squad-validation-7552", "mrqa_squad-validation-6284", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-776", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-1352", "mrqa_naturalquestions-validation-8279", "mrqa_triviaqa-validation-2510", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-3505"], "SR": 0.578125, "CSR": 0.6631944444444444, "EFR": 1.0, "Overall": 0.8315972222222222}, {"timecode": 9, "before_eval_results": {"predictions": ["the Yassa", "An attorney", "The Quasiturbine", "\u22122, \u22124,...", "creates immunological memory", "three", "coal", "pseudorandom", "average teacher salaries", "the same message routing methodology as developed by Baran", "the Mongols beyond the Middle Kingdom saw them as too Chinese.", "Temecula and Murrieta", "antibodies", "Korean", "the revolution could only succeed in Russia as part of a world revolution.", "William of Orange", "the Horn of Africa", "lipid monolayer", "\"Hymn for the Weekend\"", "Colonel Monckton", "Gymnosperms", "7 January 1943", "1.7 million", "ABC News Now", "Boomer Esiason and Dan Fouts", "Eintracht Frankfurt", "former U.S. secretary of state", "1,073 immigration detainees", "10", "the 84-year-old Mugabe has been the country's only ruler.", "Islamabad", "Haleigh Cummings,", "90", "Zimbabwe President Robert Mugabe", "the 12th on the Blue Monster course at Doral", "terminal brain cancer.", "death squad killings", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Leaders of more than 30 Latin American and Caribbean nations", "the Beatles", "Citizens are picking members of the lower house of parliament,", "the 6.2-mile Moffat Tunnel,", "(l-r)", "the 50-year-old King of Pop has agreed to a series of summer concerts at the O2.", "led the weekend box office, grossing $55.7 million during its first weekend.", "The planned Kingdom City project", "DBG", "United Arab Emirates", "Bobby Jindal", "Polo because \"it was the sport of kings.", "pro-democracy activists", "haute, bandeau-style little numbers", "Aniston, Demi Moore and Alicia Keys", "pesos ($193 million)", "Larry King,", "Afghanistan", "Landon Jones", "Transvaginal ultrasonography", "Wikia", "Ben Hogan", "400", "\"Nebo Zovyot\"", "the Chesapeake Bay", "the list of dos & don'ts"], "metric_results": {"EM": 0.5625, "QA-F1": 0.640086163379749}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.36363636363636365, 1.0, 0.125, 1.0, 0.6153846153846153, 0.0, 0.0, 0.631578947368421, 0.5555555555555556, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10128", "mrqa_squad-validation-3304", "mrqa_squad-validation-9912", "mrqa_squad-validation-8880", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2128", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2016", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-2219"], "SR": 0.5625, "CSR": 0.653125, "EFR": 1.0, "Overall": 0.8265625}, {"timecode": 10, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3412", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4048", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-100", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1886", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-8001", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9754", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10128", "mrqa_squad-validation-10155", "mrqa_squad-validation-10162", "mrqa_squad-validation-10167", "mrqa_squad-validation-1018", "mrqa_squad-validation-10198", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10269", "mrqa_squad-validation-10272", "mrqa_squad-validation-1029", "mrqa_squad-validation-103", "mrqa_squad-validation-10310", "mrqa_squad-validation-10315", "mrqa_squad-validation-10326", "mrqa_squad-validation-10345", "mrqa_squad-validation-1036", "mrqa_squad-validation-10380", "mrqa_squad-validation-10413", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10476", "mrqa_squad-validation-1048", "mrqa_squad-validation-1053", "mrqa_squad-validation-1088", "mrqa_squad-validation-1097", "mrqa_squad-validation-1119", "mrqa_squad-validation-1131", "mrqa_squad-validation-1197", "mrqa_squad-validation-1222", "mrqa_squad-validation-1231", "mrqa_squad-validation-1255", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-139", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1521", "mrqa_squad-validation-1537", "mrqa_squad-validation-1546", "mrqa_squad-validation-1561", "mrqa_squad-validation-1592", "mrqa_squad-validation-1611", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1831", "mrqa_squad-validation-1834", "mrqa_squad-validation-1876", "mrqa_squad-validation-1940", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-20", "mrqa_squad-validation-2048", "mrqa_squad-validation-2048", "mrqa_squad-validation-2087", "mrqa_squad-validation-2116", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2188", "mrqa_squad-validation-2235", "mrqa_squad-validation-2250", "mrqa_squad-validation-2374", "mrqa_squad-validation-239", "mrqa_squad-validation-2391", "mrqa_squad-validation-2403", "mrqa_squad-validation-2419", "mrqa_squad-validation-2422", "mrqa_squad-validation-2447", "mrqa_squad-validation-2462", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2580", "mrqa_squad-validation-2640", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2723", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-2797", "mrqa_squad-validation-282", "mrqa_squad-validation-2835", "mrqa_squad-validation-2848", "mrqa_squad-validation-2870", "mrqa_squad-validation-2873", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-30", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3048", "mrqa_squad-validation-3084", "mrqa_squad-validation-3086", "mrqa_squad-validation-3141", "mrqa_squad-validation-316", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3299", "mrqa_squad-validation-3304", "mrqa_squad-validation-3309", "mrqa_squad-validation-3319", "mrqa_squad-validation-3358", "mrqa_squad-validation-3368", "mrqa_squad-validation-3390", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3511", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3849", "mrqa_squad-validation-3932", "mrqa_squad-validation-3948", "mrqa_squad-validation-4032", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4165", "mrqa_squad-validation-4176", "mrqa_squad-validation-4186", "mrqa_squad-validation-4248", "mrqa_squad-validation-4265", "mrqa_squad-validation-4274", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4413", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4482", "mrqa_squad-validation-4488", "mrqa_squad-validation-4493", "mrqa_squad-validation-4562", "mrqa_squad-validation-4611", "mrqa_squad-validation-4623", "mrqa_squad-validation-4627", "mrqa_squad-validation-465", "mrqa_squad-validation-4698", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-4971", "mrqa_squad-validation-4976", "mrqa_squad-validation-501", "mrqa_squad-validation-506", "mrqa_squad-validation-5079", "mrqa_squad-validation-5113", "mrqa_squad-validation-5133", "mrqa_squad-validation-5150", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5214", "mrqa_squad-validation-5230", "mrqa_squad-validation-5295", "mrqa_squad-validation-5343", "mrqa_squad-validation-5355", "mrqa_squad-validation-5457", "mrqa_squad-validation-5478", "mrqa_squad-validation-5490", "mrqa_squad-validation-5499", "mrqa_squad-validation-55", "mrqa_squad-validation-5544", "mrqa_squad-validation-5563", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5642", "mrqa_squad-validation-5664", "mrqa_squad-validation-567", "mrqa_squad-validation-5698", "mrqa_squad-validation-5708", "mrqa_squad-validation-5762", "mrqa_squad-validation-5820", "mrqa_squad-validation-5835", "mrqa_squad-validation-586", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5978", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6008", "mrqa_squad-validation-6011", "mrqa_squad-validation-6079", "mrqa_squad-validation-6109", "mrqa_squad-validation-6124", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-616", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6223", "mrqa_squad-validation-6247", "mrqa_squad-validation-6267", "mrqa_squad-validation-6273", "mrqa_squad-validation-6284", "mrqa_squad-validation-6350", "mrqa_squad-validation-6362", "mrqa_squad-validation-6382", "mrqa_squad-validation-6421", "mrqa_squad-validation-6452", "mrqa_squad-validation-6475", "mrqa_squad-validation-6509", "mrqa_squad-validation-6535", "mrqa_squad-validation-6561", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6643", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6869", "mrqa_squad-validation-6879", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-7021", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7062", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7250", "mrqa_squad-validation-7306", "mrqa_squad-validation-7474", "mrqa_squad-validation-7521", "mrqa_squad-validation-7540", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7591", "mrqa_squad-validation-7592", "mrqa_squad-validation-7598", "mrqa_squad-validation-7653", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7733", "mrqa_squad-validation-7738", "mrqa_squad-validation-7751", "mrqa_squad-validation-7758", "mrqa_squad-validation-7775", "mrqa_squad-validation-778", "mrqa_squad-validation-7827", "mrqa_squad-validation-7842", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7937", "mrqa_squad-validation-7941", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8023", "mrqa_squad-validation-8028", "mrqa_squad-validation-8066", "mrqa_squad-validation-813", "mrqa_squad-validation-8132", "mrqa_squad-validation-8174", "mrqa_squad-validation-8213", "mrqa_squad-validation-8221", "mrqa_squad-validation-8222", "mrqa_squad-validation-824", "mrqa_squad-validation-8298", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8436", "mrqa_squad-validation-8446", "mrqa_squad-validation-8458", "mrqa_squad-validation-8466", "mrqa_squad-validation-8475", "mrqa_squad-validation-85", "mrqa_squad-validation-8505", "mrqa_squad-validation-8507", "mrqa_squad-validation-8533", "mrqa_squad-validation-8538", "mrqa_squad-validation-855", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8606", "mrqa_squad-validation-8636", "mrqa_squad-validation-8656", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8790", "mrqa_squad-validation-8790", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8836", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8880", "mrqa_squad-validation-890", "mrqa_squad-validation-8941", "mrqa_squad-validation-8960", "mrqa_squad-validation-8962", "mrqa_squad-validation-8978", "mrqa_squad-validation-9008", "mrqa_squad-validation-9101", "mrqa_squad-validation-9144", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9297", "mrqa_squad-validation-9308", "mrqa_squad-validation-9343", "mrqa_squad-validation-9388", "mrqa_squad-validation-9431", "mrqa_squad-validation-9470", "mrqa_squad-validation-9499", "mrqa_squad-validation-9567", "mrqa_squad-validation-9638", "mrqa_squad-validation-9661", "mrqa_squad-validation-9692", "mrqa_squad-validation-973", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9865", "mrqa_squad-validation-9912", "mrqa_squad-validation-9923", "mrqa_squad-validation-9935", "mrqa_squad-validation-9975", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-955"], "OKR": 0.908203125, "KG": 0.409375, "before_eval_results": {"predictions": ["France", "Turkey", "League of Augsburg", "Marburg Colloquy", "1951", "nearly three hundred years", "enter the priesthood", "32%", "Southwest Fresno", "receptions, gatherings or exhibition purposes", "marry secretly and keep quiet about the matter", "prime numbers", "a six membraned chloroplast", "Timucua people", "capturing prey", "the Presiding Officer", "1521", "5 million", "a supervisory church body", "the member state cannot enforce conflicting laws", "Ed Asner", "bitstrings", "\"Quiet Nights,\"", "Friday,", "\"momentous discovery\"", "\"We Found Love\"", "five dead bodies", "\"It was a comment that shouldn't have been made and certainly one that he wished he didn't make.\"", "Karl Kr\u00f8yer", "Austin Wuennenberg,", "133", "\"Barbarian Queen\" and \" Amazon Women on the Moon.\"", "\"Mad Men\"", "the area where the single-engine Cessna 206 went down", "water continues flow through the river channel and not spread out over land.", "series like \"Rent,\" \"Cabaret\" and \" Proof,\"", "Mutassim,", "Jennifer Arnold and husband Bill Klein,", "\"the incitement of sectarian hatred or involved in the acts of violence\"", "ambassadors", "\"the evidence and investigatory effort has minimized the likelihood that Haleigh's disappearance is the work of a strangers.\"", "the picturesque Gamla Vaster neighborhood", "17", "said she also told FBI agents Lisa's parents never mentioned anyone wanting to harm them.\"", "series \"Friends\" and Kristin Hahn, who was the executive producer of \"The Departed.\"", "make an emotional connection to their lost loved ones.", "a rambling news conference that he was having an affair with a woman in Argentina.", "the rape and murder of a 13-year-old girl and ending in a lynching.", "Fullerton, California,", "the BBC's central London offices", "a few months", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\"", "Arab Emirates", "Lashkar-e-Tayyiba", "July in the Philippines", "a U.S. military helicopter", "San Antonio", "February 6, 2005", "Falkland Islands", "water hardly ionizes,", "\"Lions for Lambs\"", "Carver Dana Andrews", "an improvement of 160 SAT points or 4 ACT points on your score,", "an American funk rock band"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5854948970985155}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.15999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 0.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.08333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.10526315789473682, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2587", "mrqa_squad-validation-9047", "mrqa_squad-validation-2468", "mrqa_squad-validation-4272", "mrqa_squad-validation-1676", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-2195", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-2268", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-3760", "mrqa_newsqa-validation-976", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-3405", "mrqa_naturalquestions-validation-1479", "mrqa_triviaqa-validation-5158", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-15716"], "SR": 0.515625, "CSR": 0.640625, "EFR": 0.9032258064516129, "Overall": 0.7293170362903225}, {"timecode": 11, "before_eval_results": {"predictions": ["manned lunar landings", "The Better Jacksonville Plan", "the perceived difficulty of its tune", "zero", "\"Hymn for the Weekend\"", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "\"Blue Harvest\" and \"420\"", "$105 billion", "Samarkand", "27", "an occupancy permit", "1774", "circuit switching", "TEU articles 4 and 5", "plastoglobulus", "Pakistan", "San Jose State", "the mouth and pharynx", "allowed local area networks to be established ad hoc", "erosion", "Kim Clijsters", "African National Congress Deputy President", "12-hour-plus shifts", "Kit of Elsinore", "28", "\"intense nervous shock and internal bleeding in the chest cavity.\"", "Kim Clijsters", "not feelMisty Cummings has told them everything she knows.", "Sub-Saharan Africa", "a shorter-range missile can be \"rolled out on a dime,\"", "the man was dead,", "Dr. Jennifer Arnold and husband Bill Klein,", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "North Korean Foreign Ministry spokesman described U.S. Vice President Dick Cheney as a \"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "his parents", "International Polo Club", "an antihistamine and an epinephrine auto-injector", "UNICEF", "Casey Anthony,", "a skilled hacker", "Leo Frank,", "the Niger Delta", "Cash for Clunkers", "Turkey", "the i report form", "the shipping industry", "41,280 pounds", "Natalie Cole", "not", "Fayetteville, North Carolina,", "25", "Rolling Stone", "shark River Park", "\"Stagecoach\"", "Chinese", "The Man", "ensure party discipline in a legislature", "the driver", "La Toya", "Lincoln Memorial University", "international producers", "the owls", "sack- cloth", "20 - year period"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6939119560994561}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.18181818181818182, 0.4, 0.4444444444444445, 0.0, 0.23076923076923078, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6248", "mrqa_squad-validation-4789", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-3862", "mrqa_triviaqa-validation-319", "mrqa_hotpotqa-validation-3136", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-2726"], "SR": 0.5625, "CSR": 0.6341145833333333, "EFR": 1.0, "Overall": 0.7473697916666666}, {"timecode": 12, "before_eval_results": {"predictions": ["green", "it was developed to explore alternatives to the early ARPANET design and to support network research generally", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "15 June 1899", "Ismail El Gizouli", "by department", "the Rhine-Ruhr region", "the trans-Atlantic wireless telecommunications facility", "a bachelor's degree", "two", "Death wish Coffee", "The French Protestant Church of London", "Edward Burne-Jones", "Francisco de Orellana", "\"missing self.\"", "Bruno Mars", "Sybilla of Normandy", "420,000", "plate tectonics", "the liver and kidneys", "InterContinental Hotels Group", "Walter Brennan", "Warren Hastings", "Poems : Series 1", "April 1917", "the New Testament", "Aristotle", "in a thousand years", "the first No. 1 seed to lose to a No. 16 seed since the field expanded to 64 teams in 1985", "2015", "April 2, 2018", "that men use violence within relationships to exercise power and control", "Zoe", "16 June", "the head of Lituya Bay in Alaska", "Thomas Jefferson", "the Indian Olympic Committee", "HTTP / 1.1", "Labour Party", "Roger Dean Stadium", "May 2002", "northern latitudes", "a ranking used in combat sports", "the portal tomb", "the team", "mainland greece", "159", "Missouri River", "Brazil", "318", "Kim Basinger", "Domhnall Gleeson", "the chest, back, shoulders, torso and / or legs", "the skin", "religious Hindu musical theatre styles", "andrew jackson", "duck", "Super Junior's", "Nanyue", "18", "Iran", "hockey", "the New York City Russian-Jewish community for the setting and characters", "drew their"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6452328943701227}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.24000000000000002, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4786", "mrqa_squad-validation-1384", "mrqa_squad-validation-5422", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-976", "mrqa_triviaqa-validation-6508", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-4596", "mrqa_newsqa-validation-3741", "mrqa_searchqa-validation-3679", "mrqa_searchqa-validation-9108"], "SR": 0.578125, "CSR": 0.6298076923076923, "EFR": 0.9629629629629629, "Overall": 0.739101006054131}, {"timecode": 13, "before_eval_results": {"predictions": ["the Ominde Commission", "2,000", "2005", "the university's off-campus rental policies", "seven", "Thomas Edison and George Westinghouse", "southern Suriname", "dispensing substandard products", "automobiles", "landed on the Moon", "Cam Newton", "KGPE", "the difference in potential energy between two different locations in space", "prime elements", "Julia Butterfly Hill", "five", "Surficial geology", "ViennaVienna", "40", "Sir John Nott", "Spice Girls", "the hose", "Sandi Toksvig,", "Salvador Allende", "Paris", "Arkansas", "\"The Blind Side\"", "hyposmia", "Dennis Potter", "Burma", "peregrines", "poblano", "pochard", "MauritaniaMauritania", "a Chopin's Prelude No. 4 in E minor,", "Hudson Hawk", "Bill Clinton's", "Charlie Sheen", "a full fat pasteurised cow's milk soft cheese made by Charles Martell & Son", "sheep", "Chile's", "the Republic of Upper Volta", "Laurie Lee", "Karl Marx and Friedrich Engels", "John Mortimer", "Beaujolais", "Humphrey Bogart", "corey piedis", "Kansas", "Amy", "Carl Sagan and his wife and co-writer, Ann Druyan", "Alex Turner", "a St. Tropez drag-show nightclub owned by Georges", "commitment", "Paul Monti, whose son, Medal of Honor recipient Jared, was killed in Afghanistan while trying to save a fellow soldier", "2001 -- 2002 season", "\"Back to December\"", "Michael Lewis Greenwell", "the remaining rebel strongholds in the north of Sri Lanka,", "a pool of blood beneath his head", "(Prince)`Attab", "a doctor that specializes in", "an Academy Award in the category Best Sound for the film \"Under Siege\"", "International Boxing Federation"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6281892586580087}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 0.4, 0.8571428571428571, 0.0, 0.0, 0.0, 0.33333333333333337, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-6342", "mrqa_squad-validation-3899", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-546", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-4630", "mrqa_naturalquestions-validation-9499", "mrqa_hotpotqa-validation-3507", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-530", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-14224", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-47"], "SR": 0.578125, "CSR": 0.6261160714285714, "EFR": 1.0, "Overall": 0.7457700892857142}, {"timecode": 14, "before_eval_results": {"predictions": ["Iberia", "that contemporary accounts were exaggerations", "as an auditor", "Victoria", "erosion", "Cricket", "2", "Gabriel Zwilling", "\"The Day of the Doctor\"", "through confirmation", "in the city of Deabolis", "Wijk bij Duurstede", "June 6, 1951", "24 September 2007", "18", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "Teri Garr", "Malayalam", "lamb", "for the red - bed country of its watershed", "DNA was the genetic material", "Steve Russell", "Africa", "The first Twenty20 match held at Lord's, on 15 July 2004 between Middlesex and Surrey", "Dalveer Bhandari", "boy", "carl sagan", "Michael Jackson and Lionel Richie", "6 March 1983", "\"One day, while listening to what seems to be a crossed telephone connection, she hears two men planning a woman's murder", "Glenn Close", "Andreas Vesalius", "1959", "in a forest", "in the year 2026", "in a fifty - year - old woman he called Auguste D.", "Uttar Pradesh", "electors", "1834", "Indian Standard Time", "New York University", "song rose to number 4 on Billboard's Pop Singles chart and number one for two weeks on the R&B Singles charts on August 14 through to August 27, 1971", "Sophia Akuffo", "Harry", "2", "April 29, 2009", "regulatory", "ball is fed into the gap between the two forward packs", "\"s surprise attack on Pearl Harbor the prior day", "Wembley Stadium", "1992", "Jonathan Cheban", "blue", "the # 4 School of Public Health in the country", "concepc", "Arabah", "May 27, 2016", "frigate", "\"AAAARRRRGGG", "Lonnie", "duck", "carlicity", "Lake Placid, New York", "Russell T Davies"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5506076388888889}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4921", "mrqa_squad-validation-10078", "mrqa_squad-validation-1076", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-3977", "mrqa_newsqa-validation-1372", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-11598", "mrqa_hotpotqa-validation-2357"], "SR": 0.46875, "CSR": 0.615625, "EFR": 0.9411764705882353, "Overall": 0.731907169117647}, {"timecode": 15, "before_eval_results": {"predictions": ["The Writers Guild of America", "every five years,", "NFIL3", "826", "Treaty of Logstown", "mid-Cambrian", "Cabot Science Library, Lamont Library, and Widener Library", "Palestine", "PNU and ODM camps", "The waxy cuticle of many leaves,", "2,200", "KMBC-TV and KQTV", "whether he stood by their contents", "cloven hoof, cleft, as in a cleft palate or cleaved.", "Hudson River", "relationship between two people or groups that work with and depend on each other", "a doll", "The Physical Basis of Long-Range Weather Forecasts", "Southern elephant Seal", "Sally Field", "1908-1993", "Nautilus", "ruby red slippers", "sports that whites are no good at", "Agamemnon", "jedoublen/jeopardy", "to raise money for the Muscular Dystrophy Association (MDA)", "japan", "Boeing", "Saskatchewan", "Larry Bird", "fortress on top of a mountain plateau called Masada", "undercard", "Willa Cather", "a piece of furniture", "Keith Urban", "French", "deer", "jedoublen/jeopardy", "drowsiness", "change to a boat", "parrots, gorillas, and tarantulas", "Rick Springfield", "The Anti-Lebanon Range", "70% isopropyl alcohol", "playground", "Custer", "phantom limb syndrome", "kabbalah", "palomino", "piedmont glacier", "French Lgion d' Honneur", "Lohengrin", "Rent", "Camping World Stadium in Orlando, Florida", "John Cooper Clarke", "President Woodrow Wilson", "Nicola Adams", "1943", "Pearl Jam", "\"It is not acceptable.", "there is not a process to ensure that auto owners comply with recalls.", "two", "16\u201321"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5750311609686609}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14814814814814817, 0.15384615384615385, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4802", "mrqa_squad-validation-6435", "mrqa_searchqa-validation-12940", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-12272", "mrqa_searchqa-validation-9570", "mrqa_searchqa-validation-8796", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-10319", "mrqa_searchqa-validation-4536", "mrqa_searchqa-validation-1446", "mrqa_searchqa-validation-14247", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-1624", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-2200", "mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-14789", "mrqa_hotpotqa-validation-1238", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2358", "mrqa_hotpotqa-validation-5438"], "SR": 0.484375, "CSR": 0.607421875, "EFR": 1.0, "Overall": 0.7420312499999999}, {"timecode": 16, "before_eval_results": {"predictions": ["the Church of England", "Lenin", "a qualified majority vote", "36 cameras", "Brough Park in Byker", "2012", "Stress", "quantum mechanics", "Jonathan Stewart", "George Westinghouse", "human", "significantly increased British military resources in the colonies", "Aquitaine", "the Lord of the Rings", "Florida", "national forests", "the Netherlands", "Marriott International", "drink wine", "mask", "(J. Bullock)", "the Sons of Liberty", "movie house", "National Security Agency", "Ugly Betty", "the Devil", "the Key deer", "the All-New Blue Ribbon Cookbook", "flowers", "Pheonix", "Amy Fisher", "A Portrait of the Artist as a Young Man", "an acrobat", "guttural", "polio", "Meg Tilly", "the Mausoleum", "George III", "Annie Braddock", "(EX)TRA)", "the Firmament", "dark places", "Maharaja", "Wendy Beckett", "Ferris B Mueller's", "the flyer", "(Richard) Rodgers", "CNN", "Samuel Goldwyn", "Annika Sorenstam", "(Asparagus)", "Thurman Munson", "Washington", "anthropologie", "the International Border", "1783", "chilis", "Ray Robinson", "McComb, Mississippi", "Bea Arthur", "Tutsi ethnic minority", "last week.", "18", "\"Twilight\" book series"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6317708333333333}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4216", "mrqa_squad-validation-586", "mrqa_squad-validation-5456", "mrqa_squad-validation-10388", "mrqa_squad-validation-10158", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-1961", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-10263", "mrqa_searchqa-validation-11531", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-16832", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-366", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-9957", "mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-2405", "mrqa_naturalquestions-validation-1169", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-2627", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-1853"], "SR": 0.46875, "CSR": 0.5992647058823529, "EFR": 1.0, "Overall": 0.7403998161764705}, {"timecode": 17, "before_eval_results": {"predictions": ["49\u201315", "113", "protein structure prediction", "Deformational events", "August 2004", "Department of State Affairs", "Prague", "attempted to enter the test site knowing that they faced arrest", "in these schools the preservation of public order is easier and more efficient than anywhere else", "M\u00f6ngke Khan", "by using net wealth (adding up assets and subtracting debts),", "Tiger Woods", "snow", "the Marines", "Romeo and Juliet", "Jane Addams", "Rand McNally & Company", "President Harry S. Truman", "the pound sterling", "Auguste Rodin", "the Andes", "Sherlock Holmes", "the Taj Mittal", "trampolining", "a axe", "a rice", "Constantine", "Daniel Inouye", "krypton", "a Buddhist monastery", "Kung Fu", "glitter", "a cocktail", "The GNTCE", "Milwaukee", "silver", "Bangkok", "the Soviet Union", "a Scotch", "a young Belfast-born actor and director", "a known quantity", "Frank Sinatra", "the Columbus brothers", "the King of the Hill", "a spoiled, wealthy woman who joins the US", "Stephen King", "Lord Byron", "Japan", "Joan of Arc", "Jaguar", "the Oompa-Loompas", "a hound", "R Stanton Avery", "at U.S. Bank Stadium in Minneapolis, Minnesota", "The Golden Gate Bridge", "Jim hacker", "Jim Branning", "May 10, 1976", "(IATA: VNO, ICAO: EYVI)", "fluoroquinolones", "murder", "Parlophone Records", "once", "Nazi concentration camps"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6324652777777777}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6702", "mrqa_squad-validation-1828", "mrqa_squad-validation-7554", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-7825", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-10022", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-12714", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-2546", "mrqa_searchqa-validation-4402", "mrqa_naturalquestions-validation-5674", "mrqa_hotpotqa-validation-3728", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-57", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-5499"], "SR": 0.515625, "CSR": 0.5946180555555556, "EFR": 1.0, "Overall": 0.739470486111111}, {"timecode": 18, "before_eval_results": {"predictions": ["hunter's garb", "from the official declaration of war in 1756 to the signing of the peace treaty in 1763", "two forces", "a computational problem", "up to three-fourths of the population of the Iranian Plateau, possibly 10 to 15 million people", "the number of foot-pounds of work delivered by burning one bushel (94 pounds) of coal", "Establishing \"natural borders\"", "at his Houston Street lab", "Excellent job opportunities", "\"Turks\" (Muslims) and Catholics.", "an United States Internal Revenue Service form that provides the public with financial information about a nonprofit organization", "the Northeast Monsoon", "April 3, 1973", "Fa Ze", "July 14, 1969", "Krypton", "Mandarin", "in the bone marrow", "Ukraine", "Coldplay", "Yugoslavia", "head coach", "May 19, 2017", "T - Bone Walker", "April 2, 2018", "Rose Stagg ( Valene Kane )", "Doug Diemoz", "Iran", "in southern Anatolia", "classical architecture", "the pyloric valve", "1546", "100,000 writes", "the cella of the Parthenon", "16 seasons", "Long Island", "lacteal", "the Beldam / Other Mother", "1987", "Elliot Scheiner", "in Kent ( which produces Kent Goldings hops ), Herefordshire, and Worcestershire", "Jikji", "Panning", "the RAF", "Pepsi", "Detective Superintendent Dave Kelly", "Isabela Moner", "Ethel Merman", "provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "New Orleans", "The Outback", "an unmasked and redeemed Anakin Skywalker", "Russell Huxtable", "Namibia", "an egg", "all-time leader in total passing yards, touchdowns, and completions", "Rawlings", "23", "Los Angeles", "North by Northwest", "Naples", "Loon", "Edward VI", "Lutherhaven"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5768631415728097}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.962962962962963, 0.6666666666666666, 1.0, 0.5263157894736842, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.6956521739130436, 0.5714285714285715, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10121", "mrqa_squad-validation-10395", "mrqa_squad-validation-6251", "mrqa_squad-validation-3300", "mrqa_squad-validation-1482", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-3727", "mrqa_hotpotqa-validation-466", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-9438"], "SR": 0.453125, "CSR": 0.587171052631579, "EFR": 0.9714285714285714, "Overall": 0.73226679981203}, {"timecode": 19, "before_eval_results": {"predictions": ["Thomas Vasey and Richard Whatcoat", "reminding their countrymen of injustice", "sex offenders register", "Republic of Kenya", "violence that subsequently engulfed the country", "aristocracy", "1905", "Saul Alinsky", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "division", "February 1834", "Jonathan Goldstein", "Yahya Khan", "potatoes", "BC Jean and Toby Gad", "Nacio Herb Brown ( music ) and Arthur Freed", "Anatomy", "Pyeongchang County, Gangwon Province, South Korea", "Jacqueline MacInnes Wood", "Melissa Ringwald", "after resting starters for the final two games, the 13 -- 3 Eagles soared past the Minnesota Vikings and the Atlanta Falcons in the playoffs", "a child with Treacher Collins syndrome trying to fit in", "Andaman and Nicobar Islands -- Port Blair -- Chandigarh   Dadra and Nagar Haveli -- Silvassa   Lakshwadweep -- Kavaratti", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals ), blood plasma and lymph in the`` intravascular compartment ''", "member states on a voluntary basis", "in the United Kingdom", "Rocinante", "1978", "May 1, 2018", "Hans Christian Andersen", "painting", "Cheitharol Kummaba", "1", "alveolar process", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F ) at Vostok Station", "October 14, 2017", "16 for females and 18 for males", "July 21, 1861", "a candidate state must be a free market democracy", "45 %", "in the bible", "bypasses", "Sally Dworsky", "Robert E. Lee", "Clarence L. Tinker", "Soviet Russia defaulted on all of Imperial Russia's commitments to the Triple Entente alliance", "the church at Philippi", "federal republic", "Thomas Edison", "mitosis", "an oxidant, usually atmospheric oxygen, that produces oxidized, often gaseous products, in a mixture termed as smoke", "pit road speed during the warm - up laps", "Pyeongchang County, Gangwon Province, South Korea", "a loop", "Gianni Versace", "David Simon", "1999", "Madrid's Barajas International Airport", "Seoul,", "a chalk", "serving the tea", "three people", "the Dalai Lama", "Christopher Savoie"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6496412858539165}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6, 0.43750000000000006, 0.8333333333333333, 0.8, 1.0, 0.0, 1.0, 0.35294117647058826, 0.0, 0.0, 1.0, 1.0, 0.8421052631578948, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.4615384615384615, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9640", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-10617", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-5984", "mrqa_triviaqa-validation-2196", "mrqa_newsqa-validation-649", "mrqa_searchqa-validation-10274", "mrqa_searchqa-validation-2516", "mrqa_newsqa-validation-1535"], "SR": 0.515625, "CSR": 0.58359375, "EFR": 0.967741935483871, "Overall": 0.7308140120967741}, {"timecode": 20, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1705", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4329", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-466", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-12", "mrqa_naturalquestions-validation-1436", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3376", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3950", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9809", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3578", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10708", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-11360", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11598", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14247", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16636", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1924", "mrqa_searchqa-validation-1928", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2887", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-3899", "mrqa_searchqa-validation-4133", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4750", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-494", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6233", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9561", "mrqa_searchqa-validation-9570", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10158", "mrqa_squad-validation-10162", "mrqa_squad-validation-10198", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10471", "mrqa_squad-validation-1076", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1188", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1330", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1424", "mrqa_squad-validation-1506", "mrqa_squad-validation-1540", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1611", "mrqa_squad-validation-1703", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1834", "mrqa_squad-validation-1908", "mrqa_squad-validation-1976", "mrqa_squad-validation-2015", "mrqa_squad-validation-2025", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2111", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2250", "mrqa_squad-validation-2395", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2532", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-3001", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3193", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-331", "mrqa_squad-validation-3368", "mrqa_squad-validation-3449", "mrqa_squad-validation-3493", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3626", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3948", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4159", "mrqa_squad-validation-4176", "mrqa_squad-validation-4248", "mrqa_squad-validation-4248", "mrqa_squad-validation-4272", "mrqa_squad-validation-4274", "mrqa_squad-validation-4301", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4623", "mrqa_squad-validation-4686", "mrqa_squad-validation-4698", "mrqa_squad-validation-4765", "mrqa_squad-validation-4789", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-501", "mrqa_squad-validation-5133", "mrqa_squad-validation-5157", "mrqa_squad-validation-5214", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-55", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5664", "mrqa_squad-validation-5715", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5897", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6251", "mrqa_squad-validation-6253", "mrqa_squad-validation-6264", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6435", "mrqa_squad-validation-6452", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7191", "mrqa_squad-validation-7226", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7592", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7751", "mrqa_squad-validation-7775", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7889", "mrqa_squad-validation-7932", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8010", "mrqa_squad-validation-8019", "mrqa_squad-validation-8199", "mrqa_squad-validation-8213", "mrqa_squad-validation-826", "mrqa_squad-validation-8278", "mrqa_squad-validation-8298", "mrqa_squad-validation-830", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8383", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-861", "mrqa_squad-validation-8612", "mrqa_squad-validation-8636", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8786", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9308", "mrqa_squad-validation-9315", "mrqa_squad-validation-9322", "mrqa_squad-validation-9388", "mrqa_squad-validation-9405", "mrqa_squad-validation-9431", "mrqa_squad-validation-9495", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9640", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9865", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1646", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5140", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6531", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-88"], "OKR": 0.8515625, "KG": 0.46015625, "before_eval_results": {"predictions": ["the dukes", "saw the Muslim faith as a tool of the devil,", "Ed Lee", "40%", "post-World War I", "8000", "can produce both eggs and sperm at the same time.", "\u00d6gedei Khan", "June 24, 1935", "Kohlberg K Travis Roberts", "John McClane", "Selected Writings by Steve Biko", "1910s", "between 11 or 13 and 18", "Deepak Tijori", "Let's Make Sure We Kiss Goodbye", "Martin McCann", "an American astronaut, naval aviator, test pilot, and businessman.", "Donald Trump's presidential campaign team", "December 17, 1974", "\"The Royal Family\"", "President of Botswana", "Mike Holmgren", "S7", "T. E. Lawrence", "Don DeLillo", "the Oakland Raiders", "a co-op of grape growers", "South America", "five", "Heart", "Bank of China", "Harry Meadows", "sexual attraction, or sexual behavior toward both males and females, or romantic or sexual attraction to people of any sex or gender identity", "\"Winnie the Pooh\"", "Excalibur Hotel and Casino", "Rigoletto", "Knoxville, Tennessee", "Americana Manhasset", "Omega SA", "1978", "Pim Fortuyn", "eastern shore", "Todd McFarlane", "M. Night Shyamalan", "Magdalen College", "Eric Allan Kramer", "1969 in New York City, United States", "Province of New York", "royal palace", "Jamaica", "Michael Seater", "1933", "Human expression meaning `` mind your manners ''", "Charles Darwin", "Brisbane Road", "Katherine Parr", "his past and his future", "British", "Engelbert Humperdinck", "the Colorado River", "John Denver", "The Princess Bride", "a republic in W Africa"], "metric_results": {"EM": 0.5, "QA-F1": 0.562077067669173}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.3157894736842105, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2293", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-498", "mrqa_hotpotqa-validation-891", "mrqa_naturalquestions-validation-4109", "mrqa_newsqa-validation-3889", "mrqa_triviaqa-validation-6564"], "SR": 0.5, "CSR": 0.5796130952380952, "EFR": 1.0, "Overall": 0.724360119047619}, {"timecode": 21, "before_eval_results": {"predictions": ["punt", "ended the true Islamic system, something for which it blames \"the disbelieving (Kafir) colonial powers\" working through Turkish modernist Mustafa Kemal Atat\u00fcrk.", "pharmacy practice science and applied information science", "thought it may have been a combination of anthrax and other pandemics.", "1912", "rubisco", "the Huguenot rebellions", "Psych", "German", "August 11, 1946", "Protestant Christian", "Queens, New York", "Erreway", "Oakland County", "Madeleine L'Engle", "FAI Junior Cup", "1966 US tour", "February 18, 1965", "Sydney", "Cuban descent", "1951", "Tricia Helfer", "Mickey's PhilharMagic", "Eielson Air Force Base", "Taylor Swift", "Italy", "Tel Aviv University", "the Donny & Marie Showroom, at the Flamingo Las Vegas", "John of Gaunt", "William Clark Gable", "Kmart", "C. J. Cherryh", "Spanish professional footballer who plays as a striker for La Liga club Atl\u00e9tico Madrid and the Spain national team.", "51", "Axl Rose", "Daniil Shafran", "Soha Ali Khan Khemu", "2001", "four", "13 October 1958", "70", "3 May 1958", "The 2007 Trail Appliances Autumn Gold Curling Classic", "Vancouver", "Marlborough", "Fountains of Wayne", "Ealdorman of Devon", "sulfur mustard H or HD blister gas", "The Saturdays", "the Chick tract of the same name", "southwestern", "The Campbell Soup Company", "The Gang", "April 2010", "The vascular cambium", "A", "dark, spicy", "Rod Blagojevich", "Iraq", "bhakti", "Harley-Davidson", "75", "$60 billion", "feels a nose,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5596656424321865}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.5185185185185185, 1.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 0.28571428571428575, 0.0, 0.0, 0.33333333333333337, 0.1111111111111111, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.33333333333333337, 1.0, 0.4, 1.0, 0.25, 0.0, 1.0, 1.0, 0.28571428571428575, 0.4444444444444445, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.13333333333333336]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_squad-validation-9918", "mrqa_squad-validation-5029", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-4304", "mrqa_hotpotqa-validation-1287", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-51", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-5300", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2301", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1784", "mrqa_searchqa-validation-7453", "mrqa_searchqa-validation-2193", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-1093"], "SR": 0.4375, "CSR": 0.5731534090909092, "EFR": 1.0, "Overall": 0.7230681818181818}, {"timecode": 22, "before_eval_results": {"predictions": ["one of the daughters of former King of Thebes, Oedipus", "environmental determinism", "WBT-FM (99.3 FM)", "Articles 106 and 107", "corpses", "Cobb Lecture Hall", "Cortina d'Ampezzo", "Eric Edward Whitacre", "1983", "Pittsburgh, Pennsylvania", "Ruth Westheimer", "Giotto", "Nickelodeon", "Brad Wilk", "Bobby Hurley", "11", "PEN America: A Journal for Writers and Readers", "March 19, 2017", "Disney California Adventure", "Anah\u00ed Giovanna Puente Portilla de Velasco", "Nicholas John \"Nic\" Cester", "Anne Elizabeth Alice Louise", "Harry Robbins \"Bob\" Haldeman", "247,597", "deities, beings, and heroes", "directed several episodes of the popular sitcom \"Friends\"", "40 million", "Billund, Denmark", "Africa", "William Cavendish, 7th Duke of Devonshire", "from 20 March to 1 May 2003", "born 20 May 1973", "Kinnairdy Castle", "Ken Rutherford", "\"The Catcher in the Rye\"", "Indianapolis Motor Speedway", "Marjorie Jacqueline \"Marge\" Simpson", "1926 April 1972", "Transporter 3", "Richard Street", "\"Queen In-hyun's Man\"", "Steve Carell", "Green Chair", "The Frog Prince", "1st Baron Dowding", "German", "November 10, 2017", "CTV Television Network", "Australian", "Rafael Palmeiro", "Eric Allan Kramer", "\"Orchard County\"", "raise the relative humidity to 100 % and create clouds and, under the right conditions, precipitation", "Reproductive system", "Coroebus of Elis", "\u201cMy Favorite Martian,\u201d", "Benjamin Disraeli", "red", "Sunday,", "5 1/2-year-old", "one day,", "an orchestra", "the Supreme Court", "Pledge of Allegiance"], "metric_results": {"EM": 0.46875, "QA-F1": 0.628783195970696}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true], "QA-F1": [0.42857142857142855, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.5714285714285715, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4444444444444445, 1.0, 1.0, 0.2857142857142857, 1.0, 0.3333333333333333, 0.0, 1.0, 0.888888888888889, 0.2, 1.0, 0.0, 0.6666666666666666, 1.0, 0.923076923076923, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6640", "mrqa_squad-validation-688", "mrqa_squad-validation-4772", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5636", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-5662", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3629", "mrqa_naturalquestions-validation-581", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-4189", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-13691"], "SR": 0.46875, "CSR": 0.5686141304347826, "EFR": 1.0, "Overall": 0.7221603260869565}, {"timecode": 23, "before_eval_results": {"predictions": ["Article 17(3)", "Great Khan", "Department of Justice", "tidal currents", "returned the ball 19 yards to the Panthers 39-yard line with 1:55 left on the clock.", "The Eleventh Doctor", "Malware", "Claims adjuster", "late to mid-2000s", "Caleb", "Arunachal Pradesh", "1998", "Richard of Shrewsbury", "the Roman Empire", "1985", "Chaka Khan", "18", "to form a higher alkane", "Percy Jackson", "Gil", "final scene of the fourth season", "UMBC", "the customer's account", "two reservoirs in the eastern Catskill Mountains", "Elena Anaya", "January 2018", "a prison", "a cake", "Woodrow Wilson", "Commander in Chief of the United States Armed Forces", "Waylon Jennings", "the Italian / Venetian John Cabot", "the Boston Red Sox", "protect the genome", "Turducken", "a balance sheet", "San Francisco Bay", "Mickey Mantle", "the ACU", "1956", "around 2011", "Toot - Toot", "2014 Olympic Games in Sochi, Krasnodar Krai, Russia", "David Gahan", "the town of Acolman", "South Asia", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "turkey", "note number 60", "New England Patriots", "Tristan Rogers", "novella", "The Brady Bunch", "Oliver Stone", "Mexico", "Apsley George Benet Cherry-Garrard", "the Ecumenical Award", "Brig Gen Augustine Warner Robins", "Florida", "Department of Homeland Security Secretary Janet Napolitano", "Kerstin Fritzl", "Colorado", "gusto", "Ulysses"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6145368500201349}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.5, 0.0, 1.0, 0.6976744186046512, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8230", "mrqa_squad-validation-818", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-10586", "mrqa_newsqa-validation-4156"], "SR": 0.546875, "CSR": 0.5677083333333333, "EFR": 0.9655172413793104, "Overall": 0.7150826149425287}, {"timecode": 24, "before_eval_results": {"predictions": ["the first two series", "Doctor of Theology", "c1750", "60%", "a deterministic Turing machine", "Henry III of England", "Henkel", "Bowie", "Handel", "Pol Pot", "the Pyrenees mountains", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Motel 6", "a restaurant in New York\u2019s Greenwich Village", "the Old Dominion", "Isaac", "a crystal ball", "Satire", "the gallbladder", "Thabo Mbeki", "1921", "a dog", "Robert Schumann", "The Benedictine Order", "(Ernie Els)", "translator", "King County Executive", "Scotland", "The Penguin", "The Sahara Desert", "Mata Hari", "rings", "the Brisbane River", "The Aidensfield Arms", "armada", "Liechtenstein", "the Rolling Stones", "Rodney", "Simpson", "the UK", "Prokofiev", "horses", "\"Stutter Rap (No Sleep til Bedtime)\"", "The State of Kansas", "Australia", "the Free Dictionary", "\"Little Red Rented Rowboat\"", "smell", "Jesse of Bethlehem", "Miss Daisy", "the Isles of the Blessed", "Anthropocene", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Darren McGavin", "Sir Henry Cole", "Scottish", "Sarah Hurst", "1964", "Manchester United", "Jason Chaffetz", "the former Libyan leader, Gadhafi's son, Mutassim, and his former defense minister, Abu Baker Yunis.", "analog watch", "Tootsie", "Alexander Haig Jr."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6016369047619048}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.3333333333333333, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-2126", "mrqa_squad-validation-1819", "mrqa_triviaqa-validation-5569", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-1702", "mrqa_naturalquestions-validation-3189", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-1053", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-2385", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-1228"], "SR": 0.515625, "CSR": 0.565625, "EFR": 0.9032258064516129, "Overall": 0.7022076612903225}, {"timecode": 25, "before_eval_results": {"predictions": ["The next architect to work at the museum was Colonel (later Major General) Henry Young Darracott Scott, also of the Royal Engineers", "floor function", "QuickBooks", "Westinghouse Electric", "The Bachelor", "statue", "2018", "Auburn Tigers football team", "Honor\u00e9 Mirabeau", "England and Wales", "Charles Habib Malik, Lebanon", "radioisotope thermoelectric generator", "1908", "honey bees", "Samantha Jo `` Mandy '' Moore ( born March 28, 1976 in St. Louis, Missouri and raised in Breckenridge, Colorado )", "Edward Douglass White, Charles Evans Hughes, Harlan Fiske Stone, and William Rehnquist", "Mangal Pandey", "Woodrow Strode", "Woodrow Wilson, with the notable exception of Herbert Hoover, has made at least one State of the Union report as a speech delivered before a joint session of Congress", "four of the 50 states of the United States in their full official state names", "tissues in the vicinity of the nose", "season two", "January 17, 1899", "Copernicus, Galileo and Kepler", "left Behind", "British Ultra code - breaking intelligence", "602", "inverted", "5.7 million customer accounts", "Vincent Price", "Steve Russell", "Janis Joplin", "1871", "September 9, 2012", "Ahmad ( Real ) selected Doll, while Kamal ( Chance ) selected Hot Wings", "Haiti", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "October 2017", "2026", "2018", "The Outback", "Deputy Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha", "2004", "Australia", "eleven", "Master Christopher Jones", "Around 1200", "12.65 m ( 41.50 ft ) long", "RMS Titanic ( / ta\u026a\u02c8t\u00e6n\u026ak / )", "2017", "in the Tremont neighborhood of Cleveland, Ohio", "( 1927, 1934, 1938, 1956 )", "Italy", "music (to be performed) in a fiery manner", "Culture Club", "These Are Special Times", "Indian", "Sofia the First", "software magnate", "Hurricane Gustav", "21", "Joseph Heller", "drake", "uranium"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5958002645502645}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true], "QA-F1": [0.5925925925925926, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.5, 0.8571428571428571, 0.0, 1.0, 0.0, 0.38095238095238093, 0.0, 0.4, 1.0, 0.14814814814814814, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.8, 0.0, 0.2, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-8004", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-734", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-6935", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7128", "mrqa_hotpotqa-validation-3423", "mrqa_hotpotqa-validation-3842", "mrqa_newsqa-validation-1234", "mrqa_searchqa-validation-3483", "mrqa_searchqa-validation-9088"], "SR": 0.46875, "CSR": 0.5618990384615384, "EFR": 1.0, "Overall": 0.7208173076923077}, {"timecode": 26, "before_eval_results": {"predictions": ["Saudi Arabia", "Thomas Commerford Martin", "seven", "labor inputs (workers)", "1996", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Raja Dhilu", "John Quincy Adams", "third season", "Christopher Columbus", "a child with Treacher Collins syndrome trying to fit in", "an intensive week - long initiation process in which the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "President of the United States", "administrative supervision over all courts and the personnel thereof", "in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form, he made the hair more `` wild '' and covered Frieza's body in red fur", "Mandy", "king Harold Godwinson", "during meiosis", "the Miracles", "Spanish / Basque origin", "in early evenings to call ( in spring and summer ) and hunt for food", "not being pushed around by big labels, managers, and agents and being true to yourself creatively", "January 2, 1971", "Hirschman", "in the Hebrew Bible, in the books of Exodus and Deuteronomy", "Lucknow", "Neuropsychology", "The User State Migration Tool", "Michelle", "May 1, 2018", "291 episodes in Japan", "the Naturalization Act of 1790", "flawed democracy", "in Pashto and Persian as \u0647\u0646\u062f\u0648\u06a9\u0634 \u202c", "last Ice Age", "Ren\u00e9 Descartes", "in people and animals that collects and stores urine from the kidneys before disposal by urination", "The First Battle of Bull Run", "oj simpson", "in The Force Awakens ( 2015", "Germany", "in London, United Kingdom", "Jeff East", "President Yahya Khan", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "6th century AD", "Arnold Schoenberg", "3.5 million years old from Idaho, USA", "111", "when a population temporarily exceeds the long term carrying capacity of its environment", "Mike Leeson and Peter Vale", "amount to a crime and deserve punishment", "Joan Crawford", "Principality of Andorra", "Ned Sherrin", "Band-e Amir National Park", "Cartoon Network", "Chrysler", "in a firefight Friday in Afghanistan,", "more than 100", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Roanoke", "dancer", "apples"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6791351669105277}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.08, 0.8205128205128205, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.8571428571428571, 1.0, 0.8, 0.0, 0.3333333333333333, 1.0, 0.7272727272727273, 1.0, 1.0, 0.5714285714285715, 0.3870967741935484, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.4210526315789474, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3771", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-9812", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-4399", "mrqa_naturalquestions-validation-1694", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-7787"], "SR": 0.53125, "CSR": 0.5607638888888888, "EFR": 0.9666666666666667, "Overall": 0.7139236111111111}, {"timecode": 27, "before_eval_results": {"predictions": ["eight days after their initial broadcast", "after sustaining an injury which would be fatal to most other species", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish.", "direct repeats", "Naples", "black, red or white,", "2009", "the Cowardly Lion", "Diego Maradona", "Harkat-ul-Jihad al-Islami ( HuJi)", "\"I'm certainly not nearly as good of a speaker as he is.\"", "\"I don't watch TV,\"", "golf", "Acura", "Floxin", "Copts", "February 12 when the opening ceremony of the 2010 Winter Games will be held at the domed BC Place Stadium.", "Roberto Micheletti,", "in the last few months, where I think people are really doing it to get by,\"", "Shiza Shahid,", "Akio Toyoda", "was arrested in a federal sting after his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "African-Americans", "environmental and political events", "trying to detonate an explosive device in his underwear aboard a Christmas 2009 flight to Detroit,", "three empty vodka bottles,", "Euna Lee,", "Angela Merkel", "Ku Klux Klan", "some dental work done, including removal of his diamond-studded braces.", "Manuel Mejia Munera", "requires police to question people if there's reason to suspect they're in the United States illegally.", "UNICEF", "club managers,", "used", "Sgt. Jason Bendett of the 3rd Platoon, A Company, 2nd Light armored Reconnaissance Battalion, based at Lejeune.", "\"I'm certainly not nearly as good of a speaker as he is.\"", "suicides", "$15 billion in 2008 and is projected to grow by 10 percent, according to PricewaterhouseCoopers.", "Karen Floyd", "Mary Phagan,", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "Cogentin and Haldol,", "$2 billion", "Both men were hospitalized and expected to survive, according to David Peterka, who was part of the film crew, but was not aboard the plane.", "Krishna Rajaram,", "\"The flooding was so fast that the thing flipped over,\"", "in the 57th-minute when striker Milito collected a pass from fellow-Argentine Javier Zanetti, before firing home a shot with the outside of his right foot.", "Dubai", "up three of the last four months.", "Sunday,", "12-hour-plus", "three times", "Canada", "South Dakota", "h Homo sapiens", "Potassium", "Something In The Air", "March 1987", "Peter Kay's Car Share", "Cyclic Defrost", "pfeffernuesse", "the furrow", "14"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5628483290610806}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, true, false, true], "QA-F1": [0.2857142857142857, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.21052631578947367, 1.0, 0.0, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.2162162162162162, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.42857142857142855, 0.07692307692307691, 1.0, 1.0, 0.0, 0.33333333333333337, 0.19999999999999998, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.0, 1.0, 0.08333333333333333, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5843", "mrqa_squad-validation-4648", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-223", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2325", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-2456", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7659", "mrqa_triviaqa-validation-2373", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-1223", "mrqa_searchqa-validation-2197"], "SR": 0.484375, "CSR": 0.5580357142857143, "EFR": 1.0, "Overall": 0.7200446428571429}, {"timecode": 28, "before_eval_results": {"predictions": ["deserts", "political parties", "discipline problems with the Flight Director's orders during their flight", "the American novel written by William Goldman about Amos McCracken, a 31-year-old man who has written a popular show tune and who is having marriage troubles.", "Cherokee Nation (\u13e3\u13b3\u13a9\u13af \u13a0\u13f0\u13b5, pronounced \"Tsalagihi Ayeli\")", "American singer Toni Braxton", "Galt\u00fcr avalanche", "Bantu", "2013", "Luis Edgardo Resto", "the Salzburg Festival", "Jay Park", "Blackpool Football Club", "New Orleans Saints", "March 11, 1993", "\"Charmed\"", "Dundalk, County Louth, Ireland", "Ashley Jensen", "Syracuse University", "E Sharon June Atkins, DBE", "Mollie Elizabeth King (born 4 June 1987)", "Esteban Ocon", "the flags of dependent territories and other areas of special sovereignty", "Ouse and Foss", "Emilia-Romagna Region in Northern Italy", "\"Casablanca\"", "The Go-Go's", "1943", "Sleepy Hollow", "Ronnie Schell", "Wandsworth, London", "Christopher Lloyd Smalling (born 22 November 1989)", "Chevron Corporation", "World Music Awards", "La Liga", "Australian", "Floyd Nathaniel \"Nate\" Hills", "Fort Hood, Texas", "Michael Phelps", "Lauren Alaina", "1961", "Droga5", "Preston, Lancashire, UK", "Prudential Center in Newark, New Jersey", "\"Krabby Road\"", "the Mach number (M or Ma)", "(3 January 1883 \u2013 8 October 1967)", "Mexico", "Chevy Motor Car Company", "wooden roller coaster", "Disco", "Theodor W. Adorno", "re-education", "Sir Rowland Hill", "complex sentence", "President Woodrow Wilson-- Wilson won the peace prize for his work in founding the League of Nations  President Theodore \"Teddy\" Roosevelt--President Roosevelt negotiated a peace treaty ending the bloody war between Japan and Russia.", "John Logie Baird,", "1066", "Joan Rivers", "did not go into further detail about her heart condition or the medical procedure.", "auction off one of the earliest versions of the Magna Carta later this year,", "The 6 Best Places to See Joan Mir - His Life and Work - Nancy Doyle Fine Art", "Windows 7,", "kids"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6340311598124098}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.3636363636363636, 0.08333333333333333, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 0.6, 1.0, 0.18181818181818182, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.8571428571428571, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2791", "mrqa_squad-validation-4060", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-2699", "mrqa_hotpotqa-validation-1997", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-1726", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-1926", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-1667", "mrqa_triviaqa-validation-4927", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-9671", "mrqa_searchqa-validation-14497"], "SR": 0.53125, "CSR": 0.5571120689655172, "EFR": 0.9666666666666667, "Overall": 0.7131932471264367}, {"timecode": 29, "before_eval_results": {"predictions": ["some teachers and parents", "July 20, 1969", "glaucophyte", "\"God and the just cause\"", "Mercury Records", "Oksana Grishuk", "Nye County", "Karolina Dean", "\"Firestorm\"", "ten", "White Knights of the Ku Klux Klan", "the Chechen Republic", "Cedar Point", "Kauffman Stadium", "1994", "Premier League club Stoke City", "Food and Agriculture Organization", "Idaho", "Jeff Meldrum", "Crossed: Dead or Alive", "Romance language", "Philip K. Dick", "over 80%", "English", "Cartoon Network Too", "David Robert Starkey", "the Cherokee River", "pop music and popular culture", "Field Marshal Lord Gort", "Hopeless Records", "Razor Ramon", "\"Godspell\"", "8 August 1907", "Boeing 757", "The Mauser C96", "Bangkok", "51st", "his exploration and settlement of what is now Kentucky,", "August 28, 1774", "eastern Afghanistan", "British", "Potomac River", "the Netherlands", "\"Love the Way You Lie\"", "Rio Gavin Ferdinand", "Boston", "Las Vegas", "actor, producer, and director", "(Rockbridge County)", "St. Louis, Missouri", "Tsung-Dao Lee", "Bay Ridge, Brooklyn", "Human anatomy", "Tyler, Ali", "at the intersection of Del Monte Blvd and Esplanade Street", "AFC Wimbledon", "The Duke of Plaza-Toro", "2, 3, 5,", "Meira Kumar", "U.S. Food and Drug Administration", "bartering", "spoiled", "a malted", "Iceland"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7377604166666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2088", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-5306", "mrqa_naturalquestions-validation-9660", "mrqa_triviaqa-validation-6131", "mrqa_triviaqa-validation-4462", "mrqa_searchqa-validation-11933"], "SR": 0.65625, "CSR": 0.5604166666666667, "EFR": 1.0, "Overall": 0.7205208333333333}, {"timecode": 30, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1079", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1430", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1524", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2823", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3382", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-3950", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4058", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4301", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4334", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4953", "mrqa_hotpotqa-validation-498", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-5313", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-932", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-999", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-9867", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1582", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3604", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3760", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-719", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-958", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-1228", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14046", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1828", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9613", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1379", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1546", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1600", "mrqa_squad-validation-1751", "mrqa_squad-validation-1819", "mrqa_squad-validation-1908", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-2025", "mrqa_squad-validation-2106", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-2848", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-3001", "mrqa_squad-validation-3103", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3449", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-4065", "mrqa_squad-validation-4132", "mrqa_squad-validation-4159", "mrqa_squad-validation-4216", "mrqa_squad-validation-4248", "mrqa_squad-validation-4274", "mrqa_squad-validation-4472", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4698", "mrqa_squad-validation-4736", "mrqa_squad-validation-4765", "mrqa_squad-validation-4772", "mrqa_squad-validation-4789", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5270", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5908", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6382", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7043", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7217", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7564", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7775", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7937", "mrqa_squad-validation-8010", "mrqa_squad-validation-8023", "mrqa_squad-validation-826", "mrqa_squad-validation-8298", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8466", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-8612", "mrqa_squad-validation-8665", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9308", "mrqa_squad-validation-9499", "mrqa_squad-validation-9594", "mrqa_squad-validation-9638", "mrqa_squad-validation-9918", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1212", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-4504", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6173", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7452", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-88"], "OKR": 0.837890625, "KG": 0.4609375, "before_eval_results": {"predictions": ["State Department", "immediately", "a second Gleichschaltung", "Las Vegas Hilton", "Recording Industry Association of America", "between 7,500 and 40,000", "mountaineer, filmmaker, author, and motivational speaker", "Belgian", "Eve Hewson", "\"Slaughterhouse-Five\"", "Allan McNish (born 29 December 1969) is a British former racing driver, commentator, and journalist from Scotland.", "William Jefferson Clinton (born William Jefferson Blythe III; August 19, 1946) is an American politician who served as the 42nd President of the United States from 1993 to 2001.", "Oldham County", "sandstone", "Channel 4", "25 December 2009", "a priest", "punk rock", "Robert \"Bobby\" Germaine, Sr. (October 1, 1925 \u2013 April 1986), the son of French-Canadian immigrants,", "Lord Byron", "Laura Elizabeth Dern", "Carrefour", "burlesque", "Argentina", "Forever Living Products", "FBI", "The Saturdays", "Indianapolis", "French", "2269", "Edinburgh", "Charles Bronson", "Oklahoma Sooners", "Orson Welles", "Sharyn McCrumb", "Robert Digges Wimberly Connor (September 26, 1878 \u2013 February 25, 1950) was an American historian and the first Archivist", "1.23 million", "Ford Motor Company", "J. K. Rowling's \"Harry Potter\" series", "Sullivan University College of Pharmacy", "Zola", "New Zealand", "January 28, 2016", "Martin Scorsese", "1979", "Charlie Kaufman", "Merrimack County", "RAF Tangmere, West Sussex", "\"Brotherly Leader\"", "Suicide Kings", "Western District", "stolperstein", "Earl ( John Doe )", "Montgomery", "Bart Howard", "South Park", "a pest", "Andre Agassi", "10 below", "Asashoryu", "heavy turbulence", "gary pizzarelli", "Clarence Darrow", "The Secret"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6557487271293052}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.20689655172413793, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.38095238095238093, 0.0, 1.0, 0.4444444444444444, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2551", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-3367", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4470", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-1999", "mrqa_hotpotqa-validation-5278", "mrqa_hotpotqa-validation-528", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-7912", "mrqa_triviaqa-validation-7167", "mrqa_newsqa-validation-1077", "mrqa_searchqa-validation-12237", "mrqa_searchqa-validation-8942"], "SR": 0.546875, "CSR": 0.5599798387096775, "EFR": 1.0, "Overall": 0.7194178427419355}, {"timecode": 31, "before_eval_results": {"predictions": ["NCAA Division I", "1985", "Royal President of the Council and Lord Lieutenant of Ireland", "2006", "25 million", "15", "Thomas Robert \"Tom\" Kitt", "The Bye Bye Man", "Hermione Baddeley", "GameStop Corp.", "Fort Frederick", "Thorgan Ganael Francis Hazard", "Robert Marvin \"Bobby\" Hull, OC", "Love Actually", "Larnelle Steward Harris", "Queensland", "Southbank", "Commanding General", "1976", "Sean Yseult", "2001", "Benjamin Andrew \"Ben\" Stokes", "newspapers, television, radio, cable television, and other businesses.", "Francis Keogh Gleason", "Royal Navy", "The Land of Enchantment", "$10\u201320 million", "the Cumberland Plain", "Formula E", "a neighborhood in Lower Manhattan, New York City,", "Province of Canterbury", "the Anhaltisches Theater in Dessau", "Alemannic German", "1932", "128", "Telugu", "1937", "Windermere", "Curtis James Martin Jr.", "Marco Fu Ka-chun, MH, JP", "a 2003 South Korean horror film", "Isabella (Belle) Baumfree", "Margaret Rose MacKenzie", "the American comedy-drama series \"Gilmore Girls\"", "Premier League", "Aqua", "St. Louis, Missouri", "Dan Castellaneta", "Bury St Edmunds", "Philip K. Dick", "Labour Party", "Greater Manchester, England", "Toby Keith", "Margaery Tyrell", "January 15, 2010", "Kenya", "Macbeth", "Sir William Hamilton", "Former Beatles", "\"Larry King Live\"", "\"foreign\" Islamic extremist groups fighting against Indian rule in the disputed Himalayan territory of Kashmir.", "The Church of Christ, Scientist", "Ronald Reagan", "East Germany"], "metric_results": {"EM": 0.5, "QA-F1": 0.686831311050061}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.15384615384615385, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5714285714285715, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.5, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2793", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2284", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-3892", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3798", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5482", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-8909", "mrqa_triviaqa-validation-2828", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-4128", "mrqa_newsqa-validation-1618", "mrqa_searchqa-validation-1275"], "SR": 0.5, "CSR": 0.55810546875, "EFR": 1.0, "Overall": 0.71904296875}, {"timecode": 32, "before_eval_results": {"predictions": ["'Licensed Local Pastor'", "a power station generator", "13", "Hebrew", "Blenheim Palace", "David Pick Up Five Stones", "Edith Cavell", "County of Cotentin", "De Lorean DMC-12", "Cold War", "Action Comics", "Queen Elizabeth II", "Shylock's daughter", "Northwestern University", "curling", "Mark Steyn", "Colorado", "Google", "Aviva plc", "oil", "Project Gutenberg", "surf", "Dr John Sentamu", "Kiel Canal", "General Sir Herbert Kitchener", "Cevennes", "Eggs Benedict", "Luigi Pirandello", "Sheffield United", "R. White's Lemonade", "Midtown", "Eddy Shah", "Hugh Laurie", "a cappella", "Dutch", "New Kids On The Block", "Red squirrels", "near the port city of Karachi", "Adam Smith", "Model T.", "Spice Girls", "Brian Blessed", "Michael Caine", "Sebastian Beach", "pig", "Bank of England", "Isaac Newton", "Pet Sounds by The Beach Boys", "the monarch", "Bangladesh", "St Clements", "Dioscuri", "Nalini Negi", "last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "Norway", "Sporting Kansas City", "University of Mississippi", "The Rebirth", "Bagosora, 67, a colonel in the Rwandan army,", "Apple", "Akio Toyoda", "Hamlet", "reddish-orange", "chicken Kiev"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5538318452380953}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1472", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-6454", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-2721", "mrqa_triviaqa-validation-958", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3536", "mrqa_triviaqa-validation-3902", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-6905", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-907", "mrqa_triviaqa-validation-299", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-1538", "mrqa_hotpotqa-validation-217", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-250", "mrqa_searchqa-validation-16717", "mrqa_searchqa-validation-10619"], "SR": 0.484375, "CSR": 0.5558712121212122, "EFR": 1.0, "Overall": 0.7185961174242423}, {"timecode": 33, "before_eval_results": {"predictions": ["NASA Administrator Webb", "Duval County", "Atlantic", "Virgin", "ohm meter", "tibet", "in the County of Gloucestershire", "1709", "Tutankhamun", "Morgan Spurlock", "iris", "Massachusetts", "Andre Agassi", "Jane Austen", "flemish", "Bruce", "nacre", "yellow", "Tbilisi", "Mrs Merton", "r.A.P.[s claims to be the first ever European champions", "Wyoming", "Catherine Cookson", "Hugh Quarshie", "Flanagan", "my Sweet Lord", "Alan Sugar", "in Dreams", "Henri Paul", "Red Sea", "Helen Gurley Brown", "Wash", "wool", "Benfica", "Mark Carney", "Eva Marie", "c\u00f4te d'Or", "Utah", "Toy Story", "David Lloyd George", "Italy", "lord Nelson", "George Osborne", "August 10, 1960", "Apollo", "Gentlemen Prefer Blondes", "yorkshire", "Ned Flanders", "Paul Gauguin", "tibet", "proton", "Demi Moore", "28 July 1914", "Spike", "1995", "Ellie Kemper", "23 June 1912", "Oryzomyini", "Ryder Russell,", "fellow Soviet republics such as Latvia, Lithuania and Estonia.", "The Washington Post", "Plymouth", "yorkshire", "Pearl Jam"], "metric_results": {"EM": 0.625, "QA-F1": 0.6674851190476191}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3929", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-2042", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-1375", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-7345", "mrqa_triviaqa-validation-5950", "mrqa_naturalquestions-validation-5304", "mrqa_hotpotqa-validation-5041", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-15915"], "SR": 0.625, "CSR": 0.5579044117647058, "EFR": 1.0, "Overall": 0.7190027573529412}, {"timecode": 34, "before_eval_results": {"predictions": ["international metropolitan region", "petroleum", "Pepsi", "nonesuch", "Dan Rather", "honest", "GIGO", "pawn", "three's Company", "bead", "bamboo", "Arthur C. Clarke", "rice", "dutch", "dutch", "aston villa", "scoop", "Led Zeppelin", "bailiwick", "Charles Lindbergh", "River Phoenix", "daffy", "NBC's Heroes", "the old Krntnertor Theatre", "Jason", "The Curse of the Play", "u.S. marine band", "AbeBooks.com Community Forum", "florence nightingale", "Profiles in Courage", "bogota", "atonement", "dutch", "Glenn Scobey \"Pop\" Warner", "dijon", "eva Baron Cohen", "coal", "jedoublen/jeopardy", "smiles", "humerus", "Harriet Tubman", "horse", "Louisa May Alcott", "scoop", "hou Williams", "Margaret Atwood", "antojitos", "Khartoum", "Joaquin Phoenix", "Winslow Homer", "Moby Dick", "The Hot Chick", "iOS", "on Mars Hill", "Aaron Harrison", "Gwyneth Paltrow", "smith", "aromatherapy", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "140 million", "1923", "Saturday", "7-1", "South Carolina Republican Party Chairwoman Karen Floyd"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5381944444444444}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_searchqa-validation-16300", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4738", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-4524", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-12487", "mrqa_searchqa-validation-15135", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-7837", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-12354", "mrqa_searchqa-validation-15325", "mrqa_searchqa-validation-16288", "mrqa_searchqa-validation-8940", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-1882", "mrqa_searchqa-validation-2976", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7832", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4753", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-30", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-2996", "mrqa_newsqa-validation-4058"], "SR": 0.46875, "CSR": 0.5553571428571429, "EFR": 1.0, "Overall": 0.7184933035714286}, {"timecode": 35, "before_eval_results": {"predictions": ["an arrow", "the Chicago Bears", "Floridians", "green and yellow", "Galicia", "Regional Rural Bank", "M2M", "Ferdinand", "extreme nationalist, and nativist ideologies, as well as authoritarian tendencies", "The Division of Cook", "July 16, 1971", "13 May 2018", "Kentucky River", "Barbara Niven", "the Ferengi bartender Quark", "Yellowcraigs", "Santiago del Estero Province", "a super-regional shopping mall", "Messiah Part II", "Abbey Road", "Mel Blanc", "the Czech Kingdom (Czech: \"\u010cesk\u00e9 kr\u00e1lovstv\u00ed\"", "March 14, 2000", "Lamar Wyatt", "Alfred Preis", "Terry Malloy", "Countess of Lovelace (\"n\u00e9e\" Byron)", "The interview", "various registries", "20th episode", "John Bingham", "January 15, 1975", "Chiwetel Umeadi Ejiofor, CBE", "Appleby-in-Westmorland", "27 November 1956", "Charles de Gaulle Airport", "The St Andrews Agreement", "Blackpool", "The Faculty of the Victorian College of the Arts and Melbourne Conservatorium of Music", "north of the Lakes Region", "Nick Cassavetes", "Cate Blanchett", "Linda Ronstadt", "January 28, 2016", "Hopi", "John Meston", "Romeo Montague", "\u00c6thelwald Moll", "The Albanian Coalition &quot;Perspective&quot", "the University of North Staffordshire", "Battle of the Rosebud", "Jaffrey", "1998", "tropical and subtropical latitudes from the Red Sea and the east African coast", "Sara Gilbert", "Elkie Brooks", "acrostic", "blue", "Asashoryu", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "1941", "Queiz", "blue", "Luxor Las Vegas"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6430431547619049}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.2222222222222222, 1.0, 0.5714285714285715, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9, 0.0, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5283", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-5266", "mrqa_hotpotqa-validation-3982", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-3644", "mrqa_naturalquestions-validation-6452", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-1457", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-15743"], "SR": 0.53125, "CSR": 0.5546875, "EFR": 1.0, "Overall": 0.718359375}, {"timecode": 36, "before_eval_results": {"predictions": ["the thylakoid space", "the Caesars Palace Grand Prix", "compact car", "Benjam\u00edn", "Koch Industries", "Enigma", "Yellow fever", "Julia Compton Moore", "Lord's Resistance Movement", "Workers' Party", "Yasiin Bey", "1763", "Bulgaria", "Swiss national team", "remake", "Oldham County, Kentucky", "The Captain Matchbox Whoopee Band", "Alec Berg", "wild boar, and red, fallow and roe deer", "Ghana's Asamoah Gyan", "Nikolai Morozov", "Rabies", "Switzerland", "Tennessee", "Godiva", "October 21, 2016", "August 1973", "Lawrence of Arabia", "The Ansonia Hotel", "1937", "the Irish Government's Health Service Executive", "Leona Lewis", "John Robert Cocker", "$7.3 billion", "Angus Brayshaw", "This technique works on the relationship between the mind and the body", "the \"Black Abbots\"", "Sarah Kerrigan", "German", "Katy Perry", "Bharat Ratna", "Wilderness Road", "75 mi southeast", "\"Nebo Zovyot\"", "\"Orchard County\"", "The Kree", "more than 110 films", "shortest player ever to play in the National Basketball Association", "The authorship of \"Titus Andronicus\"", "Denmark", "Patricia Arquette", "James Corden", "Audrey II ''", "the end of the 2015 season", "red", "(Buddha)", "Jim Braddock", "\"He is more American than German.\"", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Brazilian supreme court judge", "the New York Yankees", "Washington Redskins", "( Samuel Taylor) Coleridge", "(A - Abel)"], "metric_results": {"EM": 0.578125, "QA-F1": 0.649234068627451}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.4, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-1009", "mrqa_hotpotqa-validation-667", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-1872", "mrqa_naturalquestions-validation-7614", "mrqa_triviaqa-validation-4569", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4184", "mrqa_searchqa-validation-11491", "mrqa_triviaqa-validation-768"], "SR": 0.578125, "CSR": 0.5553209459459459, "EFR": 1.0, "Overall": 0.7184860641891893}, {"timecode": 37, "before_eval_results": {"predictions": ["the plain moraine plateau and the Vistula Valley", "londres", "leaves", "Gorky", "the James Bond", "a unit of money", "keeper of the Longstone (Fame Islands) lighthouse", "'Carlos the Jackal'", "Australia", "Margot Betti Frank", "Belgium", "Sufjan Stevens", "the isthmus", "\"The Benny Hill Show\"", "Roddy Doyle", "Kevin Spacey", "Alexandria", "the Republic of Chad", "1215", "the neck", "David Hockney", "Rudyard Kipling", "lactic acidosis", "the Central Criminal Courts", "the Netherlands", "fractal geometry", "Cosmos: A spacetime Odyssey", "the duck-billed platypus", "Aquaman", "Jean-Paul Sartre", "lite", "lite", "\"The Hat\" McVitie", "Scotch whisky", "Switzerland", "sheep", "trumpet", "Babe Ruth", "linda da Vinci", "luna", "Heston Blumenthal", "Tony Curtis", "U2", "a peasant's wife", "invaded Russia", "Harper", "Shirley Caesar", "Canada", "Buster Edwards", "Chief Inspector of Prisons", "Henley Royal Regatta", "\"Paul Lynde as Templeton, a care - free, egotistical rat who lives at Homer's farm", "the Maryland Senate's actions", "Cairo, Illinois", "Lazio", "Al D'Amato", "\" SKUM\"", "BET", "the HSH Nordbank Arena", "October 2007", "lisa", "managed to run the country in a militant...", "the Siberian Husky", "pronghorn"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5427083333333333}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false], "QA-F1": [0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-953", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5911", "mrqa_triviaqa-validation-665", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-1647", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-2340", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6211", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-7575", "mrqa_triviaqa-validation-4500", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-7109", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-8239", "mrqa_hotpotqa-validation-5893", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-731", "mrqa_searchqa-validation-10306", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-10797", "mrqa_hotpotqa-validation-2366"], "SR": 0.4375, "CSR": 0.552220394736842, "EFR": 1.0, "Overall": 0.7178659539473684}, {"timecode": 38, "before_eval_results": {"predictions": ["avionics, telecommunications, and computers.", "Dutch thriller", "Peter Yarrow", "Mount Rainier, Washington", "Zack Snyder", "Mondays", "The Cosmopolitan", "Anna Clyne", "Meghan Markle", "terrorist activity", "Commissioner", "August 1973", "Burnley", "Teen Titan", "Evey's mother", "Love and Theft", "1978", "SKUM", "Edmonton, Alberta", "Seattle", "\"The School Boys\"", "Orchard Central", "The Kennedy Center", "commanders of the Great Army", "Environmental Protection Agency", "Humberside", "Diamond Rio", "\"Falling for a Dolphin\"", "Northampton, England", "Mike Greenwell", "2017", "SAS Technical Services", "polka", "Irish Chekhov", "1860", "2006", "Ghanaian national team", "\"Coronation Street\"", "\"The Dragon\"", "Cold Spring Historic District", "Never Alone", "Sophie Monk", "The Primettes", "Southern Progress Corporation", "Melbourne Storm", "twenty", "9 November 1967", "Retina display", "technical director", "Cincinnati metropolitan area", "Captain B.J. Hunnicutt", "19 June 2018", "naos", "31 - member", "kendo", "Jeffrey Archer", "germany", "little blue booties.", "Majid Movahedi,", "Espinoza Patron's", "February 2, 2016", "ninjutsu", "witch", "Mozart"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7505208333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.4, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-668", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3208", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-2868", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-1300", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-5545", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-16163", "mrqa_searchqa-validation-3163"], "SR": 0.65625, "CSR": 0.5548878205128205, "EFR": 0.9545454545454546, "Overall": 0.7093085300116551}, {"timecode": 39, "before_eval_results": {"predictions": ["problems with funding education, sanitation, and traffic control within the city limits.", "Aly Raisman", "40,400 members", "Seoul, South Korea", "American", "frontman Shane MacGowan", "William Powell \"Bill\"Leary", "Distinguished Service Cross", "Revolver", "Samuel M \"Sam\" Raimi", "\"The Manhunter from Mars\"", "Eden Valley Railway", "Wolfgang Amadeus Mozart", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Mercer University", "Dame Eileen June Atkins", "August", "\"Little Dixie\"", "1979", "1905", "member of the Kansas House of Representatives", "Loughborough University", "Los Angeles", "2.1 million members", "Granada", "Alfred Joel Horford Reynoso", "Adrian Charles \"Ade\" Edmondson", "Ghanaian", "Tony Burke", "Alcorn State", "Wilderness Road", "Alfred Edward Housman", "The Killer", "London", "nearly 8 km", "25 October 1921", "\"War & Peace\"", "2016 U.S. Senate election", "Tayeb Salih", "Mickey Gilley's Club", "Jedi", "Edward Albert Heimberger", "Whoopi Goldberg", "Gian Carlo Menotti", "Indian", "Target Corporation", "last Roman Catholic Archbishop of Canterbury", "sub-Saharan Africa", "member of the Central Coast Council local government area.", "gamecock", "Aaliyah Dana Haughton", "Hagrid", "either `` eye point '', from which the image should be viewed for correct perspective geometry", "16,801", "Lady Gaga", "married", "Georgetown", "The Palm Jumeirah", "consumer confidence", "Sobashima", "Scarlett Johansson", "Fried Green Tomatoes", "Bon Jovi", "Marcie Blane"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5739718614718614}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666665, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.0, 0.5, 0.0, 0.13333333333333336, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7246", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-4577", "mrqa_hotpotqa-validation-1285", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-735", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-139", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-4770", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-3427", "mrqa_triviaqa-validation-1074", "mrqa_newsqa-validation-101", "mrqa_searchqa-validation-2773"], "SR": 0.453125, "CSR": 0.55234375, "EFR": 1.0, "Overall": 0.717890625}, {"timecode": 40, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1611", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2331", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3188", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5538", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-5705", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-987", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4998", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-809", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12714", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5071", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9480", "mrqa_squad-validation-10044", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-10326", "mrqa_squad-validation-10425", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1231", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1472", "mrqa_squad-validation-1608", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2006", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2819", "mrqa_squad-validation-297", "mrqa_squad-validation-3001", "mrqa_squad-validation-3262", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3812", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4078", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4543", "mrqa_squad-validation-4611", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5079", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5355", "mrqa_squad-validation-5563", "mrqa_squad-validation-5597", "mrqa_squad-validation-5616", "mrqa_squad-validation-5881", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6223", "mrqa_squad-validation-6251", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-7952", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8199", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9768", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-244", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6607", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7223", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.849609375, "KG": 0.49453125, "before_eval_results": {"predictions": ["gilt bronze", "These oceans are growing crowded,", "President Obama's surge plan", "they would not be making any further comments,", "near his home in Peshawar", "$250,000", "The Valley swim Club", "the actor who created one of British television's most surreal thrillers", "in a stream in Shark River Park in Monmouth County", "air support", "Gov. Mark Sanford", "between 1917 and 1924", "Frank Ricci", "Janet Napolitano", "President Mahmoud Ahmadinejad", "\"utterly baseless\"", "Jacob", "Police", "Intel", "Lousiana", "prostate cancer", "Eintracht Frankfurt", "Tsvangirai", "the FBI", "\"GoldenEye\"", "inconclusive", "they'd get to bring a new puppy", "the Kurdish militant group in southeastern Turkey.", "The London-born movie star", "Ralph Cifaretto", "not", "nuclear warheads", "17", "a rally", "an empty water bottle", "took a crashing fall", "a head injury", "News of the World tabloid", "the Swat Valley", "200", "Manny Pacquiao", "1964", "Disney", "Mandi Hamlin", "environmental", "80", "The EU naval force", "The meter reader who led authorities last week to remains believed to be those of Caylee Anthony", "the ancient Greek site of Olympia", "Gary Player", "200", "Adam", "Erika Mitchell Leonard", "1969", "V\u00e1clav Havel", "left", "for adventure", "London", "703", "20 October 1980", "glaciers", "Daley", "Cerberus", "1967"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6456545915814491}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.631578947368421, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.05714285714285715, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.5714285714285715, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-4036", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1159", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-2427", "mrqa_searchqa-validation-2894"], "SR": 0.5625, "CSR": 0.5525914634146342, "EFR": 1.0, "Overall": 0.7254401676829269}, {"timecode": 41, "before_eval_results": {"predictions": ["Business Connect", "\"explosion of violence.\"", "40-year-old", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "detention of terror suspects at Guant Bay, Cuba,", "six alleged victims, who are relatives of the five suspects, made accusations of sexual abuse.", "Kevin Kuranyi fired Schalke up to joint second in the Bundesliga,", "Kenyan and Somali governments", "Aung San Suu Kyi", "legitimacy of that race.", "\"The Tibetan spiritual leader, who fled to India in 1959 and established a government in exile there, visited the United States earlier this month.", "inspiring people in his new book.\"", "Mashhad", "Pakistan's combustible Swat Valley,", "North Korea", "pesos", "former U.S. soldier Steven Green", "American collective consciousness and on Muslims as Americans.", "1000 square meters", "sail", "they'd get to bring a new puppy with them to the White House in January.", "27-year-old's", "Friday,", "Brazil", "Seasons of My Heart", "helping on the sandbags lines", "$17,000", "opium", "for not doing more since taking office.\"", "10 years", "$8.8 million", "Noida, located in the outskirts of the capital New Delhi.", "Lisa Brown", "at least two and a half hours.", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England.", "Zimbabwean President Robert Mugabe signed a power-sharing deal with the opposition party's breakaway faction,", "31 meters (102 feet)", "the British capital's other two airports, Stansted and Gatwick,", "104 feet long and 95 feet wide at the alcove.", "Transport Workers Union leaders", "state senators who will decide whether to remove him from office", "84-year-old", "(290 miles)", "the Southern Baptist Convention,", "Sen. Barack Obama", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Anil Kapoor", "could become a very serious risk.", "Marie-Therese Walter.", "A mother whose daughter and granddaughter attend Oprah Winfrey's school in South Africa considers the talk-show host heaven-sent,", "Hurricane Gustav", "( Felix) Baumgartner ( German : ( \u02c8fe\u02d0l\u026aks \u02c8ba\u028a\u032fm\u02cc\u0261a\u0250\u032ftn\u0250 )", "\"s Anatomy as Dr. Jo Wilson in a recurring role. In June 2013 it was announced that she would be a series regular from season ten onward", "1273.6 cm for men", "(George) Shaw", "Dutch", "the Tatars", "Toronto", "1993", "Robert L. Stone", "Nike", "Barbara Bush", "Qwerty", "Corpulent"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5658249489910674}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.33333333333333337, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.75, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 1.0, 0.4799999999999999, 0.2857142857142857, 0.5333333333333333, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.10526315789473684, 1.0, 0.5714285714285715, 0.0, 0.3, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2627", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1427", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-3430", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-3800", "mrqa_naturalquestions-validation-448", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-7458", "mrqa_triviaqa-validation-471", "mrqa_triviaqa-validation-2480", "mrqa_hotpotqa-validation-582", "mrqa_searchqa-validation-7340"], "SR": 0.453125, "CSR": 0.5502232142857143, "EFR": 0.9714285714285714, "Overall": 0.7192522321428572}, {"timecode": 42, "before_eval_results": {"predictions": ["Wittenberg", "alcohol", "Bligh", "Parkinson's", "The Frighteners", "george", "Moscow", "germany", "Portugal", "first among equals", "Friedrich Nietzsche", "the moon", "Moldova", "Zak Starkey", "Craggy Island", "the Suez Canal", "otters", "hickory", "Aberavon", "Rapa Nui", "Diary of a Tuber", "Charlie Cairoli", "Salvador Allende", "Mike Tyson", "J. M. W. Turner", "conductor", "Boyle", "divination", "a winter fur hat", "Tony Blair", "Adolf Hitler", "Jamaica", "June Brae", "heart attack", "1066", "the Three Graces", "Jesse James", "Purple Heart Medal", "cephalus", "Jessica Simpson", "bacteria", "the MacKenzie", "Robert Devereux", "NASCAR", "Canada", "Pennsylvania", "Louis Daguerre", "Argentina", "Kwame Nkrumah", "The Color Purple", "terrorism", "lithium", "can be produced with constant technology and resources per unit of time, such that more of one good could be produced only by diverting resources from the other good, resulting in less production of it", "Mariah Carey", "lifetime achievements", "unidentified flying objects", "Chicago Bears", "grizzly bear", "Turkey", "Obama", "steroids", "Ferrari", "Smoky Mountains", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6062250797448165}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false], "QA-F1": [0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.45454545454545453, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.21052631578947367]}}, "before_error_ids": ["mrqa_squad-validation-2165", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-4034", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-5069", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-2893", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-797", "mrqa_newsqa-validation-3830", "mrqa_searchqa-validation-9911", "mrqa_naturalquestions-validation-4915"], "SR": 0.515625, "CSR": 0.5494186046511628, "EFR": 0.967741935483871, "Overall": 0.7183539830270067}, {"timecode": 43, "before_eval_results": {"predictions": ["John M. Grunsfeld", "dancing with the Stars", "psychotropic drugs", "opium", "1991-1993", "Illlinois.", "Democrat", "test scores and graduation rates", "16", "iReporter Rany Freeman", "forgery and flying without a valid license,", "Bush", "15-year-old", "seven", "upper respiratory infection,\"", "543", "Kevin Kuranyi", "Amy Bishop Anderson,", "Susan Atkins", "Ameneh Bahrami", "Virgin America", "$1,500", "Al Nisr Al Saudi", "\"We Found Love\"", "his parents", "Ralph Lauren", "iWoz", "hopes the journalists and the flight crew", "North Korea", "beetle", "Old Trafford", "Bronx County District Attorneys Office", "Arabic, French and English", "Britain.", "Arsene Wenger", "the FBI's Baltimore field office", "Michael Jackson", "all day starting at 10 a.m.,", "her mom,", "South African police", "Korean-American missionary", "Palestinian Islamic Army,", "gay marriage", "was killed", "cast doubt on Woodward's assertion Tuesday in a conversation with \"American Morning\" host John Roberts.", "consumer confidence", "Phil Spector", "the District of Columbia National Guard", "Australia and New Zealand", "Steven Gerrard", "saw an unprecedented wave of buying amid the elections.", "Durham", "Hermann Ebbinghaus", "southern whites", "lion", "myxoma virus", "Wisconsin", "Battle of Britain and the Battle of Malta", "Viacom Media Networks", "five", "carbon fiber", "the Star-Spangled Banner", "Bing Crosby", "100"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6277991973304473}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.45454545454545453, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-740", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2065", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9516", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-2853", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-7435"], "SR": 0.546875, "CSR": 0.5493607954545454, "EFR": 1.0, "Overall": 0.7247940340909091}, {"timecode": 44, "before_eval_results": {"predictions": ["the Supreme Court of the United Kingdom", "Seal", "on the limbic system", "coffin-maker", "63 to 144 inches.", "the Wye", "Zorro", "USS Thresher", "the Last Post", "Karachi", "BMW", "eagle", "Morgan Spurlock", "Michael Jackson", "helps managers understand employees' needs in order to further employees' motivation.", "Prague", "Yellowstone", "Watford", "Nevada", "muezzin", "snakes", "Rihanna", "Tintin", "Alexandrina", "22", "Hector BERLIOZ", "Azerbaijan", "Ireland", "Ash", "Madness", "Dalton", "Australia", "Phil Woolas", "bats", "United States", "Penelope Keith", "Alexei Kosygin", "John Galsworthy", "Vinegar Joe", "James Van Allen", "pangram", "the Tyrrhenian Sea", "Steel Beads", "Nicaragua", "Passepartout", "of Wellington", "Lancashire", "Edouard Manet", "Burger King", "of Thebes", "Mazda", "Xiu Li Dai and Yongge Dai", "President", "David Ben - Gurion", "Chow Tai Fook Enterprises", "an organ", "Point of Entry", "1973's \" Raw Power.\"", "a violent government crackdown seeped out.\"", "Robert Mugabe", "Hungary", "China", "China", "Isabella (Belle) Baumfree"], "metric_results": {"EM": 0.625, "QA-F1": 0.6666666666666667}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6166", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-5745", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-7055", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4657", "mrqa_naturalquestions-validation-2208", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3391"], "SR": 0.625, "CSR": 0.5510416666666667, "EFR": 1.0, "Overall": 0.7251302083333333}, {"timecode": 45, "before_eval_results": {"predictions": ["trespassing at a nuclear-missile installation", "Utrecht", "six", "skull", "new york", "Justin Trudeau,", "Washington", "Nuuk", "the Philippines", "pool", "China", "Graham Henry", "cotton", "hart", "king york", "beans", "Leeds", "wood", "Elizabeth II", "dogs", "llamas", "London Underground Piccadilly Line", "new york", "la Boh\u00e8me di Giovanni delle Bande Nere", "Nepal", "scurvy", "cutters", "Indonesia", "purple coneflower", "Variations", "keane", "gauteng", "red", "Pakistan", "Uranus", "come find Yourself", "barbara burt Balaban", "my favorite Martian", "niki lauda", "petronas", "The Daily Mirror", "Eric Morley", "radio waves", "york", "notts County", "football", "Reform Club", "billy shaw", "barbara", "Trimdon", "h Hobart", "26,000 years", "US - grown fruit", "Scottish post-punk band Orange Juice", "Fifteenth", "Plato", "Richard Feynman", "Florida", "Martin Aloysius Culhane", "400 years", "freelance", "mouth", "barbara dold", "arrested, arraigned and jailed,"], "metric_results": {"EM": 0.375, "QA-F1": 0.45833625116713356}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.5714285714285715, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.11764705882352941]}}, "before_error_ids": ["mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-7559", "mrqa_triviaqa-validation-2995", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-4038", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-3153", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-5221", "mrqa_naturalquestions-validation-4437", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-2379", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-5176", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-4100", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-11241", "mrqa_newsqa-validation-3806"], "SR": 0.375, "CSR": 0.5472146739130435, "EFR": 0.95, "Overall": 0.7143648097826086}, {"timecode": 46, "before_eval_results": {"predictions": ["Konwiktorska Street,", "mashed potato", "Lalo Schifrin", "16 November 2001", "Don McMillan", "7 correct numbers", "Billy Hill", "Paul Lynde", "chlorofluorocarbons", "body - centered cubic ( BCC ) lattice", "August 27, 2002", "2010", "virtual reality simulator", "beneath the liver", "1885", "about 1500 BC", "2014", "Most days are sunny throughout the year", "caused by chlorine and bromine from manmade organohalogens", "Gamora", "1975", "differ in ingredients", "homicidal thoughts of a troubled youth", "Mockingjay -- Part 1 ( 2014 )", "the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "April 15, 2018", "Amitabh Bachchan", "1998", "nine", "Jeff Bezos", "adrenal medulla", "main type of cell found in lymph", "redox", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "produced with constant technology and resources per unit of time", "Lewis Carroll", "January 2004", "Samuel Chase", "Cetshwayo", "1960s", "Asuka", "Erica Rivera", "New York", "September 2001", "in teaching elocution", "Chris Rea", "mashed potato", "the case of disputes between two or more states", "three", "Zoe Badwi, her Thirlwall's cousin, was supporting the gigs in Australia", "Mongol - led Yuan dynasty", "candy", "1948", "Ruth Rendell", "Hidden America with Jonah Ray", "the United Kingdom", "people working in film and the performing arts", "Jean Van de Velde", "between 1917 and 1924 when he was in his late 30s and early 40s.", "his client from the death penalty argued Tuesday that former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq", "cops", "the Virgin Spring", "cookies", "uncle"], "metric_results": {"EM": 0.5, "QA-F1": 0.6413391101357812}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.35294117647058826, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.2222222222222222, 0.0, 0.0, 0.5, 1.0, 0.8, 0.2105263157894737, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.7692307692307692, 1.0, 0.08695652173913042, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.4615384615384615, 1.0, 1.0, 0.9411764705882353, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.24000000000000002, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-1427", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-6321", "mrqa_triviaqa-validation-7778", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-2156", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-7061"], "SR": 0.5, "CSR": 0.5462101063829787, "EFR": 0.9375, "Overall": 0.7116638962765958}, {"timecode": 47, "before_eval_results": {"predictions": ["Acadia National Park", "Earl Long", "the Grand Duchy of Luxembourg", "paaiyan", "Space Shuttle Challenger", "Rolling Stones", "F Thomas G Nazareth", "Lapis lazuli", "The Pentagon Building", "a valley fold", "snails", "bamboo", "the Vietnam War", "port Royal", "Gerard Mercator", "a sailor", "Barnum & Bailey Circus", "Dizzy Gillespie", "Marcia Marcia Marica", "Ernie Els", "former Yugoslav Republic of Macedonia", "gestation", "a skin", "There Will Be Blood", "Herb Alpert", "John Adams", "Rolling Stones", "Field Marshal Bernard Montgomery", "Zeus", "Fast Food Nation", "The Tasmanian devil", "a symbol of India's freedom", "a terrarium", "cyclotron", "John Alden", "a brothel", "paseo de Gracia", "a Yellow Ribbon", "the Indy 500", "\"Project Runway\"", "a boy's treble", "porter", "a dinner party", "the Atlantic Ocean", "a port town in England's county of Kent.", "Kamehameha I", "menudo", "Alan Alda", "a Strong Waterproof Clue", "a park", "sirloin", "a children's novel by American author Elizabeth George Speare", "Walter Pauk", "left atrium of the heart", "the American Civil War", "a millenarian restorationist Christian denomination with nontrinitarian beliefs distinct from mainstream Christianity.", "the Soviet Union", "Rousillon Rupes", "Obafemi Martins", "\"Twice in a Lifetime\".", "two-state solution", "Republican Party", "a communications breakdown at a Federal Aviation Administration facility,", "The Tempest"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5619791666666666}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.4, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2144", "mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-847", "mrqa_searchqa-validation-11698", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-9392", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-13199", "mrqa_searchqa-validation-4024", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-15464", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-16787", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4769", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-4236", "mrqa_searchqa-validation-11015", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-3996", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-1817", "mrqa_searchqa-validation-10881", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3174", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-109", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-904"], "SR": 0.453125, "CSR": 0.5442708333333333, "EFR": 0.9714285714285714, "Overall": 0.718061755952381}, {"timecode": 48, "before_eval_results": {"predictions": ["photoelectric", "31", "Over", "king Gautamiputra Satakarni", "a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "( such as the muscles of the limbs, abdominal, and intercostal muscles", "bohrium", "London", "Chernobyl", "Dalveer Bhandari", "Bush", "northernmost point on the Earth", "a religious covenant that is described in the Bible", "David Tennant", "a political protest and mercantile protest", "My Heart Will Go On", "Maryland", "A footling breech", "the 1964 Republican National Convention in San Francisco", "four", "Jonathon Dutton", "1990", "the Grand Cherokee", "Bob Dylan", "qualitative data, quantitative data", "Johannes Gutenberg", "to collect menstrual flow", "( USS )", "William the Conqueror", "Sir Ernest Rutherford", "Nicole Gale Anderson", "William Chatterton Dix", "April 4, 2017", "Gupta", "Freedom Day", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "Gustav Bauer", "sex hormones", "Phillip Schofield and Christine Bleakley", "English - language", "Antarctica", "Henry Haller", "revenge and karma", "the efferent nerves that directly innervate muscles", "1986", "1546", "when the victim of a crime or any wrongful act is held entirely or partially responsible for the harm that befell them", "1928", "Bachendri Pal", "John Adams", "early 2014", "the Old Vic", "Jessica Smith", "Edward Elgar", "\"Big Mamie\"", "six", "\"First Family of Competitive Eating\"", "$2 billion", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "St. Valentine's Day", "(Liza) Minnelli", "Gabriel", "opposition parties"], "metric_results": {"EM": 0.40625, "QA-F1": 0.518721005258961}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true], "QA-F1": [0.4, 0.0, 0.0, 0.0, 0.9500000000000001, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.22222222222222224, 1.0, 0.0, 0.4347826086956522, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.06451612903225806, 0.0, 1.0, 0.5, 1.0, 1.0, 0.09523809523809525, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0909090909090909, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-4001", "mrqa_hotpotqa-validation-1210", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1290", "mrqa_searchqa-validation-2795", "mrqa_searchqa-validation-16493"], "SR": 0.40625, "CSR": 0.5414540816326531, "EFR": 1.0, "Overall": 0.7232126913265307}, {"timecode": 49, "before_eval_results": {"predictions": ["1912", "Standard Oil", "kabuki", "These Boots Are Made for Walking", "pachycephalosaurs", "united states", "( Nancy) Lopez", "the ozone layer", "who\\'s the Boss", "Donnie Wahlberg", "Tasmania", "Oriole Park", "united states", "(Queen) Mary 2", "Zionism", "Prague", "dressage", "(Isa) Newton", "Toby Keith", "accordion", "the Black swan", "piaf", "the Stratosphere Tower", "parkinsonism", "Strings", "( Warren) Burger", "Guinevere", "Department of Energy", "tangerine", "the Rhineland", "the Dead Ringers", "( Johann) Strauss", "Solidarity", "(Meg) Carter", "romantic comedy", "(Charlie) Lindbergh", "water lily", "the Haunted Mansion", "blog", "the Golden Bear", "civil war", "glass", "Teen Titans Go!", "a swan", "Istanbul", "Mary Poppins", "St. Louis", "the Amish", "the executive branch", "Levi\\'s", "Badminton", "Canada", "broadcast live on BBC One on Saturday evenings", "autopistas", "Phar Lap", "a dove", "Lewis Carroll", "1983", "Excalibur Hotel and Casino", "Czech Kingdom", "Arsenal", "Abdullah Gul", "humans", "Action Comics"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7373263888888888}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.8333333333333334, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-4796", "mrqa_searchqa-validation-3755", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-12385", "mrqa_searchqa-validation-767", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-13380", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-1337", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-16200", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-15313", "mrqa_naturalquestions-validation-8350"], "SR": 0.640625, "CSR": 0.5434375, "EFR": 1.0, "Overall": 0.723609375}, {"timecode": 50, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1338", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5266", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1604", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7716", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12265", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3025", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-3764", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8267", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9911", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3635", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.875, "KG": 0.51796875, "before_eval_results": {"predictions": ["Aerosmith", "Willa Cather", "President", "the Who", "a science fiction novel", "Otto von Bismarck", "analog", "TravBuddies", "Luisa Tetrazzini", "Renoir", "the polio vaccine", "Peter Behn", "Hedonist Rothschild", "a bar exam", "Fyodor Dostoevsky", "Smucker", "a Spanish conquistador", "China and Russia", "grease", "Hollandaise", "Esau", "Dry Ice", "Martin Luther King III", "spontaneous", "a catalyst", "Kansas City", "numbers", "Uganda", "senators", "Sappho", "the Battle of Thermopylae", "Maccabean", "John Paul Jones", "Hamlet", "a flounder", "the Ganges", "New Brunswick", "Copacabana", "\"I Write the Songs\\'", "We Own the Night", "Shimon Peres", "Mr. & Mrs. Smith", "a high fever", "a strawberry", "Memphis", "Thomas Mann", "Krackel", "a dog eat dog world", "Dmitri Mendeleev", "Azkaban", "tea leaves", "thia Weil", "an Arabic masculine given name and occasional surname with the meaning `` beloved ''", "Secretary of State", "Microsoft", "General John J. Pershing", "Ross Kemp", "Taylor Swift", "September 23, 1935", "An invoice, bill or tab", "Martin \"Al\" Culhane,", "\"Harry Potter and the Order of the Phoenix\"", "Hyundai Steel", "2015"], "metric_results": {"EM": 0.578125, "QA-F1": 0.65176204004329}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.25, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15858", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-6043", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-130", "mrqa_searchqa-validation-16767", "mrqa_searchqa-validation-803", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-892", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-12656", "mrqa_searchqa-validation-13163", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-14305", "mrqa_searchqa-validation-729", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-16283", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-2780", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-7741", "mrqa_naturalquestions-validation-998", "mrqa_triviaqa-validation-6411", "mrqa_hotpotqa-validation-5801", "mrqa_newsqa-validation-762", "mrqa_newsqa-validation-1048"], "SR": 0.578125, "CSR": 0.5441176470588236, "EFR": 0.9629629629629629, "Overall": 0.7366504970043574}, {"timecode": 51, "before_eval_results": {"predictions": ["graham crackers", "Wilkie", "Chief of Staff", "the Bible", "Ugarit", "the Civil War", "the Nobel Prize", "Roussimoff", "Vampire", "Technetium", "Nazareth", "867-5309", "Miss Havisham", "Thailand", "Taft", "opal", "Taft", "dense", "air", "echidna", "water", "porcelain", "Synchronicity", "Taft", "bees", "dark matter", "Reptiles", "Conakry", "William Herschel", "Augustus", "Barbara Walters", "Jubal Anderson", "area", "pumice", "watermelon", "Cole Porter", "Taft", "Rotary", "Louis Comfort Tiffany", "Cosmopolitan", "Democratic Republic of Madagascar", "C.G. Jung", "Carnarvon", "Ontario", "Olympia", "Copernicus", "Lovesexy", "Candlestick", "the Black Death", "Google", "Defense", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "1898", "Brevet Colonel Robert E. Lee", "Don Revie", "Piave", "the Buddha", "Lancia", "1963", "Brittany Snow", "Christopher Savoie", "authorizing killings and kidnappings by paramilitary death squads.", "\"Watchmen\"", "Cork"], "metric_results": {"EM": 0.5, "QA-F1": 0.6069196428571428}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.4, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.9428571428571428, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-1496", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-5387", "mrqa_searchqa-validation-1598", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-440", "mrqa_searchqa-validation-5000", "mrqa_searchqa-validation-10284", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-9206", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11468", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-14323", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-4585", "mrqa_searchqa-validation-4971", "mrqa_naturalquestions-validation-7223", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-4667", "mrqa_hotpotqa-validation-3399"], "SR": 0.5, "CSR": 0.5432692307692308, "EFR": 1.0, "Overall": 0.7438882211538462}, {"timecode": 52, "before_eval_results": {"predictions": ["Deere", "(Ella) VICTORIA", "electrons", "the Missouri River", "brandy", "George Babbitt", "GIGO", "Dr. Bartolo", "because she comes from sinners", "Rome", "the Isle of Wight", "Colorado Springs", "hay", "Possession", "(Etonian) No.", "a contact lens", "kilometers", "the Buk missile system", "Vibe", "Pulp Fiction", "yelping", "Frederick Forsyth", "August 15, 1947", "Princess Leia", "Vietnam", "Vince Lombardi", "the global village", "Dubliners", "Sudan", "Kwanzaa", "Warren Buffett", "Charlie\\'s Angels", "President Lincoln", "imagism", "the whimper", "a word", "oscar wilde", "Taiwan", "Mickey Spillane", "Buzz Lightyear", "Jack Bauer", "ants", "kidney stones", "Necessity", "Skull", "President Eisenhower", "Atlanta", "Texas", "fat", "holidays", "tiger\\'s eye Starbuck", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Sir Donald Bradman", "The Caucasus Mountains", "Rambo", "John Darby", "antelope", "Jung Yun-ho", "7.63\u00d725mm Mauser", "united Ireland", "raping and murdering a woman in Missouri.\"", "troy Livesay", "damage from Hurricane Irene and Tropical Storm Lee in Bradford, Dauphin, Columbia, Wyoming and Luzerne counties.", "Estadio Victoria"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6125}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-3292", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-14452", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-14015", "mrqa_searchqa-validation-9935", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-12531", "mrqa_searchqa-validation-13465", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-5962", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-9744", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-8817", "mrqa_searchqa-validation-14998", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6562", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-3364"], "SR": 0.546875, "CSR": 0.5433372641509434, "EFR": 1.0, "Overall": 0.7439018278301888}, {"timecode": 53, "before_eval_results": {"predictions": ["to encourage rebellion against the British authorities", "Debbie Gibson", "three", "February 29", "Ireland", "1942", "an expression of unknown origin", "the heart", "March 26, 1973", "Ancy Lostoma duodenale", "April 7, 2016", "Gare du Nord", "number of games where the player played, in whole or in part", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "Frank Langella", "The Maidstone Studios in Maidstone, Kent", "Human fertilization", "Father Christmas", "16 seasons", "Bill Russell", "the Washington Redskins", "Donald Trump", "The vascular cambium", "Epidemiology", "1895", "the contestant", "between the Mediterranean Sea to the north", "a Border Collie", "the Washington metropolitan area", "Julie Adams", "The india's fastest train now called Gatiman express its ranges 160km / hour between Delhi to Agra In 100 min its cross 180km", "John Young", "Kevin Spacey", "novella", "a head, neck, a midpiece and a tail", "Gene MacLellan", "The Buckwheat Boyz", "2010", "on location", "Frankie Muniz", "before the first letter of an interrogative sentence or clause to indicate that a question follows", "Jack - Jack Parr", "Bailey Graffman", "not about drugs", "2017", "1978", "a loanword of the Visigothic word guma `` man ''", "Rust", "birch", "John Brown", "brothers Henry, Jojo and Ringo Garza", "Ken Barlow", "tidal", "Roman history", "a vegetarian dish called Buddha\\'s delight", "Matt Kemp", "pro-Confederate partisan rangers", "they don't feel Misty Cummings has told them everything she knows.\"", "30,000", "The bodies of Guerline Damas, 32; Michzach, 9; Marven, 6; Maven, 5; Megan, 3; and Morgan, 11 months, were discovered Saturday,", "Pablo Picasso", "(Scott) Peterson", "the Capitol", "blood pressure"], "metric_results": {"EM": 0.515625, "QA-F1": 0.639913845966515}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.06451612903225806, 0.6666666666666666, 1.0, 0.0, 1.0, 0.17391304347826084, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.631578947368421, 0.2, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.4, 0.5, 0.8571428571428572, 1.0, 0.0625, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1009", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-4240", "mrqa_triviaqa-validation-6845", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-3154", "mrqa_hotpotqa-validation-458", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3873", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-5691"], "SR": 0.515625, "CSR": 0.5428240740740741, "EFR": 0.9032258064516129, "Overall": 0.7244443511051374}, {"timecode": 54, "before_eval_results": {"predictions": ["Norway", "Ecuador", "Home Improvement", "iron", "(Oncorhynchus keta)", "Berlin", "Iago", "Fidel Castro", "Patrick Floyd Garrett", "Montana", "Harvard University", "Custer", "an arboretum", "Marie Curie", "Abnormal Psychology", "love", "American political leader who was Vice President of the United States during Abraham Lincoln's first administration", "the Italian flag", "Samuel Butler", "Nancy Reagan", "Abraham Lincoln", "teddy bears", "Crouching Tiger", "Baseball-Reference.com", "upsilon", "banknotes", "arizonensis", "Jupiter", "conformation", "Ziegfeld Girl", "David Cassidy", "volcanic cones", "the Louvre", "Cyrillic", "royal opera house", "oxygen", "the money changers", "Wessex", "the snowmobile", "Aaron Copland", "red", "voltage", "Act I of The Royal Ballet", "Nikola Tesla", "Lil Jon", "the diamond", "the plum", "Lizzie Borden", "hockey", "Pop-Tarts", "Encephalopathy", "Henry Purcell", "Mankombu Sambasivan Swaminathan", "eight", "Ethiopia", "a cactus", "Argentina", "Real Madrid and the Spain national team", "1983", "Richard Allen Street", "Brazil forward Ronaldinho", "\"Empire of the Sun,\"", "whether he should be charged with a crime,", "four"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6260416666666666}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-8125", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-2634", "mrqa_searchqa-validation-8825", "mrqa_searchqa-validation-16878", "mrqa_searchqa-validation-10404", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-6474", "mrqa_searchqa-validation-13693", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9692", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-4573", "mrqa_searchqa-validation-5952", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-13757", "mrqa_newsqa-validation-168", "mrqa_triviaqa-validation-261"], "SR": 0.546875, "CSR": 0.5428977272727273, "EFR": 1.0, "Overall": 0.7438139204545455}, {"timecode": 55, "before_eval_results": {"predictions": ["Santa Fe", "a Beanie Baby", "kick drum", "chess", "Jamaica Blue Mountain Coffee", "Berlin", "Comedy Central", "the best of Michael Phelps", "the Mighty Five", "Romeo and Juliet", "Cerberus", "the reaper", "Overland", "bullion", "Plutarch", "free skate", "the submarine", "St. Augustine", "Trinity", "Fiji", "the burnoose", "Thomas Edison", "the Mekong", "the 36th", "Valentina Tereshkova", "Canada", "the crossword", "Missouri", "ribonucleic acid", "Rubeus Hagrid", "Manitoba", "Death of a Salesman", "Chocolate", "inshallah", "Saudi Arabia", "Pamela Anderson", "Jenny Craig", "Idaho", "coppertone", "Edward VI", "the Empire State Building", "ella wheeler", "Tennessee", "the Constitution", "Toronto", "University of Exeter", "OperaA: John Ford", "Lawrence of Arabia", "Andy Warhol", "baseball games", "Tara Reid", "Big Ten Conference Champions Michigan State Spartans", "near the 48th parallel north", "B.F. Skinner", "the Nokia tune", "General Paulus", "Cyprus", "2010", "Tufts", "the Crips", "Uighurs", "he fears a desperate country with a potential power vacuum that could lash out.", "Monday", "Spc. Megan Lynn Touma,"], "metric_results": {"EM": 0.625, "QA-F1": 0.7007812499999999}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9266", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-365", "mrqa_searchqa-validation-10897", "mrqa_searchqa-validation-6437", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-438", "mrqa_hotpotqa-validation-4049", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2772"], "SR": 0.625, "CSR": 0.5443638392857143, "EFR": 0.9583333333333334, "Overall": 0.7357738095238096}, {"timecode": 56, "before_eval_results": {"predictions": ["former English English county of Humberside", "the Federal Correctional Institution, Terre Haute", "\"Dumb and Dumber\"", "the first trans-Pacific flight from the United States to Australia", "John Hunt", "Walt Disney Productions", "Reinhard Heydrich", "British", "\"Sheen Michaels Entertainment\"", "\"Rude Boy\"", "1770", "Indianola", "September 10, 1993 to June 20, 1998", "A Bug's Life", "U.S.", "6 mi", "the Qin dynasty", "Kentucky River", "fourth", "\"The Bob Edwards Show\"", "The S7 series", "White Knights of the Ku Klux Klan", "Key West, Florida", "Charlie Puth", "Fort Albany", "Best Performance by an Actress in a Mini Series award from the Australian Film Institute", "the Soviet Union", "the National Society of Daughters of the American Revolution (NSDAR)", "Martin Scorsese", "General Sir John Monash", "Protestant", "the Kentucky RiverBats", "The creator of the comic book series Molly Danger", "Agra", "50 million", "Henry II of England", "Scotty Grainger", "the improvisational style of Isadora Duncan", "a farmers' co-op", "Kairi", "Texas", "the Democratic Unionist Party (DUP)", "44", "Candice Swanepoel", "the greater risk-adjusted return of value stocks over growth stocks", "Chris \"Izzy\" Cole", "multiple awards", "McComb, Mississippi", "\"King of Cool\"", "2009", "\"Losing My Religion,\" \"Stand,\" or \"The One I Love\"", "The Royalettes", "three", "April 17, 1982", "Triumph", "'Q'", "A Race Track", "\"Britain's Got Talent.\"", "Turkey from inside northern Iraq.", "Seoul", "Charleston", "emerald", "Don Juan", "Pandora"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5658200306637806}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true], "QA-F1": [0.9090909090909091, 0.22222222222222224, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.0, 0.13333333333333333, 1.0, 0.9333333333333333, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 1.0, 0.6666666666666666, 0.28571428571428575, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-2571", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-557", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3745", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-983", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-2032", "mrqa_naturalquestions-validation-7906", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1229", "mrqa_newsqa-validation-1503", "mrqa_newsqa-validation-154", "mrqa_searchqa-validation-8091"], "SR": 0.421875, "CSR": 0.5422149122807017, "EFR": 1.0, "Overall": 0.7436773574561404}, {"timecode": 57, "before_eval_results": {"predictions": ["The first version, written in vernacular Italian in 1435 under the title Della pittura", "in a counter clockwise direction", "The episode `` Two Fathers ''", "December 2, 1942", "Ray Charles", "mid November", "when they qualify as a medical practitioner following graduation with a Bachelor of Medicine", "Daniel Suarez", "its population", "the Central and South regions", "Matthew Roberts", "The Death of Archie", "Adam Mitchell", "9.7 m", "Guwahati", "A friendly red cyclops boy", "the modern state system", "efferent nerves", "William Wyler", "Dragon Ball GT", "The Nitty Gritty Dirt Band", "Donald Fauntleroy Duck", "2013", "Friedman Billings Ramsey", "2018", "skeletal muscle", "Joe Spano", "Spanish moss", "Georges Auguste Escoffier", "Nodar Kumaritashvili", "December 27, 2015", "Madeline Reeves", "New England Patriots", "( son of Bindusara )", "Britain of Florida", "a garden in the Government House at New Delhi", "an unknown recipient", "Andy Warhol", "Elected Emperor of the Romans", "Dalveer Bhandari", "Middle Eastern alchemy", "the division of Italy into independent states", "Nalini Negi", "The Rashidun Caliphs", "Lituya Bay in Alaska", "The Demon Barber of Fleet Street", "2002 -- 2003", "George Strait", "A vanishing point", "the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "rules applied by employers or landlords are formally neutral", "Emma Chambers", "Hans Lippershey", "Thermopylae", "August 6, 1845", "an album", "Spain", "Jennifer Arnold and husband Bill Klein,", "A family friend of a U.S. soldier", "$89", "blue", "lump", "a knish", "CBS"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6454618987017884}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true], "QA-F1": [0.1, 0.6666666666666666, 0.0, 1.0, 1.0, 0.28571428571428575, 0.11764705882352942, 0.0, 0.0, 0.6666666666666666, 0.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.11764705882352941, 0.45000000000000007, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-31", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-3189", "mrqa_searchqa-validation-416"], "SR": 0.546875, "CSR": 0.5422952586206897, "EFR": 0.9655172413793104, "Overall": 0.736796875}, {"timecode": 58, "before_eval_results": {"predictions": ["in the five - year time jump for her brother's wedding to Serena van der Woodsen", "111", "every president since Woodrow Wilson", "Uralic", "22", "IBM", "John Adams", "2018", "Jesus Christ", "September 2000", "along the Californian coast", "24", "the Coriolis force", "Hugh S. Johnson", "Paul Lynde", "Erica Rivera", "Malina Weissman", "Han", "Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s", "1970", "DeWayne Warren", "the nucleus", "1996", "Egypt", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Tom Brady", "pilgrimages to Jerusalem", "1996", "Coconut Cove", "Curtis Armstrong", "Dolby Theatre in Hollywood, Los Angeles, California", "Category 4", "Rust", "Karen Gillan", "$1.09 trillion", "1,228 km / h", "Tommy Shaw", "warplanes", "Tabitha and Napoleon", "Central Germany", "Atlanta", "Ricky Nelson", "James Chadwick", "Welch, West Virginia", "Tristan Rogers", "15 February 1998", "Houston Astros", "Americans who served in the armed forces and as civilians during World War II", "the east coast of Queensland", "in the middle of the 15th century", "Gladys Knight & the Pips", "Lake Nicaragua", "the natural world and mysticism", "rue", "Delphi Lawrence", "Juan Manuel Mata Garc\u00eda", "Edward James Olmos", "vitamin injections", "Scotland", "U.S. troops involved in the operation.\"", "heart", "James Stewart", "Frank Sinatra", "Salisbury"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7148063221500721}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.3, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 0.07999999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-6061", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4134", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2503", "mrqa_searchqa-validation-8636", "mrqa_searchqa-validation-9866", "mrqa_hotpotqa-validation-3324"], "SR": 0.59375, "CSR": 0.543167372881356, "EFR": 0.9615384615384616, "Overall": 0.7361755418839635}, {"timecode": 59, "before_eval_results": {"predictions": ["three French journalists,", "Six", "that Birnbaum had resigned \"on her own terms and own volition.\"", "Frank Ricci", "the banned substance cortisone.", "Wednesday.", "Linda Hogan", "be silent.", "Crandon, Wisconsin", "Turkey", "John Demjanjuk", "piracy problem was fueled by environmental and political events", "eight", "Haiti.", "Missouri", "\"peregruzka,\"", "Haiti", "9", "many as 250,000", "Maj. Nidal Malik Hasan,", "Mobile County Circuit Judge Herman Thomas", "to stop the Afghan opium trade", "Nick Adenhart", "order", "One of Osama bin Laden's sons", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Operation Crank Call,\"", "her death in the Holmby Hills, California, mansion he rented.", "Susan Boyle", "10-person", "April.", "recent theft in Switzerland of two paintings by Pablo Picasso,", "promotes fuel economy and safety while boosts the economy", "gasoline", "to do jobs that Arizonans wouldn't do.", "a \"prostitute\"", "digging", "Tottenham", "U.S. diplomacy to prevent Iran from developing nuclear weapons", "Obama and McCain", "3-2", "\"remained at the bottom of the hill surviving on leaves and water from a nearby creek,\"", "\"Dance Your Ass Off.\"", "56", "in an interview Tuesday on CNN's \"Larry King Live.\"", "15-month", "sedative", "Summer", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "heavy turbulence", "Zachac Efron", "drivers who were Daytona Pole Award winners, former Clash race winners, Former Daytona 500 pole winners who competed full - time in 2017, and drivers who qualified for the 2017 Playoffs", "the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "show", "2", "John Buchan", "NCAA Division I Football Bowl Subdivision", "Kris Kristofferson", "Ben Savage", "Narnia Chronicles", "Bering Strait", "Dame Ninette de Valois", "Ayahuasca"], "metric_results": {"EM": 0.375, "QA-F1": 0.5170092553446743}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.9473684210526316, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.12500000000000003, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.8571428571428571, 0.0, 0.08695652173913043, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6153846153846153, 1.0, 0.5, 0.5499999999999999, 0.4799999999999999, 0.8205128205128205, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-920", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-653", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-318", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-1524", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-585", "mrqa_naturalquestions-validation-4387", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-7393", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-3871", "mrqa_searchqa-validation-16736", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-13957"], "SR": 0.375, "CSR": 0.5403645833333333, "EFR": 1.0, "Overall": 0.7433072916666666}, {"timecode": 60, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2665", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3941", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-10461", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11693", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11890", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-11925", "mrqa_searchqa-validation-12105", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12441", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13915", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14323", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14450", "mrqa_searchqa-validation-14481", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16382", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-1817", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-2634", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-5612", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-892", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9852", "mrqa_searchqa-validation-9911", "mrqa_searchqa-validation-9935", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1876", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1145", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5886", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937"], "OKR": 0.8203125, "KG": 0.4765625, "before_eval_results": {"predictions": ["$7.8 million", "prostate cancer", "Donald Duck", "Whitney Houston", "new DNA evidence", "South Africa", "consumer confidence", "Saturn", "Prague", "35,000.", "Osama", "The EU naval force", "police", "threatening messages", "Nigeria", "misdemeanor", "firefighter", "$273 million", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Haiti", "air support.", "20", "$250,000", "March 22,", "12 off-duty federal agents in southwestern Mexico,", "Blacks and Hispanics", "Australian officials", "the leader of a drug cartel that set off two grenades during a public celebration in September,", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "Rev. Alberto Cutie", "meter reader", "the man facing up, with his arms out to the side.", "Garth Brooks", "Monday", "Friday", "10,000", "Ryan Adams.", "$1.5 million", "three out of four", "Six alleged victims, who are relatives of the five suspects, made accusations of sexual abuse.", "a cancerous tumor.", "581 points", "his health and about a comeback.", "up", "\"utterly baseless.\"", "Mexicans who are unemployed or underemployed", "Caylee, who was 2", "0-0", "her boyfriend,", "Lonnie", "the first", "the words spoken to Adam and Eve after their sin", "Tokyo for the 2020 Summer Olympics", "access to US courts", "Doncaster Rugby League", "Usain Bolt", "1973", "2,099", "PPG Paints Arena", "2017", "lactic acid", "The Greatest Show on Earth", "The republic", "Germany"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7545445261437909}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.22222222222222224, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-110", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-3229", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7987", "mrqa_triviaqa-validation-4950"], "SR": 0.671875, "CSR": 0.5425204918032787, "EFR": 1.0, "Overall": 0.7170978483606557}, {"timecode": 61, "before_eval_results": {"predictions": ["Windows Easy Transfer", "John Cooper Clarke", "Charlotte of Mecklenburg - Strelitz", "two", "Judi Dench", "accomplish the objectives of the organization", "Omar Khayyam", "P.V. Sindhu", "1665 to 1666", "Saturday", "1982", "Siddharth Arora / Vibhav Roy", "1948", "the mitotic error called tripolar mitosis", "Butter Island off North Haven, Maine in the Penobscot Bay", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Pat McCormick", "10,605", "U.S. Electoral College", "American country music artists Reba McEntire and Linda Davis", "August Darnell", "the southwest and along the Yangtze", "New York City", "July 21, 1861", "Nashville, Tennessee", "Paspahegh Indians", "the chryselephantine statue of Athena Parthenos", "about 375 miles ( 600 km ) south of Newfoundland", "April 12, 2017", "October 2012", "Tom Burlinson, Red Symons and Dannii Minogue", "John Joseph Patrick Ryan", "49 cents", "counter clockwise", "Kit Harington", "Human anatomy", "above the light source and under the sample in an upright microscope", "divergent tectonic", "prokaryotic", "neutral", "President pro tempore", "1877", "18", "rootlets", "norm that sovereigns had no internal equals within a defined territory and no external superiors as the ultimate authority within the territory's sovereign borders", "winter festivals", "a recognized group of people who jointly oversee the activities of an organization", "the 1820s", "The Royalettes", "Kate '' Mulgrew", "Joe Young", "Felix Leiter", "sun origin", "James Hogg", "The Nassau Herald", "\"Realty Bites\"", "national aviation branch", "Thessaloniki and Athens,", "prostate cancer,", "The Screening Room", "Chicken Little", "Saturn", "Russia's", "Isolde"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6746065828551293}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6428571428571429, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.9302325581395349, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-9204", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-1975", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-3093", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3398", "mrqa_searchqa-validation-4176", "mrqa_triviaqa-validation-6243"], "SR": 0.59375, "CSR": 0.5433467741935484, "EFR": 0.9230769230769231, "Overall": 0.7018784894540943}, {"timecode": 62, "before_eval_results": {"predictions": ["1983", "Bacon", "from 1922 to 1991", "79", "Gibraltar", "1 January 1904", "Thebes", "Brooke Wexler", "March 29, 2018", "in the 1980s", "Montgomery", "Evermoist", "in the mid - to late 1920s", "differential erosion", "Kanawha River", "Graham McTavish", "Thomas Alva Edison", "since been adopted by five other countries", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "Richard Masur", "Frankie Valli", "one", "JackScanlon", "Saturday", "sperm and ova", "sometime in 2018", "2015", "Marsh, Capen & Lyon", "Nickelback", "Ledger", "the International Border ( IB )", "in a 1945 NCAA game between Columbia and Fordham", "2017", "permanently absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "in a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Pink Floyd", "6 March 1983", "decades after its initial release", "1986", "1939", "Himadri Station", "on a beach in Malibu, California", "birch", "February 2017 in Japan and in March 2018 in North America and Europe", "FaZe Rug", "at the fictional elite conservative Vermont boarding school Welton Academy", "1973", "9 February 2018", "recessive", "94 by 50 feet", "Spanish Dominican Tom\u00e1s de Torquemada", "1945", "Kent", "Spanish", "Dusty Dvoracek", "South America", "Los Angeles", "has an inspiration: U.S. President Barack Obama.", "the foyer of the BBC building in Glasgow, Scotland", "has been no arrests related to the inauguration on the mall for various first-aid needs.", "Frederic Chopin", "spring", "pesos", "Rev. Alberto Cutie"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6983530287114846}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.5, 0.9600000000000001, 0.9523809523809523, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.2666666666666667, 1.0, 0.9411764705882353, 1.0, 1.0, 0.33333333333333337, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-2168", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-2984", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5791", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-3143", "mrqa_triviaqa-validation-4726", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-219", "mrqa_newsqa-validation-1330"], "SR": 0.546875, "CSR": 0.5434027777777778, "EFR": 1.0, "Overall": 0.7172743055555555}, {"timecode": 63, "before_eval_results": {"predictions": ["Malay", "Billy Martin", "Nova Scotia", "chainmaille", "osca", "Hiroshima", "Morocco", "Opera Flashcards", "Anne Rice", "(Henry) Ford", "Pop art", "embalming", "Port of Portland", "Mariah Carey", "Dionysus", "a coral reef fish", "symbiosis", "Planets", "Donald Trump", "When You Look Me In The Eyes", "The Lost World", "Prince Edward Island", "New York Presbyterian Hospital", "Bab el Mandeb Strait", "Red Heat", "Atlas Mountains", "kafkaesque", "Heather Mills", "polar", "Paris", "Mont Blanc On", "Rene Lacoste", "preemption", "the Nobel Prize", "summer", "osca", "Jawaharlal Nehru", "the last day", "Michelangelo", "cat", "congruent", "Spain", "toadies", "San Francisco", "Brief History of Time", "crossword", "Macy's", "Geoffrey Chaucer", "Hawthorne", "Hillary's America", "Benazir Bhutto", "CBS", "Gibraltar", "Orographic lift", "Virginia Plain", "strawberry", "Venice", "Chad", "Arnold M\u00e6rsk M\u00f8ller Foundation", "Stephen King", "Pat Quinn", "murder in the beating death of a company boss who fired them.", "\"Walk -- Don't Run\"", "Comeng and Clyde Engineering"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6557291666666667}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-7176", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-4487", "mrqa_searchqa-validation-661", "mrqa_searchqa-validation-8555", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-6802", "mrqa_searchqa-validation-14989", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-16392", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-9589", "mrqa_searchqa-validation-15031", "mrqa_searchqa-validation-4348", "mrqa_hotpotqa-validation-5688", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2308"], "SR": 0.609375, "CSR": 0.54443359375, "EFR": 1.0, "Overall": 0.71748046875}, {"timecode": 64, "before_eval_results": {"predictions": ["a person of Latin American or Iberian ancestry", "Bonnie and Clyde", "Forrest Gump", "My Favorite Mistake", "The Crossing Guard", "Thomas Beekman", "I Have No Mouth, and I Must Scream", "Friday Night Lights", "contractions", "skull and crossbones", "large country in the world", "Florida State", "Ukraine", "ship", "a Tibetan antelope", "The Godfather", "a bolt", "Australia", "Napalm", "Roald Dahl", "Mount Kenya", "John Lennon", "the Stamp Act", "Yale Basmati", "CO2", "The Battle of Thermopylae", "El final de un diario", "Mulberry Street", "Romeo & Juliet", "Prescott", "Helen Hayes", "Wesley Clark", "cobalt", "Sing Sing", "salmon", "a falling star", "Herman Melville", "Abercrombie & Fitch", "Beatrix Potter", "the Romance of the Rose", "a cassowary", "the Gadsden Purchase", "the umbilical cord", "trees", "Sweden", "the House of Lords", "the Red Cross", "terrorists", "The Sunshine Band", "the Somme", "Graceland", "David Tennant", "the southeastern coast of the Commonwealth of Virginia in the United States", "pulmonary heart disease", "9", "squash", "The first website", "the Gospel Starlighter", "500-room", "Wal-Mart Canada", "Tuesday,", "Diversity,", "were directly involved in an Internet broadband deal with a Chinese firm.", "gang rape"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6877649853801169}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true], "QA-F1": [0.0, 0.4, 1.0, 0.5, 0.0, 0.0, 0.2222222222222222, 0.8, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.8, 1.0, 1.0, 0.9473684210526316, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11200", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-15728", "mrqa_searchqa-validation-6468", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-16325", "mrqa_searchqa-validation-691", "mrqa_searchqa-validation-2585", "mrqa_searchqa-validation-16100", "mrqa_searchqa-validation-8741", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-9441", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-11743", "mrqa_searchqa-validation-2681", "mrqa_searchqa-validation-6512", "mrqa_searchqa-validation-13467", "mrqa_searchqa-validation-12394", "mrqa_searchqa-validation-6678", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-15724", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-14787", "mrqa_searchqa-validation-3727", "mrqa_naturalquestions-validation-5912", "mrqa_triviaqa-validation-1283", "mrqa_hotpotqa-validation-2565", "mrqa_hotpotqa-validation-3571", "mrqa_newsqa-validation-3111"], "SR": 0.53125, "CSR": 0.5442307692307693, "EFR": 1.0, "Overall": 0.7174399038461539}, {"timecode": 65, "before_eval_results": {"predictions": ["the Blue Ridge Parkway", "3,384,569", "Vishal Bhardwaj", "around 169 CE", "Ed O'Neill", "Milwaukee Bucks", "138,535", "Dennis Hull, as well as painter Manley MacDonald.", "Max Martin, Savan Kotecha and Ilya Salmanzadeh", "a jersey or uniform that a sports team wear in games instead of its home outfit or out of necessity.", "Love Letter", "Brazil", "1968", "1961", "Stacey Kent", "Shenandoah National Park", "Regionalliga Nord", "Galo", "Samantha Spiro", "William Shakespeare", "over 1.6 million", "Joulupukki", "Ghana", "special economic zone", "Portal A Interactive", "Graduados", "Sada Thompson", "World Health Organization", "Chow Tai Fook Enterprises", "Michelle Anne Sinclair", "2011", "9 November 2012", "Hawaii", "Lalit", "Kal Ho Naa Ho", "India", "Ronald Reagan", "Musicology", "his left hand", "1835", "1926 Paris during the period of the Lost Generation", "Erreway", "Forbes", "January 28, 2016", "69.7 million", "500-room", "Russia", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "the southern portion of Carroll County, the eastern portion of Grafton County, and the northern portions of Strafford County and Merrimack County", "Black Panther Party", "globetrotters", "in the United Kingdom ( with the exception of Scotland since August 1, 2016 )", "mitochondrial inner membrane", "Butter Island off North Haven, Maine in the Penobscot Bay", "eye", "Tolstoy", "gizzard", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "2,000", "Asashoryu", "sirloin", "Shirley Jackson", "Sue Miller", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6097129937246217}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.09999999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.4, 0.4444444444444445, 1.0, 0.4, 1.0, 0.8, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.4, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.5581395348837209, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-3731", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-3880", "mrqa_hotpotqa-validation-4092", "mrqa_hotpotqa-validation-5108", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4776", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-744", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-180", "mrqa_triviaqa-validation-3498", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-2296"], "SR": 0.484375, "CSR": 0.5433238636363636, "EFR": 0.9696969696969697, "Overall": 0.7111979166666667}, {"timecode": 66, "before_eval_results": {"predictions": ["musician", "Captain Hans Geering", "50JJB Sports Fitness Clubs and the attached retail stores", "2015", "North America", "Seventeen", "Bhushan Patel", "playback singer", "Mark O'Connor", "Kinnairdy Castle", "South African-born", "Barbara Feldon", "The 2008\u201309 UEFA Champions League", "National Hockey League", "Ontogenetic depth", "Parlophone Records", "Soldier in Truck", "eight", "Cuban", "arts manager", "The Royal Family", "girls aged 11 to 18", "Jackie Harris", "water", "the National Basketball Development League", "Operation Overlord", "invoice", "Wiltshire", "1851", "the first month of World War I", "12-year", "World War II", "Graham Payn", "Martin Truex Jr.", "twice", "Malayalam cinema", "47,818", "Every Rose Has Its Thorn", "13", "New York", "1953", "German", "Sc Coyotesat Mountain", "AC/DC founders Angus Young and Malcolm Young", "Boston Celtics", "1912", "Jewish", "Bill Curry", "LGBT rights activist", "1968", "311", "Jason Lee", "Missi Hale", "slavery", "Florida Current", "The History Boys", "a tabby", "Mexico", "three,", "'overcharged.'\"", "Dame Melba", "Gary", "Henry Hudson", "glial cells"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6181490384615385}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-4061", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-2842", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5294", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-5251", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-7417", "mrqa_newsqa-validation-2935", "mrqa_searchqa-validation-7226", "mrqa_naturalquestions-validation-3368"], "SR": 0.53125, "CSR": 0.5431436567164178, "EFR": 1.0, "Overall": 0.7172224813432836}, {"timecode": 67, "before_eval_results": {"predictions": ["Classical realism", "Dr. Alberto Taquini", "democracy and personal freedom", "Maria Szraiber", "2015", "Fu\u00dfballklub Austria Wien", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "Edward Albert Heimberger", "Squam Lake", "lambics", "Croatian retired professional basketball player", "Harpe brothers", "Adam Rex", "Marvel's Agent Carter", "Everton", "\"Holinshed's Chronicles\"", "twelfth", "Coalwood, West Virginia", "1975", "Dark Heresy", "Theodore Robert Bundy", "1943", "doctorates", "URO VAMTAC", "Malta", "East Knoyle", "Philadelphia", "(Yell!", "Jyothika", "The song peaked at #30 on the Hot R&B/Hip-Hop Songs chart", "24 January 76 \u2013 10 July 138", "Leonard Cohen", "General Theological Seminary", "BraveStarr", "25 million records", "Paul Avery", "Sunflower County", "848 km", "Ellesmere Port", "Homer Hickam, Jr.", "South America", "Montreal", "Eugene", "Chief of the Operations Staff of the Armed Forces High Command", "CBS News", "Philadelphia", "Parlophone", "June", "the best known globetrotters", "Henry Lau", "John Schlesinger", "Pasek & Paul", "Diary of a Wimpy Kid", "Ed Sheeran", "Sarah Palin", "Ub Iwerks", "The Frighteners", "onto the college campus", "14 bodies", "one American diplomat to a \"prostitute\"", "touchpad", "Muppet Labs Nose-Warmer", "Morocco", "arson"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6616003787878788}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.8, 0.4, 0.5, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-259", "mrqa_hotpotqa-validation-5082", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-2747", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-5385", "mrqa_naturalquestions-validation-714", "mrqa_triviaqa-validation-6828", "mrqa_newsqa-validation-3337", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-5330", "mrqa_newsqa-validation-1398"], "SR": 0.515625, "CSR": 0.5427389705882353, "EFR": 0.9354838709677419, "Overall": 0.7042383183111954}, {"timecode": 68, "before_eval_results": {"predictions": ["Stephen T. Kay", "Brian Graden", "Jay Gruden", "Wayman Tisdale", "New Boston Air Force Station", "the Corps of Discovery", "sarod", "Fleetwood Mac", "County Louth", "Chelmsford City", "1995", "Comedy Central", "five", "the Lazio region", "Mick Jackson", "The Livingston family", "U.S. saloon-keeper", "Fort Albany", "itty Hawk", "the Qin dynasty", "Our greatest comedienne - Australia's Lucille Ball", "Charles de Gaulle Airport", "Francophone", "Diana Canova", "cricket fighting", "Jaguar Land Rover", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "Noel Gallagher", "1966", "Vogue", "band director", "Ford Field in Detroit, Michigan", "drake v. Goldberg", "Mary O'Connell", "Bolton, England", "February 9, 1994", "Las Vegas", "Kongo", "Missouri River", "World War II", "Ector County", "Norse mythology", "Mercedes-Benz Superdome in New Orleans, Louisiana", "1979 to 2013", "Originally wrestling for WWE, Cena recorded the song in 2005 for his debut studio album, \"You Can't See Me\".", "October 12, 1962", "rural", "Lombardy region", "August 14, 1848", "Punjabi/Pashtun", "41st President", "Samaria", "Hol chose to charter a plane to reach their next venue in Moorhead, Minnesota", "1948", "4468 Mallard is a London and North Eastern Railway Class A4 4-6-2 Pacific steam locomotive", "Sherlock Holmes", "Tokyo", "in September internet giant Twitter announced its intention to set up headquarters in Dublin.", "near Warsaw, Kentucky,", "FARC", "W. Somerset Maugham", "a pine", "not a fast typist", "Thomas Jefferson"], "metric_results": {"EM": 0.375, "QA-F1": 0.5589330808080808}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.5, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.6666666666666666, 0.2222222222222222, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.7272727272727273, 0.0, 0.0, 0.8, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-309", "mrqa_hotpotqa-validation-5779", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-78", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-107", "mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-1316", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7939", "mrqa_triviaqa-validation-1118", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-1037", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-16102", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-1104"], "SR": 0.375, "CSR": 0.5403079710144927, "EFR": 1.0, "Overall": 0.7166553442028986}, {"timecode": 69, "before_eval_results": {"predictions": ["November 1999", "the senior-most judge of the supreme court", "the Norman given name Robert", "March 16, 2018", "the Maryland Senate's", "Winston Churchill", "relieves the driving motor from the load of holding the elevator cab", "The federal government", "Bongos", "December 2, 2013", "the 7th century", "from the Adventures of Luke Skywalker", "2018", "Walter Pauk", "Walter Egan", "1973", "maquila", "1959", "Schwarzenegger", "Salman Khan", "2016", "2003", "Isekai wa Sum\u0101tofon", "two - year terms", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "New Zealand", "a large roasted turkey", "currently a free agent", "Joe Pizzulo and Leeza Miller", "Times Square in New York City west", "1 BC", "Captaincy General of Guatemala", "the mid-1970s", "Florida and into the town of Coconut Cove", "the state sector", "tropical desert climate", "1988", "in the 1970s", "anembryonic gestation", "February 27, 2015", "John Hancock", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "lakes or reservoirs at high altitudes", "March 5, 2014", "FIGG Bridge Engineers", "Carol Ann Susi", "a hydrolysis reaction", "a crown cutting of the fruit", "Chris Rea", "2017", "16 August 1975", "a pistil", "Prophet Samuel", "a ship", "Dutch", "Winecoff", "India Today", "Jonathan Breeze", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship were at the Rajprasong intersection in the heart of Bangkok.", "Kabul", "Prison Break", "the Lone Ranger", "Ethiopia", "President Obama and Britain's Prince Charles"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6007083628177379}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.625, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 1.0, 0.42857142857142855, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.0, 0.26666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9563", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-3386", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-7398", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-2261", "mrqa_hotpotqa-validation-886", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2671", "mrqa_searchqa-validation-12778", "mrqa_newsqa-validation-2497"], "SR": 0.484375, "CSR": 0.5395089285714285, "EFR": 0.9393939393939394, "Overall": 0.7043743235930735}, {"timecode": 70, "UKR": 0.69921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2640", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4068", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-465", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-757", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10285", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-371", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8993", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9563", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10731", "mrqa_searchqa-validation-10872", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16049", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-226", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7278", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1330", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1949", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3687", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4301", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-562", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-6362", "mrqa_squad-validation-66", "mrqa_squad-validation-6962", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7693", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-855", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_squad-validation-9923", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4657", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-7726"], "OKR": 0.818359375, "KG": 0.50390625, "before_eval_results": {"predictions": ["jujitsu", "(Peter) Stuyvesant", "Cornell", "a cactus", "NASCAR", "Vivaldi", "8", "Grace Slick", "London", "Sweden", "Phil Lynott", "purple", "Zachary Taylor", "Kempton Park", "Leonardo", "Millais", "Belfast", "coconut shy", "Fulham", "Kent", "bryophyta", "margot fonteyn", "samuel", "a cheese wheel", "a glockenspiel", "Drew Carey", "William", "Mackinac Bridge", "The Poseidon", "\"Sunny After afternoon\"", "tea", "Joan Crawford", "red", "Alexandria", "1969", "the queen", "boxing", "Benjamin Disraeli", "Duke", "Babylon", "Nottingham", "George III", "25", "(S. Jimmy) Beck", "Antoine Lavoisier", "Australia", "Europa", "X-Men Origins: Wolverine", "Jimmy Carter", "David Mitchell", "King Henry I", "December 15, 2017", "Logan Williams", "Malibu Creek State Park", "Loretta Lynn", "National Association for the Advancement of Colored People", "gGmbH", "Fernando Gonzalez", "Alaska or Hawaii.", "\"falling space debris,\"", "jury dutyserve", "ova", "Brooke Shields", "Nepal"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6427083333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-5144", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-1914", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-3041", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-968", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6959", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-2955", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-6308", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-468", "mrqa_hotpotqa-validation-4252", "mrqa_hotpotqa-validation-1720", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-3347", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-14926"], "SR": 0.578125, "CSR": 0.5400528169014085, "EFR": 1.0, "Overall": 0.7123074383802817}, {"timecode": 71, "before_eval_results": {"predictions": ["four", "Wyoming", "tobacco", "eddie shrugging off all inhibitions and putting their total trust in their director", "Phil Spector", "Margaret Beckett", "Robin Hood Men in Tights", "Rapa Nui", "Fringillidae", "Greyfriars", "molly bernard", "scoone", "alfred ritchie", "eddie", "eddie", "True History of the Kelly Gang", "yorkshire", "China", "william shaughnessy", "baron-on-Trent", "trumpet", "$10", "king of midas", "sheep", "the French Open", "a cactus", "a child", "a pear", "greece", "a bone", "a bruise", "barber", "leonard king\u2019s green", "bernish republic", "Washington, D.C.", "Daedalus", "Tommy Roe", "the Faraglioni", "a barleycorn", "Kopassus", "Uranus", "cressida", "game", "behaviors involving the act of observing an unsuspecting person who is naked, in the process of disrobing", "a pascal", "brain", "ash", "847", "sisyphus", "william c. Taylor", "Hawaii", "in Cannes", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "diurnal and insectivorous", "Derry City F.C.", "URO VAMTAC", "Christian", "Oxfam, Save the Children", "U.S. program to assassinate terrorists in Iraq.", "on her Twitter page Tuesday.", "lexicographer", "a great blue heron", "william shakespeare", "mayor of Seoul"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4860422563547564}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.7878787878787877, 0.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.3636363636363636, 1.0, 0.5, 0.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-4377", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5335", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-5806", "mrqa_triviaqa-validation-7364", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-7241", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-7199", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-3439", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-55", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-1222", "mrqa_hotpotqa-validation-1630", "mrqa_newsqa-validation-218", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2547", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-13940", "mrqa_newsqa-validation-3686"], "SR": 0.390625, "CSR": 0.5379774305555556, "EFR": 0.9230769230769231, "Overall": 0.6965077457264958}, {"timecode": 72, "before_eval_results": {"predictions": ["co-pubescents", "Colombia", "Afghan homes and compounds", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "they'd get to bring a new puppy with them to the White House in January.", "Fifty-two", "Kurdish militant group in Turkey", "social and political", "Diana Krall", "U.S. President-elect Barack Obama", "music", "travels four hours to reach a government-run health facility that provides her with free drug treatment.", "July 18, 1994", "off the coast", "Pixar", "Friday", "state senators", "\"The Rosie Show,\"", "Gov. Arnold Schwarzenegger", "South Africa", "Brazil", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "rising disposable income and an increasing interest in leisure pursuits, a growing number of courses, more television coverage and availability of EU funds,", "Clifford Harris,", "Jeffrey Jamaleldine", "to sniff out cell phones.", "eight in 10", "five", "Karen Floyd", "around 8 p.m. local time Thursday", "15-year-old's", "150", "didn't achieve the level of fame as Tiger Woods, but we can surely learn more from his fall from grace than Tiger's.", "Ennis", "Haiti's equivalent of Mardi Gras", "to start a dialogue of peace based on the conversations she had with Americans along the way.\"", "Daniel Radcliffe", "1 million", "55-year-old", "Southeast,", "Venus Williams", "Russian bombers", "Lashkar-e-Jhangvi", "accused Herman Cain of groping her after a 1997 dinner", "Caylee Anthony", "March 22,", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "Rebecca Guerrero,", "prisoners", "Woosuk Ken Choi,", "Apple employees", "In December 1971", "Norman Greenbaum", "in August 1967", "hector's house", "l Leeds", "caesar", "Matt Winer", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "Sun Woong", "a darts", "Joe Louis", "1568", "ancient Roman civilization"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5326369598529638}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0689655172413793, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.19354838709677416, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.19047619047619047, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-4104", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-1509", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-1433", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-4180", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-857", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-3654", "mrqa_hotpotqa-validation-4367", "mrqa_hotpotqa-validation-3757", "mrqa_searchqa-validation-2301", "mrqa_searchqa-validation-10852", "mrqa_hotpotqa-validation-1958"], "SR": 0.453125, "CSR": 0.5368150684931507, "EFR": 1.0, "Overall": 0.7116598886986301}, {"timecode": 73, "before_eval_results": {"predictions": ["12", "Sodra nongovernmental organization,", "the United States", "not feel Misty Cummings has told them everything she knows.", "Mogadishu", "attack victim in Yarmouk Hospital in Baghdad, Amjad HameedThe attack -- which occurred outside the municipal building of Abu Ghraib in western Baghdad", "more than 100", "Tyler Peterson was killed by a police sniper's bullet", "Scarlett Keeling", "took on water", "two", "Omar", "John Frederick Joseph Cade", "Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "Twitter", "growing crowded, and governments are increasingly trying to plan their use.", "relatives of the five suspects,", "165-room", "U.S. Holocaust Memorial Museum,", "the simple puzzle video game,", "curfew in Jaipur", "40-year-old", "stand down.", "U.S. 93 in White Hills, Arizona,", "different women coping with breast cancer in five vignettes.", "two years,", "269,000", "Zimbabwe's main opposition party said Sunday.", "UK", "A 22-year-old college student in Boston, Massachusetts, has been charged with murder in connection with the death of a woman who may have been contacted through a Craigslist ad,", "because the Indians were gathering information about the rebels to give to the Colombian military.", "Hundreds of militants, believed to be foreign fighters, launched attacks on various military check posts in Pakistan's border with Afghanistan Saturday night and early Sunday morning,", "they'd get to bring a new puppy with them to the White House in January.", "Athens", "Austin, Texas,", "forgery and flying without a valid license,", "Kurt Cobain", "ALS6,", "he was one of 10 gunmen who attacked several targets in Mumbai", "rabbit hole,", "Larry Ellison,", "anesthetic", "10", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "39,", "supplies power to almost 9 million Americans, \"has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\"", "Old Trafford", "12.3 million", "LulzSec.", "Seasons of My Heart", "driver will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.", "Justice A.K Mathur", "somatic cell nuclear transfer", "1956", "The Parson Russell Terrier", "Chile", "Nicolas cage", "Donald Richard \"Don\" DeLillo", "10 Years", "\"Queen City\"", "Carl Sagan", "Israel", "Copacabana", "seven"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7003951586234852}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 0.0, 0.0, 0.12500000000000003, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8750000000000001, 1.0, 1.0, 0.0, 1.0, 0.08888888888888889, 0.125, 0.07407407407407407, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 0.0, 1.0, 0.0, 1.0, 0.06666666666666667, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-534", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-2397", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-345", "mrqa_searchqa-validation-8780"], "SR": 0.640625, "CSR": 0.5382179054054055, "EFR": 0.9130434782608695, "Overall": 0.6945491517332549}, {"timecode": 74, "before_eval_results": {"predictions": ["brazil", "north yorkshire", "gehrig", "Goat Island", "Loretta Lynn", "a bat", "electronic junk mail or junk newsgroup posting", "iosland Webber", "east of Eden", "Edward de Vere", "Mark Hamill", "Aslan", "kabaddi", "Merchant of Venice", "kvetch", "Austria/Habsburg Monarchy", "Harold Wilson", "Bleak House", "Handley Page", "Tina Turner", "puffer fish", "Sheffield United", "Capricorn", "President John F. Kennedy", "the bluebird", "Toy Story", "Tim Kinsella", "Gold", "Kiel Canal", "colombia", "Avro Lancaster Bomber", "Sarah Vaughan", "Abu Dhabi", "33", "Emily Davison", "Marc Brunel", "Aberystwyth", "Oasis", "Peter Sellers", "the Indus Valley", "Terneuzen", "an even break", "David Bowie", "Lorne Greene", "1655", "fusilli", "Thai", "Viola", "\u00e1stron", "Ramadan", "sewing machines", "Montgomery County", "11 January 1923", "Roman Empire", "1994", "\"Sevens\"", "Tamara Ecclestone Rutland", "Kaka,", "addressed a racially-tinged remark made by his former caddy, telling reporters Steve Williams apologized and is not a racist.\"", "three", "noncommissioned officer", "William Henry Harrison", "the shaft", "1982"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7496527777777777}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-531", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-3882", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-4672", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-15435"], "SR": 0.6875, "CSR": 0.5402083333333334, "EFR": 1.0, "Overall": 0.7123385416666667}, {"timecode": 75, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "reached an agreement", "Dead Weather's \"Horehound\"", "two", "oprah", "twice.", "Haleigh Cummings,", "eradication of the Zetas cartel", "Hamas,", "a bank", "my family comes from a Muslim background, and we're not defined by religion,\"", "against using injectable vitamin supplements because the quantities are not regulated.", "david daniels", "Arthur E. Morgan III,", "job training", "moved into her rental house", "\"Here Comes the Sun.\"", "22", "1969", "diabetes and hypertension,", "Tillakaratne Dilshan scored his sixth Test century", "Tuesday", "100,000", "collapsed apartment building", "$17,000", "President Obama", "Caylee Anthony", "BBC press office", "journalists and the flight crew will be freed,", "AMD,", "returning combat veterans", "September 21.", "by Thursday", "246", "40", "stole", "maximum \u00a380,000", "", "15", "\"Dr. No\"", "Arabic, French and English", "$40", "many as 250,000", "state senators", "Iran", "Sheik Mohammed Ali al-Moayad", "in the 1950s", "Orbiting Carbon Observatory,", "2.5 million", "5,600", "in Yemen", "desperation", "amphetamines", "contemporary Earth", "b\u00e9la bart\u00f3k", "refrigerator", "blackcurrant", "Premier League club Everton", "Captain Beefheart & His Magic Band", "Fat Man", "messenger", "the beaver", "John Huss", "Star Wars"], "metric_results": {"EM": 0.5, "QA-F1": 0.6109107905982907}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, false, false], "QA-F1": [0.4444444444444445, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.2, 1.0, 0.15384615384615385, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-3454", "mrqa_newsqa-validation-10", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-1493", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-2207", "mrqa_triviaqa-validation-7344", "mrqa_triviaqa-validation-5192", "mrqa_hotpotqa-validation-1555", "mrqa_hotpotqa-validation-5388", "mrqa_searchqa-validation-14660", "mrqa_searchqa-validation-5169"], "SR": 0.5, "CSR": 0.5396792763157895, "EFR": 0.90625, "Overall": 0.6934827302631579}, {"timecode": 76, "before_eval_results": {"predictions": ["Hawaii", "\"The Real Housewives of Atlanta\"", "former U.S. Attorney Patrick Collins,", "outbreak", "said he was released Friday and taken to the Australian embassy in Bangkok, where he stayed until leaving for Australia at about midnight.\"", "Sheikh Sharif Sheikh Ahmed", "two", "club-themed", "was gunned down as a gang tried to steal his BMW car in the early hours of Sunday morning.", "Niger Delta,", "Ameneh Bahrami", "the chief executive officer,", "Daytime Emmy Lifetime Achievement Award.", "launch", "said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\"", "35,000.", "Roger Federer", "March 22,", "Venezuela", "assassinations", "sanctions against Zimbabwe,", "Operation Crank Call,\"", "the body of the aircraft", "Rima Fakih", "more than 100", "Obama and McCain camps", "an Iranian court", "about 5:20 p.m. at Terminal C when a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "President Bill Clinton", "Haleigh Cummings,", "Transportation Security Administration", "pulling on the top-knot of an opponent,", "response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "J. Crew,", "being gunned down itself,", "Kindle Fire", "Islamabad", "three", "Nigeria,", "and attorney, James Whitehouse,", "fill a million sandbags and place 700,000 around our city,\"", "customers of the dependable Camry", "15-year-old's", "normal maritime traffic", "daniels", "15,000", "looked depressed", "Franklin,", "said such joint exercises between nations are not unusual.", "Authorities in Fayetteville, North Carolina,", "Saturday.", "Kanab, Utah", "Lew Brown", "Rockwell", "andrew lloyd", "Neighbours", "Pesach", "Walcha", "Player's No 10, Skol, Leyland Cars, Velocity, Daily Mirror, TNT Sameday and Dunlop", "Hern\u00e1n Crespo", "Iceland", "Giuseppe Verdi", "a lion", "Gaul"], "metric_results": {"EM": 0.5, "QA-F1": 0.5957920831408734}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.7692307692307693, 1.0, 0.5806451612903226, 1.0, 1.0, 0.0, 0.0909090909090909, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7096774193548387, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.8, 0.0, 0.5, 1.0, 0.0, 0.18181818181818185, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2396", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-2519", "mrqa_naturalquestions-validation-8526", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-648", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-5708", "mrqa_searchqa-validation-1821", "mrqa_searchqa-validation-11541", "mrqa_triviaqa-validation-925"], "SR": 0.5, "CSR": 0.539163961038961, "EFR": 0.96875, "Overall": 0.7058796672077923}, {"timecode": 77, "before_eval_results": {"predictions": ["Thursday,", "The mother (Charlotte Gainsbourg) is consumed with grief and guilt.", "Patrick McGoohan,", "flooding and debris", "Vicente Dale Coutinho, commander of Brazil's 4th Army, reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Woosuk Ken Choi,", "a head injury.", "1994,", "Math teacher Mawise Gumba", "a federal sting after his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "at least 25 dead", "At least 15", "at least nine", "shows the world that you love the environment and hate using fuel,\"", "\"falling space debris,\"", "Transportation Security Administration", "$10 billion", "\"The Orchid Thief\"", "102", "his business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court,", "Jaime andrade", "A receptionist with a gunshot wound in her stomach", "innovative, exciting skyscrapers", "the United States", "financial gain,", "Arnold Drummond", "trading goods and services without exchanging money", "Yang Hyong Sop, and Kim Kye Gwan,", "Another high tide -- expected to reach about 4 meters (13 feet) high --", "July", "16", "public endorsement", "100 to 150", "reached an agreement late Thursday to form a government of national reconciliation.", "Zelaya and Roberto Micheletti,", "inferior,", "At the end of a biology department faculty meeting at the University of Alabama in Huntsville,", "prison inmates.", "Cannes Film Festival,", "Mark Obama Ndesandjo", "Ronald Reagan UCLA Medical Center,", "two", "Afghan forces in destroying drug labs, markets and convoys,\"", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "executive director of the Americas Division of Human Rights Watch,", "\"Empire of the Sun,\"", "Basel", "\"@T- Bone\"", "Jeffrey Jamaleldine", "Heshmatollah Attarzadeh", "At least 14", "Daryl Sabara", "July 2014", "Gibraltar", "Barry White", "george i", "Anita Brookner", "9", "British", "consulting", "flamboyant", "Orson Welles", "barbed wire", "staunch"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6191079721362229}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.7894736842105263, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.33333333333333337, 0.0, 0.16666666666666666, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-2803", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-1497", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-2177", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-795", "mrqa_naturalquestions-validation-9330", "mrqa_triviaqa-validation-635", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-632", "mrqa_searchqa-validation-12008"], "SR": 0.515625, "CSR": 0.5388621794871795, "EFR": 0.967741935483871, "Overall": 0.7056176979942101}, {"timecode": 78, "before_eval_results": {"predictions": ["dress shop", "call option", "in British Columbia, Canada", "45 %", "September 14, 2008", "to prevent further offense by convincing the offender that their conduct was wrong", "Mike Alstott", "the closing of the atrioventricular valves", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "a compiler", "Dalveer Bhandari", "bone marrow", "to collect menstrual flow", "Kansas City Chiefs", "`` Killer Within ''", "Filipino", "the 10th century", "Wakanda", "the Roman Empire", "Joel", "a young girl ( an illustration by Everest creative Maganlal Daiya back in the 1960s )", "Gertrude Niesen", "Australia", "1983", "Olivia Baker", "March 12, 2013", "The photoelectric ( optical ) smoke detector", "the compass", "New Orleans", "sedimentary", "$315,600", "Michael Buffer", "Eric Clapton", "115", "c. 3000 BC", "Vince Vaughn", "each team", "27 January -- 16 April 1898", "a routing table", "his rez", "Twickenham", "Glenn Close", "Mark Stephen Lett", "Tim Russert", "the anterolateral corner of the spinal cord", "a large, high - performance luxury coupe sold in very limited numbers", "displacement", "the Second Continental Congress meeting at the Pennsylvania State House ( Independence Hall ) in Philadelphia", "the winter solstice", "a mixture of phencyclidine and cocaine", "an integral membrane protein that builds up a proton gradient across a biological membrane", "Lewis Carroll", "lithium", "Apprentice", "A123 Systems", "41st President of the United States", "Henry Daniel Mills", "34", "Caylee Anthony", "Michael Brewers,", "a taster", "Williams", "Hawthorne", "Friday,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6202640241702742}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.2857142857142857, 1.0, 1.0, 0.9, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.33333333333333337, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.25000000000000006, 0.18181818181818182, 1.0, 0.4, 0.4, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.8, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-5536", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-1433", "mrqa_newsqa-validation-3438", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-7894"], "SR": 0.4375, "CSR": 0.5375791139240507, "EFR": 0.9722222222222222, "Overall": 0.7062571422292546}, {"timecode": 79, "before_eval_results": {"predictions": ["Bury, Greater Manchester, England", "the Battle of the Rosebud", "Rabat", "Potomac River", "Hermione Baddeley", "Harmony Korine", "December 1993", "New Jersey", "rock and roll", "Red and Assiniboine Rivers", "King George IV and the Duke of Wellington", "June 24, 1935", "The Indianapolis Times", "odd-eyed", "2001", "Perth's number one rating radio station, MIX 94.5.", "1999", "mid-tempo pop and R&B love song", "Presbyterian Church", "2002", "English", "Southaven", "Anheuser-Busch InBev", "Kansas City, Missouri", "Francis the Talking Mule", "Les Clark", "Giuseppe Verdi", "County Louth", "Gal Gadot", "Kurt Vonnegut Jr.", "Liga MX", "film", "Mississippi Institute of Arts and Letters", "Vince Guaraldi", "2007", "Grave Digger", "Mulberry", "Isabella (Belle) Baumfree", "Jay Park", "the final of 2011 Asian Cup", "Mel Blanc", "state Children's Health Insurance Program", "Pakistan", "\"Godspell\"", "Steven Selling", "Scunthorpe", "Australian", "Argentinian", "Robert Norton Noyce", "1970s and 1980s", "\"Personal History\",", "Timothy B. Schmit", "Kareem Abdul - Jabbar", "the direction from which the wind is blowing", "France", "Shropshire", "Sophora", "Alfredo Astiz,", "current and historic conflict zones,", "Kim", "Catherine of Aragon", "Little Boy Blue", "Cheyenne", "March 27, 2017"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7618607954545454}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8, 0.5, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.25, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-334", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-1925", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4248", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-5515", "mrqa_triviaqa-validation-4205", "mrqa_newsqa-validation-433", "mrqa_searchqa-validation-5939", "mrqa_naturalquestions-validation-5649"], "SR": 0.65625, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.712109375}, {"timecode": 80, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1288", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.80078125, "KG": 0.4921875, "before_eval_results": {"predictions": ["John Nash", "Two Greedy Italians", "Russ Conway", "the y unit", "Mel Brooks", "Agent 007", "Edward Woodward", "Scotland", "Fiat", "three", "Bob Anderson", "Andre Agassi", "dead", "India", "Piet\u00e0", "Mark Darcy", "California Chrome (who is set to be retired in January 2017 as a 6-year-old)", "Ronnie,", "Milan", "Tony Meo", "Bash Street Kids", "David Nobbs", "\" Robin Hood Men in Tights and Monty Python's A Holy Grail", "\"Dancing the Lambeth Walk\"", "Yeshua", "Augustus Caesar", "Shepherd Neame", "Titanic", "Simon Oakland", "tax collector", "Maxwell", "Mikhail Gorbachev", "Pocahontas", "Noah Beery, Jr.", "Argentina", "the axon", "myxomatosis", "breadfruit", "World War I", "Captain America", "HARIBO", "bolognese", "New Zealand", "Eva Braun", "Sindh", "Devon Loch", "Macau", "Bruce Willis", "Kwame Nkrumah", "The Preamble", "cording", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "pneumonoultramicroscopicsilicovolcanoconiosis", "Hercules", "Philip Livingston", "2010", "\"The Process\"", "Ameneh Bahrami", "Nearly eight in 10", "a tenement in the Mumbai suburb of Chembur,", "camels", "Shrek", "Seoul", "Asia"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6979166666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-6950", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-5713", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5365", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-5149", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-6967", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-1693", "mrqa_searchqa-validation-11588", "mrqa_searchqa-validation-457"], "SR": 0.671875, "CSR": 0.5407021604938271, "EFR": 1.0, "Overall": 0.7057966820987654}, {"timecode": 81, "before_eval_results": {"predictions": ["London", "about the outdoors, especially mountain-climbing", "50th anniversary of the founding of the National Basketball Association", "Roger Thomas Staubach", "World Health Organization", "Pulitzer Prize", "Argentine", "Pittsburgh Steelers", "Kim Jong-hyun", "Las Vegas", "Scandinavian design", "romantic comedy", "Anthony Davis", "Tsung-Dao Lee", "December 19, 1967", "South African", "dennis taylor", "November 1822", "1926 Paris", "Bulgarian", "Quahog, Rhode Island", "Operation Neptune", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "rolyn castle", "the German Campaign of 1813", "2015", "Violet", "Free Range Films", "Mondays", "Chrysler K platform", "Edinburgh", "Laurel, Mississippi", "Matt Flynn", "Camber Sands", "Jaguar Land Rover Limited", "the Viet Minh's base of support", "Norwood Payneham & St Peters", "Shepardson Microsystems", "The Fault in our Stars", "sixteen", "Crips", "Deftones", "Doctor of Philosophy", "Donald Richard \"Don\" DeLillo", "Claude Mak\u00e9l\u00e9l\u00e9", "#364", "Magic Band", "the Salzburg Festival", "Eurpean dynasty", "war I fighter pilot ace", "Lawrence, Nassau County, New York", "Malibu Creek State Park, northwest of Los Angeles", "You are a puzzle", "pineapple", "euthanasia", "incompetent", "Poland", "Steven Green", "Steve Williams", "Umar Farouk AbdulMutallab", "Dr. No", "legs", "Big Brown", "ties to paramilitary groups,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7320870535714286}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.4, 0.375, 0.6, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 0.28571428571428575, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-3019", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-3348", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-1830", "mrqa_hotpotqa-validation-376", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-209", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-2181", "mrqa_naturalquestions-validation-468", "mrqa_triviaqa-validation-5053", "mrqa_newsqa-validation-1205", "mrqa_searchqa-validation-11199", "mrqa_newsqa-validation-877"], "SR": 0.578125, "CSR": 0.5411585365853658, "EFR": 1.0, "Overall": 0.7058879573170731}, {"timecode": 82, "before_eval_results": {"predictions": ["Kenny Young", "40 million", "\"American Idol\"", "brother-in-law", "Martin Scorsese", "Keith Crofford", "Daimler-Benz", "Edward M. Kennedy", "private", "Lee Seok-hoon", "Two Pi\u00f1a Coladas", "hierarchiology", "democratic system of \u201crule of the majority\u201d", "Eucritta melanolimnetes", "Umberto II", "1866", "1860", "Attorney General and as Lord Chancellor of England", "Sexred", "British", "Westfield Tea Tree Plaza", "924", "1871", "Darci Kistler", "1966", "Potomac River", "Europe", "Harry Potter series", "Portal", "England", "Black Panthers", "An extended play record", "\"127 Hours\" (2010)", "May 5 to July 8, 2014", "The Supremes", "1601\u201302", "Wolf Creek", "Sky News", "the High Court of Admiralty", "Chelsea Lately", "bobsledder", "Double Agent", "Oregon Ducks", "Aksel Sandemose", "Theme Park World", "the Earth", "Eric Whitacre", "Sullivan University", "24 January 76 \u2013 10 July 138", "Marvel Comics", "Zambesi river", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "1961", "September 14, 2008", "Harry S. Truman", "The Kentucky Derby", "Gianni Versace", "London's Heathrow airport", "his former Boca Juniors teammate and national coach Diego Maradona,", "his club", "pink", "Beaker", "Claddagh", "Emperor Concerto"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7040178571428573}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9523809523809523, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-5066", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-4293", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-900", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-4974", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1461", "mrqa_searchqa-validation-10992"], "SR": 0.59375, "CSR": 0.5417921686746988, "EFR": 1.0, "Overall": 0.7060146837349397}, {"timecode": 83, "before_eval_results": {"predictions": ["Thunder Road", "from the Anglo - Norman French waleis", "between the Eastern Ghats and the Bay of Bengal", "Lincoln Park in San Francisco", "New York University", "amylase", "Eydie Gorm\u00e9", "Werner Ruchti", "Stephen Lang", "used their knowledge of Native American languages as a basis to transmit coded messages", "the English", "Himadri Station", "soon after in most cases", "Sun Tzu ( `` Master Sun '', also spelled Sunzi )", "their son Jack ( short for Jack - o - Lantern )", "The Grasshopper Lies Heavy", "Pope Gregory I the Great", "1966", "1955", "1970", "Warren Hastings", "fresh water", "ice", "June 1992", "John Vincent Calipari ( born February 10, 1959 )", "1975", "Charles Darwin", "Left Behind", "1936", "Judith Cynthia Aline Keppel ( born 18 August 1942 )", "in the 1820s", "May 18, 2018", "Russia", "the five states which the UN Charter of 1945 grants a permanent seat on the UN Security Council ( UNSC )", "a qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government", "Fusajiro Yamauchi", "Etienne de Mestre", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "pre-Christian festivals that were celebrated around the winter solstice", "Speaker of the House of Representatives", "the NFL", "Italy", "Detroit Red Wings", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral", "Captain Jones", "The idea of a harp being the arms of Ireland may have originated as a reference to a fictional character, le roi d'irelande, in the courtly legend cycle of Tristan", "Florida and into the town of Coconut Cove", "S", "September 9, 2010", "the shooter must be at least 18 or 21 years old ( or have a legal guardian present )", "The youngest sister Phoebe Halliwell in the 28 - minute unaired pilot episode", "Jennifer Eccles", "Sinclair Lewis", "Akon", "Lily Hampton", "Hawaii Five-0", "Gregg Livingston Harper", "23-year-old", "more than 200.", "Transportation Security Administration", "the Vaio Z Canvas 2-in-1", "Newman", "The Color Purple", "East Knoyle"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6847916666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06666666666666667, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.13333333333333333, 0.16, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-6308", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-8391", "mrqa_naturalquestions-validation-3968", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-989", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-3705", "mrqa_searchqa-validation-8173"], "SR": 0.609375, "CSR": 0.5425967261904762, "EFR": 1.0, "Overall": 0.7061755952380953}, {"timecode": 84, "before_eval_results": {"predictions": ["as a relay between the brain and spinal cord and the rest of the body", "lungs", "Grand Inquisition", "John Findley Wallace", "the United States, its NATO allies and others", "Allison Janney", "the s - block", "late - night", "Emma Watson", "April 10, 2018", "the New York Yankees", "Pope Gregory I the Great", "on a side table", "Ireland", "Saphira", "Pre-evaluation, strategic planning, operative planning, implementation", "Saint Peter", "active absorption", "1983", "restricted naturalization to `` free white persons '' of `` good moral character ''", "a heart rate that exceeds the normal resting rate", "Ptolemy", "1986", "Battle of Antietam", "Emma Thompson", "1939", "if the car is slowed initially by manual use of the automatic gear box", "Thomas Mundy Peterson", "Filipino Americans", "Judy Collins", "The Continental Congress", "eusebeia", "Napoleon", "Lagaan", "A blighted ovum", "September 19 - 22, 2017", "the New Testament", "Kirsten Simone Vangsness", "asphyxia", "1902", "Armpit '' Johnson", "A patent", "1999", "10,605", "the East Coast", "the Reverse - Flash", "March 1930", "John Donne", "4", "season four", "During the reign of King Beorhtric of Wessex", "Rugby School", "gold", "Doctor Dolittle", "goalkeeper", "Christian Kern", "\"50 best cities to live in.\"", "regulators in the agency's Colorado office", "Miguel Cotto", "a desperate flight from the wreckage of President Robert Mugabe's Zimbabwe", "Rocky", "the Cumberland Gap", "salinity", "George Fox"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5733046187683285}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [0.7096774193548387, 0.0, 0.5, 0.0, 0.4444444444444445, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.16666666666666669, 0.4444444444444445, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-1688", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-6118", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-4863", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-3826", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2653"], "SR": 0.46875, "CSR": 0.5417279411764706, "EFR": 0.9411764705882353, "Overall": 0.6942371323529412}, {"timecode": 85, "before_eval_results": {"predictions": ["Roger B. Taney", "Pirates of the Caribbean: At World's End", "Samuel de Champlain", "Louis XIV", "Lady Jane Grey", "the Barbary Coast", "Iceland", "the owl", "Richard Cory", "Volkswagen", "baldness", "Athens", "rum", "tea rose", "Aida", "give love a bad name", "the People's Republic of China, Myanmar", "Marie Antoinette", "rotunda", "the magnolia", "haryana", "bicentennial", "Gallia", "eBay", "the peace sign", "Michael Dell", "Pizza Hut", "the Titanic", "1972", "hurricane", "Amish", "the Rocky Mountains", "carbon", "The Subtle knife", "Boston", "the Wu-Tang Clan", "king of England", "(Jose de San) Martin", "a whale", "Salt Lake City", "Luxembourg", "Texas", "drag", "Monty Python", "a chrie", "Las Vegas", "Laura", "The New Yorker", "the Sarajevo Haggadah", "a snout beetle", "Tufts University", "The results of the Avery -- MacLeod -- McCarty experiment, published in 1944, suggested that DNA was the genetic material, but there was still some hesitation within the general scientific community to accept this", "the cast", "from the top of the leg to the foot on the posterior aspect", "Frank Chin", "Oxygen", "The Truman Show", "Robert Redford", "Great Lakes and Midwestern", "three people", "five", "said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "March 3, 2008,", "Elizabeth Birnbaum"], "metric_results": {"EM": 0.59375, "QA-F1": 0.674264705882353}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, true], "QA-F1": [0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.35294117647058826, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16592", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-12392", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-6995", "mrqa_searchqa-validation-1981", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-15481", "mrqa_searchqa-validation-14552", "mrqa_searchqa-validation-4152", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-16693", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-11933", "mrqa_searchqa-validation-3852", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-10378", "mrqa_triviaqa-validation-5727", "mrqa_triviaqa-validation-2130", "mrqa_hotpotqa-validation-3415", "mrqa_newsqa-validation-4210"], "SR": 0.59375, "CSR": 0.5423328488372092, "EFR": 1.0, "Overall": 0.7061228197674418}, {"timecode": 86, "before_eval_results": {"predictions": ["the Granite State", "Little Yellow jackets", "Horse Feathers", "Bleak House", "Chaillot", "Do the Right Thing", "coloring", "Asteroids", "a bad peace", "Yves Saint Laurent", "Wyoming", "the United Kingdom", "Lend-Lease", "Spanglish", "Monica Lewinsky", "Friday night", "Google", "Medusa", "the vest", "Prince", "the gulls", "Hammurabi", "Nixon", "60 inches", "(Albert) Kesselring", "jump", "Ned", "the 747", "Terry Bradshaw", "Chris Evert", "Azerbaijan", "Mamma Mia!", "Fallingwater", "Alanis Morissette", "the comma", "the barrel", "Etna", "a law clerk", "Faneuil Hall", "Louisiana", "George Orwell", "tea bags", "toro", "Stalin", "Metallica", "change horses", "Jeopardy", "Lafayette", "Gatsby", "Captain Kangaroo", "Kosher", "1996", "XIX", "Gabrielle - Suzanne Barbot de Villeneuve", "puff", "air", "South Africa", "Cartoon Network Too", "Via Port Rotterdam", "Bruce Grobbelaar", "Iran", "The Palm Jumeirah", "35,000.", "Mashhad"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6958333333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13093", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-4847", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-11386", "mrqa_searchqa-validation-15017", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-7312", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-2504", "mrqa_searchqa-validation-3", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-16876", "mrqa_searchqa-validation-4740", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-13220", "mrqa_searchqa-validation-10150", "mrqa_searchqa-validation-4307", "mrqa_naturalquestions-validation-288", "mrqa_triviaqa-validation-1471", "mrqa_hotpotqa-validation-337", "mrqa_newsqa-validation-3141"], "SR": 0.609375, "CSR": 0.5431034482758621, "EFR": 1.0, "Overall": 0.7062769396551725}, {"timecode": 87, "before_eval_results": {"predictions": ["Oblivion", "a chiffon", "Corpus Christi", "President Grover Cleveland", "an eye", "the Federalist Papers", "Martin Luther King", "transitive", "California", "the Central Pacific", "ACTIVE voice", "Tom Cruise", "Sicilian pizza", "a panda", "Risk", "rice", "Kansas State", "fish eggs", "1945", "Kentucky Bourbon", "a stork", "Towers", "a rat", "anime", "Daisy Miller", "Icelandic", "Man of Thieves", "Mercury", "Manghulis", "The Stars and Stripes Forever", "One Hundred Years of Solitude", "lethal", "Henry Cavendish", "vanilla", "a terminus", "Italy", "Night of the Iguana", "Rhode Island", "cricket", "Pandora", "the root", "Judges", "1066", "Sir John Soane", "a bull", "the hip", "a hearse", "City Slickers", "Ned Kelly", "souci", "the Doge", "depression", "in the southeastern United States", "5.0 - litre", "Italy", "Pakistan International Airlines", "Desert Quartet", "1998", "\"Peshwa\" ( Prime Minister)", "Sleeping Beauty", "the Beatles", "ceo Herbert Hainer", "President Bush", "Consumer Reports"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7420882936507935}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8571428571428571, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.33333333333333337, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-203", "mrqa_searchqa-validation-15068", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-4561", "mrqa_searchqa-validation-5164", "mrqa_searchqa-validation-15006", "mrqa_searchqa-validation-13444", "mrqa_searchqa-validation-11517", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-2206", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-8657", "mrqa_searchqa-validation-2267", "mrqa_searchqa-validation-3422", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12454", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-7133", "mrqa_naturalquestions-validation-1438", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3663", "mrqa_triviaqa-validation-4719", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-663"], "SR": 0.59375, "CSR": 0.5436789772727273, "EFR": 1.0, "Overall": 0.7063920454545455}, {"timecode": 88, "before_eval_results": {"predictions": ["Sputnik", "A Good Day to Die Hard", "The Kinks", "Gorbachev", "Jerez de la Frontera", "the Buncefield Depot", "a royal court", "a cable", "Westminster Abbey", "Cast", "A-K-Q-J-10", "Hawaii", "World War II", "aromatherapy", "\"S Sierra One from Sierra Oscar...\"", "Downton Abbey", "Bobby Darin", "France", "Montmorency", "Kent", "Cliff Thorburn", "St. Moritz", "15 cells", "cymbals", "a violin", "Ireland", "Venus", "a pest", "paralysis", "eight", "Japanese silvergrass", "Swindon Town", "Billy Preston", "\"Good Morning to All\"", "Everton", "a window sash", "mar", "Staraya Russa", "dennis taylor", "Mud", "dumbo", "Jimmy Knapp", "Pelham One Two Three", "31536000", "7,898 miles", "Gianfranco Ferre", "malekith, the Destroyer, and Kurse", "(Richard) Seddon", "Chiricahua", "Albert Reynolds", "Aug. 24, 1572", "in the very late 1980s", "Taylor Michel Momsen", "Malayalam", "2008", "Western Canada", "Dutch", "the peace with Israel that Sadat worked aggressively to achieve", "Jiverly Wong,", "28", "Lili Taylor", "Hannibal Barca", "jam", "Mike Mills"], "metric_results": {"EM": 0.546875, "QA-F1": 0.62109375}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.4, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2755", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2162", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-5008", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-7013", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-4019", "mrqa_hotpotqa-validation-3566", "mrqa_newsqa-validation-912", "mrqa_searchqa-validation-1265", "mrqa_searchqa-validation-9544", "mrqa_searchqa-validation-3464"], "SR": 0.546875, "CSR": 0.5437148876404494, "EFR": 1.0, "Overall": 0.7063992275280899}, {"timecode": 89, "before_eval_results": {"predictions": ["18", "A third beluga whale belonging to the world's largest aquarium has died", "Los Ticos", "reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Jennifer Aniston, Marta Kauffman, co-creator of the series \"Friends\" and Kristin Hahn,", "off Haiti's coast", "Rwanda", "dance", "new materials", "Gadahn", "10", "a reported \u00a320 million ($41.1 million) fortune", "Hong Kong's Victoria Harbor", "helping to plan the September 11, 2001, terror attacks,", "to provide security as needed.", "dominic Adiyiah", "some dental work done,", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "fill a million sandbags and place 700,000 around our city,\"", "helping on the sandbag lines", "Eric Besson", "since 1983", "133", "promotes fuel economy and safety while boosting the economy.", "growing crowded,", "Jewish", "12.3 million", "The Ski Train", "two years", "Robert Park", "Piedad Cordoba,", "eight", "Itawamba County School District", "Frank Ricci", "1959", "$249", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "1983", "13", "The Valley Swim Club", "\"still trying to absorb the impact of this week's stunning events.\"", "International Polo Club Palm Beach in Florida.", "relatives of the five suspects,", "Sharon Bialek", "not", "john Dillinger", "five", "henry faldo", "Iggy Pop", "two", "Opryland", "Garfield Sobers", "R.E.M.", "San Jose, California", "john donne", "127 Hours", "the Kiel Canal", "\"Slaughterhouse-Five\"", "1885", "Germany", "hearsay", "buffoon", "17th century", "the Book of Esther"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6660986837916985}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.23529411764705885, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-268", "mrqa_naturalquestions-validation-878", "mrqa_hotpotqa-validation-4986", "mrqa_triviaqa-validation-6731"], "SR": 0.578125, "CSR": 0.5440972222222222, "EFR": 1.0, "Overall": 0.7064756944444445}, {"timecode": 90, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.8359375, "KG": 0.4859375, "before_eval_results": {"predictions": ["More than 15,000", "Jezebel.com's", "Mark Sanford", "the Russian air force,", "Nigeria, Africa's largest producer.", "one", "being available under the inverted glass pyramid of the Louvre.", "Melbourne.", "anaphylaxis Network (FAAN)", "opium", "Tuesday.", "acute stress disorder in Iraq and that a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "the fact that the teens were charged as adults.", "Wednesday,", "fastest time in circling the globe in a powerboat.", "order", "General Motors", "St. Louis, Missouri.", "Da Vinci Code,", "Nearly eight in 10", "an actor,", "no motive has been determined", "Opry Mills,", "starting place at Old Trafford", "Jan Brewer,", "the oceans,", "Columbia", "social issues", "weren't taking it well.", "Leo Frank,", "Ralph Lauren,", "the American Civil Liberties Union", "Islamabad", "evokes childhood memories in this four-line ode to Mom.", "Tuesday", "Olympia", "\"Percy Jackson & The Olympians,\"", "Frank Ricci,", "U.S. Navy", "the Russian air force,", "they are co-chair of the Genocide Prevention Task Force,", "she wonders if part of the appeal of plus-sized", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "Jet Republic,", "Jaipur", "Venezuela", "likening one American diplomat to a \"prostitute\"", "meeting with the president to discuss her son.", "Lilla Torg.", "she also believed police were trying to cover up the truth behind her daughter's murder,", "buckling under pressure from the ruling party.", "16 seasons", "12.9 - kilometre ( 8 mi )", "1939", "Afghanistan", "Alessandro Allori", "61", "Nicolas Winding Refn", "Father Dougal McGuire", "Parliamentarians (\"Roundheads\") and Royalists (\"Cavaliers\")", "alfalfa", "Thomas Jefferson", "\"Here's Johnny\"", "Adolphe Adam"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6433583254030453}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.125, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.4, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19047619047619047, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.06060606060606061, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-1368", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-131", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-1916", "mrqa_newsqa-validation-2983", "mrqa_triviaqa-validation-3262", "mrqa_searchqa-validation-1104", "mrqa_searchqa-validation-15070"], "SR": 0.5625, "CSR": 0.5442994505494505, "EFR": 1.0, "Overall": 0.7154223901098902}, {"timecode": 91, "before_eval_results": {"predictions": ["British Airways", "Linus van Pelt", "chestnut", "almond", "Mark Twain", "Oslo", "Humphrey Bogart", "Hawaii", "glockenspiel", "George Orwell", "Goldtrail", "The Archers", "Ben Franklin", "Jack Nicholson", "photography", "ginger ale", "Taiwan", "Rijkswapen der Nederlanden", "Oliver Stone", "Neil Armstrong and Edwin \" Buzz\" Aldrin", "Oregon", "lord", "Nikola Tesla", "De Quincey", "Susie Dent", "Pancho Villa", "Crusades", "Ivan Owen", "1930", "copper", "Pickwick", "Bluebell Girls", "Columbus", "swiss", "Ann Darrow", "blue", "the Flying Pickets", "St Moritz", "swiss", "Vietnam", "1985", "Bogota", "united states", "James Murdoch", "Crystal Palace", "Belfast", "Moulin Rouge", "Greece", "Reginald Dwight", "Lake Union", "Marshalsea", "verification code ( CVC )", "\u00c9mile Gagnan and Naval Lieutenant ( `` lieutenant de vaisseau '' ) Jacques Cousteau", "Himachal", "94", "August 6, 1845 - October 6, 1931", "Ted 2", "24", "Tuesday", "opium", "Japheth", "resuscitation", "the Hudson", "Subway"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6534926470588236}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8235294117647058, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3978", "mrqa_triviaqa-validation-4026", "mrqa_triviaqa-validation-6962", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-7195", "mrqa_triviaqa-validation-5339", "mrqa_triviaqa-validation-5332", "mrqa_triviaqa-validation-3121", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-6887", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-4434", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-2175", "mrqa_searchqa-validation-417", "mrqa_searchqa-validation-13767", "mrqa_searchqa-validation-8329"], "SR": 0.59375, "CSR": 0.5448369565217391, "EFR": 1.0, "Overall": 0.7155298913043479}, {"timecode": 92, "before_eval_results": {"predictions": ["anangutans", "lowestoft", "solomon", "new zealand", "net worth", "Ernest Hemingway", "Charles II", "Greedy Italians", "the space shuttle", "France", "george Reeves", "a window", "Hitler", "a coffee house", "the little dog laugh'd", "baseball cards", "george", "Hilary Swank", "Neighbours", "kursk", "Jessica Simpson", "3-4-5-6", "blind Beggar", "a reformed man", "One Thousand and One", "Homo floresiensis", "Tony Blair", "komsomol", "Urania", "Theo Walcott", "netherish", "surrey rayner", "dennis taylor", "Oregon", "petula Clark", "Andy Birmingham, England", "dice", "Saturn", "Sinclair Lewis", "the Fleet River", "surseen Academicals", "james chadwick", "LDV", "1879", "a weasel", "table tennis", "bison", "alberich", "geomorphology", "mars", "Tina Turner", "com TLD", "abdicated", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Guangzhou", "29, 1985", "Jane Mayer", "a storm,", "a member of the band for more than 40 years", "American", "the Avanti", "Coors Field", "the Chrysler Building", "the USS \"Enterprise\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.5892795138888889}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.22222222222222218, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4018", "mrqa_triviaqa-validation-5157", "mrqa_triviaqa-validation-1504", "mrqa_triviaqa-validation-1795", "mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-6855", "mrqa_triviaqa-validation-904", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-3613", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-7470", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-3342", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-2717", "mrqa_newsqa-validation-3990", "mrqa_searchqa-validation-6690"], "SR": 0.546875, "CSR": 0.544858870967742, "EFR": 0.9655172413793104, "Overall": 0.7086377224694105}, {"timecode": 93, "before_eval_results": {"predictions": ["Robert Barnett,", "a public housing project,", "a hospital in Amstetten,", "Iran", "Zimbabwean", "a one-shot victory in the Bob Hope Classic", "on supporting full marriage equality,\"", "said Schrenker is \"an accomplished pilot\" who owns \"a couple of airplanes\" and flies regularly.", "Her husband and attorney, James Whitehouse,", "80,", "she was crying when she was talking about her daughters.", "Iraqi", "Passers-by", "subscribers to a daily publication which is the primary service of Stratfor,\"", "talk show queen Oprah Winfrey.", "Former Mobile County Circuit Judge Herman Thomas", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "heavy flannel or wool", "Diego Milito's", "house", "snowstorm", "potential revenues from oil and gas", "the wars in Iraq and Afghanistan", "two", "finance", "Indonesian", "The Charlie daniels Band,", "three out of four", "trading goods and services without exchanging money", "gasoline", "Cambodian government", "South Africa", "Harrison Ford", "Jacob,", "Cash for Clunkers", "100 meter", "Beijing", "Cipro, Levaquin, Avelox, Noroxin and Floxin.", "Chinese and international laws", "the player", "\"Great Charter\" in Latin.", "27,", "five", "Secretary of State", "the American Civil Liberties Union.", "it was unjustifiable \"for a project which does nothing more than perpetuate misconceptions about the state and its citizens.", "his father,", "\"The Sopranos,\"", "finance", "$60 billion", "managing his time.", "merengue and bachata music, both of which are the most popular forms of music in the country", "the U.S. state of Georgia", "currently a free agent", "robert glover", "bill bryson", "argument", "1960s", "Ariel Ram\u00edrez", "\"Boston Herald\" Rumor Clinic", "the Yangtze River", "Ouija", "the Andes", "season"], "metric_results": {"EM": 0.5625, "QA-F1": 0.663457736657465}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.8571428571428571, 0.5, 0.0, 0.0, 0.8, 1.0, 0.14814814814814817, 0.0, 1.0, 0.125, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.0, 0.3, 0.0, 1.0, 1.0, 1.0, 0.5, 0.125, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3233", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-3716", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-4204", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-4073", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4841", "mrqa_triviaqa-validation-3004", "mrqa_hotpotqa-validation-316", "mrqa_hotpotqa-validation-1657", "mrqa_searchqa-validation-13597", "mrqa_triviaqa-validation-2535"], "SR": 0.5625, "CSR": 0.5450465425531915, "EFR": 1.0, "Overall": 0.7155718085106383}, {"timecode": 94, "before_eval_results": {"predictions": ["Larry King", "Immigration Minister Eric Besson", "$500,000", "The Disasters Emergency Committee, which includes the British Red Cross, Oxfam, Save the Children and 10 other charities,", "bankruptcy protection,", "At least 38", "hooked up with Mildred, a younger woman of about 80, in March.", "Eleven people", "41,", "McDonald's", "And he won it after facing various challenges and turning them to his advantage.", "next year,", "Democrats and Republicans are saying Meehan shouldn't be using a 9/11 image to make a political point.", "Zac Efron", "The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,\"", "Sylt", "the National Restaurant Association", "Congress", "Cash for Clunkers", "11th year in a row.", "three thousand", "the \"face of the peace initiative has been attacked.\"", "after Wood went missing off Catalina Island,", "grossed $55.7 million", "Tulsa, Oklahoma.", "almost 100", "Homeland Security Secretary Janet Napolitano", "56,", "There's no chance", "Barack Obama,", "as part of its 18-month journey around the world.", "On Wednesday evening he walked into the Central Methodist Church in downtown Johannesburg and joined a long queue of people waiting for shelter and food.", "30,000", "31 meters (102 feet) long and 15 meters (49 feet) wide,", "$14.1 million.", "The individual named is Thamer Bin Saeed Ahmed al-Shanfari.", "cartel from the state of Veracruz, Mexico,", "Orbiting Carbon Observatory,", "Caster Semenya", "Pakistan", "Kim Clijsters", "the chaos and horrified reactions after the July 7, 2005, London transit bombings", "Daryeel Bulasho Guud", "onto the college campus.", "The remains of Cologne's archive building", "Opry Mills,", "Caylee Anthony,", "in the neighboring country of Djibouti,", "18", "school,", "Bush-era Justice Department", "to absorb menstrual flow, often with an adhesive backing to hold the pad in place", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "Massachusetts", "Wembley", "Muhammad Ali", "Ub Iwerks", "chernan and nantahala", "4,613", "English rock band", "raytheon", "The Lion King", "Xurbia Endless", "Johnny Got His Gun"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5955026429176322}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5581395348837208, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.08, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.3529411764705882, 0.24489795918367346, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-335", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-821", "mrqa_naturalquestions-validation-2406", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-644", "mrqa_triviaqa-validation-2079", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-3036", "mrqa_searchqa-validation-9921"], "SR": 0.484375, "CSR": 0.544407894736842, "EFR": 0.9696969696969697, "Overall": 0.7093834728867623}, {"timecode": 95, "before_eval_results": {"predictions": ["Ford", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "around 3,000 - 5,000 program - erase cycles, but some flash drives have single - level cell ( SLC ) based memory that is good for around 100,000 writes", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "providing torque to all its wheels simultaneously", "October 6, 2017", "autopistas", "northwest of Bemis Heights", "Fusajiro Yamauchi", "1857", "Tim McGraw", "Aibak", "the Devastator", "Abraham Gottlob Werner", "lacteal", "Fix You", "In some cases, there was a transitional stage where toilets were built into the house but accessible only from the outside", "Britain", "House of Representatives", "architecture", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "the arms of Ireland", "pop ballad", "2010", "December 10, 2017", "Annette Strean", "Charles Path\u00e9", "endocytosis", "during World War II", "dromedary", "2003", "the meaning used by the British associationists", "the fall of 2015", "Rodney Crowell", "Spanish moss", "Guant\u00e1namo", "to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "961", "The Lightning Thief", "cutting", "The Cornett family", "Allison Janney", "around 2011", "Patrick Swayze", "McFerrin, Robin Williams, and Bill Irwin", "Ann Gillespie", "Lula", "2017", "March 15, 1945", "Adwaita, an Aldabra giant tortoise", "Oliver Goldsmith", "Brussels", "iron", "Kye Bumzu", "Cartoon Network", "What You Will", "Kurt Cobain", "Madonna", "Brian Smith.", "Shakespeare", "Bob Hope", "three", "$40 and a loaf of bread."], "metric_results": {"EM": 0.546875, "QA-F1": 0.653053637125566}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.2222222222222222, 1.0, 1.0, 0.2758620689655173, 0.6666666666666666, 1.0, 1.0, 0.5333333333333333, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8181818181818181, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.8125000000000001, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-53", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-3309", "mrqa_triviaqa-validation-3684", "mrqa_hotpotqa-validation-4167", "mrqa_newsqa-validation-1963", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-7262"], "SR": 0.546875, "CSR": 0.54443359375, "EFR": 0.9655172413793104, "Overall": 0.7085526670258621}, {"timecode": 96, "before_eval_results": {"predictions": ["the game", "stood 6 feet 6 inches,", "Another high tide -- expected to reach about 4 meters (13 feet) high", "April 22,", "\"Hawaii Five-O\"", "additional information", "30", "U.S. senators", "The son of Gabon's former president", "chairman of the House Budget Committee,", "could be secretly working on a nuclear weapon", "Philippine National Police.", "Islamabad", "the U.S. Holocaust Memorial Museum,", "bicycles", "hot and humid", "a hospital", "in Fayetteville, North Carolina,", "promised federal help for those affected by the fires.", "Ewan McGregor", "September,", "anything could have stopped Robert Hawkins from going on a murderous rampage at an Omaha, Nebraska, shopping mall on Wednesday.", "genocide, crimes against humanity, and war crimes.", "any person who has been abused by any priest of the Diocese of Cloyne during my time as bishop or at any time,\"", "Barney Stinson,", "19-year-old", "Body Works", "the guerrillas", "the U.S. Holocaust Memorial Museum,", "sportswear,", "soluble fiber, peppermint oil, and antispasmodic drugs", "the latest effort by the government to foster national reconciliation between religious and ethnic groups.", "Unseeded Frenchwoman Aravane Rezai", "researchers have developed technology that makes it possible to use thoughts to operate a computer, maneuver a wheelchair or even use Twitter -- all without lifting a finger.", "five of us for the United States and two against us because they were stranded in Japan\" when the war came.", "30-minute", "Grease.", "in good spirits, especially comforted to be receiving care from talented doctors in a world-class hospital named in honor of her late husband,\"", "gunned down four Lakewood, Washington, police officers Sunday.", "the home,", "the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "between 1917 and 1924", "some work rule issues.", "Kerstin", "salary", "\"CNN Heroes: An All-Star Tribute\"", "to clean up Washington State's decommissioned Hanford nuclear site,", "over 1,000 pounds", "3,000 kilometers (1,900 miles),", "UH-60 Blackhawk helicopters", "Marc Jacobs", "Benzodiazepines", "the Coppolas and, technically, the Farrow / Previn / Allens", "To capitalize on her publicity", "ecclesiastical", "The Men Behaving Badly", "plants that will do well with the sun, soil, and water you usually get", "\u00c6thelwald Moll", "The Bears", "Prince Nikolai Sergeyevich Trubetzkoy", "the red group", "Charles Dana Gibson", "the Constitution", "snail"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5623965163717002}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, true], "QA-F1": [0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2, 0.0, 0.4, 0.0, 0.1, 1.0, 1.0, 0.5454545454545455, 1.0, 0.0, 0.125, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.8, 0.9411764705882353, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.2857142857142857, 0.22222222222222224, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-1781", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2130", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-3543", "mrqa_hotpotqa-validation-5590", "mrqa_searchqa-validation-114"], "SR": 0.4375, "CSR": 0.5433311855670103, "EFR": 1.0, "Overall": 0.7152287371134021}, {"timecode": 97, "before_eval_results": {"predictions": ["Ferrari", "Spaniard Carlos Moya", "Six", "girls", "David Bowie,", "1957,", "in all of Lifeway's 100-plus stores nationwide", "\"We want to reset our relationship and so we will do it together.'\"", "heavy brush,", "1959.", "Phoenix, Arizona,", "\"project work\"", "Buddhism", "40", "that the automaker was slow to respond to safety issues related to sudden acceleration.", "one of Africa's most stable nations.", "Molotov cocktails, rocks and glass.", "Alfredo Astiz,", "on the 11th anniversary of the September 11, 2001,", "an airport runway", "the wars in Iraq and Afghanistan", "$106,482,500", "Obama should have met with the Dalai Lama.", "Matthew Fisher,", "autonomy.", "head", "censorship remain rife", "Guinea, Myanmar, Sudan and Venezuela.", "Brazil,", "Uzbekistan.", "45 minutes, five days a week.", "urged NATO to take a more active role in countering the spread of the narcotics trade.", "took on water", "Four bodies", "July", "aren't allowed", "A lock break", "Diego Milito's", "a music video on his land.", "Robert Mugabe", "gang rape", "\"wacko.\"", "threatening messages", "Mexico", "more than 1.2 million people.", "Sunday.", "tennis", "Osama bin Laden's sons", "Chesley \"Sully\" Sullenberger", "Gary Player,", "Karen Floyd", "Asuka", "Authority", "Prince William, Duke of Cambridge", "Astor family", "Gryffindor", "Rowan Atkinson", "841", "Adelaide", "neither", "Windsor Castle", "Tad Hamilton", "Israel", "Robin Williams"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6907366071428571}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true], "QA-F1": [0.0, 0.8, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1362", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-1775", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-2337", "mrqa_triviaqa-validation-5311", "mrqa_hotpotqa-validation-2217"], "SR": 0.578125, "CSR": 0.543686224489796, "EFR": 1.0, "Overall": 0.7152997448979591}, {"timecode": 98, "before_eval_results": {"predictions": ["Cambodian officials", "monarchy's", "placed the \"black box\" warning on Cipro and other fluoroquinolones,", "AbdulMutallab", "partially submerged in a stream in Shark River Park in Monmouth County", "consumer confidence", "sedative", "Madonna", "Wigan Athletic", "Iran's parliament speaker", "18", "as soon as 2050,", "U.S. State Department and British Foreign Office", "Janet Napolitano", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "break up ice jams.", "is likely to top $60 million by the time the Presidents Day holiday weekend is over.", "without the restrictions congressional Democrats vowed to put into place", "Islamabad", "the 3rd District of Utah.", "36", "Nineteen", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "nearly $2 billion", "near Garacad, Somalia,", "Al-Shabaab,", "February 12", "Kenneth Cole", "Clifford Harris,", "heavy turbulence", "killing of a 15-year-old boy", "sniff out cell phones.", "California, Texas and Florida,", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "\"It hurts my heart to see him in pain, but it enlightenedens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Revolutionary Armed Forces of Colombia,", "suicides", "Jaime Andrade", "Aung San Suu Kyi", "$81,1203", "Patrick McGoohan,", "\"GoldenEye\"", "J.Crew,", "Dublin.", "1995", "Chester Arthur Stiles, 38,", "hopes the journalists and the flight crew will be freed,", "a skilled hacker", "$7.8 million", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "remains committed to British sovereignty", "his last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles ( 2010 -- 14 )", "September 2017", "Norman occupational surname ( meaning tailor ) in France", "Monopoly", "\u201cI came, I saw, I conquered\u201d", "hydrocephalus", "India Today", "Eliot Cutler", "7 January 1936", "dennis morris", "Istanbul", "Washington Redskins", "Mrs. Miniver"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7153544944388892}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.25, 0.8888888888888888, 1.0, 0.9, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.9565217391304348, 0.0, 0.35294117647058826, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.9333333333333333, 1.0, 1.0, 1.0, 0.21052631578947364, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3460", "mrqa_newsqa-validation-3645", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-690", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-3888", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-8858", "mrqa_triviaqa-validation-3864", "mrqa_searchqa-validation-8905", "mrqa_searchqa-validation-6956"], "SR": 0.59375, "CSR": 0.5441919191919191, "EFR": 1.0, "Overall": 0.7154008838383838}, {"timecode": 99, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2871", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1961", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1781", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-739", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-1867", "mrqa_triviaqa-validation-2079", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4698", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5253", "mrqa_triviaqa-validation-5339", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6998", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-904"], "OKR": 0.8125, "KG": 0.4890625, "before_eval_results": {"predictions": ["detritus", "1648 - 51", "The final venues were confirmed, along with the tournament's schedule, on 2 May 2013", "Scott Schwartz", "a writ of certiorari", "Lana Del Rey", "semi solid", "Madison", "May 29, 2018", "Dan Stevens", "AD 95 -- 110", "Bulgaria", "zinc", "11 February 2012", "2018", "an upright triangle", "2010", "September 2017", "Clarence Anglin", "Ariana Clarice Richards", "James Intveld", "Rita Darlene Cates", "one season", "mashed potato", "2018", "in the dress shop", "Eric Clapton", "Seton Hall Pirates men's basketball", "July 2014", "Universal Pictures and Focus Features", "Cyanea capillata", "enabled business applications to be developed with Flash", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "Lee County, Florida, United States", "the digestive systems of many organisms", "Theodosius I", "Christopher Allen Lloyd", "reared", "Mark Jackson", "Fall 1998", "TLC", "Inequality of opportunity was higher", "In 1929", "Abid Ali Neemuchwala", "Bhupendranath Dutt", "May 19, 2017", "the people of France", "two to three barrel vaults", "Brazilian state of Mato Grosso", "Gustav Bauer", "either in front or on top of the brainstem", "leicestershire", "nevyson", "15", "Mike Pence", "Soviet Union", "Heinkel Flugzeugwerke", "she returned to Pakistan in October after President Pervez Musharraf signed an amnesty lifting corruption charges.", "Zac Efron", "Aryan Airlines Flight 1625", "Ocean's Twelve", "Trajan's", "Anne Rice", "Wilkie Collins"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7087795586130627}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5000000000000001, 0.0, 1.0, 0.6666666666666666, 0.761904761904762, 0.5245901639344263, 0.5714285714285715, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-5094", "mrqa_newsqa-validation-846", "mrqa_searchqa-validation-15139"], "SR": 0.59375, "CSR": 0.5446875, "EFR": 0.9230769230769231, "Overall": 0.6944903846153847}]}