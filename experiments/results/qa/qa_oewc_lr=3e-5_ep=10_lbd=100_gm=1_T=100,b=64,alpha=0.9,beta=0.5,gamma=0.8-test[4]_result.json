{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=100_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=100.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=100_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[4]_result.json', stream_id=4, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4190, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["fall of 1937", "The Skirmish of the Brick Church", "beliefs of Sunni Islamic thinkers", "\"The Lodger\"", "Londonistan", "a high-level marketing manager", "Houston, Texas", "cone-shaped", "San Francisco Bay Area's Levi's Stadium", "Ren\u00e9 Lalique", "absolution", "$105 billion", "ABC Cable News", "trial division", "their belief in the validity of the social contract", "Hyde Park", "four years", "Grey Street", "most of the items in the collection, unless those were newly accessioned into the collection", "literacy and numeracy", "Luther", "prime elements", "the Aveo", "one week", "Steymann v Staatssecretaris van Justitie", "1937", "The governments of the United States, Britain, Germany and France", "mother-of-pearl", "cholera", "Tower District", "ring theory", "Euclid's fundamental theorem of arithmetic", "Tony Hawk", "Beyonc\u00e9", "The Book of Discipline", "USSR", "Schmalkaldic League", "2006", "70%", "Einstein", "Genghis Khan", "four half-courses per term", "2011", "Brownlee", "Tracy Wolfson", "the wisdom and prudence of certain decisions of procurement", "1971", "the Uighurs surrendered to the Mongols first", "cnidarians", "CBS", "842 pounds", "two of Tesla's uncles", "up to \u00a332,583", "the City council", "three", "shopping", "4 weeks", "propulsion, electrical power and life support", "William Smilie", "George Westinghouse", "1279", "complexity classes", "\"everything that smacks of sacrifice\"", "a system to function"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7404040404040404}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9630", "mrqa_squad-validation-7687", "mrqa_squad-validation-4836", "mrqa_squad-validation-131", "mrqa_squad-validation-2297", "mrqa_squad-validation-5505", "mrqa_squad-validation-1802", "mrqa_squad-validation-9136", "mrqa_squad-validation-9061", "mrqa_squad-validation-116", "mrqa_squad-validation-5877", "mrqa_squad-validation-6294", "mrqa_squad-validation-7214", "mrqa_squad-validation-3699", "mrqa_squad-validation-8247", "mrqa_squad-validation-4419", "mrqa_squad-validation-553", "mrqa_squad-validation-3811", "mrqa_squad-validation-2092"], "SR": 0.703125, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 1, "before_eval_results": {"predictions": ["the Pulfrich effect", "semi-legal", "Westinghouse Electric", "Pax Mongolica", "The date of 2035", "internal strife", "the Marburg Colloquy", "Northumbria University", "non-cryogenic", "the defense and justification of empire-building", "the Carm Michael numbers", "1999", "Scorpion", "October 16, 2012", "a commune", "the metal locking screw on the camera lens", "Eldon Square Shopping Centre", "type III secretion system", "$680 billion", "296", "New Collegiate Division", "four", "18 million volumes", "15,100", "Warner Bros. Presents", "V\u03b39/V\u03b42 T cells", "1985", "the Augustinian friars", "third", "2012", "gold", "force model that is independent of any macroscale position vector", "378", "tourism", "the Jews", "all", "many celebrated seasons", "Charles-Fer Ferdinand University", "The Nationals", "a computational problem where a single output (of a total function) is expected for every input", "Katharina von Bora", "1888", "the middle of the continent", "Schmalkaldic League", "4:51", "Knaurs Lexikon", "constant pressure", "detective shows", "the southern and central parts of France", "Maria Fold and thrust Belt", "making it seem like climate change is more serious by overstating the impact", "1945", "1876", "Elway", "spring of 1349", "Extreme Makeover: Home Edition", "the Wesleyan Holiness Consortium", "it has settled as one of the pillars of history", "The four Railroads are fairly lucrative properties", "The Sphinx would devour anyone who could not answer her riddle", "The Smashing Pumpkins are an American alternative rock band from Chicago, Illinois, formed", "the 107th justice to serve on the United States Supreme Court", "32-year-long investigation into the enigmatic hijacker", "If the citizen's heart was heavier than a feather they would face torment in a lake of fire"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8365891167494787}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.2666666666666667, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 0.18181818181818182, 0.3076923076923077, 0.0, 0.0, 0.2105263157894737]}}, "before_error_ids": ["mrqa_squad-validation-8546", "mrqa_squad-validation-9024", "mrqa_squad-validation-10466", "mrqa_squad-validation-1030", "mrqa_squad-validation-1189", "mrqa_squad-validation-1600", "mrqa_squad-validation-4287", "mrqa_squad-validation-2166", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-1274", "mrqa_hotpotqa-validation-3713"], "SR": 0.78125, "CSR": 0.7421875, "EFR": 0.9285714285714286, "Overall": 0.8353794642857143}, {"timecode": 2, "before_eval_results": {"predictions": ["Solim\u00f5es Basin", "the seal of the Federal Communications Commission", "Islamism", "the E. W. Scripps Company", "Grand Canal d'Alsace", "food security", "exothermic", "Newcastle Diamonds", "the wisdom and prudence of certain decisions of procurement", "D\u00fcrer", "Grover Cleveland", "Erg\u00e4nzungsschulen", "concrete", "to promote advanced research and education networking in the United States", "The Newlywed Game", "the German-Swiss border", "the \"blurring of theological and confessional differences in the interests of unity.\"", "microbes", "300", "the electrostatic force", "Jim Nantz and Phil Simms", "1530", "from 12:00 to 6:00 p.m. Eastern Time", "employ consultant pharmacists and/or provide consulting services", "the murder of Christ", "1708", "the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)", "the Religious Coalition for Reproductive Choice", "The Arrow", "intractable problems", "yellow fever", "silver and inlaid with gold", "Richard Wilkinson and Kate Pickett", "elsewhere in the Northern United Kingdom", "president and CEO", "Von Miller", "the weak force", "a diverse phylum of bacteria capable of carrying out photosynthesis", "vaccination", "the plague theory", "the loss of soil fertility and weed invasion", "the fact (Fermat's little theorem)", "11th", "the most popular show", "Robert of Jumi\u00e8ges", "student populations", "German", "Persia", "superheaters", "two", "Johann von Staupitz", "lectures", "Short Short", "What a wonderful World", "the White House", "Dugout canoe", "Ganges", "the Heritage 1981 brand", "Britney Spears", "the title My Fair Lady", "Don Bradman", "Ford Motor Co.", "Fidenza", "Charles Scribner's"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7465411324786324}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5521", "mrqa_squad-validation-4847", "mrqa_squad-validation-597", "mrqa_squad-validation-5828", "mrqa_squad-validation-10460", "mrqa_squad-validation-8777", "mrqa_squad-validation-4974", "mrqa_squad-validation-9023", "mrqa_squad-validation-1188", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-9859", "mrqa_searchqa-validation-6857", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-3869"], "SR": 0.6875, "CSR": 0.7239583333333333, "EFR": 1.0, "Overall": 0.8619791666666666}, {"timecode": 3, "before_eval_results": {"predictions": ["Winter Film Capital of the World", "clear boundaries", "collenchyma tissue", "24 March 1879", "the Convention", "constant factors and smaller terms", "1996", "CBS", "genetic branches", "on a religious basis", "14,000", "Killer T cells", "about 11 million", "third", "The Earth's mantle", "expansions", "Necessity-based", "glaucophyte chloroplasts", "artisans and farmers", "inverted repeat", "pharmacists", "September 2007", "a declining state of mind", "G", "civil disobedience", "Sociologist", "World News Tonight", "the carriage of their respective basic channels", "cytotoxic", "Liao, Jin, and Song", "Tyneside Classical", "nine", "18 million", "four", "Christian Whiton", "his mother", "Johann Gerhard", "Korean", "The Time of the Doctor", "7 January 1900", "90\u00b0", "the Natives of these localities are very badly disposed towards the French, and are entirely devoted to the English", "stolen", "Centrum", "$200,000", "4,686", "economic", "1950s", "Karl Marx", "President of the United States of America", "Disneyland", "the king", "South Africa", "fibre optics", "Lawrence Brooks", "a cone-shaped utensil", "Yasser Arafat", "Pigeon", "voodoo", "a dowry", "Macduff", "Jean Halliwell", "Prince James, Duke of York and of Albany ( later King James II & VII )", "Charles Perrault"], "metric_results": {"EM": 0.640625, "QA-F1": 0.702951388888889}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1947", "mrqa_squad-validation-1714", "mrqa_squad-validation-9597", "mrqa_squad-validation-10107", "mrqa_squad-validation-8703", "mrqa_squad-validation-6409", "mrqa_squad-validation-3958", "mrqa_squad-validation-6670", "mrqa_squad-validation-6884", "mrqa_squad-validation-7083", "mrqa_squad-validation-2406", "mrqa_squad-validation-10186", "mrqa_squad-validation-1509", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-15033", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-495"], "SR": 0.640625, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 4, "before_eval_results": {"predictions": ["Mars", "education", "Kingdom of Prussia", "May 18, 1756", "odd prime", "economic growth by collecting resources from colonies, in combination with assuming political control by military and political means", "quantum electrodynamics", "topographic", "Hassan al Banna", "regional burden sharing", "Indianapolis Colts", "Pole Mokotowskie", "a school or other place of formal education", "Francis Blackburne", "black earth", "photolysis of ozone by light of short wavelength", "smart ticketing", "State Route 99", "F and \u2212F are equal in magnitude and opposite in direction", "free", "Air", "1,548", "whether the bill is within the legislative competence of the Parliament", "two", "Galileo Galilei", "Catholic", "patient care rounds drug product selection", "vicious and destructive", "greater scarcity", "The Eleventh Doctor", "86", "Arizona Cardinals", "Not designed to fly through the Earth's atmosphere or return to Earth", "2015", "one hunting excursion", "Deacons", "to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church", "socialist realism", "July 23, 1963", "1162", "a method which pre- allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes", "Barbara Walters", "as soon as 2050", "Red River", "the hundreds", "London and Buenos Aires", "Sub-Saharan Africa", "D, E or F", "the Carrousel du Louvre", "$1.5 million", "3 to 17", "Isabella", "Obama", "World Wide Village", "Preah Vihear temple", "Ralph Lauren", "Noriko Savoie", "T.I.", "(Zed)", "the Kenyan and Somali governments issued a joint communique declaring Al-Shabaab \"a common enemy to both countries.\"", "battles, political intrigue, and the characters", "oldpatricktoe-end", "Mulberry", "Shaft"], "metric_results": {"EM": 0.6875, "QA-F1": 0.751541832010582}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29629629629629634, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.25, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9926", "mrqa_squad-validation-8634", "mrqa_squad-validation-1891", "mrqa_squad-validation-10333", "mrqa_squad-validation-3706", "mrqa_squad-validation-2564", "mrqa_squad-validation-4746", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-2234", "mrqa_naturalquestions-validation-7134", "mrqa_triviaqa-validation-712", "mrqa_searchqa-validation-8929"], "SR": 0.6875, "CSR": 0.7, "EFR": 0.9, "Overall": 0.8}, {"timecode": 5, "before_eval_results": {"predictions": ["visitation of the Electorate of Saxony", "United States", "11", "May", "to employ limited coercion in order to get their issue onto the table", "1798", "3D printing technology", "Waterlogged", "Tim Allen", "wealth and income", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "40", "LeGrande", "filaments", "Joanna Lumley", "Energiprojekt AB in Sweden", "DuMont Television Network", "1913", "Egyptian Islamic Jihad organization", "consumer prices", "petroleum", "1870", "27.7 million tons", "the remainder of the British Isles", "Jean Auguste Dominique Ingres,", "Eliot Ness", "to obey the temporal authorities", "Edgar", "experience and extra responsibilities", "chameleon circuit", "All-Channel Receiver Act", "secular powers", "at the opposite end from the mouth", "areas controlled by Russia in 1914", "kinescope", "University Athletic Association (UAA)", "three", "Chebyshev", "reality television", "Disco", "Christopher Lloyd Smalling", "Mary Harron", "Polk", "Minette Walters", "1983", "79 AD", "1993 to 2001", "Major League Soccer", "1669", "University of Vienna", "\"lo Stivale\" (the Boot)", "Richa Sharma", "Centennial Olympic Stadium", "Violet", "Vernier, Switzerland", "October 21, 2016", "an Indian cricketer and former captain of the Indian cricket team", "international association football competitions", "The conversation", "9 February 2018", "Canada", "teenage", "an increase in dew point", "Donna Mills"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8171875000000001}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6846", "mrqa_squad-validation-3345", "mrqa_squad-validation-5519", "mrqa_squad-validation-2322", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4614", "mrqa_triviaqa-validation-4029", "mrqa_newsqa-validation-2982", "mrqa_searchqa-validation-4118"], "SR": 0.765625, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 6, "before_eval_results": {"predictions": ["woodblocks", "General Hospital", "the dot", "Denver Broncos", "129 MSPs", "Lenin", "completed (or local) fields", "\"push\" motivations", "alone", "stem cells", "John Pell, Lord of Pelham Manor", "quickly", "induction motor", "Holy War", "pressure terms", "ten million", "Tommy Lee Jones", "nine", "13.34% (116.7 sq mi or 302 km2)", "kilopond", "water level", "1981", "rules that conflict with morality", "early as the sixteenth century", "R\u00fcdesheim", "an epidemiological account of the plague", "time or space", "Jacksonville Consolidation", "Aristotle", "1724", "mid-Cambrian period", "Canada", "Stanford University", "Reuben Townroe", "small forward", "Alamo Bowl", "The King of Chutzpah", "Charles Russell", "German", "Minette Walters (born 26 September 1949)", "St. Patrick's Day in 1988", "Dulwich", "Michael Sheen", "Hungary", "ITV", "Ella Fitzgerald", "\"Confessions of a Teenage Drama Queen\"", "EBSCO Information Services", "Dutch", "Marc Bolan", "\"The Nightmares Before Christmas\" (1992)", "John Mills", "1992", "Saint-Domingue", "Airline Deregulation Act", "\"Kill Your Darlings\"", "University of Kansas", "The Land of Enchantment", "2001", "AnCIENT Seven Wonders of The World", "illegal", "Billy Bob Thornton (born August 4, 1955)", "Charles Martel", "The Blues Brothers"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7227306547619047}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5774", "mrqa_squad-validation-9430", "mrqa_squad-validation-7614", "mrqa_squad-validation-2567", "mrqa_squad-validation-5303", "mrqa_squad-validation-7476", "mrqa_squad-validation-9718", "mrqa_squad-validation-9098", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-4387", "mrqa_triviaqa-validation-2856", "mrqa_newsqa-validation-696", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-12796"], "SR": 0.65625, "CSR": 0.703125, "EFR": 0.9545454545454546, "Overall": 0.8288352272727273}, {"timecode": 7, "before_eval_results": {"predictions": ["Germany", "1985", "William Iron Arm", "Muhammad Abd al-Salaam Farag", "comedies", "lack of understanding", "British East Africa", "the Atlantic", "Jingshi Dadian", "economic instability", "Jean- Marc Bosman", "Ismailiyah", "Wiesner", "polynomial time", "quarterback", "ten times their own weight", "primes", "light", "success", "TGIF", "often married outside their immediate French communities", "George Westinghouse", "color confinement", "certification", "Alberto Calder\u00f3n", "Mercury", "Marconi successfully transmitted the letter S from England to Newfoundland", "private", "Cadeby", "Ten", "vice president", "Steven Gerrard", "the international community", "Hine's school", "Obama", "Silvio Berlusconi", "London Heathrow's Terminal 5", "at the House of Blues", "football", "sharia law", "Chandler Keys", "Sonia Sotomayor", "Kurdish militant group", "pilot", "Wednesday", "$50", "back at work", "flooding", "1971", "composer", "50,000", "to best your own fuel economy achievements", "Bhola district", "SSM Cardinal Glennon Children's Medical Center", "military", "he has no plans to fritter his cash away", "Secretary of State Hillary Clinton", "7th century", "Wigan", "2004", "the Mormon Tabernacle Choir", "Hagai Amir", "The Little Foxes", "Lord Tennyson"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5463834972394754}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.08695652173913045, 0.05714285714285715, 0.33333333333333337, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6034", "mrqa_squad-validation-6925", "mrqa_squad-validation-8339", "mrqa_squad-validation-4289", "mrqa_squad-validation-4692", "mrqa_squad-validation-9614", "mrqa_squad-validation-2160", "mrqa_squad-validation-8771", "mrqa_squad-validation-3069", "mrqa_squad-validation-10445", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_naturalquestions-validation-8664", "mrqa_hotpotqa-validation-2330", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-4905", "mrqa_searchqa-validation-1341"], "SR": 0.484375, "CSR": 0.67578125, "EFR": 0.9696969696969697, "Overall": 0.8227391098484849}, {"timecode": 8, "before_eval_results": {"predictions": ["2 July 1505,", "beginning in early September and ending in mid-May", "high risk preparations and some other compounding functions", "Non Governmental and Intergovernmental Organizations", "chief electrician position", "1671", "United States", "12 January 1943", "his own men", "Sonia Shankman Orthogenic School", "Innate immune systems", "the manufacturing sector", "Wardenclyffe Tower project", "horizontal compression", "Edgar Scherick", "Imperial", "Moscone Center", "Great Yuan", "Since the 1980s", "oxygen-16", "Lessing", "one advanced lay servant course", "the Tyne Tunnel", "Threatening government officials", "ca. 22,000\u201314,000 yr BP,", "two", "Lek", "New England Patriots", "work rule issues", "28 of those now on hardcourt surfaces.", "July 4.", "the Catholic League", "more than two years,", "Newark's Liberty International Airport,", "We Found Love", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "a kidney transplant", "foster national reconciliation between religious and ethnic groups", "Addis Ababa,", "Christian bookstores across the country that carry the publication.", "military trials", "to launch a group that will serve as an alternative to the Organization of American States.", "to pay him a monthly allowance", "Tutsi and Hutu rivalry", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Draquila -- Italy Trembles.", "hooked up with Mildred, a younger woman of about 80, in March.", "his former Boca Juniors teammate and national coach Diego Maradona,", "17 Again", "five", "75", "Kgalema Motlanthe,", "preventing our public-owned seas from turning into sprawling, watery versions of Houston, Texas, or Atlanta, Georgia.", "about 3,000 kilometers (1,900 miles)", "two courses", "NATO fighters", "Austin, Texas,", "two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "Royals", "Anah\u00ed", "Ceausescu and his wife", "Nikkei 225 Stock Average", "Surrey", "David Bowie"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6015660396824801}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1818181818181818, 0.8, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333336, 1.0, 1.0, 0.0, 0.7142857142857143, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.058823529411764705, 0.2608695652173913, 0.1818181818181818, 0.28571428571428575, 0.06451612903225806, 0.5, 0.0, 0.9523809523809523, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8526", "mrqa_squad-validation-1279", "mrqa_squad-validation-3113", "mrqa_squad-validation-589", "mrqa_squad-validation-1570", "mrqa_squad-validation-6128", "mrqa_squad-validation-7377", "mrqa_squad-validation-8084", "mrqa_squad-validation-2405", "mrqa_squad-validation-10083", "mrqa_squad-validation-5357", "mrqa_squad-validation-6671", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-490", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-93", "mrqa_searchqa-validation-8602", "mrqa_triviaqa-validation-1"], "SR": 0.46875, "CSR": 0.6527777777777778, "EFR": 1.0, "Overall": 0.8263888888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["Private Education Student Financial Assistance", "philanthropy", "in an unmarked grave", "Daily Mail", "heard her songs; he followed the fishermen and captured the mermaid.", "action-reaction", "Combined Statistical Area", "orogenic wedges", "eleven", "Grissom, White, and Chaffee", "Amtrak San Joaquins", "two", "1", "Barbara Walters", "temperate", "girls", "by citizens", "by up to 3 pence in the pound", "Lessing", "the courts of member states and the Court of Justice of the European Union", "1887", "new laws or amendments to existing laws", "487", "the Presiding Officer", "expansion", "between 3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "in the axial skeleton ( 28 in the skull and 52 in the torso )", "Thebes", "moral", "The Maidstone Studios in Maidstone, Kent", "husky", "National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1986", "Charles Darwin", "Germanic god Wodan, who was associated with the pagan midwinter event of Yule and led the Wild Hunt, a ghostly procession through the sky.", "in Poems : Series 1", "1927", "merengue", "lamina dura", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "Proposition 103", "1997 and the Middle East in 2000", "at the intersection of Mud Mountain Road and Highway 410", "United States", "Tom Brady", "2017", "Duck", "three levels", "Sylvester Stallone", "every 23 hours", "never made", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year reported on the accompanying income statement", "1963", "costume party", "altitude", "1 point", "sow", "in feces or vomit", "John Joseph Travolta", "Allies of World War I", "U.S. 93", "have a smile on her face when her kids were around.", "mike tyson", "Utah"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6615295203699809}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.7499999999999999, 1.0, 0.3636363636363636, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428572, 0.16666666666666669, 0.0, 0.6666666666666666, 1.0, 0.0, 0.631578947368421, 1.0, 1.0, 0.09523809523809523, 0.3076923076923077, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.2857142857142857, 0.13333333333333333, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5937", "mrqa_squad-validation-805", "mrqa_squad-validation-2717", "mrqa_squad-validation-3922", "mrqa_squad-validation-9641", "mrqa_squad-validation-2404", "mrqa_squad-validation-9452", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-893", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-1173", "mrqa_triviaqa-validation-2329", "mrqa_hotpotqa-validation-3886", "mrqa_newsqa-validation-4179"], "SR": 0.515625, "CSR": 0.6390625, "EFR": 0.9354838709677419, "Overall": 0.7872731854838709}, {"timecode": 10, "UKR": 0.7734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1735", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1877", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3503", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3994", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4504", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6984", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3717", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-4000", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-69", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10783", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14603", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16779", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2470", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-350", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-8978", "mrqa_searchqa-validation-9187", "mrqa_squad-validation-10015", "mrqa_squad-validation-10052", "mrqa_squad-validation-10068", "mrqa_squad-validation-1008", "mrqa_squad-validation-10083", "mrqa_squad-validation-10103", "mrqa_squad-validation-10107", "mrqa_squad-validation-10116", "mrqa_squad-validation-10125", "mrqa_squad-validation-10186", "mrqa_squad-validation-10210", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-1030", "mrqa_squad-validation-10308", "mrqa_squad-validation-10333", "mrqa_squad-validation-10333", "mrqa_squad-validation-10344", "mrqa_squad-validation-10367", "mrqa_squad-validation-10374", "mrqa_squad-validation-104", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10466", "mrqa_squad-validation-10493", "mrqa_squad-validation-1051", "mrqa_squad-validation-1052", "mrqa_squad-validation-1068", "mrqa_squad-validation-1113", "mrqa_squad-validation-116", "mrqa_squad-validation-1165", "mrqa_squad-validation-1178", "mrqa_squad-validation-1188", "mrqa_squad-validation-1193", "mrqa_squad-validation-1200", "mrqa_squad-validation-1207", "mrqa_squad-validation-1211", "mrqa_squad-validation-1257", "mrqa_squad-validation-1269", "mrqa_squad-validation-1279", "mrqa_squad-validation-131", "mrqa_squad-validation-1330", "mrqa_squad-validation-1348", "mrqa_squad-validation-1368", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1509", "mrqa_squad-validation-1527", "mrqa_squad-validation-1536", "mrqa_squad-validation-1541", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1714", "mrqa_squad-validation-1769", "mrqa_squad-validation-1802", "mrqa_squad-validation-1891", "mrqa_squad-validation-1947", "mrqa_squad-validation-1967", "mrqa_squad-validation-2030", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2166", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-2297", "mrqa_squad-validation-2331", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2405", "mrqa_squad-validation-2409", "mrqa_squad-validation-2438", "mrqa_squad-validation-25", "mrqa_squad-validation-2554", "mrqa_squad-validation-2559", "mrqa_squad-validation-2564", "mrqa_squad-validation-2567", "mrqa_squad-validation-2576", "mrqa_squad-validation-2579", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2717", "mrqa_squad-validation-2778", "mrqa_squad-validation-2822", "mrqa_squad-validation-2827", "mrqa_squad-validation-2870", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-3050", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-313", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3261", "mrqa_squad-validation-3269", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3388", "mrqa_squad-validation-3445", "mrqa_squad-validation-3492", "mrqa_squad-validation-3603", "mrqa_squad-validation-3617", "mrqa_squad-validation-365", "mrqa_squad-validation-3699", "mrqa_squad-validation-3759", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3815", "mrqa_squad-validation-3833", "mrqa_squad-validation-3837", "mrqa_squad-validation-3844", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3922", "mrqa_squad-validation-3938", "mrqa_squad-validation-3958", "mrqa_squad-validation-3976", "mrqa_squad-validation-4030", "mrqa_squad-validation-4086", "mrqa_squad-validation-4191", "mrqa_squad-validation-4231", "mrqa_squad-validation-4232", "mrqa_squad-validation-4248", "mrqa_squad-validation-4269", "mrqa_squad-validation-43", "mrqa_squad-validation-4419", "mrqa_squad-validation-4480", "mrqa_squad-validation-4491", "mrqa_squad-validation-4560", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4746", "mrqa_squad-validation-475", "mrqa_squad-validation-4765", "mrqa_squad-validation-4836", "mrqa_squad-validation-4847", "mrqa_squad-validation-4896", "mrqa_squad-validation-4935", "mrqa_squad-validation-5009", "mrqa_squad-validation-5075", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5164", "mrqa_squad-validation-5180", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5221", "mrqa_squad-validation-5272", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5357", "mrqa_squad-validation-5363", "mrqa_squad-validation-5424", "mrqa_squad-validation-5451", "mrqa_squad-validation-5455", "mrqa_squad-validation-5471", "mrqa_squad-validation-5505", "mrqa_squad-validation-5519", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5541", "mrqa_squad-validation-5616", "mrqa_squad-validation-5651", "mrqa_squad-validation-5670", "mrqa_squad-validation-5774", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-583", "mrqa_squad-validation-5840", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-5877", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5908", "mrqa_squad-validation-5937", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-5971", "mrqa_squad-validation-5976", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6048", "mrqa_squad-validation-6083", "mrqa_squad-validation-6098", "mrqa_squad-validation-6098", "mrqa_squad-validation-6128", "mrqa_squad-validation-6158", "mrqa_squad-validation-618", "mrqa_squad-validation-6238", "mrqa_squad-validation-6294", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6381", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6506", "mrqa_squad-validation-6527", "mrqa_squad-validation-6530", "mrqa_squad-validation-6569", "mrqa_squad-validation-6580", "mrqa_squad-validation-6605", "mrqa_squad-validation-6670", "mrqa_squad-validation-6681", "mrqa_squad-validation-6707", "mrqa_squad-validation-6754", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-69", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-6996", "mrqa_squad-validation-7002", "mrqa_squad-validation-7020", "mrqa_squad-validation-7022", "mrqa_squad-validation-7034", "mrqa_squad-validation-7080", "mrqa_squad-validation-7083", "mrqa_squad-validation-7092", "mrqa_squad-validation-7094", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7303", "mrqa_squad-validation-7304", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7372", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7420", "mrqa_squad-validation-7476", "mrqa_squad-validation-7502", "mrqa_squad-validation-7614", "mrqa_squad-validation-7687", "mrqa_squad-validation-7690", "mrqa_squad-validation-7704", "mrqa_squad-validation-775", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-7886", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7981", "mrqa_squad-validation-805", "mrqa_squad-validation-8052", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8197", "mrqa_squad-validation-8247", "mrqa_squad-validation-829", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8364", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8416", "mrqa_squad-validation-8479", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8526", "mrqa_squad-validation-8546", "mrqa_squad-validation-8580", "mrqa_squad-validation-8600", "mrqa_squad-validation-863", "mrqa_squad-validation-8680", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8777", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8953", "mrqa_squad-validation-8957", "mrqa_squad-validation-8965", "mrqa_squad-validation-9002", "mrqa_squad-validation-9012", "mrqa_squad-validation-902", "mrqa_squad-validation-9023", "mrqa_squad-validation-9024", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9136", "mrqa_squad-validation-9141", "mrqa_squad-validation-9208", "mrqa_squad-validation-9254", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9337", "mrqa_squad-validation-9411", "mrqa_squad-validation-9430", "mrqa_squad-validation-9452", "mrqa_squad-validation-9457", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9527", "mrqa_squad-validation-9569", "mrqa_squad-validation-9597", "mrqa_squad-validation-9614", "mrqa_squad-validation-9615", "mrqa_squad-validation-9624", "mrqa_squad-validation-9635", "mrqa_squad-validation-9641", "mrqa_squad-validation-9665", "mrqa_squad-validation-9718", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_squad-validation-9845", "mrqa_squad-validation-985", "mrqa_squad-validation-9926", "mrqa_squad-validation-9940", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.904296875, "KG": 0.4140625, "before_eval_results": {"predictions": ["the Miller\u2013Urey experiment", "five", "Mughal emperors", "a series of strikes by coal miners and railroad workers over the winter of 1973\u201374 became a major factor in the change of government.", "\" Informal\" and \"informal\" imperialism", "HO", "increased settlement and deforestation", "Afranji, meaning \"Franks.\"", "Abercynon in south Wales", "drinking water", "permafrost", "10 to 15 million", "Cabot Science Library, Lamont Library, and Widener Library", "the Henry Cole wing", "a unicameral Warsaw City Council (Rada Miasta), which comprises 60 members. Council members are elected directly every four years.", "The time and space hierarchy theorems", "The innate immune system", "Cow Counties", "11", "Royal Institute of British Architects", "the 6th century", "The owner", "United Parcel Service, Inc.", "Ernie", "Sapporo", "The Rosetta Stone", "m\u00e0hjeung", "Tonight", "the right hand side of the second line of letters, the semi colon key is swapped for the M key.", "n. pl. Japanese", "William Boyd", "Wolf Hall", "a Golden set", "the gums", "The House That Hoban Built", "Richmond, Va.", "Wawrinka", "Humphrey Bogart", "Nigel Hawthorne", "\" animal", "Auric Goldfinger", "Hell Upside Down: The Making of The Poseidon Adventure", "5", "Mary Poppins", "Neil Armstrong", "the centre bull", "Metropolitan Borough of Oldham, in Greater Manchester, England", "British Defence Secretary", "Old Ironsides", "Bullnose", "blue", "the skull", "\"Maljanne\"", "gold", "Brainy", "New Zealand", "Paige O'Hara", "Chris Martin", "Ishtar Gate", "neo-Nazi", "several weeks", "an animal tranquilizer", "Robin Givens", "the mouth"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5996660337985769}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6206896551724138, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3722", "mrqa_squad-validation-9808", "mrqa_squad-validation-1134", "mrqa_squad-validation-6223", "mrqa_squad-validation-962", "mrqa_squad-validation-944", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5918", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-865", "mrqa_triviaqa-validation-6822", "mrqa_triviaqa-validation-1579", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-515", "mrqa_searchqa-validation-2154"], "SR": 0.546875, "CSR": 0.6306818181818181, "EFR": 1.0, "Overall": 0.7444957386363636}, {"timecode": 11, "before_eval_results": {"predictions": ["\"Old Briton\"", "Tiffany & Co.", "1985", "y. p. orientalis", "4k + 3", "Samuel Reshevsky", "mujahideen Muslim Afghanistan", "July 1977", "coal", "programmes", "Stanford Stadium", "friction", "his work", "monatomic", "MetroCentre", "SAP Center", "composite", "the courts of member states", "German-language publications", "27", "6", "summer", "Durham Cathedral", "Konakuppakatil Gopinathan Balakrishnan", "Warsaw Radio Mast", "March 14, 1942", "book and architecture", "Bart Howard", "along with a cover slip or cover glass", "m moscovium", "Britney Spears", "2005", "1612", "February 29", "transmission", "216", "Dr. Addison Montgomery", "old English pyrige", "food and clothing", "the Kansas City Chiefs", "the Indians", "Jennifer Parker", "fascia surrounding skeletal muscle", "the amount of surface", "October 2004", "old York Yankees'third baseman Alex Rodriguez", "`` Love Will Keep Us Alive ''", "Jaydev Shah", "Jodie Foster", "13,000 astronomical units", "1978", "wisdom", "Thomas Chisholm", "Rocinante", "before the first letter of an interrogative sentence", "Wikia", "Sunday Mirror", "invoice", "the Cumberland Plain", "1,500", "mayor of Seoul from 2002 to 2004", "Alexander Pushkin", "richmond", "The Time Machine"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6395442231379732}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.15384615384615383, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10142", "mrqa_squad-validation-4963", "mrqa_squad-validation-3450", "mrqa_squad-validation-9024", "mrqa_squad-validation-3947", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-4549", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-3602", "mrqa_triviaqa-validation-412", "mrqa_hotpotqa-validation-2913", "mrqa_newsqa-validation-3686", "mrqa_searchqa-validation-14088", "mrqa_searchqa-validation-2383"], "SR": 0.53125, "CSR": 0.6223958333333333, "EFR": 0.9666666666666667, "Overall": 0.7361718749999999}, {"timecode": 12, "before_eval_results": {"predictions": ["2014", "Muslim state", "Frederick William", "herbal remedies", "Mongols and the Semuren", "96.26%", "19th", "Raoul Pierre Pictet", "more than $45,000", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "n < p < 2n \u2212 2", "illegal acts", "A", "The European Court of Justice", "chlorophyll a and phycobilins", "German-language publications", "1992", "25 percent of all money it raises for philanthropic causes in the Bay Area", "in the stems and roots of certain vascular plants", "Richardson", "1998", "was an incident on King Street by the British", "Judiththia Aline Keppel", "Ted '' Levine", "gregororio", "noble gas", "A rotation", "Jaydev Shah", "Polly Walker", "Raya Yarbrough", "Darlene Cates", "2014 Winter Olympics in Sochi", "in the scientific literature for the evolution of light hair", "2020 National Football League ( NFL ) season", "his brother", "11 November 1918", "Jean F Kernel", "Haiti", "1956", "Sauron", "Chandan Shetty", "Bill Henderson", "Liam Cunningham", "Nancy Jean Cartwright", "Max", "Jonathan Goldstein", "a premalignant flat", "in the brain", "his friends", "Ren\u00e9 Descartes", "Clarence Anglin", "Jason Lee", "October 1941", "2013", "on the continent of Antarctica", "Bath and Wells", "John Nash", "Alistair Grant", "1989", "nearly $2 billion", "gacaca", "Inkerman", "Brownsville", "Ralph Lauren"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6762362637362638}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.2, 0.0, 1.0, 0.15384615384615383, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.16666666666666666, 1.0, 1.0, 0.6, 0.4, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8206", "mrqa_squad-validation-8412", "mrqa_squad-validation-6655", "mrqa_squad-validation-394", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-5259", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-7346", "mrqa_triviaqa-validation-6797", "mrqa_hotpotqa-validation-2319", "mrqa_newsqa-validation-3661", "mrqa_searchqa-validation-3744"], "SR": 0.59375, "CSR": 0.6201923076923077, "EFR": 1.0, "Overall": 0.7423978365384615}, {"timecode": 13, "before_eval_results": {"predictions": ["early 1990s", "the 17th century", "Four thousand", "the class ofNP-complete problems", "the seal of the Federal Communications Commission", "gentrification of older neighbourhoods", "a historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\"", "1206", "2011", "in his lab and elsewhere", "their bright colors sometimes override the chlorophyll green", "Radio Corporation of America", "up to three-fourths", "partial funding", "Open Door Policy", "1538", "Cricket", "a minimum adequate diet", "Tony Almeida", "Calvin Coolidge", "Saint Wenceslaus", "coal", "Union Pacific & the Central Pacific", "to deliver the standard Late Show apology", "\"Peking Man\"", "Latin", "War of the Worlds", "Luxor", "to work hard", "the Osmonds", "a stuffed animal", "butterflies", "the ballerina", "The Real Apprentices", "Calypso", "Richard", "A Million Little pieces", "\"Give Me Liberty Or Give Me Death\"", "\"Imagine\"", "the Billy Goats Gruff", "palindrome", "\"bacon strip\"", "Saturn", "the Urals", "Amsterdam", "Etna", "Oahu", "Richard Nixon", "\"Under The Sea\"", "a letter T", "George Carlin", "Che Guevara", "a raven", "Kurmanji", "Nicholas Sparks", "Lizzy Greene", "B\u00e9la Bart\u00f3k", "Michael Owen", "English Electric Canberra", "the era of Texan history", "Krishna Rajaram", "at the Lindsey oil refinery in eastern England.", "60 euros", "processing data"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6323223039215686}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.25, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1804", "mrqa_squad-validation-9575", "mrqa_squad-validation-8229", "mrqa_squad-validation-6250", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-6844", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-16654", "mrqa_searchqa-validation-5858", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-2755", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5059", "mrqa_hotpotqa-validation-1876", "mrqa_newsqa-validation-2801"], "SR": 0.578125, "CSR": 0.6171875, "EFR": 1.0, "Overall": 0.741796875}, {"timecode": 14, "before_eval_results": {"predictions": ["Pittsburgh Steelers", "J. S. Bach", "Denver", "from January 1964, until it achieved the first manned landing in July 1969,", "unicellular organisms", "The Quasiturbine", "Dave Logan", "Roger NFL", "a cascade method", "December 2014", "Ladner", "the Ming dynasty", "Newton", "15,100", "Ronnie Hillman", "Masha Skorobogatov", "Daniel A. Dailey", "on the urinary floor", "31 December 1600", "National Park Service's Shenandoah National Park", "1916", "Milira", "in the absence of a catalyst", "a comic book series", "ceramists or potters", "government monopoly", "Tbilisi", "a candidate state must be a free market democracy", "Yondu Udonta", "religious Hindu musical theatre styles", "Rocinante", "one", "John Roberts", "Ray Charles", "British", "a routing table", "the donor organ", "Bobby Darin", "2 Fast 2 Furious", "No Secrets", "Kim Basinger", "photoelectric", "5,534", "Valmiki", "September 19", "President pro tempore", "February 16, 2016", "Wisconsin", "Merry Clayton", "greatly determine the tenderness of meat", "Florida", "a salvation story", "Nepal", "Austria - Hungary", "Word Options", "Australia", "June 2, 2008", "Reverend Timothy \"Tim\" Lovejoy", "Arthur E. Morgan III", "a Muslim and a Coptic family", "Australia", "Oxford Committee for Famine Relief", "a 2vaganzas", "ivan owkosh"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5526413690476191}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.2857142857142857, 0.8, 0.0, 0.3333333333333333, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-32", "mrqa_squad-validation-6453", "mrqa_squad-validation-690", "mrqa_squad-validation-80", "mrqa_squad-validation-8062", "mrqa_squad-validation-362", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-9517", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-1161", "mrqa_triviaqa-validation-5650", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2435", "mrqa_searchqa-validation-11788", "mrqa_searchqa-validation-913", "mrqa_searchqa-validation-9115"], "SR": 0.453125, "CSR": 0.60625, "EFR": 1.0, "Overall": 0.739609375}, {"timecode": 15, "before_eval_results": {"predictions": ["chest pains", "thermodynamic", "The Lone Ranger", "broken arm", "Arthur Woolf", "a noble man who respected people's dignity and lives", "Manning", "Warren Buffett", "Malkin Athletic Center", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "planktonic", "Lucas Horenbout", "a 22-yard throw to receiver Andre Caldwell", "Spektor", "writ of certiorari", "on the table", "Holly Bell", "flawed democracy", "Southwest Florida International Airport ( RSW )", "1987", "temporal lobes", "Missi Hale", "their son Jack ( short for Jack - o - Lantern )", "a Norwegian town circa 1879", "the Dutch United Provinces", "Television demonstrations", "26 days in 1994 and 19 days in 1998", "1937", "Go KU", "gastrocnemius", "a god of the Ammonites", "Christmas Day", "24th match", "273.16 K", "a major earthquake", "Isaiah Amir Mustafa", "Southern California Timing Association ( SCTA )", "The photoelectric ( optical ) smoke detector", "New York University", "mascot", "the ball is fed into the gap between the two forward packs", "microscope's stage", "9 February 2018", "31 October 1972", "Jesse McCartney", "4.25 inches ( 108 mm )", "Lady Gaga", "Director of National Intelligence", "April 1979", "mainland greece", "at least 18 or 21 years old ( or have a legal guardian present )", "the Royal Air Force ( RAF )", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "a single peptide bond or one amino acid with two peptide bonds", "197 million square miles", "Alberich", "Debbie Reynolds", "\"Jawbreaker\"", "about 100 light bulbs", "58", "the ulna", "2", "Monday night", "a fight outside of an Atlanta strip club"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5457878221839361}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7368421052631579, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.1818181818181818, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6976744186046512, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.30769230769230765]}}, "before_error_ids": ["mrqa_squad-validation-3190", "mrqa_squad-validation-845", "mrqa_squad-validation-9399", "mrqa_squad-validation-4636", "mrqa_squad-validation-800", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8215", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-4064", "mrqa_naturalquestions-validation-437", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-6481", "mrqa_triviaqa-validation-2131", "mrqa_newsqa-validation-1789", "mrqa_searchqa-validation-7968", "mrqa_searchqa-validation-10092", "mrqa_newsqa-validation-85"], "SR": 0.453125, "CSR": 0.5966796875, "EFR": 0.9428571428571428, "Overall": 0.7262667410714286}, {"timecode": 16, "before_eval_results": {"predictions": ["427", "Michael Mullett", "impact process effects", "since 2001", "double or triple non-French linguistic origins", "German", "a course of study, lesson plan, or a practical skill", "Super Bowl XXXIII", "a protest that blocked heavy traffic passing over the A13, Brenner Autobahn, en route to Italy.", "unequal", "in whole by charging their students tuition fees", "directly every four years", "Gatsby", "the Swallows", "crawdads", "Lewis and Clark", "a cross", "The Age of Innocence", "Logan International Airport", "a thirst", "parabola", "Canada", "an asylum", "high school", "Liza Minnelli", "yod-dalet", "airplanes", "Detroit", "bay leaf", "John", "\"Madama Butterfly\"", "a home", "carbon dioxide", "San Diego", "the Venus landing", "Saturn", "nickel", "a lichen", "Lake Baikal", "Brazil", "a railroad", "Laos", "Chang Apana", "the Jeckle", "Erma Bombeck", "Clinton", "Beyonc", "King Herod", "Iran", "a serve", "Touch of Evil", "blonde", "Nightingale", "the Golden Age of Murder", "Daryl Sabara", "holiday", "William Shakespeare", "Nick Berry", "split 7\"", "North Dakota", "Turkey", "sumo wrestling", "more than 26,000", "Fat Man"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4995679302422723}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5263157894736842, 1.0, 0.4, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4267", "mrqa_squad-validation-4065", "mrqa_squad-validation-1844", "mrqa_squad-validation-4332", "mrqa_squad-validation-6983", "mrqa_squad-validation-964", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-3445", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-16782", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-3214", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-9973", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-15522", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-15665", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-1397", "mrqa_searchqa-validation-12143", "mrqa_searchqa-validation-519", "mrqa_naturalquestions-validation-94", "mrqa_hotpotqa-validation-1701", "mrqa_newsqa-validation-1120", "mrqa_hotpotqa-validation-5388"], "SR": 0.421875, "CSR": 0.5863970588235294, "EFR": 1.0, "Overall": 0.7356387867647058}, {"timecode": 17, "before_eval_results": {"predictions": ["forceful taking of property", "Robert Iger", "Central business districts", "occupations necessary to sustain the community as distinct from the indigenous population", "prime number theorem", "nonphotosynthetic eukaryote engulfed a chloroplast-containing alga", "Santa Clara", "Trevathan", "General Hospital", "being drafted into the Austro-Hungarian Army in Smiljan", "friction", "a statue", "Titanic", "Queen Anne", "Heroes", "Lord Hamlet", "a dog eat dog world", "Sir Anthony Eden", "Judgment City", "quaerere", "Jalisco", "Santa Fe", "San Diego Comic-Con", "Muddy Waters", "the First Telegraphic Message", "a candidate or ballot measure for which at... his or her intention to do so, unless the candidate is seeking a nonpartisan office.", "Manfred von Richthofen", "cowboys", "Michael Collins", "John J. Pershing", "a balloon", "San Antonio", "the Jesuit order", "Javier", "(AAT 49)", "an anatomical animation", "New York City", "Hampshire College", "less than 60 beats per minute", "a bachelor pad", "a coal", "diastole", "rabbit", "livestock", "Jack London", "Vladimir Putin", "Hillary Clinton", "Nikola Tesla", "gingerbread", "Elza", "a transporter", "the Hubble Space Telescope", "David Tyree", "statistical estimation or statistical inference", "The federal government", "the Greater Antilles", "Doncaster Rovers", "the City of Onkaparinga", "A simple iron boar crest", "Monday night", "(Sarkozy)", "needle - like teeth", "3", "relieve families who had difficulty finding jobs during the Great Depression in the United States"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5321822853072853}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.962962962962963]}}, "before_error_ids": ["mrqa_squad-validation-3106", "mrqa_squad-validation-8638", "mrqa_squad-validation-1232", "mrqa_squad-validation-10287", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-7483", "mrqa_searchqa-validation-10653", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-8777", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-9231", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-222", "mrqa_newsqa-validation-295", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-9856"], "SR": 0.390625, "CSR": 0.5755208333333333, "EFR": 1.0, "Overall": 0.7334635416666666}, {"timecode": 18, "before_eval_results": {"predictions": ["308", "Trajan's Column", "the solution", "the Danube", "scrutinise legislation", "Ralph Woodward", "the major cities", "19th Century", "high density", "the Ten Commandments", "amyotrophic lateral sclerosis (ALS)", "Jefferson Memorial", "Magic Johnson", "somewhere between 7,500 and 40,000", "The Soloist", "2012", "ethic group", "feats of exploration", "Abdul Razzak Yaqoob", "Norwegian", "1868", "Churchill", "Hopeless Records", "Saturday Night Live", "the National Basketball Association", "Ang Lee", "Lake County, Illinois", "1971", "former Chelsea and Middlesbrough striker Jimmy Floyd Hasselbaink", "Arkansas", "2009", "The Ministry of Utmost Happiness", "Samurai Uzumaki", "Appalachians", "March 30, 2025", "Miller Brewing", "Mike Greenwell", "Daimler-Benz", "1994", "John of Gaunt", "classical", "1,521", "capitol building", "one", "role-playing game", "Leofric", "Australian Electoral Division", "Argentinian", "the International Hotel", "American black bear", "Mot\u00f6rhead", "Richa Sharma", "An impresario", "Lake Michigan", "Tulsa", "local people", "Jessica Smith", "Hungary", "five days a week", "Maryland", "all correct", "the Civil War", "The Case-Book of Sherlock Holmes", "kidnapping"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6037833694083694}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.18181818181818182, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9479", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-525", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-4297", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-2241", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-4001", "mrqa_newsqa-validation-4077", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-10375"], "SR": 0.515625, "CSR": 0.5723684210526316, "EFR": 1.0, "Overall": 0.7328330592105263}, {"timecode": 19, "before_eval_results": {"predictions": ["toward the end of his life", "motivated students", "native tribes", "Cuba", "Dai \u00d6n Ulus", "Parliamentary time", "Private Bill Committees", "photolysis of ozone", "Eliot Ness", "Joseph Stalin", "Bobby Eli", "the Eurasian Plate", "Anglican", "10.5 %", "living - donor", "eastern coast of Australia", "silk floss", "Tom Waits", "Cherbourg in France and Queenstown ( now Cobh ) in Ireland", "Olympic - class ocean liners", "T.J. Miller", "full '' sexual intercourse", "1995", "Paradise, Nevada", "Mike Nesmith", "The Chinese Bunkhouse at the Steveston Shipyards in Richmond, BC", "Office of Inspector General", "Skat", "1878", "Rockwell", "Scott Bakula as Dwayne `` King '' Cassius Pride", "October 2008", "Loving Father", "the theory of T\u0101\u1e47\u1e0dava dance ( Shiva )", "Heather Stebbins", "the Portuguese version of this surname is Tavares", "supervillains who pose catastrophic challenges to the world", "1999", "Geraldine Margaret Agnew - Somerville", "Joe Lawrence", "October 2, 2017", "Stephen Curry of Davidson", "December 15, 2016", "a star", "armored fighting vehicle", "the Overlook Hotel", "Marty Robbins", "Albert Einstein", "early 2017", "Ed Sheeran", "state ownership of the means of production", "HTTP / 1.1", "A substitute", "DCI John Barnaby", "Hawaii", "Hopeless Records", "death sentence", "Seasons of My Heart", "wife Linda.", "To Build a Fire", "Humorous poetry with irregular rhythm", "MBA", "Vancouver", "Hudson"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5259701797385621}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.2, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.11764705882352941, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-6584", "mrqa_triviaqa-validation-4187", "mrqa_hotpotqa-validation-4897", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-566"], "SR": 0.421875, "CSR": 0.56484375, "EFR": 1.0, "Overall": 0.731328125}, {"timecode": 20, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-2139", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2467", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4387", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10135", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2025", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-3628", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1789", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11788", "mrqa_searchqa-validation-12143", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-1407", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-15689", "mrqa_searchqa-validation-15696", "mrqa_searchqa-validation-1576", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-16645", "mrqa_searchqa-validation-16723", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3321", "mrqa_searchqa-validation-3450", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6061", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-6844", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-6969", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-8029", "mrqa_searchqa-validation-8063", "mrqa_searchqa-validation-8332", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-8829", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_squad-validation-10052", "mrqa_squad-validation-10107", "mrqa_squad-validation-10125", "mrqa_squad-validation-10149", "mrqa_squad-validation-10186", "mrqa_squad-validation-1027", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10333", "mrqa_squad-validation-10341", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10445", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-10493", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-116", "mrqa_squad-validation-1193", "mrqa_squad-validation-1257", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-147", "mrqa_squad-validation-1503", "mrqa_squad-validation-1527", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1684", "mrqa_squad-validation-1754", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2092", "mrqa_squad-validation-2166", "mrqa_squad-validation-2288", "mrqa_squad-validation-2302", "mrqa_squad-validation-232", "mrqa_squad-validation-2322", "mrqa_squad-validation-2324", "mrqa_squad-validation-2344", "mrqa_squad-validation-2406", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2559", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2625", "mrqa_squad-validation-2737", "mrqa_squad-validation-2778", "mrqa_squad-validation-2827", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-32", "mrqa_squad-validation-3217", "mrqa_squad-validation-3233", "mrqa_squad-validation-3241", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3506", "mrqa_squad-validation-3617", "mrqa_squad-validation-362", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3848", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3913", "mrqa_squad-validation-3916", "mrqa_squad-validation-3923", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-401", "mrqa_squad-validation-4086", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4248", "mrqa_squad-validation-4287", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-4697", "mrqa_squad-validation-4836", "mrqa_squad-validation-4974", "mrqa_squad-validation-5012", "mrqa_squad-validation-5088", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5338", "mrqa_squad-validation-5379", "mrqa_squad-validation-5451", "mrqa_squad-validation-5521", "mrqa_squad-validation-5526", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-585", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5907", "mrqa_squad-validation-5950", "mrqa_squad-validation-5964", "mrqa_squad-validation-597", "mrqa_squad-validation-6001", "mrqa_squad-validation-6012", "mrqa_squad-validation-6069", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6250", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6389", "mrqa_squad-validation-6409", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6605", "mrqa_squad-validation-6671", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6843", "mrqa_squad-validation-6846", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-6935", "mrqa_squad-validation-6981", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7476", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-78", "mrqa_squad-validation-7839", "mrqa_squad-validation-7861", "mrqa_squad-validation-789", "mrqa_squad-validation-7897", "mrqa_squad-validation-7901", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-800", "mrqa_squad-validation-805", "mrqa_squad-validation-8084", "mrqa_squad-validation-8159", "mrqa_squad-validation-8193", "mrqa_squad-validation-8197", "mrqa_squad-validation-8307", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-845", "mrqa_squad-validation-852", "mrqa_squad-validation-8580", "mrqa_squad-validation-8696", "mrqa_squad-validation-8771", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8798", "mrqa_squad-validation-8861", "mrqa_squad-validation-8897", "mrqa_squad-validation-8915", "mrqa_squad-validation-8935", "mrqa_squad-validation-8953", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9087", "mrqa_squad-validation-9098", "mrqa_squad-validation-9141", "mrqa_squad-validation-9254", "mrqa_squad-validation-9270", "mrqa_squad-validation-929", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9457", "mrqa_squad-validation-9479", "mrqa_squad-validation-9510", "mrqa_squad-validation-9523", "mrqa_squad-validation-9630", "mrqa_squad-validation-964", "mrqa_squad-validation-9718", "mrqa_squad-validation-9766", "mrqa_squad-validation-9768", "mrqa_squad-validation-985", "mrqa_squad-validation-9968", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1751", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2106", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-93"], "OKR": 0.857421875, "KG": 0.4421875, "before_eval_results": {"predictions": ["\" Behind the Sofa\"", "Jean Fran\u00e7ois de Troy, Jean-Baptiste Pater", "the Privy Council", "During the Second World War", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "assembly center", "Leonardo da Vinci", "end of 1350", "two Nobel Peace Prizes", "as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "June 12, 2017", "Prussian", "Gatwick Airport", "27 November 1956", "Dan Conner", "\"The Ring\" series", "Billy Joel", "The Times Higher Education Guide", "The conversation", "Al D'Amato", "veto power", "Lush Ltd.", "Port Macquarie", "non-alcoholic recipe", "Big & Rich", "Brea, California", "World War II", "Brazilian Jiu-Jitsu", "Biola University", "Columbia Pictures", "February 22, 1968", "Erreway", "1997", "Kim Bauer", "the Joint Chiefs of Staff", "The Postal Service", "1955", "1993", "near Philip Billard Municipal Airport", "Sam Rockwell", "John Boyd Dunlop", "Iran", "The Cherokee\u2013American wars", "a creek", "Edward Trowbridge Collins Sr.", "Province of New York", "Archie Andrews", "January 28, 2016", "Most Valuable Player Award", "Steve Martin", "Israeli Declaration of Independence in 1948", "Sleepy Hollow", "Mumbai", "1996", "Scheria", "The National Council", "Doubting Castle", "Noida", "genocide", "\"Pride and Prejudice\"", "Howard Robard Hughes Jr.", "Dairy Queen", "AC/DC", "the Iberian Peninsula"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7126984126984127}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5489", "mrqa_squad-validation-8189", "mrqa_squad-validation-5061", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2669", "mrqa_hotpotqa-validation-5047", "mrqa_hotpotqa-validation-5727", "mrqa_triviaqa-validation-5418", "mrqa_newsqa-validation-3563", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11847", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-12943"], "SR": 0.640625, "CSR": 0.5684523809523809, "EFR": 1.0, "Overall": 0.7146279761904762}, {"timecode": 21, "before_eval_results": {"predictions": ["Several thousand", "Paris", "Islamic Republic", "different subject specialists each session during the week", "Sonia Shankman Orthogenic School", "Newton's Second Law", "alternating current", "1002", "Guardians of the Galaxy Vol.  2", "January 7, 1964", "arts manager", "software programmer", "UHF channel 44 (or virtual channel 6 via PSIP)", "the number of men killed and the manner of the attacks", "bassline house", "Ireland", "Valley Falls", "U\u00ed \u00cdmair", "54 Ry\u016bky\u016ban sailors in Qing-era Taiwan", "Dallas", "a leg injury", "Hirsch index rating", "neo-Nazi", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "2008", "Michael Crawford", "Hugh de Kevelioc, 5th Earl of Chester", "Nicholas \" Nick\" Offerman", "American", "the Dominican Republic", "29,000", "Bullets power forward/center Wes Unseld", "Mr. Church", "1990", "Ronald Wilson Reagan", "Raymond Albert Romano", "\"The Flying Doctors\"", "Kennedy Road", "Nazareth", "tenor Peter Yarrow", "\"From Here to Eternity\"", "Kentucky RiverBats", "\"The Royal Family\"", "Backstreet Boys", "Lynn Minmei", "Sir Matthew Arundell", "Derek Jacobi", "August 14, 1848", "Catwoman", "The Hindu Group", "Detroit Lions and the Los Angeles Rams", "Audi", "September 3, 1858", "Phillip Schofield and Christine Bleakley", "USS Chesapeake", "Jupiter", "Seal", "CNN", "Somali", "\"Walking on Sunshine\"", "Joe Jackson", "Michael Arrington", "Krishna Rajaram", "at least 12 months"], "metric_results": {"EM": 0.46875, "QA-F1": 0.56640625}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.2, 0.8, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1894", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-4843", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-4792", "mrqa_naturalquestions-validation-1786", "mrqa_triviaqa-validation-2997", "mrqa_newsqa-validation-3178", "mrqa_searchqa-validation-6538", "mrqa_searchqa-validation-939", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-274"], "SR": 0.46875, "CSR": 0.5639204545454546, "EFR": 1.0, "Overall": 0.7137215909090909}, {"timecode": 22, "before_eval_results": {"predictions": ["1,320 kilometres (820 miles)", "anti-colonial movements", "The Middle and Modern Family", "An increase in imported cars", "immunomodulators", "inequality", "Duval County", "Univision", "Pieter van Musschenbroek", "\"Beauty and the Beast\"", "every aspect of public and private life", "Missouri Tigers", "Acela Express", "German", "Cartoon Network", "Aamir Khan", "trans-Pacific", "Kristina Ceyton and Kristian Moliere", "Ginger Rogers", "Dan Castellaneta", "from 1995 to 2012", "Saint Petersburg Conservatory", "Columbine", "one", "Colonel", "Ars Nova Theater", "Donna Paige Helmintoller", "Detroit, Michigan", "CBS News", "1838", "Assistant Director Neil J. Welch", "near Philip Billard Municipal Airport", "ZZ Top", "2", "The Handmaid's Tale", "Rick and Morty", "second largest", "Bamyan Province", "Melbourne", "before 1638", "247,597", "jurisdiction", "Yoruba", "Madonna Louise Ciccone", "Telugu", "1800000 sqft", "Malayalam", "New York State Route 908M", "Man Booker Prize", "child actor", "October 21, 2016", "Taeko Ikeda", "Kate", "abbreviation", "Thomas Chisholm", "Brazil", "Superman", "Wales", "since 1983", "three", "threatening messages", "New Zealand", "\"Hey Ya!\"", "SS Mayaguez"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7732886904761904}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.4, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3366", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-2446", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-1384", "mrqa_naturalquestions-validation-8493", "mrqa_naturalquestions-validation-4308", "mrqa_triviaqa-validation-5792", "mrqa_newsqa-validation-377", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-6136"], "SR": 0.703125, "CSR": 0.5699728260869565, "EFR": 1.0, "Overall": 0.7149320652173914}, {"timecode": 23, "before_eval_results": {"predictions": ["28.5\u00b0E", "the Saracens", "seven", "BBC Dead Ringers series", "Mnemiopsis", "Dr. George E. Mueller", "salmon", "Gerald Gerald", "heating", "Gerald Robert Newhart", "Constellations", "chile chile", "chile chile", "Gerald Dostoyevsky", "The Plaza Hotel", "Stephen Hawking", "Rodeo", "Oahu", "a sustained pull", "Gerald Coleman", "Gerald Nicolai", "M1 Abrams", "Earth", "an egg in front of the opening", "Gerald IV", "Gerald Gerald", "Gerald Schwarzenegger", "Gerald E. Neugebauer", "College of William and Mary", "Who's Afraid of Virginia Woolf", "aluminum", "jamais", "Barnard College", "Caracalla", "Gerald Goodstein", "B. Bilbo Baggins", "chicken", "a stone, 1000, May 12, 2008.", "George W. Bush", "chile chile", "James Cook", "Bosom Buddies", "Gerald Cantor Foundation", "Gerald Retriever", "Walter Crawford Kelly, Jr.", "Edith Wharton", "Rapa Nui National Park", "Nike+ iPod Sport Kit", "gravity", "an equatorial diameter", "chile", "Gerald Asscherick", "Bay of Montevideo", "The U.S. state of Georgia", "1820s", "Gerald ibn Ezra", "QWERTY", "Gerald", "Las Vegas Boulevard", "Slaughterhouse-Five", "Food and Agriculture Organization", "further reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "economic growth and creating opportunity for our people.", "2050"], "metric_results": {"EM": 0.375, "QA-F1": 0.47700892857142857}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-7993", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-4758", "mrqa_searchqa-validation-15407", "mrqa_searchqa-validation-5554", "mrqa_searchqa-validation-12149", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-13231", "mrqa_searchqa-validation-9639", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-14893", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-2225", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-596", "mrqa_searchqa-validation-14432", "mrqa_searchqa-validation-7080", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-4374", "mrqa_searchqa-validation-16777", "mrqa_searchqa-validation-7572", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-15964", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-3087", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-1639"], "SR": 0.375, "CSR": 0.5618489583333333, "EFR": 0.975, "Overall": 0.7083072916666666}, {"timecode": 24, "before_eval_results": {"predictions": ["ca. 2 million", "Catholic", "The waxy cuticle of many leaves", "theta intermediary form", "subsequent long-run economic growth", "Military Commissions Act", "two-state solution", "WFTV", "\"an Afghan patriot\"", "Juliet", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "eight", "World Wide Village", "Rima Fakih", "Indian Ocean", "at least 25 dead", "18", "Sunday", "a rally at the State House next week", "Herman Cain", "humans", "Friday", "Saluhallen", "murder", "the Swat Valley", "two years", "228", "Keating Holland", "be open \"by the end of the year.\"", "Steve Williams", "Apple", "Nazi Germany", "\"It's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "30 years ago.", "five", "in July", "\"Oprah: A Biography,\"", "100 percent", "whites", "Abu Sayyaf", "The Palm", "\"I don't think it'll get to the court.\"", "Egypt", "Jeddah, Saudi Arabia", "poor families", "Russia", "2011", "use of torture and indefinite detention", "skull", "Iran", "a review of state government practices", "stripper pole photos", "Michael Schumacher", "The chief executive of West Virginia", "Massachusetts", "d", "natural world and mysticism", "The Lion King", "7pm", "1966", "Wendell Erdman Berry", "Twelfth Night", "Shabbat", "Brazilian national flag"], "metric_results": {"EM": 0.453125, "QA-F1": 0.543843814699793}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.08695652173913043, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.7142857142857143, 0.28571428571428575, 1.0, 0.6666666666666665, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6435", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2201", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-1419", "mrqa_naturalquestions-validation-4207", "mrqa_triviaqa-validation-802", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-173", "mrqa_searchqa-validation-2076", "mrqa_searchqa-validation-14268"], "SR": 0.453125, "CSR": 0.5575, "EFR": 0.9428571428571428, "Overall": 0.7010089285714286}, {"timecode": 25, "before_eval_results": {"predictions": ["primes", "UNESCO's World Heritage list", "lower incomes", "Wankel engine", "five or more seats in the Parliament", "the Bronx", "Peshawar", "backbreaking labor", "Oregon", "Democrats and Republicans are saying Meehan shouldn't be using a 9/11 image to make a political point.", "Fullerton, California,", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "The minister later apologized, telling CNN his comments had been taken out of context.", "Carl Froch", "10 below", "The mother whose daughter and granddaughter attend Oprah Winfrey's school in South Africa considers the talk-show host heaven-sent,", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "had settled in to a job he liked at the U.S. Holocaust Memorial Museum,", "The pilot, whose name has not yet been released,", "The International Red Cross Committee, the U.N. High Commissioner for Refugees and UNICEF", "The office of Muqtada al-Sadr accused Iraqi and U.S. forces of attacking Sadr City", "air support", "Marie-Therese Walter.", "neither Sudanese nor orphans,", "it should stay that way.", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "Swansea", "U.S. Court of Appeals for the District of Columbia.", "Michael Jackson", "Kurdistan Freedom Falcons,", "insurgent small arms fire", "schools", "a lump in Henry's nether regions", "the WGC-CA Championship", "glamour and hedonism", "I am sick of life", "a peace sign.", "the Southeast", "Diego Milito", "a grizzly bear", "Human Rights Watch", "Florida", "Christopher Savoie", "rebels", "three Ghanaians, two Liberians and a Togo national", "How I Met Your Mother", "London", "Mafia", "Stratfor", "The BBC", "CBS, CNN, Fox and The Associated Press.", "Ashley \"A.J. Jewell,", "The stratum lucidum", "Gettysburg College", "the Italian / Venetian John Cabot", "Celsius", "75", "supreme religious leader", "Charles Quinton Murphy", "Rio Gavin Ferdinand", "more than 110 films", "Athens", "Craig", "artesian"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5393106020817419}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.33333333333333337, 0.23529411764705882, 0.5, 0.0, 1.0, 0.4, 0.2666666666666667, 0.0, 0.0, 0.09090909090909091, 0.2666666666666667, 0.0, 0.2222222222222222, 0.1111111111111111, 0.24000000000000002, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.5, 0.0, 1.0, 0.3333333333333333, 0.8, 0.0, 0.0, 0.0, 0.375, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9459", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-561", "mrqa_naturalquestions-validation-8585", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-1390", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-4004", "mrqa_searchqa-validation-1916", "mrqa_searchqa-validation-16961"], "SR": 0.390625, "CSR": 0.5510817307692308, "EFR": 1.0, "Overall": 0.7111538461538462}, {"timecode": 26, "before_eval_results": {"predictions": ["enhanced transit infrastructure, possible shuttles open to the public, and park space which will also be publicly accessible.", "Katy\u0144 Museum", "two", "a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship.", "David Anthony O'Leary", "1822", "Stephanie Plum", "Dominican", "Evgeni Arkadievich Platov", "Knoxville, Tennessee", "Enkare Nairobi", "Field Marshal Stapleton Cotton,", "Shari Shattuck", "Si Da Ming Bu", "The Blue Album", "Apsley George Benet Cherry-Garrard", "A basilica", "Blender", "Odawa", "Columbus Crew SC", "Yoo Seung-ho", "1989 until 1994,", "Phil Spector", "Steve Cuden", "pornographicstar", "Montreal", "Sam the Sham", "John Nicholas Galleher", "San Francisco 49ers", "Prince of Cambodia Norodom Sihanouk", "Durham, North Carolina", "Double Crossed", "19th-century", "Donald McNichol Sutherland", "provides its services in the Japanese market.", "Security Management", "14,372", "musician", "Cersei", "Fort Worth", "Dutch", "\"Holinshed's Chronicles\"", "North Atlantic Treaty Organisation (NATO)", "\"The Comic Strip Presents...\"", "Tabasco", "Sunday, November 2, 2003,", "1853", "1.5 million", "Neymar", "Plymouth Regional High School", "Tainted Love", "Dissection", "celebrity alumna Cecil Lockhart", "MercyMe", "in a thousand years", "a ride cymbal", "Al Jazeera", "Anabaptists and the non-sectarians", "38 feet", "identity documents", "Shenzhen in southern China.", "Brigham Young", "Silly Putty", "Government National Mortgage Association"], "metric_results": {"EM": 0.5, "QA-F1": 0.6328221006144393}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8387096774193548, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.8571428571428571, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6426", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3749", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-111", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-3889", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-2131", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-5740", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-3954", "mrqa_newsqa-validation-875", "mrqa_searchqa-validation-10970"], "SR": 0.5, "CSR": 0.5491898148148149, "EFR": 1.0, "Overall": 0.710775462962963}, {"timecode": 27, "before_eval_results": {"predictions": ["low-light conditions", "1550", "60 days", "$60,000 in cash and stock", "four", "Steve Goodman", "Jonathan Goldstein", "Hans Raffert", "Secretary of Homeland Security", "1996", "Tristan Rogers", "Mase Dinehart", "counter clockwise direction", "Paul Hogan", "A seed is a competitor or team in a sport or other tournament who is given a preliminary ranking for the purposes of the draw", "Matt Monro", "IIII", "Fa Ze Rug", "8ft", "Robert Irsay", "to manage the characteristics of the beer's head", "one of The Canterbury Tales", "2019", "Scott Drever", "Southern Ocean", "The Exploratorium continues to hold Pi Day celebrations", "his guilt in killing the bird", "1", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "December 1, 2017", "James Watson and Francis Crick", "Sara Gilbert", "Tokyo / Helsinki", "the courts", "John Travolta", "Djokovic", "Rigg", "Ant & Dec", "Nepal", "Joanne Wheatley", "UNESCO / ILO", "September 24, 2012", "lowest air temperature record", "summer", "between the Eastern Ghats and the Bay of Bengal", "Daniel Suarez", "1 - 2 spinal nerve segments above the point of entry", "16 August 1975", "useless, time - wasting", "Kyla Pratt", "Lori McKenna", "Beorn", "Douglas MacArthur", "The Daily Mail's", "Scotland", "Forbes", "2006", "Fife", "April 28", "Russia and the United States", "one American diplomat to a \"prostitute\"", "unassisted triple play", "Ben Kingsley", "New Mexico"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6437571543040292}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.8, 1.0, 0.15384615384615385, 1.0, 0.0, 0.4, 0.0, 1.0, 0.7692307692307692, 0.0, 1.0, 0.0, 0.125, 0.0, 0.0, 1.0, 0.09090909090909093, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.4, 0.0, 0.4, 0.9090909090909091, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1313", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7208", "mrqa_triviaqa-validation-6967", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-506", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-2350", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-12174"], "SR": 0.515625, "CSR": 0.5479910714285714, "EFR": 0.9354838709677419, "Overall": 0.6976324884792626}, {"timecode": 28, "before_eval_results": {"predictions": ["Matt Smith", "1206", "Genghis Khan", "on Monday to meet with the Shanghai mayor and hold a town hall-style meeting with \"future Chinese leaders\"", "24", "Monday night where it left off in September with what Sedgwick called \"a fantastic five episodes.\"", "a delegation of American Muslim and Christian leaders", "September 21.", "1,500", "\"solid credentials,\"", "269,000", "Eden Park", "The Ski Train is a 68-year-old local favorite that shuttles about 750 people between Denver and Winter Park.", "a sailboat matching the description of the missing 38-foot boat was found overturned about 5:15 p.m. Saturday,", "the man facing up, with his arms out to the side. He is wearing socks but no shoes.", "to step up attacks against innocent civilians", "dogs who walk on ice in Alaska.", "his father's", "guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels", "Haitians", "President Obama", "militant group declared an \"all-out war\" on the government", "energy-efficient light-emitting diodes", "it is currently home to 15 African and Asian elephants.", "to secure more funds", "short- and medium-range missile tests,", "Karl Eikenberry", "defaulted on the mortgage and the house fell into foreclosure.", "A member of the group dubbed the \"Jena 6\"", "among a growing number of state governments going after them", "Anil Kapoor", "Illlinois.", "by raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond.", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Nazi Germany", "robert Schechter, who has studied the HIV/AIDS epidemic since 1989, has often take the forms of candid public awareness ads with slogans like \"Be good in bed, use a condom,\"", "gun", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "don't believe the U.S. assertion that the system is needed to guard against imminent threats from Iran or North Korea.", "David McKenzie", "citizenship because he was depriving his wife of the liberty to come and go with her face uncovered,", "embankment in the Angeles National Forest", "through a facility in Salt Lake City, Utah,", "that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis,", "remains unknown,", "get out of the game, and I wondered what will they do now?\"", "Alwin Landry's supply vessel Damon Bankston", "along the equator between South America and Africa", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case is recovering well and wants to see the ocean and a pop concert,", "an empty tub, his face blue and purple and a chain around his neck,", "in a tenement in the Mumbai suburb of Chembur, with eight people living together in a single room.", "California, Texas and Florida,", "Rigg", "usually restricted to the lower motor neurons, the efferent nerves that directly innervate muscles", "Mexico", "The Duchess,", "Bahrain", "Narita", "Danny Lebern Glover", "William Shakespeare", "British", "an assault rifle", "rookoe noted that you've studied fine art or taken four", "Turtle Wax"], "metric_results": {"EM": 0.3125, "QA-F1": 0.47602909050687475}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.23529411764705882, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.13333333333333333, 0.0, 0.923076923076923, 0.25, 1.0, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.25, 0.6666666666666666, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.3076923076923077, 0.4615384615384615, 1.0, 0.0975609756097561, 1.0, 0.0909090909090909, 0.08333333333333333, 1.0, 0.1111111111111111, 0.8333333333333333, 1.0, 0.9333333333333333, 1.0, 0.1, 0.5, 0.8750000000000001, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7868", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-301", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-2338", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-7484", "mrqa_triviaqa-validation-1993", "mrqa_hotpotqa-validation-1922", "mrqa_searchqa-validation-9212", "mrqa_searchqa-validation-6288"], "SR": 0.3125, "CSR": 0.5398706896551724, "EFR": 1.0, "Overall": 0.7089116379310345}, {"timecode": 29, "before_eval_results": {"predictions": ["Disney\u2013ABC Domestic Television", "2011", "Soviet", "buprenorphine", "sense of smell", "Brigit Forsyth", "Florence", "Wrigley", "Oprah Winfrey", "2004", "a Great Dane", "Director General of the Security Service", "(Maiquet\u00eda)", "Rock Follies of \u201977", "Do I Hear a Waltz?,", "The Nobel Prize in Literature", "Celtic", "Northwestern University", "the best value diamond for your money", "The Star Spangled Banner", "aperto", "Ibrox Stadium", "New York", "(born 17 September 1929)", "Micael Caine", "Llyn Padarn", "Little Dorrit", "Tacitus", "apples", "Chekhov", "Chris Evans", "John Keats", "Declaration of Independence", "Lome", "a condor", "Belgium", "Pilgrim's Progress", "Plato", "Fulham Football Club", "graphite", "Australia", "Alaska", "God", "the town of Jerez de la Frontera", "the Michelin brothers had a stand,", "Tesco", "\u201cZ.\u201d", "Toms, The Bear and The andy griffith", "Clio Awards", "Pygmalion", "Watford", "Trainspotting", "English", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Vicente Fox", "Central-Eastern Europe", "1919", "August 24, 1983", "ensuring that all prescription drugs on the market are FDA approved,", "International Polo Club Palm Beach in Florida.", "more than 200.", "Sappho", "the White House", "Steely Dan"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6597213955026455}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.4, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.4, 0.9600000000000001, 1.0, 0.0, 1.0, 0.5, 0.07407407407407408, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7651", "mrqa_triviaqa-validation-2203", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-4186", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-566", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-1236", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-1069", "mrqa_triviaqa-validation-6429", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-45", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-3697", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-4547", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-4122"], "SR": 0.5625, "CSR": 0.540625, "EFR": 0.9285714285714286, "Overall": 0.6947767857142858}, {"timecode": 30, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1665", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2030", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2688", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-3249", "mrqa_hotpotqa-validation-3288", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3526", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3655", "mrqa_hotpotqa-validation-3701", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4364", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-5047", "mrqa_hotpotqa-validation-505", "mrqa_hotpotqa-validation-5213", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5645", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-775", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-988", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1852", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4308", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-994", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1925", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2553", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2751", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3750", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-919", "mrqa_searchqa-validation-10110", "mrqa_searchqa-validation-10347", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-10793", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-11289", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-1274", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12964", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-14141", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-14268", "mrqa_searchqa-validation-14735", "mrqa_searchqa-validation-15000", "mrqa_searchqa-validation-15385", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-16393", "mrqa_searchqa-validation-1664", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3677", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-5040", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-600", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-662", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-977", "mrqa_searchqa-validation-9837", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-9949", "mrqa_squad-validation-10052", "mrqa_squad-validation-10125", "mrqa_squad-validation-1028", "mrqa_squad-validation-10289", "mrqa_squad-validation-10300", "mrqa_squad-validation-10308", "mrqa_squad-validation-10344", "mrqa_squad-validation-10366", "mrqa_squad-validation-10374", "mrqa_squad-validation-1042", "mrqa_squad-validation-10455", "mrqa_squad-validation-10460", "mrqa_squad-validation-1052", "mrqa_squad-validation-1113", "mrqa_squad-validation-1134", "mrqa_squad-validation-1159", "mrqa_squad-validation-1193", "mrqa_squad-validation-1309", "mrqa_squad-validation-1356", "mrqa_squad-validation-1368", "mrqa_squad-validation-1503", "mrqa_squad-validation-1541", "mrqa_squad-validation-1595", "mrqa_squad-validation-1600", "mrqa_squad-validation-1627", "mrqa_squad-validation-1681", "mrqa_squad-validation-1769", "mrqa_squad-validation-1959", "mrqa_squad-validation-1967", "mrqa_squad-validation-2071", "mrqa_squad-validation-2074", "mrqa_squad-validation-2166", "mrqa_squad-validation-2324", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2567", "mrqa_squad-validation-2580", "mrqa_squad-validation-2580", "mrqa_squad-validation-2603", "mrqa_squad-validation-2778", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3", "mrqa_squad-validation-3069", "mrqa_squad-validation-3113", "mrqa_squad-validation-3125", "mrqa_squad-validation-3259", "mrqa_squad-validation-3309", "mrqa_squad-validation-331", "mrqa_squad-validation-3319", "mrqa_squad-validation-3345", "mrqa_squad-validation-3445", "mrqa_squad-validation-3464", "mrqa_squad-validation-3474", "mrqa_squad-validation-3722", "mrqa_squad-validation-3811", "mrqa_squad-validation-3831", "mrqa_squad-validation-3849", "mrqa_squad-validation-3849", "mrqa_squad-validation-3861", "mrqa_squad-validation-3916", "mrqa_squad-validation-394", "mrqa_squad-validation-3947", "mrqa_squad-validation-4065", "mrqa_squad-validation-4092", "mrqa_squad-validation-4155", "mrqa_squad-validation-4191", "mrqa_squad-validation-4248", "mrqa_squad-validation-4332", "mrqa_squad-validation-4434", "mrqa_squad-validation-4746", "mrqa_squad-validation-4836", "mrqa_squad-validation-5009", "mrqa_squad-validation-5088", "mrqa_squad-validation-5108", "mrqa_squad-validation-5113", "mrqa_squad-validation-5128", "mrqa_squad-validation-5180", "mrqa_squad-validation-5221", "mrqa_squad-validation-5303", "mrqa_squad-validation-5521", "mrqa_squad-validation-5781", "mrqa_squad-validation-5828", "mrqa_squad-validation-5840", "mrqa_squad-validation-589", "mrqa_squad-validation-5894", "mrqa_squad-validation-5964", "mrqa_squad-validation-6001", "mrqa_squad-validation-6069", "mrqa_squad-validation-6082", "mrqa_squad-validation-6158", "mrqa_squad-validation-6256", "mrqa_squad-validation-6356", "mrqa_squad-validation-6375", "mrqa_squad-validation-6453", "mrqa_squad-validation-6455", "mrqa_squad-validation-6501", "mrqa_squad-validation-6592", "mrqa_squad-validation-6605", "mrqa_squad-validation-6707", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-6898", "mrqa_squad-validation-69", "mrqa_squad-validation-690", "mrqa_squad-validation-6925", "mrqa_squad-validation-7009", "mrqa_squad-validation-7034", "mrqa_squad-validation-709", "mrqa_squad-validation-7092", "mrqa_squad-validation-7193", "mrqa_squad-validation-7214", "mrqa_squad-validation-7229", "mrqa_squad-validation-7286", "mrqa_squad-validation-7295", "mrqa_squad-validation-7303", "mrqa_squad-validation-7476", "mrqa_squad-validation-7485", "mrqa_squad-validation-7502", "mrqa_squad-validation-7578", "mrqa_squad-validation-7614", "mrqa_squad-validation-768", "mrqa_squad-validation-7704", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-7965", "mrqa_squad-validation-7987", "mrqa_squad-validation-8159", "mrqa_squad-validation-8213", "mrqa_squad-validation-8339", "mrqa_squad-validation-8359", "mrqa_squad-validation-8374", "mrqa_squad-validation-8395", "mrqa_squad-validation-8402", "mrqa_squad-validation-8580", "mrqa_squad-validation-8681", "mrqa_squad-validation-8696", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8915", "mrqa_squad-validation-8920", "mrqa_squad-validation-8935", "mrqa_squad-validation-8967", "mrqa_squad-validation-9012", "mrqa_squad-validation-9061", "mrqa_squad-validation-9141", "mrqa_squad-validation-9270", "mrqa_squad-validation-9300", "mrqa_squad-validation-9363", "mrqa_squad-validation-9399", "mrqa_squad-validation-9421", "mrqa_squad-validation-9430", "mrqa_squad-validation-944", "mrqa_squad-validation-9510", "mrqa_squad-validation-9569", "mrqa_squad-validation-964", "mrqa_squad-validation-9759", "mrqa_squad-validation-9766", "mrqa_squad-validation-9974", "mrqa_triviaqa-validation-1269", "mrqa_triviaqa-validation-1329", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-576", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-855", "mrqa_triviaqa-validation-874", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-996"], "OKR": 0.865234375, "KG": 0.4390625, "before_eval_results": {"predictions": ["1947", "Madame de Pompadour", "$125 per month", "doubting castle", "Newbury", "Ruth Elizabeth", "pennor horn", "doubting castle", "Lisieux", "Astor family", "heddlu Dyfed-Powys", "canned Heat", "John Huston", "c.I.D.", "Christopher Lee", "cabot", "the Advisory Council of Science and Industry", "Patrick Kielty", "Norfolk Island", "d'Ivoire", "Mexico", "Worcester Cathedral", "\"The French Connection\"", "Albert Finney", "carrefour", "Ken Russell", "the garden of Gethsemane", "John Galliano", "Swallow Sidecar Company", "mitzi Gaynor", "mickey Mouse", "sun protection factor", "Plato", "Bugsy Malone", "1812", "\"Hoagy\" Carmichael", "basil", "llanet", "cfs", "lyonesse", "raclette", "AllStars", "pennsylvania state", "Copenhagen", "hokkaido", "mambo jambo", "George Walker Bush", "Nicky Henderson", "comedian and writer David Mitchell", "Zachary Taylor", "\"Old Ironsides\"", "furrow", "Total Drama World Tour", "General George Washington", "Real Madrid", "two", "Balvenie Castle", "January 18, 1977", "threatening messages", "London", "\"The Real Housewives of Atlanta\" reunion special,", "llanises", "Communist Party", "doubting castle"], "metric_results": {"EM": 0.484375, "QA-F1": 0.569705815018315}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.5714285714285715, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5491", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2299", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5311", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-6367", "mrqa_triviaqa-validation-6074", "mrqa_triviaqa-validation-3052", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4499", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3285", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-1468", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-3517", "mrqa_hotpotqa-validation-1351", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-85", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-3674"], "SR": 0.484375, "CSR": 0.5388104838709677, "EFR": 1.0, "Overall": 0.7080745967741936}, {"timecode": 31, "before_eval_results": {"predictions": ["nine", "Lower Rhine", "Pet Shop Boys", "rugby", "mexologist", "Budapest", "michael barysh", "dame", "joan crawford", "I'm Sorry,", "Jean-Paul Gaultier", "javelin throw", "silvery", "heavy birds", "French", "finland", "buttock", "isambard", "Turandot", "Alan B'Stard", "Pete Ham", "dora maar", "apples", "440 hertz", "the Spanish", "finster", "durness", "mexico", "Sweden", "Russia", "depression", "chile", "barrister", "mexico", "finland", "lance", "Paul Merton", "Anna", "Paris", "boisterous", "finado Tuerto, Argentina", "Charlie Chaplin", "The Perfect Storm", "middies", "pottery", "mexico", "newbury", "Abraham", "vice-admiral", "1936", "eulb Yvi", "joan crawford", "Rachel Kelly Tucker", "the BBC", "in pilgrimages to Jerusalem", "a co-op of grape growers", "Tim Allen", "Anatoly Vasilyevich Lunacharsky", "an empty water bottle", "Japanese officials", "The Rev. Alberto Cutie", "united Asian", "joan crawford", "bd al-Wahhab"], "metric_results": {"EM": 0.34375, "QA-F1": 0.3770032051282051}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.46153846153846156, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-4054", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-4765", "mrqa_triviaqa-validation-657", "mrqa_triviaqa-validation-5237", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-1633", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-1261", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-5259", "mrqa_triviaqa-validation-7645", "mrqa_triviaqa-validation-5549", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-6730", "mrqa_hotpotqa-validation-3921", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-12801", "mrqa_searchqa-validation-2441"], "SR": 0.34375, "CSR": 0.53271484375, "EFR": 0.9523809523809523, "Overall": 0.6973316592261904}, {"timecode": 32, "before_eval_results": {"predictions": ["teachers are now selling their lesson plans to other teachers through the web in order to earn supplemental income, most notably on TeacherspayTeachers.com.", "provisional elder/deacon", "fiber", "Prince Henry of Wales", "Mujib", "Bart\u00f3k", "roosevelt", "astronaut", "Salt Lake City", "four", "teacher", "brazil", "Canada", "Peter Nichols", "American Family Publishers", "seven", "Microsoft", "Brigit Forsyth", "Celsius", "Poincar\u00e9 conjecture", "amalthea", "leicestershire", "Boris Johnson", "HMS Conqueror", "Tamar", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "irons", "the inner ear", "Gloucestershire", "Norway", "gymnastics", "ginger Rogers", "Ishmael", "bluebell", "Norway", "Prokofiev", "cyclone", "Dan Brown", "horses", "Newcastle United", "Thank you", "millais", "second year", "William Neil Connor", "ryadh", "charles chaplin", "horse-racing", "\"Araf\"", "femur", "dragon", "peregrines", "1938", "Anirudh Sinha", "Tim Rice", "five", "The Walking Dead", "1979", "1999", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "Apple employees", "suicide", "the Battle of Marston Moor", "chess", "Athol Fugard"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6489949136008919}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true], "QA-F1": [0.08695652173913045, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2238", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-2608", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-1838", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-7671", "mrqa_triviaqa-validation-2258", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-6726", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-5666", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-6815", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-10161", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-1889", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-6553"], "SR": 0.609375, "CSR": 0.5350378787878788, "EFR": 1.0, "Overall": 0.7073200757575757}, {"timecode": 33, "before_eval_results": {"predictions": ["1,100", "since 2001", "horseshoe", "(Barry) Briggs", "table tennis", "archibald haddock", "b\u00e9la Bart\u00f3k", "19-9", "Harold Shipman", "The Undertones", "michael hordern", "Richmondshire", "yeast", "michael Anthony Holding", "cenozoic", "Thank you", "nipples", "Queen Mary", "lola", "muscle tissue", "Surrealism", "Shinto", "sewing machines", "Morgan Spurlock", "John Buchan", "algae", "workington", "Andrea Grimes", "stenographer", "Altamont Speedway", "fourteen", "jack Sprat", "Battle of Dunkirk", "Tony Washington, Willie Woods and Victor Thomas", "marc", "Praseodymium", "tla\u010denica or \u0161vargla", "50p", "al rosmarino (focaccia with rosemary)", "chairman of the Federal Reserve", "devil andrew", "Rita Hayworth", "the Observer", "Sir Isaac Newton", "Turnbull & Asser, Hawes & Curtis, Thomas Pink, Harvie & Hudson, Charles Tyrwhitt and T. M. Lewin", "bullfighting", "Arthur C. Clarke", "Marc Warren", "entropy", "Chad", "Earring", "Taggart", "on the microscope's stage", "( 3 gold, 5 silver, 1 bronze )", "Norway", "Virgin", "Philadelphia Naval Shipyard", "Afro-Caribbean", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "Stewart Miller", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "a hook and ladder truck", "Hill Street Blues", "white"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5607665121336997}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5384615384615384, 0.0, 0.0625, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-1410", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-7142", "mrqa_triviaqa-validation-368", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-1431", "mrqa_triviaqa-validation-7043", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-3060", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-1369", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1538", "mrqa_hotpotqa-validation-1813", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-1141", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-11071"], "SR": 0.46875, "CSR": 0.5330882352941176, "EFR": 0.9705882352941176, "Overall": 0.7010477941176471}, {"timecode": 34, "before_eval_results": {"predictions": ["second Gleichschaltung", "two of Tesla's uncles", "Nissan", "kinks", "Massachusetts", "lyonesse", "six", "peter david hewson", "an \"ink sac\"", "Toy Story", "house sparrow", "devolution", "sheep", "Independence Day", "Charlie Brooker", "South Africa", "phobia", "Glasgow", "thomas peterrose", "Florence", "Wat Tyler", "Tony Meo", "black", "vomiting", "Ennio Morricone", "NBA", "enid blyton", "Benjamin Franklin", "horseradish", "chile", "1066", "thomas", "$1", "queen Margaret College", "daniel fossey", "port", "checkers", "Norman Mailer", "an action figure", "jura", "edward shannon", "Chatsworth House", "pongo", "quant pole", "Scooby Doo", "Pennine Way", "margo eddington", "lorne Greene", "thomas Jefferson", "frans hals", "charles", "edward thomas", "the University of Oxford", "Matt Monro", "Greensleeves", "Belarus", "\"Big Fucking German\"", "Welterweight", "Harrison Ford", "The EU naval force", "Miami Beach, Florida,", "2,000,000 years", "39 Steps", "dynamo"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6060049019607843}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4416", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-4366", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-2232", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-1202", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6896", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-4210", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-587", "mrqa_hotpotqa-validation-1891", "mrqa_searchqa-validation-14545", "mrqa_searchqa-validation-2191"], "SR": 0.546875, "CSR": 0.5334821428571428, "EFR": 1.0, "Overall": 0.7070089285714285}, {"timecode": 35, "before_eval_results": {"predictions": ["Tracy Wolfson", "2013", "Russ Conway", "green", "london", "York", "Austria", "a poster", "john Mellencamp", "Sir Humphry Davy", "Saint Aidan", "an order of about 600 men", "beans", "annie leibovitz", "Pinot Noir", "dennis pipino", "london", "duke of York", "Dick Whittington", "peacock", "Pisces", "ishmael", "smell", "Brad Pitt", "Eleanor Rigby", "The Simpsons", "One Direction", "yellows", "Cornell University", "michael kray", "vinegar Joe", "london", "Follicle-stimulating hormone", "paramita", "ichak", "ned arthur", "Saturday", "china", "racecar", "Costa Concordia", "the Dominican Republic", "Sarah Keays", "chile", "true or false", "horseshoes", "George III", "w", "oil capital of Europe", "orange", "a crow", "Yassir Arafat", "Black Sea", "Jerry Houser", "49 cents", "1", "Juan Francisco Antonio Hilari\u00f3n Zea D\u00edaz", "Wings of Desire", "actor and former fashion model", "Airbus A330-200", "an account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "they can demonstrate they have been satisfactorily treated for at least 12 months.", "Roland Garros", "rhinoplasty", "market capitalism"], "metric_results": {"EM": 0.453125, "QA-F1": 0.525}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-584", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-168", "mrqa_triviaqa-validation-1757", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3593", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-5010", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-121", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-5128", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-276", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11091"], "SR": 0.453125, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.7065625}, {"timecode": 36, "before_eval_results": {"predictions": ["independent schools", "cymbal", "Djibouti and Yemen", "river valley", "Barcelona", "Salma Hayek", "llandudno", "a beggar woman", "glaciers", "Delaware", "rodents", "crow", "The Flintstones", "fire", "gelatine", "Ecuador", "Cyprus", "an orphan", "homeless", "Dublin", "london", "walker", "doesn't include additional costs such as insurance or business rates", "Google", "lulu", "Pembrokeshire Coast National Park", "Tripoli", "Jack Johnson", "Dreamgirls", "Opus Dei", "Belize", "Civil Law", "The Press Gang", "london", "Zephyr", "a goat", "Dubai", "Sydney", "orange", "Lehman Bros International", "they reconciled", "Ordovices", "Robert Devereux,", "Dealings with the Firm of Dombey and Son", "mexico", "pascal", "John Galsworthy", "Boris Becker", "Dr. No", "Amsterdam", "Peter Ustinov", "24", "Ford", "between 27 July and 7 August 2021", "94", "Kang and Kodos", "La Liga", "three different videos", "five minutes before commandos descended from ropes that dangled from helicopters,", "St. Louis, Missouri,", "milk", "1st", "Goodson", "IJsselmeer"], "metric_results": {"EM": 0.484375, "QA-F1": 0.544140625}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.625, 0.8, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-275", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-7626", "mrqa_triviaqa-validation-6236", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7186", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-7594", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-562", "mrqa_naturalquestions-validation-5647", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-621", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-12212"], "SR": 0.484375, "CSR": 0.5299831081081081, "EFR": 1.0, "Overall": 0.7063091216216215}, {"timecode": 37, "before_eval_results": {"predictions": ["uncivilized", "jerry zaks", "8", "m\u00e1laga", "julius hefner", "Tiananmen Square", "noises off", "julius humphy", "Till Death Us Do Part", "javier Bardem", "1720", "australia", "endometriosis", "The Hague", "red", "circular line", "julay vtoroy", "mary jimison", "g\u00e9rard Depardieu", "jimily Carter", "lily", "herpes zoster", "humble pie", "zoom", "Angela dothea Kasner", "mary cuthbert", "long-term exposure", "davy humphrey asquith,", "spectator", "2", "Aslan", "cushite", "Vancouver Island", "purdy", "juliet", "six", "double-hung", "mary Trepanier", "central Stockholm", "salford", "beta", "julius hew", "Basil Fawlty", "blue", "mary west", "zephryos", "humbling loss,", "violins and cellos", "Lady Gaga", "chardonnay", "colleen McCullough", "retinal ganglion cell axons and glial cells", "2026", "davy maryn shayn Solberg", "50th anniversary of the founding of the National Basketball Association", "The Cherokee Nation", "domileia t\u014dn Rh\u014dmai\u014dn", "45th anniversary.", "emily, Ava, Emily, Madison, Abigail, Chloe and Mia.", "the man facing up, with his arms out to the side.", "muskrat", "Santo Versace", "trisha yearwood", "B.J. Thomas"], "metric_results": {"EM": 0.34375, "QA-F1": 0.43589409722222217}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.375, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-7331", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4963", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-5616", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-5100", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-4129", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-856", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-4819", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-3899", "mrqa_triviaqa-validation-6813", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-6638", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-1958", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-1855", "mrqa_searchqa-validation-12111", "mrqa_searchqa-validation-15107"], "SR": 0.34375, "CSR": 0.5250822368421053, "EFR": 0.9761904761904762, "Overall": 0.7005670426065163}, {"timecode": 38, "before_eval_results": {"predictions": ["71", "(Cecil) Rhodes", "constant", "theology", "a triangle", "jedoublen/jeopardy", "crescent", "Anne", "root beer", "honey Nut Cheerios", "Venus", "de Berry", "jimmy carter", "secretary of state", "density", "merkel", "Ocean's twelve", "Barack Obama", "stockholm", "jimmy carter", "crescent", "macau", "light speed", "jaffa", "Dan Marino", "boston", "viola", "Grand duchy of Luxembourg", "Alice", "Scotch whisky", "stockholm", "groucho Marx", "deuteronomy", "zounds", "ix", "yellow", "the Crimean War", "an obelisk", "gothic", "concave", "caspian", "barbie doll", "sugar Ray", "de cain", "hypnotic", "South Dakota", "cops", "6", "ned faldo", "Munich Crisis", "Samuel Roxy Rothafel", "border between the Cocos Plate and North American Plate", "Erica Rivera", "Jonas Roberts", "Wuthering Heights", "Craggy Island", "greece", "boston", "Belgian", "Esteban Ocon", "Roberto Micheletti", "greece", "Twelve", "laysan"], "metric_results": {"EM": 0.421875, "QA-F1": 0.46510416666666665}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-3544", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-15961", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-13762", "mrqa_searchqa-validation-16868", "mrqa_searchqa-validation-8389", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-7647", "mrqa_searchqa-validation-10062", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-11967", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-385", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8997", "mrqa_searchqa-validation-5544", "mrqa_searchqa-validation-9662", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-11364", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-6285", "mrqa_hotpotqa-validation-4023", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-857"], "SR": 0.421875, "CSR": 0.5224358974358974, "EFR": 1.0, "Overall": 0.7047996794871795}, {"timecode": 39, "before_eval_results": {"predictions": ["inequality in wealth and income", "Princess Aisha bint Hussein", "Baugur Group", "Westchester County", "City Mazda Stadium", "841", "American", "Lonestar", "Kaep", "Charmed", "Southern Rhodesia", "Alonso L\u00f3pez", "made into a TV series", "Voni Morrison", "Galleria Vittorio Emanuele II", "October Sky", "Annette Ragsdale Camp", "neuro-orthopaedic", "the City of Westminster, London", "British", "Albert", "6,241", "Dan Bilzerian", "STS-51-L.", "an American business magnate, investor, and philanthropist", "Crackle", "Kristy Lee Cook", "Perth", "Love Streams", "1935", "5.3 million", "Upper Manhattan", "Dara Grace Torres", "Lincoln green", "actress", "a fictional world", "May 5, 2015", "Red and Assiniboine Rivers", "Ephedrine", "Neymar", "The Jefferson Memorial", "Strange Interlude", "Bothtec", "2,099", "a body of water", "35,402", "2004 Paris Motor Show", "1996", "33", "1999", "Axl Rose", "5", "John McConnell", "George Harrison", "Bruno Mars", "Wyre", "Azzurri", "Heshmatollah Attarzadeh", "a one-shot victory in the Bob Hope Classic", "700", "The Untouchables", "Monopoly", "a cookie jar", "November 1"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7242931547619048}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-3495", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-2707", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-943", "mrqa_triviaqa-validation-1097", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1230"], "SR": 0.65625, "CSR": 0.52578125, "EFR": 1.0, "Overall": 0.70546875}, {"timecode": 40, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1616", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1938", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3708", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5353", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5696", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3626", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-612", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12704", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15107", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-2178", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-480", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7865", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9870", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10149", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1682", "mrqa_squad-validation-1844", "mrqa_squad-validation-1967", "mrqa_squad-validation-2049", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2549", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2975", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3428", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3815", "mrqa_squad-validation-3836", "mrqa_squad-validation-3837", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4135", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-503", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5209", "mrqa_squad-validation-5338", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-5859", "mrqa_squad-validation-5893", "mrqa_squad-validation-6012", "mrqa_squad-validation-6034", "mrqa_squad-validation-6098", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6435", "mrqa_squad-validation-6506", "mrqa_squad-validation-6671", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7002", "mrqa_squad-validation-7193", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7377", "mrqa_squad-validation-7395", "mrqa_squad-validation-7704", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8084", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-852", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-8935", "mrqa_squad-validation-902", "mrqa_squad-validation-9254", "mrqa_squad-validation-9300", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9479", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9773", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3640", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3874", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4239", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4527", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6152", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-657", "mrqa_triviaqa-validation-6762", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7400", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-996"], "OKR": 0.873046875, "KG": 0.496875, "before_eval_results": {"predictions": ["2006", "Ghana's Asamoah Gyan", "Dublin", "Ribhu Dasgupta", "George Gordon Byron", "The Deer Hunter", "Irish", "sulfur mustard", "12\u201318", "Mike Biden", "Jeffrey William Van Gundy", "People!", "Ballarat Bitter", "Bank of China Building", "(January 11, 1881 \u2013 March 24, 1938)", "about 26,000", "Comeng and Clyde Engineering", "Gillian Leigh Anderson", "Steve Carell", "The Second City", "Lauren Alaina", "Louisiana Tech University", "1943", "Fountains of Wayne", "Saint Paul, Minnesota", "composer of both secular and sacred music", "(91.1 FM)", "October 20, 2017", "Zimbabwe", "Larry Eustachy", "Barbara Niven", "German philosopher Friedrich Nietzsche", "Rabat", "Straits of Gibraltar", "American burlesque", "Russell T Davies", "Jay Schottenstein", "over 600", "(born April 30, 1982),", "Martin \"Marty\" McCann", "Chinese Democracy", "Labour Party", "Orlando\u2013Kissimmee\u2013Sanford,", "Prussia", "Boston", "Bambi, a Life in the Woods", "from 1993 to 1996", "Watertown, New York", "green and yellow", "ice hockey", "an American actress, producer, singer, comic book writer, and political activist.", "3 September", "the Gaget, Gauthier & Co. workshop", "200 to 500 mg up to 7 mg", "Paul Gauguin", "Ynys M\u00f4n", "Saturday Night Live", "anaphylaxis", "General Motors", "outside his house in Najaf's Adala neighborhood", "a Panic's in thy breastie", "the Backstreet Boys", "momentum", "hemoglobin"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5739583333333333}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.5, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.22222222222222224, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-5509", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-507", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2527", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-325", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-4639", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-2482", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-8555", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-5417", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-1606", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-3357"], "SR": 0.453125, "CSR": 0.5240091463414633, "EFR": 1.0, "Overall": 0.7178487042682927}, {"timecode": 41, "before_eval_results": {"predictions": ["by conquering or creating vassal states out of all of modern-day China, Korea, the Caucasus, Central Asia, and substantial portions of modern Eastern Europe, Russia, and Southwest Asia", "the brain", "inundate", "a fisheye lens", "Bears", "Parris Island", "Nova Smoked Salmon", "She was married a staggering eight times to seven men", "She is also one of my favorite 80's actress.", "Mick Jagger", "White blood cells", "Al Capone", "Jinmen", "Stardust", "a book of Operas", "Mickey in Living Color", "Quinn", "New Haven Professional Malpractice Lawyer", "Ursus arctos", "Kareem Abdul-Jabbar", "the Police", "Bravo", "Henry Clay Frick", "Mikhail Gorbachev", "someone has a telephone call.", "New York City", "early-Atlantic Amateur Radio Club", "three-storied pagoda", "Gregory Maguire", "She assisted Jack Dawson in winning over Rose DeWitt Bukater", "Franklin D. Roosevelt", "Pluto", "the Golden Fleece", "Alzheimer's", "Chuck Yeager", "barney stinson", "anvil", "a toilet called the 'Ajax", "Vermont", "She was named World Swimmer of the Year", "a rocket launcher", "one's head", "the Whig Party", "Vietnam", "Ectoplasm", "by the German label Behind s.r.b.b", "Old North Church", "binocular", "I Can't Help Myself", "Legally Blonde", "Scorpio", "Sunni Islam", "Kaley Christine Cuoco", "H ions", "Salix", "Pearl Slaghoople", "The Bible of Proverbs", "Crown Holdings", "New Orleans Saints", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "said during a conference call about the act last October.", "Aldgate East.", "because the Indians don't want to get involved in the armed struggle.", "crossword"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5010416666666666}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [0.06666666666666667, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.8, 1.0, 0.2, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5936", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-13266", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-9985", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-6579", "mrqa_searchqa-validation-9793", "mrqa_searchqa-validation-1329", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-1899", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-12679", "mrqa_searchqa-validation-7833", "mrqa_searchqa-validation-16343", "mrqa_searchqa-validation-591", "mrqa_searchqa-validation-12899", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-1471", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-1035", "mrqa_triviaqa-validation-987"], "SR": 0.421875, "CSR": 0.5215773809523809, "EFR": 1.0, "Overall": 0.7173623511904762}, {"timecode": 42, "before_eval_results": {"predictions": ["Westminster", "Ratatouille", "Ecclesiastes", "Catherine de' Medici", "the Bohemia", "Ecuador", "Microsoft", "Katharine Hepburn", "binocular", "the forest", "London", "Little Boy Blue", "cotton", "Alexander", "Seinfeld", "John Paul Jones", "the sound barrier", "Spider-Man", "Le Morte d'Arthur", "Hudson Bay", "Hamlet", "an axiom", "Patrick\\'s Cathedral", "a cereal", "George III", "David", "Wheaton", "Heart of Darkness", "Rastafari", "Beverly Cleary", "a pizza crust", "Hound Dog", "andorra", "a pillar", "Pisa", "\"Like V for Victory in World War II\"", "Bangkok", "Cuba Gooding", "Russia", "Burt Lancaster", "diagonals", "the Communists", "a sacristy", "Israel", "the Moor", "Alabama", "Making the Band 3", "Martinique", "Sure deodorant", "the Leatherstocking Tales", "the Lion King", "Florida", "Norman", "After tentatively courting each other in `` Entropy ''", "a dragonfly", "\"Rarely is the question asked,", "firethorn", "alt-right", "Australian", "Hawaii House of Representatives", "Negotiators for Zelaya and Roberto Micheletti,", "paid tribute to pop legend Michael Jackson.", "a Daytime Emmy Lifetime Achievement Award.", "Australia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5881696428571428}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714285, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-1830", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-854", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-1179", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-8651", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-314", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-8416", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-8515", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-1452", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-10697", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-616", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-755", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-1351"], "SR": 0.515625, "CSR": 0.5214389534883721, "EFR": 1.0, "Overall": 0.7173346656976745}, {"timecode": 43, "before_eval_results": {"predictions": ["jellyfish", "enforcing racially separated educational facilities", "the Primal rib", "February", "Monk's Caf\u00e9", "Christopher Lloyd", "Morgan Freeman", "1912", "over 74 languages", "John Vincent Calipari", "Palmer Williams Jr.", "2007", "James Madison", "provided majority of members present at that time approved the bill either by voting or voice vote", "1917", "Vikare", "Brad Johnson", "the Red Sea", "1982", "1956", "Harry", "the coffee shop Monk's", "the Atchafalaya River", "Pete Seeger", "low mood", "Autobots", "the closing of the atrioventricular valves and semilunar valves", "Kevin Sumlin", "Bonnie Lipton", "American drama film", "two parallel planes", "gastrocnemius", "all transmissions are in clear text, and usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "Coconut Cove", "most junior enlisted sailor", "March 9, 2018", "937 total weeks", "Johannes Gutenberg", "cadmium", "1961", "2 September 1990", "white oak", "cephalopods", "1939", "March 26, 1973", "Stephen Foster", "Charles Carson", "the Bee Gees", "the Brazilian state of Mato Grosso", "The White House Executive Chef", "3.9 and 5.5 mmol / L ( 70 to 100 mg / dL )", "Mike Danger", "conductor", "a game of bridge.", "Crawley Town", "Skipton Castle", "Andrew Lloyd Webber, Jim Steinman, Nigel Wright", "Elena Kagan", "a treadmill", "Egyptian State TV ran footage Thursday of the assassination of President Mohamed Anwar al-Sadat", "Ernesto \"Che\" Guevara", "pinnipeds", "Heather Locklear", "Centers for Medicare & Medicaid Services"], "metric_results": {"EM": 0.375, "QA-F1": 0.5349620646298993}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.16666666666666669, 0.4, 1.0, 1.0, 1.0, 0.8, 1.0, 0.7499999999999999, 0.0, 1.0, 0.6086956521739131, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.21052631578947367, 0.0, 0.9333333333333333, 0.0, 0.5, 0.0, 0.5, 0.6666666666666666, 0.14814814814814814, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.9090909090909091]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-946", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-4874", "mrqa_newsqa-validation-912", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-8799", "mrqa_hotpotqa-validation-1803"], "SR": 0.375, "CSR": 0.5181107954545454, "EFR": 0.925, "Overall": 0.7016690340909092}, {"timecode": 44, "before_eval_results": {"predictions": ["Confucianism", "April 2011", "James Intveld", "frontal lobe", "Haliaeetus ( sea eagles )", "International Orange", "a low concentration in pigmentation", "Carol Ann Susi", "the septum", "a premalignant flat ( or sessile ) lesion of the colon", "Arnold Schoenberg", "a jazz funeral without a body", "asexually", "mid November", "Deposition", "Dimitar Berbatov and Carlos Tevez", "George Strait", "boiling water reactor ( BWR ) units", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "October 2012", "201", "Rafael Nadal", "The Bangladesh -- India border", "Melissa Disney", "the Hebrew Bible, in the books of Exodus and Deuteronomy", "Gene MacLellan", "the fingers on either side of the mouth ( usually with the knuckles facing the observer )", "all the world's a stage", "water ice", "Michael Schumacher", "on the microscope's stage", "living - donor", "North Atlantic Ocean", "John Hancock", "silk floss tree", "around 100,000", "Triple threat", "Clarence Darrow", "alpha efferent neurons", "in teaching elocution", "the optic disc to the optic chiasma", "Butter Island off North Haven, Maine in the Penobscot Bay", "a combination of genetics and the male hormone dihydrotestosterone", "British Columbia, Canada", "4.5, a fluorapatite - like remineralized veneer is formed over the remaining surface of the enamel", "Halliwell, French, Timomatic and Sandilands", "Frankie Valli", "908 mbar ( hPa ; 26.81 inHg )", "1939", "Pyeongchang County, Gangwon Province, South Korea", "Utah, Arizona, Wyoming, and Oroville, California", "Quito", "Lidice", "Augustus Caesar", "Ars Nova Theater", "French", "1902", "Najaf.", "a monthly allowance,", "(3 degrees Fahrenheit),", "flatware", "a Caesar", "Possession is nine-tenths of the law", "last summer."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6469274648962149}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.25, 1.0, 0.3333333333333333, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.3333333333333333, 0.4615384615384615, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8160", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-9196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-8484", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-2863", "mrqa_newsqa-validation-1076", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-6194"], "SR": 0.53125, "CSR": 0.5184027777777778, "EFR": 0.9333333333333333, "Overall": 0.7033940972222222}, {"timecode": 45, "before_eval_results": {"predictions": ["adaptive immune system", "Andaman and Nicobar Islands", "Jacques Cousteau", "a mid-size four - wheel drive luxury Mercedes -Benz GL - Class", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "a pair of compasses", "Lake Wales, Florida", "Anakin", "Megan Park", "Michael Jeter", "Tulsa, Oklahoma", "Broken Hill and Sydney", "John Goodman", "the right side of the heart", "during the period of rest ( day )", "Oklahoma", "11 January 1923", "Rocky Mountains in southwestern Colorado and northwestern New Mexico", "Ann Gillespie", "the Himalayas", "Master Christopher Jones", "during the united monarchy of Israel and Judah", "Claudia Grace Wells", "Jerry Leiber and Mike Stoller", "1995 Toyota Supra", "Natural - language processing", "Sir Alex Ferguson", "around 1872", "Jane Addams", "2011", "Within two weeks of the second devaluation the dollar was left to float", "Cairo, Illinois", "comic", "Abanindranath Tagore CIE", "Coldplay", "a 0.3 mm diameter rod - free area with very thin, densely packed cones which quickly reduce in number towards the periphery of the retina", "Empiricism", "1,149 feet ( 350 m )", "Lana Del Rey", "Mutt Lange", "a total of six degrees of freedom", "December 1886", "Ludacris", "A costume", "950 pesos ( approximately $ 18 ) in the Philippines or $60 abroad", "Frankie Muniz", "Freddie Highmore", "the somatic nervous system and the autonomic nervous system", "Andy Cole", "late staged at the Edinburgh Festival Fringe in 1966", "Scar's henchmen", "an abnormal visual condition that makes colorless objects appear tinged with color", "Perth", "a bramble fruit", "England", "two", "6,241", "Roy Foster", "share personal information.", "Stephen Tyrone Johns", "\"Austen\"tatious", "a spoon", "Sir Francis Drake", "Charles Swinburne"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6157907196969697}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.8, 0.4444444444444445, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.08333333333333333, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.13333333333333333, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-3855", "mrqa_naturalquestions-validation-3018", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5041", "mrqa_hotpotqa-validation-5438", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-2549", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-6979", "mrqa_searchqa-validation-2528"], "SR": 0.484375, "CSR": 0.5176630434782609, "EFR": 1.0, "Overall": 0.7165794836956522}, {"timecode": 46, "before_eval_results": {"predictions": ["The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316,", "78 %", "an ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "Cliff Richard", "McKim Marriott", "The British Indian Association", "foreign investors", "Redenbacher family", "British and French Canadian fur traders", "committed and effective Sultans", "Jules Shear", "from 13 to 22 June 2012", "Tandi, in Lahaul", "H.L.A. Hart", "Janie Crawford's `` ripening from a vibrant, but voiceless, teenage girl into a woman with her finger on the trigger of her own destiny", "West Norse sailors", "2005", "2012", "in the front of the body", "Kenny Rogers", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "because in Christianity, it is a symbol of the resurrection of Christ, which is celebrated during Eastertide", "it failed to enforce its rule, and its vast territory was divided into several successor polities", "Buffalo Lookout", "Aristotle", "December 1800", "John Donne", "around the time when ARPANET was interlinked with NSFNET in the late 1980s, that the term was used as the name of the network, Internet, being the large and global UDP / IP network", "Cristeta Comerford", "104 colonists and Discovery", "6 - 7 % average GDP growth annually", "Arnold Schoenberg", "Identification of alternative plans / policies", "The Outback", "quartz or feldspar", "Wisconsin", "85 %", "Long Island", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned, and to give his or her advice", "ten", "The NFL owners", "October 19, 2005", "William Shakespeare's play Romeo and Juliet", "the United States Court of Appeals for the Armed Forces", "gathering money from the public, which circumvents traditional avenues of investment", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "2018", "the contestant makes a thirty - second call to one of a number of friends ( who provide their phone numbers in advance ) and reads them the question and answer choices, after which the friend provides input", "the Sharks initially played their home games at the Cow Palace, before they moved to their present home, the SAP Center at San Jose in 1993", "Phil Mickelson", "Indo - Pacific", "Shaft", "Denise van Outen", "West Virginia", "Syracuse", "Girls' Generation", "Manchester, England", "Authorities in Fayetteville, North Carolina,", "three out of four Americans are angry about the way things are going in the country.", "If a security officer were to pull a gun on an armed individual in a mall, it could result in \"the gunfight at the 'OK corral,'", "Jericho", "James Garfield Davis", "Catherine", "Ashley \"A.J.\" Jewell,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6205124041595466}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.2222222222222222, 0.6666666666666666, 0.08333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.8, 1.0, 0.9090909090909091, 1.0, 0.0, 0.09090909090909091, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5283018867924527, 0.9655172413793104, 0.72, 1.0, 0.0, 1.0, 1.0, 0.5789473684210525, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.676470588235294, 0.0, 1.0, 0.0, 0.2857142857142857, 0.18181818181818182, 0.8695652173913044, 0.4615384615384615, 1.0, 1.0, 0.16, 0.0, 0.05714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.5, 0.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-259", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-2578", "mrqa_hotpotqa-validation-4117", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-982", "mrqa_searchqa-validation-16615"], "SR": 0.4375, "CSR": 0.5159574468085106, "EFR": 0.9444444444444444, "Overall": 0.705127253250591}, {"timecode": 47, "before_eval_results": {"predictions": ["mumps", "The Last King of Scotland", "Kazakhstan", "Wilkie Collins", "the lung", "Knutsford", "Burma", "Jim Broadbent", "a falcon", "Jesus vs. Santa", "Shylock", "Canada", "Phil Spector", "Champagne", "Tiny Tim", "the Dada movement", "Boston", "Roddy Doyle", "geography", "Operation Frequent Wind", "Berlin", "Charlie Chan", "Wanderers", "Pinwright's Progress", "a hat", "Lady Gaga", "a butterfly", "Christian Wulff", "the Kinks", "the Queen of Comedy", "Debbie Rowe", "Sir Herbert Kitchener", "a centaur", "iodine deficiency", "14", "Margaret Beckett", "James Hogg", "Welsh", "George Bernard Shaw", "table tennis", "Woolton pie", "the Florida Current", "Crane, Poole & Schmidt", "Brighton", "Gandalf", "1930", "Motown", "Canada", "Pope Benedict XVI", "a dove", "John T. Cable", "a rearrangement of chromosomal material between chromosome 21 and another chromosome", "Abraham Lincoln's war goals", "$2 million in 2011", "Amy Poehler", "Eric Allan Kramer", "Koninklijke Ahold N.V.", "via YouTube days after 35 bodies were found in two trucks during rush hour in the city of Boca del Rio.", "Donald Duck", "debris", "Sweden", "Nancy Drew", "schizophrenia", "in order to prevent heat exhaustion or heat stroke"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6960565476190476}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-2555", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-3098", "mrqa_triviaqa-validation-2077", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-7274", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2944", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-304", "mrqa_naturalquestions-validation-9093", "mrqa_hotpotqa-validation-4091", "mrqa_newsqa-validation-2797", "mrqa_searchqa-validation-14439"], "SR": 0.671875, "CSR": 0.5192057291666667, "EFR": 1.0, "Overall": 0.7168880208333335}, {"timecode": 48, "before_eval_results": {"predictions": ["eukarya", "eight", "April 1st", "June 1992", "won", "Jane Fonda -- Chelsea Thayer Wayne", "Kimberlin Brown", "March 31, 2017", "vincent gogh", "New York City", "George Strait", "John Adams", "a major fall in stock prices", "to Lands End", "Charles Path\u00e9", "Phillip Paley", "from statute or the Constitution itself", "18", "Game 1", "those colonists of the Thirteen Colonies who rebelled against British control during the American Revolution and in July 1776 declared the United States of America an independent nation", "Abraham Gottlob Werner", "session Initiation Protocol", "the 18th century", "Lesley Gore", "around 1200", "thirteen if Plank", "mongrel female", "the Beldam / Other Mother", "John Quincy Adams", "August 1991", "Uralic", "dromedary", "Bhupendranath Dutt", "2011", "a substance that fully activates the receptor that it binds to )", "Bill Russell", "Battle of Antietam", "pickup trucks", "Hunter Tylo", "Buffalo Bill", "by the early 3rd century", "James Rodr\u00edguez", "around 10 : 30am", "Jack Barry", "the White Sox", "45 %", "to condense the steam coming out of the cylinders or turbines", "Bill Russell", "1984", "the problems", "the longest rotation period ( 243 days ) of any planet in the Solar System and rotates in the opposite direction to most other planets ( meaning the Sun would rise in the west and set in the east )", "northern line", "cilla black", "epernay", "her grandmother Hanako Muraoka", "Taoiseach", "The Los Angeles Dance Theater", "Kurdistan Freedom Falcons", "AbdulMutallab", "hank mary", "typewriter", "calico", "mandolin", "Gary Player"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6856456304112554}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.12500000000000003, 0.10714285714285715, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0606060606060606, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-183", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-4655", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3102"], "SR": 0.59375, "CSR": 0.5207270408163265, "EFR": 1.0, "Overall": 0.7171922831632653}, {"timecode": 49, "before_eval_results": {"predictions": ["Napoleon", "iron", "Patrick Warburton", "Toot - Toot", "pneumonoultramicroscopicsilicovolcanoconiosis", "Charles Crozat Converse", "Andrew Garfield", "July 4, 1776", "Keith Thibodeaux", "Jesus Christ", "Charles Path\u00e9", "eleven", "President alone, and the latter grants judicial power solely to the federal judiciary", "Johannes Gutenberg", "O'Meara", "first published in the United States by Melvil Dewey in 1876", "marley", "fourth season", "four", "The First Battle of Bull Run ( the name used by Union forces )", "rejecting the null hypothesis given that it is true", "slavery", "It's Gonna Take a Miracle ''", "United States federal law that imposes a federal employer tax used to help fund state workforce agencies", "The Outback", "its vast territory was divided into several successor polities", "Louis XV", "2017", "genome", "Beorn", "the ability to influence somebody to do something that he / she would not have done ''", "nationalists of the Union proclaimed loyalty to the U.S. Constitution", "in response to the Weimar Republic's failure to continue its reparation payments in the aftermath of World War I", "government used by the British and French to control parts of their colonial empires, particularly in Africa and Asia, through pre-existing local power structures", "Zachary John Quinto", "the governor of West Virginia", "Wednesday, September 21, 2016, on NBC and finished on Wednesday, May 24, 2017, with a two - hour season finale", "ninth w\u0101", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "milling", "November 27, 2017", "1939", "1992", "Miller Lite", "Felix Baumgartner", "Donald Gets Drafted", "c. 3000 BC", "Bart Howard", "Paris", "1966", "the vascular cambium", "Child of the 1980's", "Leeds", "the Netherlands", "Coleman Hawkins", "Samuel Joel \" Zero\" Mostel", "Ellesmere Port, United Kingdom", "the Genocide Prevention Task Force,", "a lone 50-year-old man", "U.S. Naval Forces Central Command,", "Menoetius", "Treasure Island", "Sergei Diaghilev", "sailing"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6114458878160144}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5454545454545454, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.8837209302325582, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-1290", "mrqa_naturalquestions-validation-6888", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-1366", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-6021", "mrqa_triviaqa-validation-460", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2428", "mrqa_searchqa-validation-9956", "mrqa_searchqa-validation-15480"], "SR": 0.515625, "CSR": 0.520625, "EFR": 0.967741935483871, "Overall": 0.7107202620967742}, {"timecode": 50, "UKR": 0.6875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1373", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-75", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10692", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5387", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8709", "mrqa_naturalquestions-validation-8819", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3368", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3674", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5277", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6529", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6920", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9885", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10107", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1257", "mrqa_squad-validation-1403", "mrqa_squad-validation-1536", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2404", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3230", "mrqa_squad-validation-3259", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4538", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4847", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5348", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-5753", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7295", "mrqa_squad-validation-7339", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-9575", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3529", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4187", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5685", "mrqa_triviaqa-validation-5744", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99"], "OKR": 0.841796875, "KG": 0.49921875, "before_eval_results": {"predictions": ["Pelias", "Trainspotting", "Rockin' at the Hops", "jute fibres", "spark-ignition", "Concorde", "Al Jazeera", "French", "team", "sand lance fish", "1925", "Goldfinger", "Midway", "Flower", "Gerald R. Ford", "Dengue fever", "Japan", "Ted Turner", "phobia", "Cowslip primrose", "Mount Everest", "Strangeways", "Carthage", "Wensum", "Robben Island", "United Kingdom", "Taekwondo", "foot", "apple", "sixth Wimbledon championship", "Nelson Mandela", "George Orwell", "Andrew Jackson", "Muriel Spark", "table tennis", "Entwistle Reservoir", "DeLorean", "six", "Perseus", "Yakutat", "United Nations of Football", "specialist insurance", "muscle", "transuranic elements", "John Buchan", "Tesco", "Lolita", "jukebox", "Indus Valley", "duck", "Pickwick", "Nancy Jean Cartwright", "Watson and Crick", "Authority", "his superhero roles", "German", "Che Guevara", "Robert Barnett", "five", "Jet Republic", "the Lexus 460", "John Lennon", "Murder by Death", "Republicans"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6973958333333333}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-139", "mrqa_triviaqa-validation-6499", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-6476", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-204", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-5363", "mrqa_triviaqa-validation-3982", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-112", "mrqa_naturalquestions-validation-3468", "mrqa_hotpotqa-validation-881", "mrqa_searchqa-validation-5409"], "SR": 0.640625, "CSR": 0.5229779411764706, "EFR": 0.9565217391304348, "Overall": 0.701603061061381}, {"timecode": 51, "before_eval_results": {"predictions": ["Flatbush Zombies", "British", "\"Traumnovelle\"", "Denmark", "Royce da 5'9\"", "Bellagio and The Mirage", "Mr. Basketball", "more than 20 principal operations", "Guthred", "The New Yorker", "jerrydavy", "St. Louis Cardinals", "NXT Tag Team Championship", "Song Kang-ho", "as many as 16 universities in the eastern half of the United States from 1979 to 2013", "February 1", "capital crimes or capital offences", "Let Me Be the One", "March", "Chuck Noll", "Cate Blanchett", "California", "Atlas ICBM", "Democratic", "Sun Woong", "Beatles", "22,500 acres", "Trey Parker", "Kew", "Albany", "IX", "Wembley Stadium", "Shameless", "Brigadier General Raden Panji Nugroho Notosusanto", "skiing and mountaineering", "Indian", "Comedy Film Nerds", "cruiserweight", "five books", "Leofric", "Bigfoot", "March 17, 2015", "Yubin", "5249", "U.S. Representative for Oklahoma's 4 congressional district", "seven members", "28 November 1973", "La Nouba", "londonderry", "Santiago Herrera", "jewelry designer", "Steve Russell", "old pronunciation of Gaultier or Walter", "while studying All My Sons by Arthur Miller, a play about a man whose choice to send out faulty airplane parts for the good of his business and family", "Pegasus", "sunday", "barildon", "15-year-old's", "July 23.", "Baghdad", "yermo", "penis", "T.S. Eliot", "lottie"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5147915040333073}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false], "QA-F1": [0.22222222222222224, 1.0, 0.5, 0.0, 0.6, 1.0, 0.0, 0.33333333333333337, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.819672131147541, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-3270", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-1260", "mrqa_hotpotqa-validation-3712", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-248", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-1886", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-7514", "mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-4557", "mrqa_searchqa-validation-6192", "mrqa_searchqa-validation-834", "mrqa_triviaqa-validation-4216"], "SR": 0.421875, "CSR": 0.5210336538461539, "EFR": 1.0, "Overall": 0.7099098557692307}, {"timecode": 52, "before_eval_results": {"predictions": ["Frank Ocean", "Brookhaven", "2010", "Ryukyuan people", "Robert L. Stone", "Mexican", "The King of Hollywood", "five times", "1968", "Charles Eug\u00e8ne Jules Marie Nungesser, MC", "Kim Yoon-seok and Ha Jung-woo", "Jennifer Grey", "1978", "M2M", "Mark Neveldine and Brian Taylor", "The special digital single", "Beauty and the Beast", "Odorama", "a professor at the Jon M. Huntsman School of Business", "Larnelle Steward Harris", "Total Nonstop Action Wrestling", "Lambic", "Bit Instant", "Tom Jones", "Leon Czolgosz", "Secrets and Lies", "Hard rock", "Ludwig van Beethoven", "Peter Kay's Car Share", "Orph\u00e9e et Eurydice", "Dirt track racing", "Frederick Barbarossa", "Karakalpaks", "Walldorf", "Han Solo", "Campbellsville", "Shinjuku", "1933", "Delphi Lawrence", "Philadelphia", "December 13, 2015", "The New York Stock Exchange (abbreviated as NYSE and nicknamed \"The Big Board\")", "Las Vegas", "Russell T Davies", "four", "a Kensan-Devan Wildlife Sanctuary", "Mickey\\'s Christmas Carol", "2018\u201319 UEFA Europa League group stage", "Argentinian", "76,416", "Burning Man", "President of the United States", "Ra\u00fal Eduardo Esparza", "October 30, 2017", "Live and Let Die", "Carrie", "on Mars", "Reggae legend Lucky Dube, one of South Africa's most famous musicians,", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "the Internet", "a stick amok", "Yes", "East Germany", "a heart"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7606856684981684}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-847", "mrqa_hotpotqa-validation-1263", "mrqa_triviaqa-validation-7133", "mrqa_newsqa-validation-594", "mrqa_searchqa-validation-15932", "mrqa_triviaqa-validation-3362"], "SR": 0.671875, "CSR": 0.523879716981132, "EFR": 1.0, "Overall": 0.7104790683962264}, {"timecode": 53, "before_eval_results": {"predictions": ["doubles", "New York", "the northern part", "muezzin", "nippon Sangyo", "a binder", "James Hogg", "Sarajevo", "Darby and Joan", "2010 BAFTA Awards", "Stanley Kubrick's Full Metal jacket", "Blur", "chicken Marengo", "Nelson Mandela", "General Sir Herbert Kitchener", "white", "a bodice", "grizzly bear", "bukwus", "Jesuit", "rowing", "his death in 1975", "Nowhere Boy", "Donald Trump", "Gorbachev", "Popeye", "John Key", "Charlie Brooker", "pennsylvania state university", "the Atlantic", "Delilah", "Aegle", "dynamite", "Take That", "Jean Alexander", "Lew Hoad", "David Hockney", "La Toya Jackson", "Jimmy Carter", "Greek Home Management", "chicago", "Edinburgh", "Today", "Rose Atoll, American Samoa", "Bolton", "Norwegian Ibsen", "Super Bowl", "\"Stutter Rap (No Sleep til Bedtime)\"", "Vladimir Putin", "Belle", "a double dip recession", "Augustus Waters", "1967", "alpaca fiber and mohair from Angora goats", "Ghana", "Peter Kay's Car Share", "Miller Brewing", "The son of Gabon's former president", "more than two years,", "auction off one of the earliest versions of the Declaration of Independence", "argyle", "a judgment, viewpoint, or statement that is not conclusive", "Secretary Robert A. McDonald", "Unseeded Frenchwoman Aravane Rezai"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6376190832194122}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7368421052631577, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5884", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-5000", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-5848", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-2385", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5531", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-3499", "mrqa_newsqa-validation-3285"], "SR": 0.5625, "CSR": 0.5245949074074074, "EFR": 1.0, "Overall": 0.7106221064814815}, {"timecode": 54, "before_eval_results": {"predictions": ["Emanuel Schwartz", "gatsby", "germany", "San Francisco", "the First World War", "germany", "prince andrew", "Duke of Edinburgh", "the Soviet Union", "Ross MacManus", "the Appalachians", "Skylab", "John Poulson", "michael cavassoni", "shoes", "great depression", "corsets", "Queen Anne", "outer one", "dicken\\'s dream", "Swansea City", "argon", "the silurian", "meatloaf", "non-Orthodox synagogues", "j.M.W. Turnerer", "The Lone Gunmen", "gold", "Duncan Jones", "the north-west corner of the central business district", "Wonderwall Music", "basketball", "carburet", "germany", "corsets", "germany", "Jean- Martin Charcot", "winged horse", "Charlie Chaplin, Jr.", "bathe", "dora peggotty", "Scotland", "germany", "Arthur C. Clarke", "Buzz Aldrin", "power outage", "the Russian army", "index fingers", "Blenheim Palace", "Rihanna", "cumbria", "28 July 1914 to 11 November 1918", "Fix You", "the spectroscopic notation for the associated atomic orbitals", "Copa Airlines", "My Beautiful Dark Twisted Fantasy", "Soha Ali Khan Khemu", "for vitamin injections that promise to improve health and beauty.", "debris", "digging ditches.", "typhoid fever", "France", "the Edict of Nantes", "in Austin and Pflugerville"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5243303571428571}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-1536", "mrqa_triviaqa-validation-1767", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-7740", "mrqa_triviaqa-validation-2312", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-2243", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-4795", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-2879", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-7712", "mrqa_hotpotqa-validation-303", "mrqa_newsqa-validation-3330", "mrqa_searchqa-validation-5331", "mrqa_naturalquestions-validation-3995"], "SR": 0.453125, "CSR": 0.5232954545454546, "EFR": 0.9428571428571428, "Overall": 0.6989336444805194}, {"timecode": 55, "before_eval_results": {"predictions": ["aviva plc", "Venezuela", "Mozart", "Naples", "2001: A Space Odyssey", "Catherine Cookson", "almonds", "Barack Obama,", "Geneva", "wesley shatner", "mark", "Persian Gulf", "bad", "Ascot", "seine", "mary poppins", "sheryl Crow", "winnie Mae", "Spain", "quietly", "amorites", "graphite", "Narragansett Bay", "Moby Dick", "The Scream", "gingerbread", "Boddington", "king henry III", "good luck", "raspberries", "island", "surfer", "oakum", "blancmange", "rochdale", "penhaligon", "Black September", "9,926", "Germany", "shoe", "metal", "Herald of Free Enterprise", "professor brians", "fur", "9", "joints", "Bolivia", "jewish communities", "Jordan", "Hans Lippershey", "mark", "Christina Giles", "statistical advantage for the casino that is built into the game", "in order to halt it following brake failure", "Teriade", "Japan", "the last living pilot", "Megan Lynn Touma, 23,", "Madeleine K. Albright", "three", "walrus", "grotesque", "Liam Neeson", "centaur"], "metric_results": {"EM": 0.53125, "QA-F1": 0.56328125}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-4616", "mrqa_triviaqa-validation-5454", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1324", "mrqa_triviaqa-validation-1674", "mrqa_triviaqa-validation-3231", "mrqa_triviaqa-validation-6866", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3275", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-3897", "mrqa_triviaqa-validation-2442", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-1365", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-6964", "mrqa_triviaqa-validation-4164", "mrqa_triviaqa-validation-768", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5510", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-2524", "mrqa_searchqa-validation-2027"], "SR": 0.53125, "CSR": 0.5234375, "EFR": 0.9, "Overall": 0.6903906249999999}, {"timecode": 56, "before_eval_results": {"predictions": ["sarah cox", "Judy Garland", "william hartnell", "friedrich n Nietzsche", "Ben Affleck", "jamaican", "magical moptops", "brazil", "Cyclopes", "purple", "1961", "norway", "florence", "tony meo", "georgia coltrane", "Antoine Lavoisier", "30th", "meerkat", "tara", "Henri Rousseau", "albania", "the Beatles", "sorghia", "florence", "non-vascular", "paddington bear", "tarn", "motorway", "tidal bay", "robbie", "alastair Cook", "peterronas", "cribbage", "1960s", "north Yorkshire", "LMFAO", "robbie coltrane", "the Kinks Are the Village Green Preservation Society", "Tony Blackburn", "spain", "rebecca", "india", "Pink Floyd", "bobby brown", "miles Morales", "annette earhart", "British", "Tokyo", "postmortem", "mono", "Augustus Caesar", "the south coast of eastern New Guinea", "Lady Gaga", "revenge", "\"Secrets and Lies\"", "October 3, 2017", "Morris Barney Dalitz", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "Unseeded", "Kingman Regional Medical Center", "Walter coltrane", "the fairway", "the Wikimedia Foundation, Inc.", "2018"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5685267857142857}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-223", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-4390", "mrqa_triviaqa-validation-346", "mrqa_triviaqa-validation-1665", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-3252", "mrqa_triviaqa-validation-188", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-4757", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-1527", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6935", "mrqa_triviaqa-validation-3716", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-9821", "mrqa_hotpotqa-validation-4161", "mrqa_newsqa-validation-3287", "mrqa_searchqa-validation-16213", "mrqa_searchqa-validation-11582"], "SR": 0.515625, "CSR": 0.5233004385964912, "EFR": 1.0, "Overall": 0.7103632127192983}, {"timecode": 57, "before_eval_results": {"predictions": ["morocco", "Illinois", "Edward Hopper", "robocop", "papal secretary", "Quentin Blake", "bazaar", "shrove sunday", "new york", "Hamlet", "Chris Smalling", "007", "iccadilly", "hobbits", "Jordan", "Tangled", "So Far Away", "united states", "crossword puzzle", "Sheree Murphy", "morocco", "Robin Ellis", "tomato Basil Conchiglie Pasta", "davy cockett", "war and peace", "paphos", "three", "east of Eden", "de quincey", "Zaragoza", "Phil Woolas", "argentina", "king eddy", "Belgium", "Scrooge", "bridge", "elliptical", "Koblenz", "bernard", "blood", "zips", "Isar", "Roman history", "Thor", "Admiral Vernon", "Florence", "woodstock", "birds", "nijinsky", "p Preston", "drogba", "the most senior position in the Bank of England", "B.J. Thomas", "65,535 bytes", "Prince Amedeo,", "Richardson, Texas", "hulder", "the Obama administration", "Senate Democrats", "some of the Awa", "Manhattan Island", "Patrick Henry", "chronic daytime drowsiness", "the Eagles"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6382965686274509}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.11764705882352941, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1493", "mrqa_triviaqa-validation-3686", "mrqa_triviaqa-validation-4243", "mrqa_triviaqa-validation-910", "mrqa_triviaqa-validation-4367", "mrqa_triviaqa-validation-6237", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-1563", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7378", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-2177", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-1787", "mrqa_hotpotqa-validation-2399", "mrqa_newsqa-validation-1550", "mrqa_searchqa-validation-10934", "mrqa_searchqa-validation-384"], "SR": 0.59375, "CSR": 0.5245150862068966, "EFR": 1.0, "Overall": 0.7106061422413793}, {"timecode": 58, "before_eval_results": {"predictions": ["the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "10th Cavalry Regiment", "Claude Mak\u00e9l\u00e9l\u00e9", "John Robert Cocker", "Taylor Swift", "mountaineer", "\"Lonely\"", "Garrett Morris", "October 5, 1937", "1692", "Jay Hanna \"Dizzy\" Dean", "Target Corporation", "British Labour Party", "Bandai", "William Harold \"Bill\" Ponsford", "Ward Bond", "Myst3ry", "every Rose Has its Thorn", "Cleveland Browns", "Jacking, or the jack", "1910", "My Beautiful Dark Twisted Fantasy", "Broadcasting House in London", "20", "Amway", "Congo River", "Minneapolis", "Alemannic and the Bavarian-Austrian dialects of German", "illnesses", "XVideos", "1967", "1967", "pinball machine", "Lawrence of Arabia", "The Fault in Our Stars", "Gareth Jones", "head of the Cabinet of Bluhme I", "J35-A-23", "Scotty Grainger", "balloons Street, Manchester", "Somerset County, Pennsylvania", "Italy", "Psych", "Gateways", "Iran", "Veneto", "Empire Falls", "Fitzroya cupressoides", "Vernon L. Smith", "Dan Rowan", "Bohemia", "March 18, 2005", "1978", "Austria", "Switzerland", "the Treaty of Waitangi", "1930", "a U.S. military helicopter", "African National Congress Deputy President Kgalema Motlanthe", "new DNA evidence", "blue whale", "Christopher Darden", "out-of-print books", "bullfight"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6129712301587301}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.5, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.8, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-2209", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-243", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2004", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-10135", "mrqa_triviaqa-validation-5517", "mrqa_newsqa-validation-1382", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-1481", "mrqa_triviaqa-validation-6175"], "SR": 0.484375, "CSR": 0.5238347457627119, "EFR": 0.9696969696969697, "Overall": 0.7044094680919363}, {"timecode": 59, "before_eval_results": {"predictions": ["the first integrated circuit", "Oracle Corporation", "Levittown, New York", "Mako", "seven", "Ashanti", "1934", "Cheshire", "1980", "blood sport involving the fighting of male crickets", "Dachshunds", "the Mersey in the north almost to Nantwich in the south, and from the Gowy in the west to the Weaver in the east", "Duncan Kenworthy", "the Stern-Plaza in Potsdam", "the Netherlands", "Continental Army", "various deities, beings, and heroes", "Henry Lau", "1", "Russian Empire", "the Catholic Church in Ireland", "people working in film and the performing arts", "Lykan HyperSport", "1989", "Gareth Barry", "1999", "Warner Animation Group", "Margarine Unie", "David Naughton", "A123 Systems, LLC", "Ian Fleming", "Minneapolis", "14", "anvil", "50 Greatest Players in National Basketball Association History", "James G. Kiernan", "Dizzy Dean", "Magnus Carlsen", "\"The Independent\", \"BBC Focus\" and \"Wired\"", "Home", "1958", "World War II", "Jenn Brown", "\"Glee\"", "Purdue University", "Indianapolis", "the recording debut of future AC/DC founders Angus Young and Malcolm Young.", "Bury, Greater Manchester, England", "\"Agent Vinod\"", "Marxist and a Leninist", "George Timothy Clooney", "Laura Jane Haddock", "when the cell is undergoing the metaphase of cell division ( where all chromosomes are aligned in the center of the cell in their condensed form )", "2011", "giraffe", "ghee", "Wagner", "to step up.", "\"The group, Lashkar-e-Jhangvi,", "Andrew Morris,", "Home on the Range", "meter", "Donnie Wahlberg", "1918"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7077563061938061}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false], "QA-F1": [0.8, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.22222222222222224, 1.0, 0.07692307692307691, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3054", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-1148", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-2810", "mrqa_hotpotqa-validation-1055", "mrqa_naturalquestions-validation-8159", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-2789"], "SR": 0.609375, "CSR": 0.5252604166666667, "EFR": 1.0, "Overall": 0.7107552083333333}, {"timecode": 60, "UKR": 0.685546875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1113", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1494", "mrqa_hotpotqa-validation-1532", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1624", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1917", "mrqa_hotpotqa-validation-1971", "mrqa_hotpotqa-validation-2048", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-2958", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3009", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3293", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3700", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-4534", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-4728", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5839", "mrqa_hotpotqa-validation-5859", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-637", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-768", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-921", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-10651", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1252", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1490", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1726", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2383", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-3088", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3379", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3990", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4312", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7673", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7829", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9585", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1667", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2612", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-879", "mrqa_newsqa-validation-910", "mrqa_searchqa-validation-10446", "mrqa_searchqa-validation-10449", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-10697", "mrqa_searchqa-validation-10846", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-11303", "mrqa_searchqa-validation-11327", "mrqa_searchqa-validation-11488", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-1204", "mrqa_searchqa-validation-12174", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12796", "mrqa_searchqa-validation-12885", "mrqa_searchqa-validation-13107", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13608", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-16109", "mrqa_searchqa-validation-16722", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-1862", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-2415", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-2614", "mrqa_searchqa-validation-2755", "mrqa_searchqa-validation-3095", "mrqa_searchqa-validation-3346", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-4291", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4948", "mrqa_searchqa-validation-5384", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5518", "mrqa_searchqa-validation-5796", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6950", "mrqa_searchqa-validation-7026", "mrqa_searchqa-validation-7424", "mrqa_searchqa-validation-7470", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-8075", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-9231", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-9949", "mrqa_searchqa-validation-9973", "mrqa_squad-validation-10286", "mrqa_squad-validation-1042", "mrqa_squad-validation-1045", "mrqa_squad-validation-1134", "mrqa_squad-validation-1188", "mrqa_squad-validation-1189", "mrqa_squad-validation-1193", "mrqa_squad-validation-1207", "mrqa_squad-validation-1403", "mrqa_squad-validation-1627", "mrqa_squad-validation-1967", "mrqa_squad-validation-2092", "mrqa_squad-validation-2159", "mrqa_squad-validation-2168", "mrqa_squad-validation-2188", "mrqa_squad-validation-2288", "mrqa_squad-validation-232", "mrqa_squad-validation-2377", "mrqa_squad-validation-2523", "mrqa_squad-validation-2570", "mrqa_squad-validation-2579", "mrqa_squad-validation-2841", "mrqa_squad-validation-2884", "mrqa_squad-validation-2916", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3296", "mrqa_squad-validation-3360", "mrqa_squad-validation-3462", "mrqa_squad-validation-3506", "mrqa_squad-validation-3836", "mrqa_squad-validation-3849", "mrqa_squad-validation-3922", "mrqa_squad-validation-394", "mrqa_squad-validation-4107", "mrqa_squad-validation-4560", "mrqa_squad-validation-4578", "mrqa_squad-validation-4636", "mrqa_squad-validation-4935", "mrqa_squad-validation-4974", "mrqa_squad-validation-5061", "mrqa_squad-validation-5128", "mrqa_squad-validation-5154", "mrqa_squad-validation-5191", "mrqa_squad-validation-5207", "mrqa_squad-validation-5357", "mrqa_squad-validation-5444", "mrqa_squad-validation-5471", "mrqa_squad-validation-5489", "mrqa_squad-validation-5491", "mrqa_squad-validation-5508", "mrqa_squad-validation-553", "mrqa_squad-validation-5630", "mrqa_squad-validation-6012", "mrqa_squad-validation-6158", "mrqa_squad-validation-6251", "mrqa_squad-validation-6256", "mrqa_squad-validation-6506", "mrqa_squad-validation-6746", "mrqa_squad-validation-6815", "mrqa_squad-validation-6884", "mrqa_squad-validation-7347", "mrqa_squad-validation-7395", "mrqa_squad-validation-7839", "mrqa_squad-validation-8062", "mrqa_squad-validation-8213", "mrqa_squad-validation-8229", "mrqa_squad-validation-8507", "mrqa_squad-validation-8680", "mrqa_squad-validation-8703", "mrqa_squad-validation-8771", "mrqa_squad-validation-9254", "mrqa_squad-validation-9392", "mrqa_squad-validation-9399", "mrqa_squad-validation-9411", "mrqa_squad-validation-962", "mrqa_squad-validation-964", "mrqa_squad-validation-9766", "mrqa_squad-validation-9780", "mrqa_squad-validation-9839", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1182", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1317", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1580", "mrqa_triviaqa-validation-1632", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1768", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-198", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-2235", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2243", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2547", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2954", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3432", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-3869", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4168", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-4351", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4372", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-4968", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5228", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5366", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6098", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6536", "mrqa_triviaqa-validation-6780", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-6830", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6875", "mrqa_triviaqa-validation-7046", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7174", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7462", "mrqa_triviaqa-validation-7603", "mrqa_triviaqa-validation-7606", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-99", "mrqa_triviaqa-validation-997"], "OKR": 0.830078125, "KG": 0.49921875, "before_eval_results": {"predictions": ["September, Bianchi's death during childbirth", "five minutes before commandos descended", "Arsene Wenger", "some of the most gigantic pumpkins in the world,", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "co-chair of the Genocide Prevention Task Force.", "New York City crackdown on suspects allegedly involved in forged credit cards and identity theft led authorities to a $13 million global crime ring,", "4.6 million people", "sports cars", "Vicente Carrillo Leyva,", "discovery\" for the museum \"of the last 90 years.\"", "Communist Party of Nepal (Maoist)", "581 points", "Molotov cocktails, rocks and glass.", "1994,", "10 years in prison", "Gulf", "average of 25 percent of U.S. consumers who get recall notices don't follow through and fix their vehicles.", "Orbiting Carbon Observatory,", "then-Sen. Obama", "Claude Monet", "more than 4,000 commercial farmers", "apartment building", "Pittsburgh", "Former Mobile County Circuit Judge Herman Thomas", "Daytime Emmy Lifetime Achievement Award", "South Africa.", "Barack Obama's", "dual nationality", "They are, of course, shattered.", "Cash for Clunkers", "It will be the golfer's first public appearance since his November 27 car crash outside his home near Orlando, Florida.", "couple's surrogate", "prostate cancer,", "Zimbabwe", "Britain.", "fire or punch a hole through the aircraft structure,\"", "10 percent", "eight or nine young girls, some younger then 18, who were returned to their families.", "Jaipur", "cancer", "Mrs. Graham,", "salutes the \"People of Palestine\" and calls on them to fight back against Israel in Gaza.", "forgery and flying without a valid license,", "GeorgeWashington", "Lavau's son, Sean,", "three out of four questioned say that things are going well for them personally.", "poems", "Fourth time lucky in Atlanta in 1996.", "environmental efforts make even more impact than Harrison Ford's chest.", "South African captain Graeme Smith", "2009", "William Jennings Bryan", "anteriorly by an acrosome, which contains enzymes used for penetrating the female egg.", "Barry Humphries", "Mozambique Channel", "Ede & Ravenscroft", "Adelaide", "punk rock", "Great Northern Railway", "Otis Elevator Company", "Iberian Peninsula", "George Balanchine", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.5961101672450357}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, true], "QA-F1": [0.33333333333333337, 1.0, 1.0, 0.0, 0.3, 0.0, 0.14285714285714288, 0.8, 0.5, 1.0, 0.15384615384615383, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.14285714285714288, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-3297", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-4092", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-56", "mrqa_triviaqa-validation-2468", "mrqa_hotpotqa-validation-3984", "mrqa_searchqa-validation-15121"], "SR": 0.484375, "CSR": 0.5245901639344263, "EFR": 1.0, "Overall": 0.7078867827868852}, {"timecode": 61, "before_eval_results": {"predictions": ["Donald Duck", "Iran's parliament speaker", "Department of Homeland Security Secretary Janet Napolitano", "18", "India", "World leaders", "Casalesi Camorra clan", "managing his time.", "his club", "we seek a new way forward, based on mutual interest and mutual respect.", "$50", "collaborating with the Colombian government,", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "1,500", "Jada", "200", "Karen Floyd", "space shuttle Discovery,", "Brazil", "EU naval force", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "Harrison Ford", "Sunday", "28", "The Falklands, known as Las Malvinas in Argentina,", "New York City Mayor Michael Bloomberg", "Department of Homeland Security Secretary Janet Napolitano", "two", "\"We have to condition the dogs to the shoes,\"", "30-minute", "338", "UNICEF", "eight", "Daniel Radcliffe", "Department of Homeland Security", "\"the most publicity the breed has ever had since its introduction into the U.S. in the late 1960s,\"", "tanker", "lightning strikes", "if he did cheat on you", "Afghan lawmakers", "the Dalai Lama", "Colombia", "nine-wicket", "nearly 100", "1616", "to sniff out cell phones.", "Casey Anthony,", "people look at the content of the speech, not just the delivery.", "2005", "root out terrorists within its borders.", "a point for Bayern Munich as the German Bundesliga leaders were held to a 1-1 draw by Cologne on Saturday.", "March 31 to April 8, 2018", "Veterans Committee", "Bobby Beathard", "St Paul's Cathedral", "15", "Rome", "University of Vienna", "Dutch", "Naomi Campbell", "Earhart", "cricket", "Saturday Night Live", "the courts"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6267400874248301}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.0, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8823529411764706, 1.0, 0.0, 1.0, 0.25, 0.2857142857142857, 0.4444444444444445, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 0.5, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-417", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-2145", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-4318", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-880", "mrqa_hotpotqa-validation-3500", "mrqa_searchqa-validation-13347"], "SR": 0.53125, "CSR": 0.5246975806451613, "EFR": 0.9333333333333333, "Overall": 0.6945749327956989}, {"timecode": 62, "before_eval_results": {"predictions": ["Golden Valley, Minnesota,", "Emmy and four", "small forward", "Southern Rock Allstars", "Araminta Ross", "Mach number", "eight", "August 17, 2017", "Al Capone", "Atomic", "St Augustine's Abbey", "Vilyam \"Willie\" Genrikhovich Fisher", "minister and biographer", "Carl Michael Edwards", "\"the most influential private citizen in the America of his day\"", "rhythm and blues", "Standard Oil", "over 1.6 million passengers", "British Labour Party", "September 8, 2017", "Obafemi Martins", "Charles Edward Stuart", "HackThis Site", "Steve Carell", "Saint Motel", "Dr. Bernadette Rostenkowski-Wolowitz", "Flyweight", "Levon Helm", "Jean Acker", "attack on Pearl Harbor", "Fountains of Wayne", "Nick Offerman", "Sam Raimi, and Tom Spezialy", "SAS", "Double Crossed", "Edmonton, Alberta", "8,211", "KXII", "Wikimedia Foundation", "Greek-American", "Mika H\u00e4kkinen", "Debbie Isitt", "Los Angeles", "1999", "Outside", "Food and Agriculture Organization", "Wojtek", "King Edward I", "Los Angeles", "West Point Iron and Cannon Foundry", "New York City", "Speaker of the House of Representatives", "13", "Hans Christian Andersen", "arrowhead", "Corin Redgrave", "a son of Amram and Jochebed,", "UNICEF", "Bob Bogle,", "she was humiliated by last month's incident,", "The sheriff of Sparta", "Mead", "David", "6"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7365166900093371}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6, 0.28571428571428575, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-5444", "mrqa_hotpotqa-validation-4237", "mrqa_hotpotqa-validation-4866", "mrqa_hotpotqa-validation-4539", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-2552", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2993", "mrqa_naturalquestions-validation-839", "mrqa_triviaqa-validation-6039", "mrqa_triviaqa-validation-5231", "mrqa_newsqa-validation-390", "mrqa_searchqa-validation-12442"], "SR": 0.640625, "CSR": 0.5265376984126984, "EFR": 1.0, "Overall": 0.7082762896825396}, {"timecode": 63, "before_eval_results": {"predictions": ["African National Congress", "Ronald Cummings", "five", "Bob Bogle", "Bob Bogle,", "for the creation of an Islamic emirate in Gaza,", "suppress the memories and to live as normal a life as possible;", "Caster Semenya", "Ashley \"A.J.\" Jewell,", "the BBC's central London offices", "Kgalema Motlanthe,", "as he tried to throw a petrol bomb at the officers,", "Karl Eikenberry", "at the age of 23", "Asian qualifying Group 2", "Elena Kagan", "Harrison Ford", "Christmas parade", "a facility in Salt Lake City, Utah,", "eight in 10", "your ex's loved ones ask why", "2", "racial intolerance.", "acid attack by a spurned suitor.", "23 million square meters (248 million square feet)", "part of the proceeds", "bicycles", "landed in Cameroon,", "Akshay Kumar", "August 19, 2007.", "a strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television.", "should have met with the Dalai Lama.", "would require an act of Congress,", "the test results", "shows the world that you love the environment and hate using fuel,", "\"Dancing With the Stars.\"", "Brown-Waite", "we have no real procedure for sectioning off the rear-frame rails,", "40", "strife in Somalia,", "protest child trafficking and shout anti-French slogans", "a colonel in the Rwandan army,", "1918-1919.", "in the mouth.", "cancer", "Susan Atkins", "137", "the shoreline of the city of Quebradillas.", "for pulling on the top-knot of an opponent,", "at the University of Alabama in Huntsville,", "secretary of state", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "the temperature at which it becomes semi solid and loses its flow characteristics", "a medium", "Touchstone", "Quentin Tarantino", "uric goldfinger", "Thomas Mawson", "Charles Ellis Schumer", "Peel Holdings", "Java", "aaron", "the zodiac", "Harriet the Spy"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5678096962430843}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 0.8, 0.8571428571428571, 1.0, 0.0, 1.0, 0.5714285714285715, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 0.7894736842105263, 0.0, 0.125, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2724", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-10403", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-6865", "mrqa_hotpotqa-validation-3529", "mrqa_searchqa-validation-2650"], "SR": 0.46875, "CSR": 0.525634765625, "EFR": 1.0, "Overall": 0.7080957031249999}, {"timecode": 64, "before_eval_results": {"predictions": ["Laurel and Hardy", "Great British Bake Off", "Gary Havelock", "Lance Corporal", "Fiji", "Natty Bumppo", "John Hurt", "morocco", "Aleister Crowley", "Tom Snyder's", "\"Barefoot Bandit\"", "Ytterby", "sheep-like", "lithium", "Braves", "yokai", "Joan Crawford's", "1825", "argentina", "Vatican City", "Ascot", "Mark Twain", "Charlie Cairoli", "sunil gavaskar", "Zeitgeister", "the capitol", "William Caxton", "Neil Armstrong", "a cocktail", "a brownish-black fossil fuel", "Dutch", "the Reform Club", "amicus", "Saint Cecilia", "the Netherlands", "the Joker", "pistil", "The World as Will and Idea", "Thomas Cranmer", "the Mad Hatter", "Nick Clegg", "Virginia", "a blessing gesture used by the Jewish priests (kohanim) during the worship service", "the largest buttock", "Nikola Tesla", "adrian Edmondson", "Persian Empire", "the innermost digit of the forelimb", "Boyle\u2019s law", "J. S. Bach", "Bachelor of Science", "lamina dura", "July 2014", "the French CYCLADES project directed by Louis Pouzin", "Massachusetts", "Princess Jessica", "supply chain management", "Dr. Jennifer Arnold and husband Bill Klein,", "2nd Lt. John Auer,", "that Birnbaum had resigned \"on her own terms and own volition.\"", "Spmi", "Private Benjamin", "the Rhine & the Main", "Amber Heard"], "metric_results": {"EM": 0.421875, "QA-F1": 0.43854166666666666}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-1020", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-6566", "mrqa_triviaqa-validation-5536", "mrqa_triviaqa-validation-105", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-84", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-195", "mrqa_triviaqa-validation-4258", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-1949", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5009", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-1171", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-1001", "mrqa_newsqa-validation-1862", "mrqa_searchqa-validation-7466", "mrqa_hotpotqa-validation-652"], "SR": 0.421875, "CSR": 0.5240384615384616, "EFR": 0.972972972972973, "Overall": 0.7023710369022869}, {"timecode": 65, "before_eval_results": {"predictions": ["hemlock", "paul boyle", "rowevelt", "eyes", "sierra leone", "dennis hockney", "sierra leone", "p Preston", "sandown", "constellation of Scorpio", "Maine", "Coalbrookdale", "borgia", "the Periodic Table", "jimmy connors", "bread", "japan", "j Jakarta", "spike", "Nakajima", "the Isthmian Canal Commission (I.C.C.)", "1960", "feet", "apples", "lug", "paul crawford", "Hamelin", "Harold II", "Kuwait", "leicestershire", "100km", "green", "The Grapes of Wrath", "Coldplay", "pamphlets, posters, ballads", "Rugrats", "chancellor of the Exchequer", "bottle ring toss", "fat", "Austria", "\"Inspector\u2026 (Acorn Media)", "fool", "paul boyle", "Markus Aemilius Lepidus", "business", "tall", "stanley da ponte", "Nikita Khrushchev", "blue ivy", "high-Fructose Corn Syrup (HFCS)", "molecular structure of nucleic acids", "Schwarzenegger", "Wakanda", "lamina dura", "stand-up", "Robert L. Stone", "Haitian Revolution", "sierra leone", "to launch a group that will serve as an alternative to the Organization of American States.", "64", "Nagpur", "tuna tune-up Casserole", "tall girl", "Old English pyrige ( pear tree )"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5273841873706003}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.34782608695652173, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-922", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-1899", "mrqa_triviaqa-validation-4758", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-5514", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-5504", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-2057", "mrqa_triviaqa-validation-4521", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-3962", "mrqa_triviaqa-validation-4255", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6015", "mrqa_newsqa-validation-3226", "mrqa_newsqa-validation-2224", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-16126", "mrqa_searchqa-validation-884"], "SR": 0.421875, "CSR": 0.5224905303030303, "EFR": 1.0, "Overall": 0.707466856060606}, {"timecode": 66, "before_eval_results": {"predictions": ["Air NEXUS card", "hillsborough", "Buddhist", "Andrew Jackson", "The Bad Beginning", "rooney mikael braves", "Red sea", "red", "sesame", "grizzly bear", "jennifer antonley", "Swiss", "The Pilgrim's Progress", "duke", "terence Edward \" Terry\" Hall", "acetone", "San Francisco", "Paris", "sewing", "Atlas", "Flanagan", "anophthalmia", "jennifer antonio", "dolores", "peter Principle", "buggles", "Frank McCourt", "jacky hoganner", "marke keeler", "blancmange", "mark", "Louis- Stanislas-Xavier", "host", "\"Good Morning to All\"", "1976", "Pride & Prejudice", "arthur golding", "dna", "Revelation", "Mr. Brainwash", "calypso", "one-eyed", "phrenology", "mary Tudor", "hong Kong phooey", "an apple", "driver", "Joan Rivers", "Mr. Humphries", "h Katherine Mansfield Beauchamp", "heineken", "as the B - side of the `` Tramp '' single in 1987, and as its own single in 1988", "1996", "Ella Mitchell", "\"Wicked Twister\"", "Lerotholi Polytechnic Football Club", "3730 km", "mild to moderate depression", "Saturday", "green-card warriors", "Bering Sea", "Charlottetown", "Agatha Christie", "former Beatles"], "metric_results": {"EM": 0.5, "QA-F1": 0.5843563988095238}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.25, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.125, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-3268", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-4903", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-6164", "mrqa_triviaqa-validation-5135", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-5391", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-3734", "mrqa_triviaqa-validation-1067", "mrqa_naturalquestions-validation-1491", "mrqa_hotpotqa-validation-758", "mrqa_newsqa-validation-150", "mrqa_searchqa-validation-140"], "SR": 0.5, "CSR": 0.5221548507462687, "EFR": 1.0, "Overall": 0.7073997201492537}, {"timecode": 67, "before_eval_results": {"predictions": ["Yuri and Maria Panteleyvna Gorbachev", "kathleen w Winslet", "joseph marlowe", "Camino Franc\u00e9s", "fox", "linda coren Mitchell", "\"sound and light\"", "coffee", "tomato", "fred west", "wrought iron", "Columba", "peter paul", "1215", "1937", "13th", "michelle foot", "king george IV", "nahuatl", "Venice", "Calvors", "Massachusetts", "nikkei 225", "Nutbush", "robert shumann", "jape", "NASCAR", "Jordan", "linda evans", "llanberis", "Battle Marengo", "darshaan", "asthma", "Nicaragua", "louis crawford", "par-5", "oklahoma", "Jason Bourne", "Venus", "bauxite", "protein", "antelopes", "Nevada", "SW19", "Mizrahi Jews", "craig", "reclaim Our Streets", "british", "gower", "london", "Aquaman", "Justin Bieber", "2017 season", "eleven", "CD Castell\u00f3n", "Shaftesbury, Dorset", "Jan Kazimierz", "2.5 million", "Vivek Wadhwa,", "Wednesday.", "parody", "animal house", "Lake Victoria", "helicopters and unmanned aerial vehicles"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5453125000000001}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false], "QA-F1": [0.0, 0.5, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-565", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-6399", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-2021", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-5176", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-7545", "mrqa_triviaqa-validation-506", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3917", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-1443"], "SR": 0.453125, "CSR": 0.5211397058823529, "EFR": 1.0, "Overall": 0.7071966911764705}, {"timecode": 68, "before_eval_results": {"predictions": ["fort boyard", "Richard Seddon", "16", "1,200 Archers scripts,", "st james cephalonia", "top cat", "fotheringhay", "tungsten", "New Zealand", "fenn street school", "Kristiania", "South Pacific", "kurt hummel", "mozart", "thalia", "paddy mcinness", "woodstock", "mel Blanc", "chicago", "jack", "dog sport", "alfresco", "Sarajevo", "Hokkaido", "Norman Mailer", "david boyard", "florence", "apple", "braille", "PC", "stockholm", "george w", "Switzerland", "moose merkel", "pressure", "fort boyard", "daniel ostroff", "peter boyard", "dr ichak adizes", "1936", "honda", "st james", "Dunfermline", "cribbage", "midtown", "the Library of Congress", "quarter note", "osmium", "pear", "cunard", "elton john", "peptide bond", "William the Conqueror", "Aslan", "Gregory Carlton \" Greg\" Anthony", "\"Pete and Gladys\"", "Lowe's Companies, Inc.", "india", "Cash for Clunkers", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "nassau", "Maurice Jarre", "degaussing", "10 Years"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6692708333333333}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-4762", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-3851", "mrqa_triviaqa-validation-5077", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-1862", "mrqa_triviaqa-validation-5611", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-5498", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-536", "mrqa_triviaqa-validation-7144", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-7518", "mrqa_naturalquestions-validation-3016", "mrqa_hotpotqa-validation-3119", "mrqa_hotpotqa-validation-1129", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-3611"], "SR": 0.609375, "CSR": 0.5224184782608696, "EFR": 0.96, "Overall": 0.6994524456521739}, {"timecode": 69, "before_eval_results": {"predictions": ["victoria plum Brit", "Ronald Searle", "dennis taylor", "loki", "The Avengers", "ivied", "insulin", "Lilac", "Matt Kowalski", "laryngeal prominence", "Andes", "banshee", "Hawaii", "eutrophication", "heraldry", "good life", "japan", "spanish", "Sherlock Holmes", "Ida Noddack", "Rocky and Bullwinkle", "vindaloo", "South Africa", "Hep Stars", "mark Twain", "Holly Johnson", "salt", "khaki", "british", "joseph w", "dunfermline athletic", "9", "joseph caiaphas", "penrhyn", "new south Wales", "African violet", "ourselves alone", "James Dean", "eva Herzigov\u00e1", "drizzle", "chiropractic", "wicker man", "stieg Larsson", "james devlin", "Orecchiette", "boston university", "hypertext", "Croatia", "cete", "greyfriars school", "Mr. chips", "John Locke", "seattle in the semi-finals", "Matt Monro", "comic", "Disha Patani", "USS \"Enterprise\"", "Charles Lock", "Kenneth Cole", "kryptonite", "solar eclipse", "Cher", "the Black Sea", "Crank Yankers"], "metric_results": {"EM": 0.609375, "QA-F1": 0.671875}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2531", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-1801", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-4485", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-1322", "mrqa_triviaqa-validation-6999", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-335", "mrqa_triviaqa-validation-4829", "mrqa_triviaqa-validation-6456", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-4581", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-7614", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-110", "mrqa_searchqa-validation-9522", "mrqa_searchqa-validation-14235"], "SR": 0.609375, "CSR": 0.5236607142857144, "EFR": 1.0, "Overall": 0.7077008928571429}, {"timecode": 70, "UKR": 0.693359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3054", "mrqa_hotpotqa-validation-306", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3863", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-242", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3680", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-975", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2364", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3910", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4791", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1461", "mrqa_squad-validation-147", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2564", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3473", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3923", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5884", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6670", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-6981", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7083", "mrqa_squad-validation-7094", "mrqa_squad-validation-7339", "mrqa_squad-validation-78", "mrqa_squad-validation-7868", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-9002", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9293", "mrqa_squad-validation-9300", "mrqa_squad-validation-9344", "mrqa_squad-validation-9411", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1514", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1555", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1801", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-196", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4615", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5117", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5949", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6285", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.8203125, "KG": 0.4984375, "before_eval_results": {"predictions": ["the Philippines", "silurian", "ricky gervais", "snooty", "vermouth", "sixth", "perry pear", "lyon", "gold", "Tina Turner", "Sparks", "nissan", "washing", "mexico", "Benjamin Britten", "Eric Coates", "st Pancras", "beer", "Toronto", "cevennes", "lady Gaga", "phil Glenister", "carbon copy", "1979", "Donald Trump", "volume", "Tomorrow Never Dies", "tea", "sandra getz", "moonee ponds,", "bullfighting", "Autobahn", "Kiss Me Kate", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Hindenburg", "sandff Graff", "sandra", "Tangled", "spanish", "Morrissey", "red stockings", "ooperatiou", "smallpox", "old lady from Candide", "harvard city", "violin", "nipples", "lady Thatcher", "Temple of Artemis", "abietic acid", "Achille Lauro", "Frank Langella", "anion", "Rose Stagg", "Hilo", "Objectivism", "16,116", "CEO", "Daniel Radcliffe", "Eleven people", "Harold M. Ickes", "bone marrow", "Smilla", "France"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6258184523809524}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-4689", "mrqa_triviaqa-validation-5348", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-6736", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-5521", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-3327", "mrqa_triviaqa-validation-2633", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-2648", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-1591", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-2372", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-2351", "mrqa_hotpotqa-validation-4382", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-335", "mrqa_searchqa-validation-15744"], "SR": 0.53125, "CSR": 0.5237676056338028, "EFR": 1.0, "Overall": 0.7071753961267606}, {"timecode": 71, "before_eval_results": {"predictions": ["31536000 seconds", "Suez canal", "robert boyer", "spain", "Paris", "john poulson", "breadfruit", "1963", "hughmannsthal", "st james", "sandi Tok svig", "james hardouin-Mansart", "robert davies", "bette davis", "fringe", "spitze", "Arabah", "bessie shaw", "ut\u00f8ya", "lesley Garrett", "Ty Hardin", "b\u00e4umer", "2240 gallons", "sandstone", "Bristol Aeroplane", "charliesheen", "anita Brookner", "keyhole", "endometriosis", "william lite", "d", "eight", "Pizza Express", "Lilo & Stitch", "Hugh Quarshie", "billie holiday", "Hindi", "estimate", "Eric Morley", "sandstone", "Assault on Precinct 13", "daniel day-Lewis", "steam engines", "Yemen", "antelope", "relativistic mass", "james", "muskets", "bajan", "barley", "hoagland", "Jonny Buckland", "Minti Gorne", "an African - American woman in her early forties", "teen volleyball", "Milk Barn", "nursery rhyme", "Russian air force,", "President Obama and Britain's Prince Charles", "byproducts", "Captains Courageous", "I", "kayak", "Wisconsin"], "metric_results": {"EM": 0.34375, "QA-F1": 0.43229166666666663}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.26666666666666666, 0.4, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-4932", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-64", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-2746", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3511", "mrqa_triviaqa-validation-3506", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-4467", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3639", "mrqa_triviaqa-validation-4048", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-3499", "mrqa_triviaqa-validation-4979", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7629", "mrqa_triviaqa-validation-875", "mrqa_triviaqa-validation-854", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-10194", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5346", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1051", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-3830"], "SR": 0.34375, "CSR": 0.5212673611111112, "EFR": 1.0, "Overall": 0.7066753472222222}, {"timecode": 72, "before_eval_results": {"predictions": ["Cambridge", "eye lenses", "Poland", "washington", "apple", "high jump", "horizontal desire", "Hungary", "port Talbot", "panurge", "dancewear", "Not So Much a Programme", "Sydney", "charlie chaplin", "smell", "russell", "judy holliday", "cupressaceae", "mary connelly", "geyser park", "blue ivy", "prince andrew", "Israel", "blackburn rovers", "1943", "Elizabeth Taylor", "daimler", "daniel", "herzigovina", "james hargreaves", "antonia pinter", "peter stuyvesant", "South Africa", "libero armani", "charf\u00fcrstendamm", "mark Twain", "surfer", "ever decreasing circles", "quito", "Sensurround", "grayson", "sandown", "goat", "lady", "Ottoman Empire", "bb", "treacherous", "mental floss", "Kajagoogoo", "Carly Simon", "Robin Hood Airport", "March 12, 2013", "What's Going On", "into the intermembrane space", "Ronnie Schell", "La Familia Michoacana", "thermal shielding material", "in the neighboring country of Djibouti,", "cervical cancer", "along the equator between South America and Africa.", "Warsaw", "City Slickers", "exes", "Richa Sharma"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6071180555555555}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.8, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-1196", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-7446", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-5510", "mrqa_triviaqa-validation-4025", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-4148", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-4215", "mrqa_triviaqa-validation-249", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-4498", "mrqa_triviaqa-validation-3836", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-180", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-188", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-1370", "mrqa_newsqa-validation-2784", "mrqa_searchqa-validation-1212"], "SR": 0.546875, "CSR": 0.5216181506849316, "EFR": 1.0, "Overall": 0.7067455051369863}, {"timecode": 73, "before_eval_results": {"predictions": ["flesh and the Devil", "c\u00e9vennes", "lilo and stitch", "Tacitus", "best", "loki", "bagram", "pink", "charlie chaplin", "ostrich", "ireland", "mozambique", "silk warp", "Swaziland", "cartoonist, author, art critic and stage designer", "jack the Ripper", "jaws", "dodo bird", "Imola", "albus white", "brazil", "Thailand", "america", "worcester cathedral", "curvature", "Superman", "wales", "christwick", "michael square garden", "The Equals", "baffin", "woodstock", "molybdenum", "plate tectonic", "Hungary", "apollon", "Matterhorn", "gold hallmarks", "tide-wise", "genesis", "trumpet", "South Carolina", "ourselves alone", "james chadwick", "coffee house", "Apocalypse Now", "pilgrimage", "volkswagen", "althorp", "Pyrenees mountains", "noah", "Steveston Outdoor pool in Richmond, BC", "October 1, 2015", "Flamborough Head", "25 November 2015", "Jesper Myrfors", "Vancouver", "Rod Blagojevich,", "Mary Procidano,", "opium", "Louis XIV", "Minnesota", "quid", "Ugly Betty"], "metric_results": {"EM": 0.59375, "QA-F1": 0.684375}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-80", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-1967", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-606", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-4216", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-3373", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-5018", "mrqa_newsqa-validation-3632", "mrqa_searchqa-validation-7502"], "SR": 0.59375, "CSR": 0.5225929054054055, "EFR": 1.0, "Overall": 0.706940456081081}, {"timecode": 74, "before_eval_results": {"predictions": ["jamaica", "get well soon", "jordan bennard", "rudolph", "halloween", "Compundyne", "Samson", "copenhagen", "selene", "western Caribbean", "bathtub curve", "john nNapier", "Japanese silvergrass", "macbeth", "eton", "geomagnetic", "Diego Garcia", "lighthouse keeper", "ritchie jennison", "robert boyle", "phobos", "Sphinx", "madison", "william Morris", "pennsylvania state university", "Father Brown", "henry Ford", "jet", "dihydrogen monoxide", "brian wilson", "rudolph", "Alison Krauss", "aprigs", "four", "neurons", "Poland", "banjo", "cricketer", "time bandits", "The Hague", "One Foot in the Grave", "rudolph", "copper", "george offenbach", "speed camera", "self-actualized", "blue", "passport", "florence", "Fancy Dress Shop", "jabba the Hutt", "the 15th century", "1937", "turkey", "business", "boxer", "The Handmaid's Tale", "Stanford University", "Sgt. Barbara Jones", "because the Indians were gathering information about the rebels to give to the Colombian military.", "George Babbitt", "Emanuel Swedenborg", "nod", "Norway"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5921875}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-4919", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-1912", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-4433", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-3070", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-4603", "mrqa_triviaqa-validation-53", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-5806", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-957", "mrqa_naturalquestions-validation-2830", "mrqa_hotpotqa-validation-2388", "mrqa_newsqa-validation-2236"], "SR": 0.515625, "CSR": 0.5225, "EFR": 0.967741935483871, "Overall": 0.7004702620967742}, {"timecode": 75, "before_eval_results": {"predictions": ["rugby", "hyperbole", "North by Northwest", "danelaw", "mahatma Gandhi", "for Gallantry", "senators", "colette", "willow", "eurozone", "Separate Tables", "harry Spencer", "Ulysses S. Grant", "1929", "aviva", "Antarctica", "hurt locker", "Douglas MacArthur", "harry edward", "zager and evans", "c\u00e9vennes", "genesis", "sirhan Sirhan", "handball", "jeSuisCharlie", "judy gumm", "dark blood", "lowestoft", "washington", "evolution", "lulu", "erinyes", "faggots", "mount Godwin Austen", "Angus Deayton", "david bowie", "Chester", "tchaikovsky", "faversham", "Jimmy Knapp", "arisios", "to the north", "anton\u00e9 de force", "butcher", "edward Woodward", "priesthood", "violins", "charlie", "eucalyptus", "1883", "harry of free Enterprise", "3000 BC", "A lacteal", "Renishaw Hall, Derbyshire, England", "Jefferson Memorial", "96", "New Orleans, Louisiana", "JBS Swift Beef Company, of Greeley, Colorado,", "Silicon Valley.", "10-person", "vlaimir Ilyich Lenin", "beta blockers", "Yoko", "oracle"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7703993055555556}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5856", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-7341", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-4075", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-5109", "mrqa_naturalquestions-validation-10408", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-4936"], "SR": 0.703125, "CSR": 0.524876644736842, "EFR": 0.9473684210526315, "Overall": 0.6968708881578947}, {"timecode": 76, "before_eval_results": {"predictions": ["andra day o'Connor", "propeller", "joe louis", "thirteen", "peter hurd", "Louvre", "feminist", "potatoes", "grayson and gromit", "anorthos", "Mozambique", "blue Nile", "troy", "\"Timber!\"", "reptiles", "actual sales data", "coconut", "Imaginext", "indonesia", "Lord Bill Astor", "Finland", "Making the Band", "pennies", "Colorado", "stereoscopic", "time", "Library of Congress", "Hawaii", "austere trombone", "Georgetown University", "kidney maladies", "intavano", "Colin Colin Kaepernick", "madison county", "kennebunkport", "A Room with a View", "an eye", "Africa", "Ingenue", "Notre-Dame de Paris", "scientific review", "grief", "paul mcc McCartney", "Iberian peninsula", "bionic", "trip", "baccarat", "Drums Along the Mohawk", "Wallis Warfield Simpson", "grapevine", "Coco Chanel", "December 14, 2017", "Virginia Dare", "a loanword of the Visigothic word guma `` man", "chronicles of indonesia", "panama canal", "Will Smith", "1950", "Undecided", "An aircraft", "Britain and France", "Alwin Landry's supply vessel Damon Bankston", "federal ocean planning.", "funchal"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5289434523809524}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.4, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-16515", "mrqa_searchqa-validation-11059", "mrqa_searchqa-validation-5372", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-5110", "mrqa_searchqa-validation-8417", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-16759", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12916", "mrqa_searchqa-validation-737", "mrqa_searchqa-validation-5637", "mrqa_searchqa-validation-9899", "mrqa_searchqa-validation-4056", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-13006", "mrqa_searchqa-validation-12225", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-5273", "mrqa_searchqa-validation-15174", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-3019", "mrqa_triviaqa-validation-5219", "mrqa_hotpotqa-validation-2065", "mrqa_hotpotqa-validation-2730", "mrqa_newsqa-validation-660", "mrqa_newsqa-validation-4169"], "SR": 0.4375, "CSR": 0.5237418831168832, "EFR": 1.0, "Overall": 0.7071702516233767}, {"timecode": 77, "before_eval_results": {"predictions": ["Graceland", "Bob Fosse", "S", "mexico", "Wynton Marsalis", "volleyball", "Havana", "Edwin Hubble", "Einstein", "Lhasa", "U.S. Census Bureau", "New Kids on the Block", "Manila Bay", "Lady Chatterley", "molasses", "Hard Knock Life", "a crumpet", "Douglas MacArthur", "Fred Thompson", "Sappho", "the Netherlands", "Texas", "Donald Trump", "the Hippocratic Oath", "the Taliban", "Solidarity", "Kookaburra", "the Hastings", "examination of one's own thought and feeling", "Craftsman", "delete", "W.H. Auden", "\"Johnny B. Goode\"", "Cal Ripken", "diaphragm", "Marquis de Sade", "Louis Comfort Tiffany", "a tornado", "the joker", "New Zealand", "a glove", "Jutland", "Kindergarten", "feet", "Titanic", "San Francisco", "Gulliver's Travels", "a carriage", "Billy Bathgate", "Richmond", "steel", "1988", "Cheryl Campbell", "Ohio newspaper", "Joe Brown", "Funchal", "Virgil", "Adam Sandler", "WANH", "Avoca Lodge", "Daniel Cain,", "A staff sergeant", "\"procedure on her heart,\"", "55th district"], "metric_results": {"EM": 0.625, "QA-F1": 0.6912202380952381}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14018", "mrqa_searchqa-validation-2395", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-16708", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-1687", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-1963", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14160", "mrqa_searchqa-validation-11728", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-3821", "mrqa_searchqa-validation-388", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-12470", "mrqa_searchqa-validation-10670", "mrqa_naturalquestions-validation-6665", "mrqa_hotpotqa-validation-2238", "mrqa_hotpotqa-validation-5500", "mrqa_newsqa-validation-563", "mrqa_newsqa-validation-2547", "mrqa_hotpotqa-validation-5006"], "SR": 0.625, "CSR": 0.5250400641025641, "EFR": 0.9583333333333334, "Overall": 0.6990965544871794}, {"timecode": 78, "before_eval_results": {"predictions": ["Flint, Michigan.", "a president who understands the world today, the future we seek and the change we need.", "about 5:20 p.m. at Terminal C", "Former Mobile County Circuit Judge Herman Thomas", "\"Top Gun\"", "Daniel Radcliffe", "Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "Tennessee", "Intensifying", "the death of Prince George's County police Cpl. Richard Findley,", "Dubai", "gun", "CNN's Larry King", "talk show queen Oprah Winfrey.", "bankruptcies", "repression and dire economic circumstances.", "African National Congress", "1.2 million people.", "Haiti.", "police", "Zuma", "your secrets", "US Airways Flight 1549", "the estate with its 18th-century sights, sounds, and scents.", "forcing them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "A lock break", "The Rosie Show", "Between 1,000 and 2,000", "Tuesday in Los Angeles.", "Moscow, to offer a diverse range of reports.", "6-2 6-1", "a convenient location is another factor that makes it more likely that someone will actually show up for an annual mammogram appointment.", "Diego Milito's", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "\" Maria\"", "the Democratic VP candidate", "to block the release of photos showing prisoners allegedly being abused by U.S. personnel in Iraq and Afghanistan,", "Friday.", "a judge to order the pop star\\'s estate to pay him a monthly allowance,", "consumer confidence", "Afghanistan and India", "Hugo Chavez", "Expedia", "coastal development destroys 20,000 acres of estuaries and near-coast fish habitat.", "bronze", "two", "JBS Swift Beef Company, of Greeley, Colorado,", "as many as 250,000 unprotected civilians", "state senators", "Friday,", "Jaime Andrade", "headdresses", "In cases of the federal death penalty, the power to seek the death penalty rests with the Attorney General", "Sons of Liberty", "Arkansas", "Adam Smith", "troposphere", "Manchester Victoria station", "communist", "1896", "the ceiling", "Jonathan Swift", "John Molson", "Sir Adrian Boult"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5789440822569019}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.0, 0.9565217391304348, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.36363636363636365, 0.5, 1.0, 0.0, 0.4, 0.23529411764705882, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.23076923076923078, 0.0, 0.7058823529411764, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.1, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-4205", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-368", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-642", "mrqa_triviaqa-validation-6410", "mrqa_hotpotqa-validation-375", "mrqa_searchqa-validation-3681", "mrqa_searchqa-validation-15735", "mrqa_triviaqa-validation-5099"], "SR": 0.4375, "CSR": 0.5239319620253164, "EFR": 0.9444444444444444, "Overall": 0.6960971562939522}, {"timecode": 79, "before_eval_results": {"predictions": ["an Italian and six Africans", "Daniel Radcliffe", "the remaining rebel strongholds in the north of Sri Lanka,", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Samoa", "Cartoon Network", "in the Iraq's autonomous region of Kurdistan.", "Adam Yahiye Gadahn,", "mental health and recovery.", "75.", "co-wrote", "2005.", "12", "Phil Spector", "Iran", "attempted robbery", "70,000", "severe flooding", "56,", "frozen world located in the Gaslight Theater.", "AbdulMutallab", "in a medical setting", "Saturday's Hungarian Grand Prix.", "Sub-Saharan Africa", "Manny Pacquiao", "Aung San Suu Kyi", "Aniston, Demi Moore and Alicia Keys", "hand-painted Swedish wooden clogs", "Michael Schumacher", "Austin Wuennenberg", "1983", "the U.S. Holocaust Memorial Museum,", "the college campus.", "Elena Kagan", "\"fusion teams,\"", "misdemeanor", "Thursday", "the man facing up, with his arms out to the side.", "golf", "Israel", "prostate cancer,", "the Iraqi and U.S. soldiers were attacked by small-arms, machine-gun and RPG fire", "$1.45 billion", "people look at the content of the speech, not just the delivery.", "eco videos", "for strategy, plans and policy on the Army staff.", "walk", "a city of romance, of incredible architecture and history.", "regulators in the agency's Colorado office", "Chancellor Angela Merkel", "\"Nothing But Love\"", "Games", "Audrey II", "the benefits of the US privacy Act to Europeans and gives them access to US courts", "troposphere", "arthur ashe", "Hippos", "2004", "Tim \"Ripper\" Owens", "Brad Silberling", "Mother Vineyard", "Mars", "the Capitol", "Out - With"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7725823443412454}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 0.9523809523809523, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.4, 0.14285714285714288, 1.0, 0.9411764705882353, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.5263157894736842, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2835", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-1706", "mrqa_naturalquestions-validation-7987", "mrqa_triviaqa-validation-4441", "mrqa_searchqa-validation-15009", "mrqa_naturalquestions-validation-582"], "SR": 0.671875, "CSR": 0.52578125, "EFR": 1.0, "Overall": 0.7075781250000001}, {"timecode": 80, "UKR": 0.666015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4547", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-512", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11854", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-15841", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3141", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4918", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6120", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-7075", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7798", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8799", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-10210", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3861", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5679", "mrqa_squad-validation-5840", "mrqa_squad-validation-5859", "mrqa_squad-validation-5964", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-6925", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9270", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9575", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1750", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4749", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5646", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6311", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7035", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-984"], "OKR": 0.8359375, "KG": 0.5078125, "before_eval_results": {"predictions": ["Tyler \"Ty\" Mendoza", "\" Anne of Green Gables\"", "four", "professional footballer", "five", "Julie Taymor", "Greg Anthony", "Drifting", "the eastern section of the country", "Rebirth", "a personalized certificate, an official pin, medallion, and/or a congratulatory letter", "Harpe brothers", "Bedknobs and Broomsticks", "Martin \"Marty\" McCann", "Yubin, Yeeun", "Herbert Ross", "Elena Verdugo", "melodic hard rock", "9 February 1971", "Chancellor of Austria", "Taylor Swift", "SARS", "the son of writer William F. Buckley Jr.", "1345 to 1377", "\"Cs\u00e1sz\u00e1ri \u00e9s Kir\u00e1lyi Hadsereg\"", "Noel Gallagher", "India Today", "North Dakota", "2006", "\"Histoires ou contes du temps pass\u00e9\"", "Charles Reed Bishop", "mixed martial arts", "Yarrow and Stookey", "Mathieu Kassovitz", "Dame Eleen June Atkins", "Summerlin, Nevada", "Jean- Marc Vall\u00e9e", "Klasky Csupo", "1950s", "Prussia", "Newfoundland and Labrador", "Tom Kartsotis", "Knowlton", "shock cavalry", "Manhattan", "Professor Frederick Lindemann, Baron Cherwell", "dementia", "Hugh Hefner", "seven", "July 11, 2016", "\"Vision of Love\"", "5 - 7 teams", "40 %", "March 1, 2018", "trumpet", "pink Panther", "G\u00e9rard Depardieu", "is a businessman, team owner, radio-show host and author.", "dozens", "more than 2.5 million copies,", "root", "Emperor Maximillian", "Lake Michigan", "hot Chocolate"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6100446428571429}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-1975", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-3650", "mrqa_hotpotqa-validation-3898", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-399", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-4131", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-8301", "mrqa_naturalquestions-validation-190", "mrqa_triviaqa-validation-1295", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-863", "mrqa_searchqa-validation-6921", "mrqa_triviaqa-validation-85"], "SR": 0.515625, "CSR": 0.5256558641975309, "EFR": 1.0, "Overall": 0.7070842978395062}, {"timecode": 81, "before_eval_results": {"predictions": ["views", "The White Shadow", "Hungary", "HIV/AIDS", "Nepal", "second", "Sanjaya", "Fauvism", "Dresden", "Turkish", "The Shirley Temple Story", "flavor Flav", "Mike Nichols", "backcountry", "blue blood", "acetylene", "32", "Harriet the Spy", "Wheaton Iceman", "a splash of cranberry juice", "Amsterdam", "Grover Cleveland", "Clyde", "James Naismith", "Harold Godwinson", "North Carolina", "Job", "1969", "The Greatest", "pickles", "Stand by Me", "lead", "Nokia Company Detaiils", "Bernard Malamud", "Cyprus", "top run", "Neil Diamond", "Munich", "Babe Ruth", "wildebeest", "Sicilian pizza", "Pirates of the Caribbean", "Atlantic bluefin tuna", "Arts and Crafts", "lm", "Subclue 2", "Uvula", "Biloxi", "James Flint", "Robots", "A hope chest", "communication modalities following acquired brain injury", "Robber baron", "NFL owners", "Jane Seymour", "Guy", "george bernard shaw", "Tom Kitt", "Yewell Tompkins", "Wolfgang Amadeus Mozart", "House-passed bill that eliminates the 3% withholding requirement for government contractors --", "seven", "16", "has some limits, however."], "metric_results": {"EM": 0.609375, "QA-F1": 0.6583333333333333}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-6682", "mrqa_searchqa-validation-6307", "mrqa_searchqa-validation-7539", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-14093", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-1314", "mrqa_searchqa-validation-2623", "mrqa_searchqa-validation-7545", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-15450", "mrqa_searchqa-validation-8240", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-16773", "mrqa_searchqa-validation-7174", "mrqa_naturalquestions-validation-3840", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-4597", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4028"], "SR": 0.609375, "CSR": 0.5266768292682926, "EFR": 1.0, "Overall": 0.7072884908536585}, {"timecode": 82, "before_eval_results": {"predictions": ["Jaws 2", "the leg", "Ovid", "Coal mining", "a knight", "Tudor", "Australia", "sugar cane juice", "sheep", "Washington for Jobs and Freedom", "lily", "Hammurabi", "Isle of Wight", "gung ho", "Dale Earnhardt", "Johns Hopkins", "Hashemite Kingdom of Jordan", "Tiger Woods", "North Africa", "Celesta", "Stephen Hawking", "James Madison", "X-Ray", "Disturbia", "Michael Moore", "The Indianapolis 500", "I, Daniel Blake", "tap chorine", "a cross moline", "Johannesburg", "carbon", "Philistines", "Deep Brain Stimulation", "Louis Chevy", "Morocco", "I melt for no one", "Hieronymus Bosch", "Neil Diamond", "Cardinal Richelieu", "Malaysia", "bionic", "Wherein I'll catch the conscience of the King", "Lance Armstrong", "a chicken Fried Steak", "Edith Wharton", "the Berlin Wall", "Uranus", "The Sopranos", "a telephone operator", "a bonnet", "Henry Moore", "wintertime", "the Great Crash", "Elena Anaya", "Brisbane Road", "China Expeditionary Forces", "austria", "Girl Meets World", "three", "\"Pour le M\u00e9rite\"", "The meter reader who led authorities last week to remains believed to be those of Caylee Anthony", "New York-based Human Rights Watch", "used", "Don Draper"], "metric_results": {"EM": 0.5625, "QA-F1": 0.643092757936508}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-734", "mrqa_searchqa-validation-7268", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-802", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-15533", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-14165", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-13445", "mrqa_searchqa-validation-6865", "mrqa_searchqa-validation-14350", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-319", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8837", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-5807", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2964"], "SR": 0.5625, "CSR": 0.5271084337349398, "EFR": 0.9642857142857143, "Overall": 0.7002319546041308}, {"timecode": 83, "before_eval_results": {"predictions": ["Atlanta", "Dmitri Mendeleev", "Calligraphy", "Duke Ellington", "Maria Sharapova", "Chile", "glow", "John Waters", "Aristophanes", "the Americans with Disabilities Act", "freelance", "konishiki", "Thurman Munson", "a barrel", "Chippewa", "Rooster Cogburn", "5", "Richard Burton", "transmission", "Meringue", "the Dying Swan", "Big Bang", "winter", "Alyssa Milano", "Tahiti", "Herbert Hoover", "Keith Urban", "an isosceles triangle", "the Jinx", "Neil Armstrong", "the Netherlands", "Kelly Clarkson", "Michael Douglas", "an aquiline nose", "Troy weight", "Neil Simon", "Candidate", "trespasser", "Ronald Reagan", "Patrick Henry", "the light bulb", "the Cold War", "viola", "ostrich", "I Love Rock 'n'roll", "the American Mind", "America", "Ziploc", "Hannibal", "Eva D. Bowles", "Beethoven", "Gene Barry", "a heart rate that exceeds the normal resting rate", "a 1993 American comedy - drama film directed by Fred Schepisi", "the American Civil War", "mexico", "Denise Richards", "India Today", "Distinguished Service Cross", "Genderqueer", "raping and murdering a woman in Missouri.", "Jeanne Tripplehorn\\'s", "Zelaya and Roberto Micheletti,", "New York Islanders"], "metric_results": {"EM": 0.703125, "QA-F1": 0.772172619047619}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4451", "mrqa_searchqa-validation-11935", "mrqa_searchqa-validation-9367", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-7651", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-9041", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-15147", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-16490", "mrqa_searchqa-validation-1448", "mrqa_searchqa-validation-3237", "mrqa_searchqa-validation-6308", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-4354", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-435"], "SR": 0.703125, "CSR": 0.5292038690476191, "EFR": 0.9473684210526315, "Overall": 0.6972675830200501}, {"timecode": 84, "before_eval_results": {"predictions": ["Hoffmann", "Ford", "dextrin", "Grover\\'s Corner", "President Lincoln\\'s", "topaz", "Universal City", "surrender", "phosphates", "subtraction", "Harpy", "New York City", "numerals", "fur", "Titan", "Crossword Clue", "quick picks", "Batista", "Glacier Bay", "makrama", "Toy Story", "fight", "the Ark of the Covenant", "the Nivernais", "Granite", "the Emperor", "dipping square slices of homemade ice cream", "a dove", "GALCIT", "Francis Scott Key", "Eminem", "Tarzan of the Apes", "Diebold", "cheese", "Puncak Jaya", "Queen Latifah", "the Liberty Bell", "anchovy", "a saint", "Clarence Thomas", "the day of Mars", "nacreous", "whimper", "Prison Break", "Iberia", "the ceiling", "Go-Kart", "Kilimanjaro", "koala", "the Great Circus", "Extradition", "1979", "Lou LaRue", "Allan Holdsworth", "pasta carbonara", "finger", "Robert Schumann", "Pacific Place", "1941", "yellow fever is transmitted by mosquitoes", "stay on track and get me through prison,\"", "Polo", "Friday,", "2009"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5338541666666666}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, true, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-10313", "mrqa_searchqa-validation-9264", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-3217", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-14803", "mrqa_searchqa-validation-12869", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-12518", "mrqa_searchqa-validation-13886", "mrqa_searchqa-validation-3882", "mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-828", "mrqa_searchqa-validation-9830", "mrqa_searchqa-validation-4630", "mrqa_searchqa-validation-8767", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-7275", "mrqa_searchqa-validation-14709", "mrqa_searchqa-validation-10541", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-5264", "mrqa_triviaqa-validation-7214", "mrqa_hotpotqa-validation-3149", "mrqa_newsqa-validation-1008", "mrqa_naturalquestions-validation-1856"], "SR": 0.46875, "CSR": 0.5284926470588236, "EFR": 0.9705882352941176, "Overall": 0.7017693014705882}, {"timecode": 85, "before_eval_results": {"predictions": ["15", "cancer", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "Picasso's muse and mistress, Marie-Therese Walter.", "a motor scooter", "iTunes", "supermodel", "South Africa", "Missouri.", "Anthony Chambers", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "in Hong Kong's Victoria Harbor", "at least 13", "2,073 immigration detainees had \"medical escorts\" for deportation since 2003.", "10 a.m.", "acid attack by a spurned suitor.", "Bowie", "\"The Kirchners have been weakened by this latest economic crisis,\" said Robert Pastor, who was a Latin America national security adviser for former President Carter.", "a number of calls,", "summer", "Tuesday afternoon.", "peppermint oil", "Molotov cocktails, rocks and glass.", "Biden, Sen. Evan Bayh of Indiana and Virginia Gov. Tim Kaine", "April.", "2008", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "three", "the L'Aquila earthquake, which killed nearly 300 people and devastated the city when it struck last year,", "88", "Cash for Clunkers", "$249", "cancerous tumor.", "American", "Piers Morgan Tonight", "several weeks,", "$1.4 million,", "next year", "the death of a pregnant soldier whose body was found Saturday morning in a hotel, police said.", "the 11th year in a row.", "will look at how the universe formed by analyzing particle collisions.", "Saturn owners", "dead at her north London home July 23.", "tuatara", "Alfredo Astiz,", "drug cartels", "some of the Awa", "two remaining crew members", "Sabina Guzzanti", "more than 4,000", "Robert Gates", "the national flag of the United States", "Fiona Hampton", "to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "151", "Gaston Leroux", "Volkswagen", "\u00c6thelwald Moll", "Lufthansa heist", "white and orange", "epiphyte", "Re- Animator", "a stride", "Girls' Generation"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6532396541152345}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.33333333333333337, 1.0, 0.5714285714285715, 0.6666666666666666, 0.057142857142857134, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.19047619047619044, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.9473684210526316, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.1818181818181818, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3186", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-164", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-3537", "mrqa_searchqa-validation-9547"], "SR": 0.546875, "CSR": 0.5287063953488372, "EFR": 1.0, "Overall": 0.7076944040697675}, {"timecode": 86, "before_eval_results": {"predictions": ["cement (or concrete)", "the Cayman Islands", "orsche", "coax", "haiku", "waive", "China", "loverly", "economics", "Graceland", "funnel", "Beverly Hills", "Irish Coffee", "a live young chicken", "electric cars", "Isaac Newton", "Billy Budd", "John Brown", "Communist", "Gene Krupa", "\"For the Love of God\"", "13", "Smashing Pumpkins", "cruller", "I", "Ma Barker", "Northanger Abbey", "Wyatt Earp", "Star Trek", "Mensa", "febreze", "Portrait", "mutton", "Philip Seymour Hoffman", "an idea that we don't like him", "Wayne Gretzky", "amu", "Michael Irvin", "Gap", "salt", "the Tower of London", "Arbor Day", "Westinghouse Electric", "a salad Dressing", "The Fugitive", "Sisyphus", "Java", "Juan Ponce", "bioluminescence", "Rococo", "the First Barbary War", "Pakistan", "961", "Linda Davis", "a giant frog", "Northumberland", "Louis XVI", "July 16, 1971", "pastels and oil painting", "12", "monarchy.", "Africa", "Donald Trump", "Johnny cage"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6833333333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16965", "mrqa_searchqa-validation-8424", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-5358", "mrqa_searchqa-validation-12385", "mrqa_searchqa-validation-6460", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-12090", "mrqa_searchqa-validation-14681", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-6245", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-13204", "mrqa_searchqa-validation-2161", "mrqa_searchqa-validation-5579", "mrqa_searchqa-validation-5867", "mrqa_triviaqa-validation-6116", "mrqa_hotpotqa-validation-680", "mrqa_hotpotqa-validation-5667", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1586", "mrqa_hotpotqa-validation-4514"], "SR": 0.640625, "CSR": 0.529992816091954, "EFR": 1.0, "Overall": 0.7079516882183908}, {"timecode": 87, "before_eval_results": {"predictions": ["Washington, Jay and Franklin", "no more than 4.25 inches ( 108 mm )", "the external genitalia", "prosperity", "eleven", "the third ventricle", "Cody Fern", "the most", "in 2007 and 2008", "New York City", "Schadenfreude", "Indian Standard Time", "a Nativity scene", "Johannes Gutenberg", "The Mecca", "Rocky Dzidzornu", "Jennifer Grey", "Experimental neuropsychology", "Yosemite National Park", "April 3, 1973", "John C. Reilly", "administrative supervision over all courts and the personnel thereof", "1997", "Emma Watson", "near Flamborough Head", "The Way You Move", "the President of India", "John J. Flanagan", "nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "the House of Representatives", "Session Initiation Protocol", "bowel obstruction, short bowel syndrome, gastroschisis, prolonged diarrhea regardless of its cause, high - output fistula, very severe Crohn's disease or ulcerative colitis, and certain pediatric GI disorders", "presbyters / bishops", "Tessa Peake - Jones", "Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie, Jennifer O'Neill as Hermie's mysterious love interest, and Katherine Allentuck", "September 19 - 22, 2017", "the 2001 -- 2002 season", "1773", "Randy VanWarmer", "senators", "Teri Garr", "the producers, businesses, and workers of the import - competing sector in the country from foreign competitors", "13", "10.5 %", "Gene MacLellan", "the length of their main span", "Brad Dourif", "the previous season", "Sanchez Navarro", "in 2009", "23 September 1889", "My Fair Lady", "Armageddon", "Thailand", "\u00c6thelstan", "Eugene", "The Highwaymen", "Israel", "The judge", "abduction of minors.", "Dag Hammarskjld", "Erin Go Bragh", "Art Garfunkel", "three"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6619412586929034}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true], "QA-F1": [0.4210526315789474, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9189189189189189, 0.0, 1.0, 0.2631578947368421, 0.4, 1.0, 0.14814814814814814, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962962962962963, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-6157", "mrqa_naturalquestions-validation-8061", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-7138", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8439", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-687", "mrqa_newsqa-validation-3595", "mrqa_searchqa-validation-3012"], "SR": 0.578125, "CSR": 0.5305397727272727, "EFR": 0.9259259259259259, "Overall": 0.6932462647306397}, {"timecode": 88, "before_eval_results": {"predictions": ["2013", "the Triple Alliance of Germany, Austria - Hungary, and Italy", "John von Neumann", "Joanne Wheatley", "Alicia Vikander", "the Federated States of Micronesia and the Indonesia", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "Mickey Rourke", "4.37 light - years ( 1.34 pc )", "eight hours ( UTC \u2212 08 : 00 )", "John Joseph Patrick Ryan", "36 months old", "1988", "Malayalam", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "the Old Testament", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "Krypton", "DJ Twist and the Fly Girls", "a crust of mashed potato", "a central place in Christian eschatology", "digestive systems", "the employer", "publishing", "1974", "from spontaneous fission is insufficient for a reliable startup, or after prolonged shutdown periods", "Nancy Jean Cartwright", "Germany", "total cost", "Justin Timberlake", "1978", "William Whewell", "1956", "Symphony No. 40 in G minor", "Majandra Delfino", "V\u1e5bksayurveda", "Fix You", "XIX", "during initial entry training", "New England ( Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont )", "Wilson Pickett", "2003", "Simone Vangsness", "Ludacris", "vehicles inspired by the Chrysler that are suitable for use on rough terrain", "tissues in the vicinity of the nose", "Yuzuru Hanyu", "Felicity Huffman", "the RAF", "1974", "National Industrial Recovery Act", "Mercury", "Bob Marley & the Wailers", "Ronseal", "model", "the 8th and 16th centuries", "Blue Origin", "2,700-acre", "to provide security as needed.", "forcibly injecting them with psychotropic drugs", "Summit", "time", "the Kansas Department of Wildlife and Parks", "gestures during a pre-election rally in Harare on Saturday."], "metric_results": {"EM": 0.4375, "QA-F1": 0.6104126477461519}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.2, 1.0, 1.0, 0.5714285714285715, 0.0, 0.625, 1.0, 0.7499999999999999, 0.6, 1.0, 0.8, 1.0, 1.0, 0.5245901639344263, 0.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.25, 1.0, 1.0, 0.0, 0.0, 0.125, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.3076923076923077, 0.0, 1.0, 0.8, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-2272", "mrqa_naturalquestions-validation-954", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-6001", "mrqa_hotpotqa-validation-141", "mrqa_newsqa-validation-2756", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-12140", "mrqa_searchqa-validation-11445", "mrqa_newsqa-validation-1133"], "SR": 0.4375, "CSR": 0.5294943820224719, "EFR": 0.9722222222222222, "Overall": 0.7022964458489389}, {"timecode": 89, "before_eval_results": {"predictions": ["Paul Lynde", "the naos", "in England, births were initially registered with churches, who maintained registers of births", "420", "the fourth ventricle", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "Steve Russell", "if the occurrence of one does not affect the probability of occurrence of the other", "full '' sexual intercourse", "the Archies", "the government - owned Panama Canal Authority", "to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "Comancheria", "Jay Baruchel", "Plank", "in the early 20th century", "in 1651", "in the red bone marrow of large bones", "Sarah Silverman", "Janie Crawford", "in 1958", "3", "supervillains who pose catastrophic challenges to the world", "in 1932", "March 15, 1945", "reproductive", "the NFL", "Biotic -- Biotic resources are obtained from the biosphere ( living and organic material ), such as forests and animals, and the materials that can be obtained from them", "1975", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "March 1995", "Austin, Texas", "Muhammad", "July 21, 1861", "Harry Potter and the Deathly Hallows", "the 1984 Summer Olympics", "9 or 10 national ( significant ) numbers after the `` 0 '' trunk code", "David Joseph Madden", "December 12, 2017", "8 December 1985", "British Indian Association", "in the body", "on the microscope's stage", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "The uvea", "he hosted a short - lived talk show in WCW called A Flair for the Gold", "1971", "at Moton Field, the Tuskegee Army Air Field", "2002 Mitsubishi Lancer OZ Rally", "Beijing", "sport utility vehicles", "Nowhere Boy", "The Beatles\u2019 Liverpool", "kidney", "Luigi Segre", "Ronald Lyle \" Ron\" Goldman", "Adrian Lyne", "543", "eight", "help the convicts find calmness in a prison culture fertile with violence and chaos.", "tanks", "the Army of the Potomac", "Horn", "Basketball"], "metric_results": {"EM": 0.625, "QA-F1": 0.7280206252081252}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.07999999999999999, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.2, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-699", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-2209", "mrqa_triviaqa-validation-519", "mrqa_hotpotqa-validation-2410", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-3695", "mrqa_searchqa-validation-1282"], "SR": 0.625, "CSR": 0.5305555555555556, "EFR": 0.9583333333333334, "Overall": 0.6997309027777778}, {"timecode": 90, "UKR": 0.681640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4877", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10403", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-669", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11297", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13228", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15077", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3812", "mrqa_searchqa-validation-384", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8093", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9877", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1068", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6318", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9098", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5570", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-697", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.802734375, "KG": 0.50234375, "before_eval_results": {"predictions": ["Eriksson", "Tiananmen", "Arthur Conan Doyle", "December 7, 1941", "frauds", "Chrysler", "shakyamuni", "Real Madrid", "York", "Norman Hartnell", "A Beautiful Mind", "Red Admiral", "meadows", "the equator", "Verona", "Isaac", "Let It Snow!", "macbeth", "throw Darts", "physics", "Poland", "Easter", "Peter Sellers", "febrile", "Milton Keynes", "a meteorite", "1955", "China", "(Etheldreda)", "fishes", "Independence Day", "English", "Keane", "Sarkozy", "Harry Potter", "mercury", "Jack Ruby", "a pot or crock", "website", "Helen Gurley Brown", "australia", "Groucho Marx", "Exile", "1664", "Shanghai", "Stieg Larsson", "five", "Saskatchewan", "Priam", "Denise van Outen", "argument", "775", "nasal septum", "August 8, 1508", "Dealey Plaza", "four months in jail", "25 December 2009", "L'Aquila", "Janet and La Toya", "breaking a window at the home but said it was strictly because he was going through intense heroin withdrawal that night and was trying to wake up his friend to get drugs.", "2C", "rain", "I Will Remember You", "the Bavarian Alps"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6171875}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-2675", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-3122", "mrqa_triviaqa-validation-4163", "mrqa_triviaqa-validation-5187", "mrqa_triviaqa-validation-2576", "mrqa_triviaqa-validation-7092", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-2330", "mrqa_triviaqa-validation-4760", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-3004", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-5474", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-4546", "mrqa_newsqa-validation-2839", "mrqa_searchqa-validation-14485"], "SR": 0.5625, "CSR": 0.5309065934065934, "EFR": 0.9285714285714286, "Overall": 0.6892393543956044}, {"timecode": 91, "before_eval_results": {"predictions": ["diddle", "lusitania", "sand on Sunday", "japan", "crash", "germany", "alza", "two", "herald of free enterprise", "bridge", "germany", "le Leicester", "joseph", "ormolu", "yellow", "Burkina Faso", "mortadella", "london", "archers", "Telegraph Media Group Limited 2017", "palladium", "leander", "national militia", "aluminium", "dorset", "1825", "lito m Mussolini", "tuscany", "wildeve", "eye", "Donald Trump", "ruritania", "scarlet tanager", "nelson", "lily Allen", "germany", "flofeld", "mozart", "head", "portugal", "one", "lolo soetoro", "de goya", "brawn", "zipporah", "carousel", "michael hordern", "Mary Poppins", "quatermass experiment", "Benjamin Disraeli", "a cappella", "in outer space", "cases that have not been considered by a lower court may be heard by the Supreme Court in the first instance", "2014", "Cielos del Sur S.A.", "a tragedy", "Eastern College Athletic Conference", "lite Babies and Children's Hospital", "The Cycle of Life", "July 23.", "a tartar sauce", "enron", "mercury", "2010"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5451388888888888}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-4425", "mrqa_triviaqa-validation-6138", "mrqa_triviaqa-validation-6622", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-684", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-4762", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-3194", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-7358", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-1023", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8092", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-2585", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2013"], "SR": 0.484375, "CSR": 0.5304008152173914, "EFR": 1.0, "Overall": 0.7034239130434783}, {"timecode": 92, "before_eval_results": {"predictions": ["maarten tromp", "indonesia", "bulgaria", "diamondbacks", "pilot of the future", "rudolph", "fat", "Singapore", "bird", "stanie mary", "Hebrew", "heisenberg", "carlsberg", "Cumberland", "Billy Connolly", "spain", "kiel canal", "australia", "bacon", "madison square garden", "deaver", "John Flamsteed", "quick brown fox jumps over the lazy dog", "croquet", "kinks", "spearchucker", "reservoirs", "stanley", "zebras", "urien", "datello", "south-West Africa", "st paul\\'s", "james reyes", "philadelphia", "stanley", "haute", "archer", "paris", "a lion", "kiki", "lord john moran", "pig", "st. Petersburg", "stronaut", "poirot", "twelve", "winds", "spider", "barra Streisand", "kipps: The Story of a Simple Soul", "in the National Basketball Association, zone defenses were prohibited until the 2001 -- 2002 season", "The location of the Super Bowl is chosen by the NFL well in advance, usually three to five years before the game", "Identity Theory", "Melbourne Storm", "A Rush of Blood to the Head", "1993", "1994", "Derek Mears", "11", "a cable bridge", "Lake Titicaca", "on a raised platform", "Austin, Texas,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5271655701754385}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.4, 0.10526315789473684, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4506", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-4337", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-6806", "mrqa_triviaqa-validation-3842", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-6743", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-5230", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-525", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-244", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-6936", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-4934", "mrqa_triviaqa-validation-1601", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-3243", "mrqa_naturalquestions-validation-1507", "mrqa_newsqa-validation-406", "mrqa_searchqa-validation-4717", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-6744", "mrqa_newsqa-validation-3021"], "SR": 0.453125, "CSR": 0.5295698924731183, "EFR": 0.9428571428571428, "Overall": 0.6918291570660522}, {"timecode": 93, "before_eval_results": {"predictions": ["a", "Hercules", "spain", "japan", "james Garner", "james bond", "flower", "kerry kitten", "lisping violet", "sinus node", "red", "out shot", "Dutch", "indonesia", "spain", "giambologna", "james abbologna", "gluteal region", "majorca", "12", "carry on leo", "Alexander Borodin", "hector bERLIOZ", "king arthur", "spain", "st aidan", "john vivgo", "(Richard) Seddon", "mole", "stiefbeen", "Prince Andrew", "parma", "cryonic suspension", "human", "tetrodotoxin", "sodor", "the Porteous Riots", "willie nelson", "giambologna", "gloster", "Sicilia", "paris", "dennis", "germany", "danish olympia", "germany", "Essex County Cricket Club", "Kaiser Chiefs", "Israelites", "nicolas cage", "philip catelinet", "Sweden's long - standing policy of neutrality", "April 1917", "Saint Alphonsa", "Takura Tendayi", "Whitesnake", "310", "Robert Barnett,", "voice-assistant software", "the end of TV's rabbit-ears era.", "anchid", "in late 1960s", "lira", "Joanna Moskawa"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5256628787878788}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-6873", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-4113", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4135", "mrqa_triviaqa-validation-5975", "mrqa_triviaqa-validation-4323", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-6771", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-4211", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-3276", "mrqa_newsqa-validation-886", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-14194"], "SR": 0.46875, "CSR": 0.5289228723404256, "EFR": 0.9705882352941176, "Overall": 0.6972459715269086}, {"timecode": 94, "before_eval_results": {"predictions": ["Steps and Accidentals", "Frida Khalo", "Tajik-Afghan-American", "Pope John Paul II", "Louisa May Alcott", "Rock Island", "Turandot", "the Bolsheviks", "cloning", "Signs", "Edward", "Teapot Dome scandal", "The Police", "a carrots anlam", "Manhattan", "Rehab", "a ballpoint pen", "tap", "Ernie Banks", "Christopher Columbus", "1975", "the Great White Way", "shrewd", "novelist", "Peter Shaffer", "the Congo", "(William) Harvey", "reptiles", "a gizzard", "Bangkok", "the Reform Party", "Catwoman", "bats", "(Giacomo) Puccini", "Omaha", "the Monitor", "antonio", "silver", "saved the original Star", "Takana", "the Silk Road", "dreams", "Google", "Jack Ruby", "Take Me Out", "(William) Shakespeare", "a palace.", "type O", "Italy", "green", "Humperdinck", "1940", "three", "January 11, 2014, and on April 16, 2014 on Super Channel in Canada", "Velvet Revolution", "taka", "Hampton Court Palace", "Marvel Comics", "three or more", "Donald Wayne Johnson", "Marcell Jansen", "At least 88 people had been hurt,", "African-Americans", "rocket"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6962425595238095}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9550", "mrqa_searchqa-validation-10265", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-5390", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-14368", "mrqa_searchqa-validation-8175", "mrqa_searchqa-validation-12939", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-9613", "mrqa_naturalquestions-validation-4594", "mrqa_hotpotqa-validation-4578", "mrqa_newsqa-validation-2068"], "SR": 0.671875, "CSR": 0.5304276315789473, "EFR": 1.0, "Overall": 0.7034292763157894}, {"timecode": 95, "before_eval_results": {"predictions": ["Clifford Roberts", "a cobra", "New York", "mexico", "Peter Paul Rubens", "Colonel (Tom) Parker", "the Netherlands", "the mu-koan", "cholesterol", "his only begotten Son", "The Associated Press", "The MIM-104 Patriot", "a sharpness", "Jewel Shuping", "Northern Exposure", "Rebecca Rolfe", "Easy Rider", "the Panama Canal", "a winged horse", "Ned Kelly", "Jakarta", "the Cherokee", "Jim Bunning", "brood", "John F. Kennedy", "Arby's", "Albert Einstein", "a bacterio", "(Victor)UGO", "fudge", "a to Zed", "cows prod", "The Gilder Lehrman", "stimulation", "an egg", "Ken Russell", "The Crucible", "the United Healthcare Workers East", "the zenith", "apogee", "Vancouver", "a semaphore", "a reverse", "Coors Field", "Edgar Rice Burroughs", "\"All for Our Country\"", "foragers", "philosopher", "3.14159", "\"The Postman Always Rings Twice\"", "Kansas City", "1979", "The Hunger Games : Mockingjay -- Part 2 ( 2015 )", "Massachusetts", "Illinois", "South Africa", "Darth vader", "George Adamski", "five", "United States Marine Corps", "two", "\"black box\" label warning", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "black"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5967948717948719}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.46153846153846156, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3709", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-7622", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-6979", "mrqa_searchqa-validation-664", "mrqa_searchqa-validation-543", "mrqa_searchqa-validation-8919", "mrqa_searchqa-validation-7412", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-7113", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-9198", "mrqa_searchqa-validation-3964", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-7194", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-1982", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3198"], "SR": 0.5625, "CSR": 0.53076171875, "EFR": 0.9642857142857143, "Overall": 0.6963532366071429}, {"timecode": 96, "before_eval_results": {"predictions": ["Flickr", "Eric Angat", "air", "Leontyne Price", "damselflies", "Charles I", "Casey Kasem", "San Juan", "sheep", "The witches of Eastwick", "Joseph Smith", "Baskerville", "May", "aOreo", "Cyrano de Bergerac", "Alaska", "birds", "the European Union", "Verdi", "Matt Lauer", "the Kremlin", "Ricardo Sanchez Robert Gates", "Frogs", "Heracles", "the President", "England", "Austin City limits", "the Sacred Cod", "Agatha Christie", "a truck", "Esther", "cat scratch fever", "New Kids on the Block", "Iraq", "country", "The Crucible", "Lincoln", "center of gravity", "Moriarty", "Simon Cowell", "magnesium", "Lenin", "a fruitcake", "nests", "Firebird", "Kansas", "a radical", "Air France", "Louis Brandeis", "a plaque", "David", "In the 1979 -- 80 season", "Virgil Ogletree", "Tom Brady", "Dick Whittington", "Leeds", "Messerschmitt Me262", "1828", "Elijah Wood", "Dizzy Dean", "The Ski Train", "Hezbollah", "a rapist", "skirts"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6614583333333333}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-2946", "mrqa_searchqa-validation-13498", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-862", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4540", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9745", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-6912", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-3093", "mrqa_triviaqa-validation-4843", "mrqa_hotpotqa-validation-4621", "mrqa_newsqa-validation-1402", "mrqa_newsqa-validation-1176"], "SR": 0.609375, "CSR": 0.5315721649484536, "EFR": 1.0, "Overall": 0.7036581829896907}, {"timecode": 97, "before_eval_results": {"predictions": ["blue ribbon", "a pig", "Fear of Flying", "War Admiral", "Abraham Lincoln", "a horizon", "McDuck", "Czechoslovakia", "Roussimoff", "Maria Callas", "Buddhism", "Theodore Roosevelt", "a deluge", "Cold Mountain", "Guignol", "a glove push", "A Night at the Roxbury", "King Henry II", "the Claddagh Ring", "Keith Richards", "the Hydra", "(Victor) Hugo", "the Bronx", "Marcia Clark", "the Lincoln Tunnel", "the albatross", "Bob Fosse", "Dictum", "Georgia", "(Nixon) Nixon", "Madame Tussaud", "\"Cloverfield\"", "Shakespeare", "Mother Jones", "King", "HatfieldMcCoy", "Walter Scott", "Pig Latin", "the Nile", "the Department of Transportation", "a mutton", "Latin", "Malcolm Ochs", "Patrick Ewing", "Vienna", "On the Origin of Species", "New Orleans", "Parody", "ccoli", "arteries", "Carol Burnett", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "a legal case in certain legal systems", "9 February 2018", "18", "Jeremy Thorpe", "blue", "A Song of Ice and Fire", "Labour", "Balvenie Castle", "Animal Planet", "St. Louis, Missouri.", "Espinoza", "ITV"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6907986111111111}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.8, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-15628", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-5057", "mrqa_searchqa-validation-13412", "mrqa_searchqa-validation-15943", "mrqa_searchqa-validation-16528", "mrqa_searchqa-validation-11879", "mrqa_searchqa-validation-8337", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6243", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-4255", "mrqa_searchqa-validation-16590", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-8347", "mrqa_searchqa-validation-13547", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-2490", "mrqa_hotpotqa-validation-5225", "mrqa_newsqa-validation-2474"], "SR": 0.578125, "CSR": 0.5320471938775511, "EFR": 1.0, "Overall": 0.7037531887755102}, {"timecode": 98, "before_eval_results": {"predictions": ["Franklin, Indiana", "Nelson County", "47", "Sir Hiram Stevens", "15 October 1988", "5 February 1976", "Boston Celtics", "Hermione Youlanda Ruby Clinton-Baddeley", "45,698", "Andries Jonker", "Ashanti Region of Ghana", "Groupe PSA", "Texas Tech University", "brigadier general", "Omega SA", "South Australia", "\"Apatosaurus\"", "Resorts World Genting", "the Beatles", "British", "1950", "six", "Future", "London", "Cuyler Reynolds", "New York City", "Toxics Release Inventory", "Figaro", "South African", "\"Apprendi v. New Jersey\"", "Bambi: Eine Lebensgeschichte aus dem Walde", "close to 50 million albums", "Whoopi Goldberg", "Transporter 3", "Disco", "Frederick I", "Afghanistan", "Thriller", "Antonio Lippi", "March 30, 2025", "English", "Azeroth", "Isabella II", "McG", "Vitor Belfort", "11 November 1821", "villanelle", "Emilia-Romagna", "Who's That Girl", "Alan Young", "(Pedro) Calomino", "Virginia Dare", "David Tennant", "efficient point", "Simeon Williamson", "Frobisher", "Margaret Smith Court", "Alwin Landry's", "World leaders", "Lashkar-e-Jhangvi,", "hoist", "Bananas", "Tallahassee", "Brooke Wexler"], "metric_results": {"EM": 0.625, "QA-F1": 0.7065972222222222}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-4963", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-3578", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-2364", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-3420", "mrqa_newsqa-validation-2205"], "SR": 0.625, "CSR": 0.5329861111111112, "EFR": 1.0, "Overall": 0.7039409722222223}, {"timecode": 99, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1438", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1813", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2243", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3415", "mrqa_hotpotqa-validation-3525", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4787", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-5156", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5687", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5783", "mrqa_hotpotqa-validation-5812", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-890", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2678", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6935", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-7987", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-8998", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1205", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1938", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3632", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10148", "mrqa_searchqa-validation-10321", "mrqa_searchqa-validation-1037", "mrqa_searchqa-validation-11038", "mrqa_searchqa-validation-11071", "mrqa_searchqa-validation-11298", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-11567", "mrqa_searchqa-validation-11642", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11797", "mrqa_searchqa-validation-11874", "mrqa_searchqa-validation-12051", "mrqa_searchqa-validation-12192", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12490", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-12602", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-12980", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13154", "mrqa_searchqa-validation-13323", "mrqa_searchqa-validation-13422", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13888", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13943", "mrqa_searchqa-validation-140", "mrqa_searchqa-validation-14210", "mrqa_searchqa-validation-1440", "mrqa_searchqa-validation-14421", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-14582", "mrqa_searchqa-validation-14620", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-14690", "mrqa_searchqa-validation-14704", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15285", "mrqa_searchqa-validation-15391", "mrqa_searchqa-validation-15409", "mrqa_searchqa-validation-15532", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-15680", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15966", "mrqa_searchqa-validation-16373", "mrqa_searchqa-validation-16475", "mrqa_searchqa-validation-16612", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2349", "mrqa_searchqa-validation-2484", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-3030", "mrqa_searchqa-validation-3057", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-3859", "mrqa_searchqa-validation-3911", "mrqa_searchqa-validation-3998", "mrqa_searchqa-validation-4158", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-4454", "mrqa_searchqa-validation-4462", "mrqa_searchqa-validation-4634", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-527", "mrqa_searchqa-validation-5289", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-5730", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-7200", "mrqa_searchqa-validation-7289", "mrqa_searchqa-validation-7315", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-7818", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7930", "mrqa_searchqa-validation-8066", "mrqa_searchqa-validation-8309", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-8876", "mrqa_searchqa-validation-9032", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9547", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-9985", "mrqa_squad-validation-10052", "mrqa_squad-validation-10149", "mrqa_squad-validation-104", "mrqa_squad-validation-1137", "mrqa_squad-validation-1309", "mrqa_squad-validation-1627", "mrqa_squad-validation-1769", "mrqa_squad-validation-2288", "mrqa_squad-validation-2322", "mrqa_squad-validation-2331", "mrqa_squad-validation-2344", "mrqa_squad-validation-2554", "mrqa_squad-validation-2884", "mrqa_squad-validation-3233", "mrqa_squad-validation-3798", "mrqa_squad-validation-3807", "mrqa_squad-validation-3947", "mrqa_squad-validation-4030", "mrqa_squad-validation-4289", "mrqa_squad-validation-4578", "mrqa_squad-validation-4615", "mrqa_squad-validation-4692", "mrqa_squad-validation-5348", "mrqa_squad-validation-5505", "mrqa_squad-validation-5605", "mrqa_squad-validation-5859", "mrqa_squad-validation-6256", "mrqa_squad-validation-6455", "mrqa_squad-validation-6671", "mrqa_squad-validation-7002", "mrqa_squad-validation-7022", "mrqa_squad-validation-7094", "mrqa_squad-validation-7886", "mrqa_squad-validation-8229", "mrqa_squad-validation-8526", "mrqa_squad-validation-8607", "mrqa_squad-validation-876", "mrqa_squad-validation-8771", "mrqa_squad-validation-902", "mrqa_squad-validation-9344", "mrqa_squad-validation-944", "mrqa_squad-validation-9597", "mrqa_squad-validation-9615", "mrqa_squad-validation-9773", "mrqa_triviaqa-validation-1", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1301", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1502", "mrqa_triviaqa-validation-1734", "mrqa_triviaqa-validation-1758", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-1969", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2159", "mrqa_triviaqa-validation-2188", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-2513", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-279", "mrqa_triviaqa-validation-2791", "mrqa_triviaqa-validation-2844", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2900", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-3225", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-3975", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4170", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-4529", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4618", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-5059", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5369", "mrqa_triviaqa-validation-5471", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5783", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-589", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-6479", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6623", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7136", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7285", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-7352", "mrqa_triviaqa-validation-7421", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7692", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-984"], "OKR": 0.837890625, "KG": 0.509375, "before_eval_results": {"predictions": ["Robert Arthur Mould", "1926", "Antonio Lippi", "1993", "Allies of World War I", "Logan International Airport", "the Big East Conference (Big East)", "Academy Award in the category Best Sound", "\"Phineas & Ferb\"", "Switzerland", "September 29, 2017", "Stern-Plaza", "Pakistan", "science fiction drama", "migold Newey", "Darkroom", "1972", "Royce da 5'9\" (Bad)", "evangelical Christian periodical", "Lionel Eugene Hollins", "water", "Harlem neighborhood", "Sophie Monk", "Love Actually", "the Commanding General of the United States Army", "Fredric John Warburg", "five", "20 March to 1 May 2003", "1993", "\"Pimp My Ride\"", "1886", "the Swiss Confederation has adopted various provisions of European Union law in order to participate in the Union's single market", "Syracuse", "Godspell (also known as \"Godspell: A Musical Based on the Gospel According to St. Matthew\")", "1755", "\"The Big Bang Theory\"", "1966", "John Paul \"Johnny\" Herbert", "Marcel \u00c9mile Verdet", "British fantasy and science fiction illustrator", "punk rock", "Atlantic Ocean", "Adelaide Laetitia \" Addie\" Miethke", "Red", "Theodore Robert Cowell", "Matthew Ryan Kemp", "Epic Records", "Roslyn Castle", "2.1 million", "Rothschild banking dynasty", "Corendon Airlines", "Claudia Grace Wells", "Hugo Weaving", "Freedom Day", "Misery", "Henry Hudson", "geometry", "Adidas", "whether to close some entrances, bring in additional officers, and make security more visible.", "the Marine Corps", "Syria", "cherries", "Halloween", "bullnose"], "metric_results": {"EM": 0.625, "QA-F1": 0.7102475649350649}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-4702", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-5238", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-2940", "mrqa_hotpotqa-validation-1711", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-1822", "mrqa_naturalquestions-validation-9150", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1859", "mrqa_searchqa-validation-14664"], "SR": 0.625, "CSR": 0.53390625, "EFR": 1.0, "Overall": 0.71646875}]}