{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_er_lr=5e-5_ep=10_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8', gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/QA_er_lr=5e-5_ep=10_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_er_lr=5e-5_ep=10_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5680, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a plug valve", "1550", "French Louisiana west of the Mississippi River", "2012", "carbon dioxide", "the Lisbon Treaty", "all colors", "in the chloroplasts of C4 plants", "An attorney", "democracy", "The Greens", "third", "Enthusiastic teachers", "expositions", "no French regular army troops were stationed in North America", "estates of the Holy Roman Empire", "Stromatoveris", "2011", "Louis Pasteur", "the owner", "Time Lord", "mosaic floors", "economic", "1893", "environmental factors like light color and intensity", "Gandhi", "deforestation", "Middle Rhine Valley", "pump this into the mesoglea", "low-light conditions", "No Child Left Behind", "one way streets", "\u20ac25,000 per year", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "unbalanced torque", "Ulaanbaatar", "power", "very weak", "Judith Merril", "Gender pay gap", "the Ilkhanate", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "University of Chicago campus", "3D printing technology", "1957", "2000", "a certain number of teacher's salaries are paid by the State", "the Dutch Republic", "San Jose Marriott", "April 20", "the Commission", "evacuate the cylinder", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "Hurricane Beryl", "temperature and light", "terra nullius", "growth", "human", "the \u2018combs\u2019", "1978", "non-Catholics", "Sanders", "vice president", "700 employees"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8576388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10143", "mrqa_squad-validation-8841", "mrqa_squad-validation-2145", "mrqa_squad-validation-739", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-7449", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-9764", "mrqa_squad-validation-7051"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["1986", "carotenoid pigment", "standardized", "50% to 60%", "Stromatoveris", "lower incomes", "Fort Duquesne", "Katharina von Bora", "Miller", "women", "Mr. Belvedere, Roseanne, Who's the Boss?, Just the Ten of Us, The Wonder Years, Full House and Perfect Strangers", "Frank Marx", "architect or engineer", "$2 million", "superintendent of New York City schools", "San Francisco Bay Area at Santa Clara, California", "Kingdom of Prussia", "the country in the same league as the Asian Economic Tigers", "Palestine", "Aristotle and Archimedes", "in the chloroplasts of C4 plants", "Outlaws", "increased blood flow into tissue", "Edgar Scherick", "14th to the 19th century", "Gibraltar and the \u00c5land islands", "the Evangelical Lutheran Church", "oxygen", "the BBC National Orchestra of Wales", "Thanksgiving", "the founding of new Protestant churches in Catholic-controlled regions", "it is impossible to determine what the acceleration of the rope will be", "Venus", "those who proceed to secondary school or vocational training", "zoning and building code requirements", "Ikh Zasag", "Central Bridge", "Europe", "King James Bible", "1935", "seven", "Grumman", "1191", "Maciot de Bethencourt", "Euclid", "case law by the Court of Justice", "long, slender tentacles", "mesoglea", "1970s", "white", "misguided", "2014", "Reconstruction and the Gilded Age", "European Parliament and the Council of the European Union", "State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Manakin Episcopal Church", "Nicholas Stone", "due to ongoing tectonic subsidence", "the release of her eponymous debut album the following year", "a form of business network", "the Capitol held its first session of the United States Congress with both chambers in session on November 17, 1800", "It is the currency used by the institutions of the European Union", "Djokovic", "a generic cover and none of the Wyeth illustrations"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7774959415584415}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8661", "mrqa_squad-validation-7332", "mrqa_squad-validation-6031", "mrqa_squad-validation-27", "mrqa_squad-validation-8459", "mrqa_squad-validation-10339", "mrqa_squad-validation-10321", "mrqa_squad-validation-3021", "mrqa_squad-validation-2328", "mrqa_squad-validation-3946", "mrqa_squad-validation-1906", "mrqa_squad-validation-5588", "mrqa_squad-validation-9166", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1187", "mrqa_searchqa-validation-2579"], "SR": 0.734375, "CSR": 0.78125, "EFR": 0.9411764705882353, "Overall": 0.8612132352941176}, {"timecode": 2, "before_eval_results": {"predictions": ["Lek", "prohibited emigration", "The Private Education Student Financial Assistance", "lower-paid professions", "Labor Party", "adding mathematical models of computation", "special efforts", "rhetoric", "the British occupation", "a year", "Genghis Khan", "a supervisory church body", "44", "a cubic interpolation formula", "King Sigismund III Vasa", "1835", "the exploitation of the valuable assets and supplies of the nation that was conquered", "poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies.", "five", "liquid oxygen", "Gosforth Park", "Metropolitan Police Authority", "18 February 1546", "1996", "1.7 billion years ago", "Mike Carey", "coal", "31 October", "Stanford University", "1991", "LOVE Radio", "juvenile delinquents and political assassins", "Khasar", "Sky Digital", "99.4", "40%", "the issue of laity having a voice and vote", "1995", "red chloroplast", "rocketry and manned spaceflight, including avionics, telecommunications, and computers.", "linebacker", "water", "Sir Edward Poynter", "oxygen", "August 1967", "Velamen parallelum", "a terrorist organisation", "three", "Lowry Digital", "T(n)", "2010", "363 feet", "Buffalo Lookout", "Missouri", "The User State Migration Tool", "1773", "Cadmium poisoning", "October 6, 2017", "during the day", "Haliaeetus", "Sir Henry Bartle Frere, 2nd Baron Chelmsford", "James Zeebo", "November", "Ewan McGregor and Israeli actress Ayelet Zurer."], "metric_results": {"EM": 0.6875, "QA-F1": 0.7572916666666667}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.8, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.7777777777777778, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7357", "mrqa_squad-validation-2886", "mrqa_squad-validation-1672", "mrqa_squad-validation-335", "mrqa_squad-validation-2538", "mrqa_squad-validation-7781", "mrqa_squad-validation-6664", "mrqa_squad-validation-6171", "mrqa_squad-validation-5064", "mrqa_squad-validation-9876", "mrqa_squad-validation-8754", "mrqa_squad-validation-1708", "mrqa_squad-validation-3909", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-6211", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-174"], "SR": 0.6875, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 3, "before_eval_results": {"predictions": ["female", "1884", "Sayyid Abul Ala Maududi", "a family member, or by anyone with knowledge or skills in the wider community setting.", "James E. Webb", "Lutheran and Reformed states in Germany and Scandinavia.", "the phycoerytherin pigment", "losing proposition to offer below or above market wages to workers.", "swimming-plates", "10 July 1856", "130 million cubic foot", "\"teleforce\" weapon", "Heinrich Himmler", "34\u201319.", "Baptism", "Decision problems", "without markings", "1953", "The Day of the Doctor", "Muhammad Khan.", "Sun Life Stadium", "the Council", "February 9, 1953", "May,", "sea gooseberry", "1961", "the Trio Tribe", "Dai Setsen", "the Late Medieval Catholic Church.", "January 1979", "phagocytic cells", "Rankine cycle", "$2.2 billion", "Seine", "Newton's Law of Gravitation", "15 February 1546", "Marquis de la Jonqui\u00e8re", "BBC Dead Ringers series.", "Kenyans for Kenya initiative", "Fresno", "Saudi", "Presiding Officer", "an intuitive understanding", "default emission factors", "wealth than half of all Americans combined", "Michael P. Millardi", "Goldman Sachs", "the BRAAVOO website", "the world's catalog of ideas.", "Jun 27, 2016", "the priest or priestess ( feminine) is a person authorized to perform the sacred rituals of a religion,", "the children were nestled all snug in their beds", "the U.N. organization raised the temples of Abu Simbel up out of the way of flooding", "The Leyden jar", "a Freudian slip", "Gaetano Rossi", "the post office said his \"Droll Stories\", part of \"La Comedie Humaine,", "70% of the Earth's water", "the employees dressed as clowns, cowboys and the sword-wielding knights.", "by joining our Keepsake Ornament Club.", "the British", "early 1960s", "April 1917", "the close quarters and poor hygiene"], "metric_results": {"EM": 0.59375, "QA-F1": 0.661610524891775}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.5454545454545454, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.8571428571428571, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1863", "mrqa_squad-validation-3270", "mrqa_squad-validation-8595", "mrqa_squad-validation-7525", "mrqa_squad-validation-2595", "mrqa_squad-validation-5860", "mrqa_squad-validation-5262", "mrqa_squad-validation-10369", "mrqa_squad-validation-7993", "mrqa_squad-validation-8449", "mrqa_squad-validation-3863", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-844"], "SR": 0.59375, "CSR": 0.7109375, "retrieved_ids": ["mrqa_squad-train-36887", "mrqa_squad-train-56976", "mrqa_squad-train-9239", "mrqa_squad-train-72241", "mrqa_squad-train-64899", "mrqa_squad-train-57523", "mrqa_squad-train-14960", "mrqa_squad-train-23071", "mrqa_squad-train-85486", "mrqa_squad-train-16405", "mrqa_squad-train-35064", "mrqa_squad-train-546", "mrqa_squad-train-77798", "mrqa_squad-train-75590", "mrqa_squad-train-4443", "mrqa_squad-train-64101", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-10339", "mrqa_squad-validation-5588", "mrqa_squad-validation-3019", "mrqa_squad-validation-10143", "mrqa_squad-validation-7449", "mrqa_squad-validation-335", "mrqa_squad-validation-8459", "mrqa_squad-validation-9876", "mrqa_naturalquestions-validation-5780", "mrqa_squad-validation-27", "mrqa_searchqa-validation-2579", "mrqa_squad-validation-10321", "mrqa_newsqa-validation-174", "mrqa_squad-validation-8754", "mrqa_naturalquestions-validation-1912"], "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 4, "before_eval_results": {"predictions": ["boom-and-bust cycles", "The Prince of P\u0142ock", "thyroid hormone", "1840", "occupational stress", "in the parts of the internal canal network", "adding their different epistemological spheres", "Tesla Electric Company", "African-American", "Thomson", "1905", "\"Nun komm, der Heiland", "John Fox", "all health care settings", "cut in half", "the study of rocks", "colonies", "lower wages", "geophysical surveys", "Fontainebleau", "social power and wealth", "2,900 kilometres", "Elie Metchnikoff", "an algorithm", "Immediately after Decision Time", "Confucian propriety and ancestor veneration", "between 25-minute episodes", "eight", "elude host immune responses", "Pusey Library", "inequality", "designs", "cytokines", "requiring his arrest", "wide sidewalks", "other members", "Air Force", "an occupancy permit", "reactive allotrope of oxygen", "Nederrijn", "multi-cultural", "pump water out of the mesoglea", "Tim Johnson", "Canberra", "a judicial officer", "mathematical model", "Henry Purcell", "Ram Nath Kovind", "anembryonic gestation", "Todd Griffin", "Sandy Knox and Billy Stritch", "Hudson Bay", "Etienne de Mestre", "a bow bridge", "from 1922 to 1991", "Nicole Gale Anderson", "1", "sedimentary", "Mrs. Wolowitz", "theory of plate tectonics", "Columbia", "Isabella II", "Kris Allen", "Yo soy Betty, la fea"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7131448412698413}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575]}}, "before_error_ids": ["mrqa_squad-validation-6649", "mrqa_squad-validation-4539", "mrqa_squad-validation-2463", "mrqa_squad-validation-2456", "mrqa_squad-validation-7338", "mrqa_squad-validation-2943", "mrqa_squad-validation-8093", "mrqa_squad-validation-7708", "mrqa_squad-validation-6957", "mrqa_squad-validation-3497", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-4002", "mrqa_triviaqa-validation-5855", "mrqa_searchqa-validation-172"], "SR": 0.640625, "CSR": 0.696875, "EFR": 1.0, "Overall": 0.8484375}, {"timecode": 5, "before_eval_results": {"predictions": ["former flooded terraces", "In the beginning of the 20th century", "1974", "ABC", "dictatorial", "Ben Johnston", "quantum", "Book of Exodus", "Synthetic aperture radar", "Battlestar Galactica and Bionic Woman", "patients' prescriptions and patient safety issues", "No, that's no good", "1697", "3 January 1521", "a new magma", "as a \"principal hostile country\"", "Jan Hus", "Newton", "Croatia", "2011", "Swynnerton Plan", "the machine gun", "Theatre Museum", "August 10, 1948", "they are distinct or equal classes", "the Lisbon Treaty", "Serge Chermayeff", "Thomas Edison", "Mnemiopsis", "the flail of God", "Woodward Park", "The Melbourne Cricket Ground", "Wednesdays", "most common", "in the stroma", "urine", "six years", "plants and algae", "the Constitution of India", "in 1913", "Yuzuru Hanyu", "Konakuppakatil Gopinathan Balakrishnan", "in 1942", "March 2016", "Texas, Oklahoma, and the surrounding Great Plains", "a balance sheet as an asset", "Mayor Hudnut", "in 1963", "William the Conqueror", "in 1922", "no embryo", "Bemis Heights", "9pm ET ( UTC - 5 )", "twice", "Joe Pizzulo and Leeza Miller", "Lituya Bay in Alaska", "Sarah", "a network connection device", "The euro", "in vitro", "2000", "KCNA", "Michael Peachtree Street", "Rodgers & Hammerstein"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6618341727716728}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.8, 1.0, 0.4615384615384615, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.5, 0.2857142857142857, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3076923076923077, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5004", "mrqa_squad-validation-5818", "mrqa_squad-validation-1827", "mrqa_squad-validation-1566", "mrqa_squad-validation-10388", "mrqa_squad-validation-7613", "mrqa_squad-validation-3770", "mrqa_squad-validation-1780", "mrqa_squad-validation-3985", "mrqa_squad-validation-4572", "mrqa_squad-validation-8904", "mrqa_squad-validation-6439", "mrqa_squad-validation-8471", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-2749", "mrqa_newsqa-validation-2404", "mrqa_searchqa-validation-14371"], "SR": 0.546875, "CSR": 0.671875, "EFR": 0.9655172413793104, "Overall": 0.8186961206896552}, {"timecode": 6, "before_eval_results": {"predictions": ["four", "2 million", "ranges from 53% in Botswana to -40% in Bahrain", "Pliocene period", "teacher who stays with them for most of the week and will teach them the whole curriculum", "LeGrande", "After the sixth sermon", "10 Cloverfield Lane", "from 6.1% to 7.8%", "about 60,000", "the University of Chicago College Bowl Team", "the decline of organized labor in the United States", "the San Jose Marriott", "oxygen chambers", "two", "two catechisms", "Cologne", "1991", "Silk Road", "the Surveyor 3 unmanned lunar probe", "145 galleries", "growth and investment", "the centers were computer service bureaus, offering batch processing services", "Vampire bats", "antiforms", "all of the U. S. flags left on the Moon during the Apollo missions were found to still be standing", "the tin had increased in weight", "as the \"father of the Mongols\" especially among the younger generation", "oil was priced in dollars", "Beyonc\u00e9 and Bruno Mars", "a university or college", "More than 1 million people", "hash tables and pseudorandom number generators", "Japan", "the Coriolis force", "Mickey Rourke", "May 2016", "Nicklaus", "Superstition Mountains", "Panama Canal Authority", "alpaca fiber and mohair from Angora goats", "the French First Republic", "two occasions", "Sebastian Vettel", "April 10, 2018", "Gorakhpur railway station", "61st overall", "the largest delegation is that of California", "December 15, 2016", "Abraham Gottlob Werner", "Jourdan Miller", "1991", "Mandy '' Moore", "the Greek name", "Broken Hill and Sydney", "159 counties", "the five permanent members", "Judiththia Aline Keppel", "medellin", "Crown Holdings", "Expedia", "a mirror 3 m in diameter", "a Columbian mammoth", "a failure of leadership at a critical moment in the nation's history"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6048318001443002}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.19999999999999998, 1.0, 0.1, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.7272727272727273, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.33333333333333337, 0.5454545454545454, 0.09523809523809523, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.25, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7445", "mrqa_squad-validation-1892", "mrqa_squad-validation-606", "mrqa_squad-validation-6965", "mrqa_squad-validation-5435", "mrqa_squad-validation-7422", "mrqa_squad-validation-327", "mrqa_squad-validation-4000", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_squad-validation-3435", "mrqa_squad-validation-6228", "mrqa_squad-validation-3718", "mrqa_squad-validation-516", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-6106", "mrqa_hotpotqa-validation-1471", "mrqa_searchqa-validation-10372", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-429"], "SR": 0.46875, "CSR": 0.6428571428571428, "retrieved_ids": ["mrqa_squad-train-9364", "mrqa_squad-train-58766", "mrqa_squad-train-54751", "mrqa_squad-train-49665", "mrqa_squad-train-30226", "mrqa_squad-train-71561", "mrqa_squad-train-53999", "mrqa_squad-train-47196", "mrqa_squad-train-57104", "mrqa_squad-train-10371", "mrqa_squad-train-64090", "mrqa_squad-train-23809", "mrqa_squad-train-68033", "mrqa_squad-train-33206", "mrqa_squad-train-22073", "mrqa_squad-train-67163", "mrqa_squad-validation-8904", "mrqa_squad-validation-1780", "mrqa_searchqa-validation-14371", "mrqa_squad-validation-5064", "mrqa_squad-validation-3985", "mrqa_squad-validation-10143", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-9597", "mrqa_squad-validation-2595", "mrqa_naturalquestions-validation-3558", "mrqa_squad-validation-9176", "mrqa_squad-validation-8459", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-4674"], "EFR": 0.9411764705882353, "Overall": 0.792016806722689}, {"timecode": 7, "before_eval_results": {"predictions": ["Director", "travel literature, cartography, geography, and scientific education", "oxygen chambers", "McManus", "Two", "1066", "2008", "Mojave Desert", "the ARPANET", "St. Lawrence and Mississippi watersheds", "27%", "4000", "Rhine Gorge", "Helicoid stromal thylakoids", "16,000", "impact process effects", "Asian, African and Caribbean", "Warner Bros. Presents.", "pharmacists", "high-frequency", "4:51", "Kabaty Forest", "the seal of the Federal Communications Commission", "strong sedimentation in the western Rhine Delta", "The European Commission", "SAP Center in San Jose", "respiration", "352", "eliminate the accusing law", "October 6, 2004", "\"The Day of the Doctor\"", "Pakistan", "November 1999", "September 6, 2019", "Chinese", "During the fourth season", "the Games have been held just once each in Germany ( 1936 ), Yugoslavia ( 1984 ), Russia ( 2014 ) and South Korea ( 2018 )", "Nick Kroll", "The Ranch", "Billy Gibbons", "Padawan", "the medulla oblongata", "31", "1970s", "the Vital Records Office of the states, capital district, territories and former territories", "Art Carney", "accomplish the objectives of the organization", "the female", "by December 1922", "Category 4", "September 2017", "3 September", "South Africa", "Terrell Owens", "since 3, 1, and 4", "five points", "Dolph Lundgren", "Hampton Court Palace", "Sela Ann Ward", "Frances Reid", "Jeopardy", "Benjamin Britten", "an ISOSCELES triangle", "NOW Magazine"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6697358630952381}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.625, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-762", "mrqa_squad-validation-4629", "mrqa_squad-validation-4348", "mrqa_squad-validation-1938", "mrqa_squad-validation-6409", "mrqa_squad-validation-1195", "mrqa_squad-validation-451", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16130", "mrqa_triviaqa-validation-6548"], "SR": 0.609375, "CSR": 0.638671875, "EFR": 1.0, "Overall": 0.8193359375}, {"timecode": 8, "before_eval_results": {"predictions": ["cytokines", "William Pitt", "North Carolina and New Mexico", "p-adic norm", "Hassan al Banna", "Gottfried Fritschel", "The logo from 1973\u201380 was used for the Third Doctor's final season and for the majority of the Fourth Doctor's tenure", "single Plastoglobulus", "pound-force", "the Yuan dynasty", "Dorothy and Michael Hintze", "The Small Catechism", "36%", "a grand sculpted tabernacle by Antonio Rossellino", "April 20", "biomass", "their belief in the validity of the social contract", "K MJ-TV", "Foreign Protestants Naturalization Act", "southern and central parts of France", "10%", "not", "Metro Trains Melbourne", "BBC 1", "greater than $2 million", "Vince Lombardi Trophy", "Galileo", "singlely, attached directly to their parent thylakoid", "meaning", "The Tiber", "1885", "James Madison", "Ryan Pinkston", "a federal republic", "a mixture of fatty acids, di - and monoglycerides", "2007", "foreign investors", "a Native American nation", "8ft", "Bartolomeu Dias", "William Wyler", "1961", "March 1930", "Julie Adams", "Thomas Jefferson", "February 2017 in Japan and in March 2018 in North America and Europe", "October 29, 2015", "USCS or USC", "a biochemist working for New York's Rheingold Brewery", "15 May 2004", "Billy Hill", "a son whom they name Ben after Obi - Wan Kenobi's pseudonym Ben", "Malina Weissman", "September 6, 2019", "1773", "A lacteal", "April 26, 2005", "Gorman, poet Roger McGough and Mike McGear, who was later revealed to be Paul McCartney's younger brother", "Albert", "a coma in a grave condition", "a tiny medieval hilltop town in Istria, Croatia", "Drew Kesse", "16", "they just were all good little soldiers and pulled right over"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6241071428571427}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8958", "mrqa_squad-validation-7876", "mrqa_squad-validation-8786", "mrqa_squad-validation-8184", "mrqa_squad-validation-5724", "mrqa_squad-validation-6706", "mrqa_squad-validation-4715", "mrqa_squad-validation-8769", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8339", "mrqa_triviaqa-validation-4881", "mrqa_hotpotqa-validation-2800", "mrqa_newsqa-validation-3043", "mrqa_searchqa-validation-2141", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-3476"], "SR": 0.5625, "CSR": 0.6302083333333333, "EFR": 1.0, "Overall": 0.8151041666666666}, {"timecode": 9, "before_eval_results": {"predictions": ["in an attempt to emphasize academics over athletics", "3,600", "nine", "the individual states and territories", "about 30 kPa at standard pressure", "one of his wife's ladies-in-waiting", "liquid phase", "their greatest common divisor is one", "Europe", "cell membrane", "a renegade Time Lord who desires to rule the universe", "Laverne & Shirley", "carbohydrates", "Genghis Khan's children", "Jean Ribault", "March 2011", "his Edison Machine Works on Manhattan's lower east side", "2010", "more equality in the income distribution", "X reduces to Y", "38", "1887", "1469", "a \"world classic of epoch-making oratory.\"", "up to half", "one octave lower in pitch", "WD-40", "a tavern", "jennifer Porgie", "an alcoholic beverage distilled at a high proof from a fermented ecc.", "William Shaksper", "The Fray", "Venus", "Helen Hayes MacArthur", "Canberra", "jennifer Lohan", "Thomas Watson", "Anna Pavlova", "a person who computes premium rates, dividends, risks,", "Joseph Paul \" Joe\" Torre", "a boy", "Chicago Cubs", "jennifer eccennifer", "a goat", "an enlarged heart", "jennifer Psi", "jennifer Wagner", "the White Nile", "jennifer Wood", "the chimney", "Andrew Jackson", "Madonna", "jenno among the wild beasts", "a sailfish", "jennifer", "jenniferini", "between 1859 and 1869", "James Hutton", "Turkey", "Shepherd Neame", "Total Nonstop Action Wrestling", "the highest Hirsch index rating of all living chemists", "the Bronx", "NATO"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5819487803862804}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.923076923076923, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7943", "mrqa_squad-validation-3687", "mrqa_squad-validation-7700", "mrqa_squad-validation-6229", "mrqa_squad-validation-1240", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-7463", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-2179"], "SR": 0.515625, "CSR": 0.61875, "retrieved_ids": ["mrqa_squad-train-84650", "mrqa_squad-train-24612", "mrqa_squad-train-36965", "mrqa_squad-train-52792", "mrqa_squad-train-18352", "mrqa_squad-train-1530", "mrqa_squad-train-67819", "mrqa_squad-train-20752", "mrqa_squad-train-39708", "mrqa_squad-train-32338", "mrqa_squad-train-22468", "mrqa_squad-train-39505", "mrqa_squad-train-17858", "mrqa_squad-train-1772", "mrqa_squad-train-56063", "mrqa_squad-train-33058", "mrqa_squad-validation-7364", "mrqa_naturalquestions-validation-5780", "mrqa_squad-validation-7613", "mrqa_squad-validation-6171", "mrqa_naturalquestions-validation-7554", "mrqa_squad-validation-7051", "mrqa_squad-validation-10388", "mrqa_squad-validation-6649", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3442", "mrqa_squad-validation-8459", "mrqa_squad-validation-4452", "mrqa_squad-validation-516", "mrqa_naturalquestions-validation-6211", "mrqa_searchqa-validation-4367", "mrqa_squad-validation-3435"], "EFR": 0.967741935483871, "Overall": 0.7932459677419355}, {"timecode": 10, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8454", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-3245", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-5035", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-10000", "mrqa_squad-validation-10115", "mrqa_squad-validation-10136", "mrqa_squad-validation-1017", "mrqa_squad-validation-10181", "mrqa_squad-validation-10184", "mrqa_squad-validation-10217", "mrqa_squad-validation-10263", "mrqa_squad-validation-10281", "mrqa_squad-validation-10290", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-10361", "mrqa_squad-validation-10369", "mrqa_squad-validation-1038", "mrqa_squad-validation-10410", "mrqa_squad-validation-10454", "mrqa_squad-validation-10496", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1181", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1226", "mrqa_squad-validation-1240", "mrqa_squad-validation-1254", "mrqa_squad-validation-1269", "mrqa_squad-validation-1371", "mrqa_squad-validation-1499", "mrqa_squad-validation-1521", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1651", "mrqa_squad-validation-166", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1748", "mrqa_squad-validation-1780", "mrqa_squad-validation-1787", "mrqa_squad-validation-1848", "mrqa_squad-validation-1863", "mrqa_squad-validation-1892", "mrqa_squad-validation-1924", "mrqa_squad-validation-1938", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-1998", "mrqa_squad-validation-2019", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-2108", "mrqa_squad-validation-2145", "mrqa_squad-validation-2209", "mrqa_squad-validation-2233", "mrqa_squad-validation-2241", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-247", "mrqa_squad-validation-2521", "mrqa_squad-validation-2545", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2642", "mrqa_squad-validation-27", "mrqa_squad-validation-2751", "mrqa_squad-validation-2820", "mrqa_squad-validation-2885", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-3039", "mrqa_squad-validation-305", "mrqa_squad-validation-3076", "mrqa_squad-validation-3144", "mrqa_squad-validation-3164", "mrqa_squad-validation-317", "mrqa_squad-validation-3184", "mrqa_squad-validation-322", "mrqa_squad-validation-3230", "mrqa_squad-validation-3270", "mrqa_squad-validation-334", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3376", "mrqa_squad-validation-3380", "mrqa_squad-validation-3392", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3497", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3687", "mrqa_squad-validation-3703", "mrqa_squad-validation-3718", "mrqa_squad-validation-374", "mrqa_squad-validation-3769", "mrqa_squad-validation-3770", "mrqa_squad-validation-381", "mrqa_squad-validation-3824", "mrqa_squad-validation-3829", "mrqa_squad-validation-3842", "mrqa_squad-validation-3848", "mrqa_squad-validation-3852", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3917", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3985", "mrqa_squad-validation-3986", "mrqa_squad-validation-3998", "mrqa_squad-validation-4000", "mrqa_squad-validation-4009", "mrqa_squad-validation-402", "mrqa_squad-validation-4031", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4181", "mrqa_squad-validation-4187", "mrqa_squad-validation-4213", "mrqa_squad-validation-4291", "mrqa_squad-validation-4312", "mrqa_squad-validation-4348", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4452", "mrqa_squad-validation-4467", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-451", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4539", "mrqa_squad-validation-4557", "mrqa_squad-validation-4557", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4838", "mrqa_squad-validation-491", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5019", "mrqa_squad-validation-5064", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-516", "mrqa_squad-validation-5262", "mrqa_squad-validation-5396", "mrqa_squad-validation-5436", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5493", "mrqa_squad-validation-5527", "mrqa_squad-validation-5546", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5602", "mrqa_squad-validation-5631", "mrqa_squad-validation-5664", "mrqa_squad-validation-5677", "mrqa_squad-validation-57", "mrqa_squad-validation-5726", "mrqa_squad-validation-5750", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5806", "mrqa_squad-validation-5818", "mrqa_squad-validation-5852", "mrqa_squad-validation-5860", "mrqa_squad-validation-5865", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6031", "mrqa_squad-validation-6066", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6176", "mrqa_squad-validation-6206", "mrqa_squad-validation-6222", "mrqa_squad-validation-6229", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6319", "mrqa_squad-validation-6330", "mrqa_squad-validation-6347", "mrqa_squad-validation-6353", "mrqa_squad-validation-6355", "mrqa_squad-validation-6409", "mrqa_squad-validation-6439", "mrqa_squad-validation-6502", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6611", "mrqa_squad-validation-6649", "mrqa_squad-validation-6664", "mrqa_squad-validation-6694", "mrqa_squad-validation-6790", "mrqa_squad-validation-6815", "mrqa_squad-validation-6838", "mrqa_squad-validation-6875", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6898", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7036", "mrqa_squad-validation-7039", "mrqa_squad-validation-7064", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7228", "mrqa_squad-validation-7260", "mrqa_squad-validation-7261", "mrqa_squad-validation-7297", "mrqa_squad-validation-7332", "mrqa_squad-validation-7338", "mrqa_squad-validation-7357", "mrqa_squad-validation-7364", "mrqa_squad-validation-7368", "mrqa_squad-validation-7380", "mrqa_squad-validation-739", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7445", "mrqa_squad-validation-7457", "mrqa_squad-validation-7470", "mrqa_squad-validation-7492", "mrqa_squad-validation-7503", "mrqa_squad-validation-7525", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-762", "mrqa_squad-validation-7693", "mrqa_squad-validation-7700", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7775", "mrqa_squad-validation-7781", "mrqa_squad-validation-7785", "mrqa_squad-validation-779", "mrqa_squad-validation-7863", "mrqa_squad-validation-7871", "mrqa_squad-validation-7917", "mrqa_squad-validation-7943", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8016", "mrqa_squad-validation-8043", "mrqa_squad-validation-8093", "mrqa_squad-validation-8125", "mrqa_squad-validation-8154", "mrqa_squad-validation-8177", "mrqa_squad-validation-8184", "mrqa_squad-validation-8192", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8365", "mrqa_squad-validation-8414", "mrqa_squad-validation-8449", "mrqa_squad-validation-8459", "mrqa_squad-validation-8471", "mrqa_squad-validation-8484", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8568", "mrqa_squad-validation-8585", "mrqa_squad-validation-8661", "mrqa_squad-validation-8670", "mrqa_squad-validation-8670", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-8841", "mrqa_squad-validation-888", "mrqa_squad-validation-8904", "mrqa_squad-validation-8925", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8958", "mrqa_squad-validation-8985", "mrqa_squad-validation-908", "mrqa_squad-validation-9095", "mrqa_squad-validation-9161", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9403", "mrqa_squad-validation-9405", "mrqa_squad-validation-9446", "mrqa_squad-validation-9464", "mrqa_squad-validation-9556", "mrqa_squad-validation-957", "mrqa_squad-validation-9594", "mrqa_squad-validation-9615", "mrqa_squad-validation-9669", "mrqa_squad-validation-9716", "mrqa_squad-validation-9717", "mrqa_squad-validation-9764", "mrqa_squad-validation-9814", "mrqa_squad-validation-9816", "mrqa_squad-validation-9876", "mrqa_squad-validation-9907", "mrqa_squad-validation-9928", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7463"], "OKR": 0.9140625, "KG": 0.365625, "before_eval_results": {"predictions": ["Northern Europe and the Mid-Atlantic", "$2 million", "fish stocks to collapse", "Chris Keates", "its many castles and vineyards", "Selznick library", "Antigone", "3.5 million", "Denver Broncos", "1997", "several A \u2192 G deamination gradients", "since 2001", "Streptococcus aureus", "1784", "Narrow alleys", "another problem", "economic", "John and Benjamin Green", "1530", "The company installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators", "Denver", "the poor", "Irish Hospitals' Sweepstakes", "(Prince) Vedder", "(Prince) Grey", "(Prince) Albert", "(Prince)steen", "Wounded Knee", "(Prince) Callas", "Henry Moore", "(Prince) Williams", "Charlotte", "(William) Hogan", "Carl Jung", "(Fred) Williamson", "the Orange River", "(Prince) needles", "the Holy Grail", "(Mellon Collie) Collie and the Infinite Sadness", "(Prince) Hunt", "Ludwig Van Beethoven", "(Prince) Victoria", "(Prince) tides", "(Prince) Root Beer", "Frederick Franklin and T. Edward Aldam", "(Prince) Cather", "Velvet Revolver", "(Prince) Bacon", "(Prince) Albert", "(William) Marx", "China", "(Prince) Lawrence", "(William) G", "(Prince) Lewis", "Franklin Pierce", "the college of Charles", "Michael Schumacher", "a four - page pamphlet in 1876", "(Prince) billiards", "(Prince) Margaret College", "Joely Kim Richardson", "Hugh Dowding", "NATO fighters", "Congress"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5620125534188034}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.888888888888889, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.26666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8990", "mrqa_squad-validation-8730", "mrqa_squad-validation-6655", "mrqa_squad-validation-1288", "mrqa_squad-validation-985", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-2642", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-3530", "mrqa_searchqa-validation-12160", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-11388", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-8760", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-6909", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-6181", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6896", "mrqa_hotpotqa-validation-1843"], "SR": 0.40625, "CSR": 0.5994318181818181, "EFR": 1.0, "Overall": 0.7250426136363637}, {"timecode": 11, "before_eval_results": {"predictions": ["North Africa", "Grumman", "to civil disobedients", "1687", "St. Johns River", "the AS-205 mission was canceled", "The President of the Council and a Commissioner can sit in on ECB meetings, but don't have voting rights", "Ismailiyah, Egypt", "floridean", "lupus erythematosus", "December 1878", "bars", "PNU and ODM camps", "T(n) = O(n2)", "Bill Clinton", "the qu", "International Crops Research Institute for the Semi-Arid Tropics", "a straight line", "Germany", "autoimmune disorders", "June 6, 1959", "his advocacy of young earth creationism", "Seoul, South Korea", "2005", "December 24, 1973", "May 21, 2000", "9.58 seconds", "January 2016", "seven", "Samuel Beckett", "Dornie", "Sonic Mania", "Homeland", "Carson City", "The Three Caesars", "Barack Obama", "Hanna-Barbera", "Washington, D.C.", "December 13, 2015", "the Front Row media program on the iSight iMac G5", "1590", "Vixen", "Revolution Studios", "a dimensionless quantity", "1990", "Michael A. Cremo", "Gangsta's Paradise", "A41", "Mary Astor", "five", "Indiana", "Esteban Ocon", "ABC", "Jean Baptiste Point DuSable", "National Lottery", "2018", "Alan Menken", "Milton Friedman", "Richard Avedon", "twice", "forward deck space", "the Sudan", "a pillar", "Yahya Khan"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5863839285714286}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.4, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9912", "mrqa_squad-validation-6759", "mrqa_squad-validation-3113", "mrqa_squad-validation-3954", "mrqa_squad-validation-4150", "mrqa_squad-validation-8594", "mrqa_squad-validation-5097", "mrqa_squad-validation-4883", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-10161", "mrqa_triviaqa-validation-358", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1700", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-971", "mrqa_naturalquestions-validation-3485"], "SR": 0.53125, "CSR": 0.59375, "EFR": 0.9666666666666667, "Overall": 0.7172395833333334}, {"timecode": 12, "before_eval_results": {"predictions": ["a Wi-Fi or Power-line connection", "water pump", "the Tesla coil", "1946", "11", "Parliamentary Bureau", "Japan and Latin America", "send missionaries, backed by a fund to financially reward converts to Catholicism", "Arizona Cardinals", "842 pounds", "1540s", "John Fox", "American Indians in the colony of Georgia", "deferring the D mission to the next mission in March 1969", "poison", "quickly", "a system of many biological structures and processes within an organism that protects against disease", "March 1896", "Percy Jackson", "Jamie '' Dornan", "W. Edwards Deming", "May", "biochemistry", "$799.4 million", "current day", "Accounting Standards Board", "Germany", "General George Washington", "a post as a Consultant, a General Practitioner ( GP )", "Djokovic", "Hundreds or even thousands", "1961", "a bronze statue", "1997", "Procol Harum", "Sheev Palpatine", "Tom Brady", "punk rock", "the septum", "the First Family", "vaskania", "the church at Philippi", "10 May 1940", "Ethel `` Edy '' Proctor", "bohrium", "considered to be unfair", "nasal septum", "the Iraq War", "Spanish American wars of independence", "Colin Atkinson", "Owen Vaccaro", "Walter Brennan", "1954", "Brad Johnson", "1992 to 2013", "King Richard II", "Liechtenstein", "Le Divorce", "Gillian Anderson", "the Kooyong Classic", "18", "Swing Low, Sweet Chariot", "blinking his left eye", "the Puget Sound"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6033878968253967}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 0.16666666666666666, 0.0, 1.0, 0.13333333333333336, 1.0, 0.0, 0.8, 1.0, 0.5, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.32, 1.0, 0.0, 0.20000000000000004, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-3130", "mrqa_squad-validation-3811", "mrqa_squad-validation-9863", "mrqa_squad-validation-3994", "mrqa_squad-validation-8164", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10620", "mrqa_hotpotqa-validation-2009", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-765", "mrqa_searchqa-validation-9822"], "SR": 0.484375, "CSR": 0.5853365384615384, "retrieved_ids": ["mrqa_squad-train-35324", "mrqa_squad-train-54566", "mrqa_squad-train-45634", "mrqa_squad-train-2260", "mrqa_squad-train-71516", "mrqa_squad-train-58947", "mrqa_squad-train-12354", "mrqa_squad-train-34665", "mrqa_squad-train-14843", "mrqa_squad-train-46621", "mrqa_squad-train-25397", "mrqa_squad-train-53194", "mrqa_squad-train-60915", "mrqa_squad-train-85333", "mrqa_squad-train-49077", "mrqa_squad-train-82171", "mrqa_squad-validation-8093", "mrqa_squad-validation-2538", "mrqa_squad-validation-7876", "mrqa_searchqa-validation-12160", "mrqa_searchqa-validation-13232", "mrqa_squad-validation-3985", "mrqa_naturalquestions-validation-4505", "mrqa_squad-validation-4539", "mrqa_squad-validation-1195", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7080", "mrqa_squad-validation-3770", "mrqa_naturalquestions-validation-6524", "mrqa_squad-validation-2456", "mrqa_naturalquestions-validation-2452", "mrqa_squad-validation-10321"], "EFR": 0.9393939393939394, "Overall": 0.7101023455710955}, {"timecode": 13, "before_eval_results": {"predictions": ["Protestantism", "Extension", "Riverside", "momentum-carrying gauge bosons", "Hamburg merchants and traders", "the Department of Justice", "water flow through the body cavity", "67.9", "Fort Duquesne", "Sports Programs, Inc.", "quality rental units", "Pittsburgh Steelers", "Edward Teller", "the geographical area it covers", "to stay", "Andrew Lortie", "Animals are divided by body plan into vertebrates and invertebrates", "Thirty years after the Galactic Civil War", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "Uplokayukta", "The Vulcan salute", "Longline fishing", "various locations in Redford's adopted home state of Utah", "Stephen A. Douglas", "April 2011", "around 1940", "between Polaris Avenue and Dean Martin Drive", "Herman Hollerith", "Dr. Sachchidananda Sinha", "Ron Harper", "hairpin corner", "over two days in July 2011", "IB Diploma Program", "when the cell is undergoing the metaphase of cell division", "it activates a relay which will handle the higher current load", "Donald Trump", "Liam Cunningham", "the spectroscopic notation for the associated atomic orbitals", "Archie Marries Veronica", "moral tale", "a pole", "Sauron's bearers", "Gustav Bauer", "2002", "Mohammad Reza Pahlavi", "the southeastern coast of the Commonwealth of Virginia", "a convergent plate boundary", "Jourdan Miller", "10,605", "84", "1773", "Jesse McCartney", "73", "the sun rises first on Mars Hill, 150 miles ( 240 km ) to the northeast", "2005", "Catherine Zeta-Jones", "Michael Crawford", "264,152", "10,000", "those missing", "Mormon Tabernacle Choir", "carbon dioxide", "Pickwick Papers", "Florida"], "metric_results": {"EM": 0.5, "QA-F1": 0.5902732090643275}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.10526315789473684, 1.0, 0.19999999999999998, 1.0, 0.0, 0.8, 0.4666666666666667, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10475", "mrqa_squad-validation-259", "mrqa_squad-validation-4435", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-4097", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-4328"], "SR": 0.5, "CSR": 0.5792410714285714, "EFR": 1.0, "Overall": 0.7210044642857143}, {"timecode": 14, "before_eval_results": {"predictions": ["a Tulku", "the Quaternary", "the Atlantic Ocean", "Broncos receiver Jordan Norwood", "Museum of the Moving Image", "BBC Dead Ringers", "1206", "Louis Pasteur", "The Brain of Morbius", "the depths of the oceans and seas", "118", "a mainline Protestant Methodist denomination", "Albert Einstein", "Vince Lombardi Trophy", "death in body and soul", "Candice Susan Swanepoel", "AT&T", "Australian", "German", "Chris Anderson", "the state of Indiana", "1949", "Red 2", "Australian", "Ben Miller", "Jena Malone", "John M. Dowd", "15", "Republican", "New York", "Southern Rock Allstars", "a tragedy", "Cricket fighting", "14th Street", "bass", "Brad Wilk", "2012", "New Orleans, Louisiana", "Lufthansa heist", "May 4, 1924", "Australian", "1966", "2012", "1926", "27th congressional district", "jennifer Ciaramello", "Greek", "VAQ-135", "1892", "Ludwig van Beethoven", "Sun Records founder Sam Phillips", "Manchester United", "Saudi Arabian", "1942", "October 6, 2017", "the lower the normal boiling point", "wolf", "Ganges", "January 24, 2006", "repression", "a pager", "Baltimore", "April 6, 1917", "in the Blue Ridge Mountains of Virginia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6058001300599984}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.10526315789473684]}}, "before_error_ids": ["mrqa_squad-validation-10168", "mrqa_squad-validation-821", "mrqa_squad-validation-7674", "mrqa_squad-validation-8229", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-10613", "mrqa_newsqa-validation-1879", "mrqa_naturalquestions-validation-1813"], "SR": 0.546875, "CSR": 0.5770833333333334, "EFR": 1.0, "Overall": 0.7205729166666667}, {"timecode": 15, "before_eval_results": {"predictions": ["1937", "trade unions", "2014", "\"Journey's End\"", "Greater Sacramento region", "John Houghton", "heterokontophyte", "polynomial hierarchy", "Jenggis", "128,843", "by a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections", "56.2%", "20\u201318", "Tower of London", "KlingStubbins", "Edward Albert Heimberger", "1st Earl Mountbatten of Burma", "Alcorn State", "American action horror-thriller film", "The Light in the Piazza", "Philadelphia, Pennsylvania", "12 members", "The A41", "Eminem, Bad Meets Evil", "imp My Ride", "1998", "casting, job opportunities, and career advice", "Mary Harron", "Flashback: The Quest for Identity", "Eenasul Fateh", "Chicago", "Australia", "2014", "the Second World War", "Lismore", "rural areas", "teenage actor", "Summerlin, Clark County, Nevada", "Lester Ben \"Benny\" Binion", "YG Entertainment", "Rusalka", "Noel Gallagher", "the \"Pour le M\u00e9rite\" 1", "Trey Parker and Matt Stone", "Riot Act", "Aqua", "American Longhair", "four operas", "Christy Walton", "Commanding General", "Hechingen in Swabia", "Black Sabbath", "footballer", "8,211", "Kristin Beth Baxter", "the nucleus through nuclear pores", "Bristol Box Kite", "1961", "Diprivan", "at the South Dakota State Penitentiary", "Douglas Fir", "drink wine or kiss a fool", "\"a striking blow to due process and the rule of law", "Philip Markoff"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6122362012987013}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5454545454545454, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.4, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.9090909090909091, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2835", "mrqa_squad-validation-1791", "mrqa_squad-validation-4298", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-5320", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-2178", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2558", "mrqa_triviaqa-validation-7461", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-13595", "mrqa_searchqa-validation-1757"], "SR": 0.484375, "CSR": 0.5712890625, "retrieved_ids": ["mrqa_squad-train-6985", "mrqa_squad-train-47268", "mrqa_squad-train-15957", "mrqa_squad-train-42679", "mrqa_squad-train-65030", "mrqa_squad-train-20967", "mrqa_squad-train-55897", "mrqa_squad-train-85573", "mrqa_squad-train-22590", "mrqa_squad-train-74333", "mrqa_squad-train-48506", "mrqa_squad-train-67217", "mrqa_squad-train-12383", "mrqa_squad-train-70566", "mrqa_squad-train-24847", "mrqa_squad-train-64469", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-15770", "mrqa_squad-validation-5097", "mrqa_naturalquestions-validation-10460", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-8983", "mrqa_squad-validation-9176", "mrqa_searchqa-validation-9148", "mrqa_hotpotqa-validation-2205", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4326", "mrqa_searchqa-validation-7829", "mrqa_squad-validation-4435", "mrqa_squad-validation-6031", "mrqa_squad-validation-7525"], "EFR": 1.0, "Overall": 0.7194140625000001}, {"timecode": 16, "before_eval_results": {"predictions": ["the Central Secretariat (Zhongshu Sheng)", "Puritanism", "James Wolfe", "March 1974", "2003", "Frederick II the Great", "Lower taxes", "the Armenians", "redistributive taxation", "the Seattle Seahawks", "paid professionals", "squaring", "revelry", "Krishna Rajaram", "Padre Alberto", "Mark Thompson", "1-0", "Kim Il Sung", "second-degree aggravated battery", "Romney", "one day", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 20", "be silent", "200", "2,000", "several weeks", "auction off one of the earliest versions of the Magna Carta later this year", "a piece of gauze", "Michael Jackson", "Caylee's", "10 below in Chicago, Illlinois", "Barbara Williams", "Manmohan Singh", "jazz", "1983", "cancer", "Al-Shabaab", "Casalesi Camorra clan", "videtaping", "Appathurai", "Eintracht Frankfurt", "opium poppies", "The Bronx County District Attorneys Office", "1,073", "Arthur E. Morgan III", "Cesar Duarte", "Las Vegas", "Pakistan", "the crew of the Bainbridge", "I, the chief executive officer", "18", "the prime minister's handling of the L'Aquila earthquake", "Sri Lanka", "in Mumbai", "Miami Heat", "a combination of genetics and the male hormone dihydrotestosterone", "Senegal", "Windermere", "Field of Dreams", "Bill Paxton", "two more stripes", "KLM", "Sex Pistols", "Literary Devices"], "metric_results": {"EM": 0.5, "QA-F1": 0.5934466575091575}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.2, 1.0, 0.0, 0.5714285714285715, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8294", "mrqa_squad-validation-7147", "mrqa_squad-validation-7296", "mrqa_squad-validation-1136", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-2899", "mrqa_triviaqa-validation-4966", "mrqa_hotpotqa-validation-5742", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-4356"], "SR": 0.5, "CSR": 0.5670955882352942, "EFR": 1.0, "Overall": 0.7185753676470589}, {"timecode": 17, "before_eval_results": {"predictions": ["higher economic inequality", "private individuals, private organizations or religious groups.", "primary education", "a glass case suspended from the lid", "phagocytic cells", "2000", "five", "weight", "Leukocytes", "3D printing technology.", "Ong Khan", "colonel", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "\"The oceans are kind of the last frontier for use and development,\"", "Wigan Athletic", "Russian air company Vertikal-T,", "Graeme Smith", "228", "the commissions as a legitimate forum for prosecution,", "her father's", "St. Francis De Sales Catholic Church", "has been the Magneto to my Wolverine,", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "air support", "two", "African National Congress", "Somali", "The oldest documented bikinis", "Bobby Darin", "Adam Yahiye Gadahn,", "Mark Sanford", "150", "weren't taking it well.", "the equator", "Chinese President Lee Myung-bak,", "183", "warning", "They're big, strong, and fierce", "CNN's", "has \"launched investigations into 150 separate incidents, including 36 criminal investigations opened thus far,\"", "11th year in a row", "stolen four Impressionist paintings worth about $163 million (180 million Swiss francs)", "fastest circumnavigation of the globe in a powerboat", "Guinea, Myanmar, Sudan and Venezuela.", "Austin Wuennenberg,", "Diversity", "forcibly injecting them with psychotropic drugs", "in the Oaxacan countryside of southern Mexico", "corruption charges", "a polo match", "more than 100", "has not been able to rise above the scandals and tragedies that have beset Japan's national sport in recent years.", "Alfredo Astiz,", "Carter Pewtersch Schmidt", "convert single - stranded genomic RNA into double - stranded cDNA", "Harrison Ford", "Andes", "Peter Robert Auty", "2009", "Lassie", "Marilyn Monroe", "\"The Who\"", "Jodi Benson's", "salsa Bell"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5048923416110916}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.1, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.25, 0.8333333333333334, 0.0, 0.0, 1.0, 0.09090909090909091, 1.0, 0.0, 0.4864864864864865, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-7136", "mrqa_squad-validation-2000", "mrqa_squad-validation-7845", "mrqa_squad-validation-6477", "mrqa_newsqa-validation-412", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1123", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-1968", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1649"], "SR": 0.421875, "CSR": 0.5590277777777778, "EFR": 1.0, "Overall": 0.7169618055555556}, {"timecode": 18, "before_eval_results": {"predictions": ["Jason Bourne", "isobaric (constant pressure) processes", "1999,", "mesoglea", "a body of treaties and legislation, such as Regulations and Directives,", "isobaric (constant pressure)", "socially owned", "Mark Twain's", "in amylopectin starch granules that are located in their cytoplasm,", "the Tower District", "\"A security tower stands over the crowd that came to watch Barack Obama's inauguration,", "pro-democracy activists clashed Friday with Egyptian security forces in central Cairo,", "a site that has yielded between 3 million and 4 million fossilized bones.", "overthrow the socialist government of Salvador Allende in Chile,", "The Pilgrims sail to Plymouth Rock", "a rally at the State House next week", "28 passengers,", "Michael Schumacher", "Ventures", "seven", "hanging a noose in a campus library,", "lost its majority in the Chamber of Deputies", "\"I'm just getting started.\"", "Utah teenager", "diplomatic relations", "chic boutiques", "Daniel Radcliffe", "Muslim with Lebanese heritage,", "five", "mother", "$10 billion", "Six members of Zoe's Ark", "Galveston, Texas,", "Millvina Dean,", "\"unapproved drug,\"", "Lucky Dube,", "children's books", "James Osterberg", "At least 40", "Afghan forces", "sliding out on the first run,", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "A grizzly bear", "International Polo Club Palm Beach in Florida.", "girls'", "cervical cancer", "The National Infrastructure Program", "88", "the creation of an Islamic emirate in Gaza,", "the Islamic militant group Harkat-ul-Jihad al-Islami ( HuJi)", "a UPS delivery box", "Manchester City", "al-Maqdessi's", "2015", "January 2017", "30", "Sweater", "Waylon Albright \"Shooter\" Jennings (born May 19, 1979)", "people working in film and the performing arts,", "candy shells", "Theiky room", "Marlborough,", "1968", "The Krypto Report"], "metric_results": {"EM": 0.359375, "QA-F1": 0.43629807692307687}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.2564102564102564, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3444", "mrqa_squad-validation-3447", "mrqa_squad-validation-7516", "mrqa_squad-validation-1235", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2733", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-5209", "mrqa_hotpotqa-validation-2986", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-3428"], "SR": 0.359375, "CSR": 0.5485197368421053, "retrieved_ids": ["mrqa_squad-train-57437", "mrqa_squad-train-26019", "mrqa_squad-train-65759", "mrqa_squad-train-49538", "mrqa_squad-train-25816", "mrqa_squad-train-28528", "mrqa_squad-train-14623", "mrqa_squad-train-35003", "mrqa_squad-train-69138", "mrqa_squad-train-54898", "mrqa_squad-train-43930", "mrqa_squad-train-68106", "mrqa_squad-train-73636", "mrqa_squad-train-39107", "mrqa_squad-train-27857", "mrqa_squad-train-67858", "mrqa_squad-validation-4000", "mrqa_hotpotqa-validation-5667", "mrqa_naturalquestions-validation-4505", "mrqa_squad-validation-7147", "mrqa_squad-validation-4348", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-5644", "mrqa_naturalquestions-validation-9453", "mrqa_squad-validation-8661", "mrqa_searchqa-validation-9822", "mrqa_naturalquestions-validation-9715", "mrqa_hotpotqa-validation-5627", "mrqa_searchqa-validation-1279", "mrqa_squad-validation-7449", "mrqa_searchqa-validation-12864", "mrqa_squad-validation-7700"], "EFR": 1.0, "Overall": 0.7148601973684211}, {"timecode": 19, "before_eval_results": {"predictions": ["the blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "an Executive Committee,", "San Francisco Bay Area's Levi's Stadium", "the death of Elisabeth Sladen in early 2011", "NFL Experience", "English and Swahili", "61%", "plastoglobulus", "three", "Turkey", "wombat", "Goa, Hanagal", "a plan", "Broncos Place", "Hope Diamond", "gin", "Pilate", "enamel", "a special grade", "Tagline", "Battle of Hastings", "the Caspian Sea", "Office", "the army", "The Times of India", "a projecting ornament", "\"It was directed by the music video director Anton Corbijn and released and aired in May 1995.", "Fes", "FIFA World Cup Final", "Interlaken", "Mystic Pizza", "Princeton University", "Mandy Well you came", "volumes", "Malay Peninsulamakes", "Herman Wouk", "Frederick IV,", "The heart of a fool", "\"The Courier\" or \"The C-J\",", "Napoleon Bonaparte", "passenger saddles", "unassisted", "thermodynamics", "Derek Smalls", "Dalits", "Harry Houdini", "(1970-1975), drugi raz z Cassandr \"Casey\" Coates (od... Jego trzeci on jest Mary Steenburgen,", "Double Vision", "dollop", "Lust for Life", "Camembert", "James Ross Clemens", "a hole", "in 1991, 1992, 1996, 1998 )", "not being pushed around by big labels, managers, and agents and being told what to do, and being true to yourself creatively", "cuban cigars", "\"The Fight of the Century\"", "North America, Australia, and India.", "841", "Ike", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "Harlem", "sovereign states", "near the city of Cairo, Illinois"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5536063762626262}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.33333333333333337, 0.9777777777777777, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.6666666666666666, 0.08333333333333334, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-127", "mrqa_squad-validation-7872", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-2769", "mrqa_newsqa-validation-3214", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-3559"], "SR": 0.390625, "CSR": 0.540625, "EFR": 0.9743589743589743, "Overall": 0.7081530448717949}, {"timecode": 20, "UKR": 0.689453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-1252", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1967", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-3519", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-782", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-920", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11270", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2607", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4469", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-971", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10143", "mrqa_squad-validation-10168", "mrqa_squad-validation-10241", "mrqa_squad-validation-10266", "mrqa_squad-validation-10370", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1141", "mrqa_squad-validation-115", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1254", "mrqa_squad-validation-127", "mrqa_squad-validation-1288", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1747", "mrqa_squad-validation-1765", "mrqa_squad-validation-1827", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-218", "mrqa_squad-validation-22", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2379", "mrqa_squad-validation-2383", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-2538", "mrqa_squad-validation-2545", "mrqa_squad-validation-257", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2886", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-305", "mrqa_squad-validation-3052", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3567", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3864", "mrqa_squad-validation-3917", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3975", "mrqa_squad-validation-3986", "mrqa_squad-validation-3994", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4187", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4883", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5097", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-5237", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5604", "mrqa_squad-validation-5677", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5859", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6347", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6594", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7034", "mrqa_squad-validation-7039", "mrqa_squad-validation-7051", "mrqa_squad-validation-71", "mrqa_squad-validation-7125", "mrqa_squad-validation-7136", "mrqa_squad-validation-7192", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7449", "mrqa_squad-validation-7521", "mrqa_squad-validation-7576", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7708", "mrqa_squad-validation-7751", "mrqa_squad-validation-7814", "mrqa_squad-validation-7863", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7881", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8471", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8670", "mrqa_squad-validation-8710", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9095", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9367", "mrqa_squad-validation-9405", "mrqa_squad-validation-942", "mrqa_squad-validation-9594", "mrqa_squad-validation-9614", "mrqa_squad-validation-9669", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-4881", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-7496"], "OKR": 0.85546875, "KG": 0.446875, "before_eval_results": {"predictions": ["the main contractor", "widespread education", "300", "an attack on New France's capital, Quebec", "two-thirds", "Decompression sickness", "1979", "Parliament Square, High Street and George IV Bridge in Edinburgh", "the union", "the Lincoln Laboratory", "grizzly bear", "Dracula", "Sid Vicious", "Nitrous oxide", "the Overture", "Frederic Remington", "La Mosquitia", "Arkansas", "an object oriented programming", "the Baby Boom", "the genie", "the Whig", "ER", "a genie", "apron", "The Princess Diaries", "Arkansas", "Mao Zedong", "a multilingual person", "a genie", "the Wells Fargo", "the Sundance Kid", "a den", "amber", "Holly", "Umbria", "a Roth IRA", "Quentin Tarantino", "the Palatine Hill", "Kentucky", "an axion", "the second Sunday", "a genie", "a geniehagerty", "Scooter Libby", "flood", "a genie", "Bioko", "a fat prisoner", "bowling", "Cecil John Rhodes", "Aerobic", "Anaheim", "Steve Hale", "the distribution and determinants of health and disease conditions in defined populations", "Belgium", "Jack Frost", "the 137th", "Merck", "Microsoft", "the Kremlin", "France", "a Hungarian Horntail", "Phil Mickelson"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6071614583333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.625, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4918", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-4028", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-16826", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-12996", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-12477", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-12188", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-3514", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-4992", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-4118"], "SR": 0.53125, "CSR": 0.5401785714285714, "EFR": 1.0, "Overall": 0.7063950892857143}, {"timecode": 21, "before_eval_results": {"predictions": ["a lesson plan", "the laws of physics", "(1893)", "Welsh", "pastors and teachers,", "criminal", "a monthly subscription", "11,000 BC", "novella", "the President of the United States", "above the light source and under the sample in an upright microscope, and above the stage and below the light sources in an inverted microscope", "November 3, 2007", "1939", "1917", "1959", "Humpty Alexander Dumpty", "September 19 - 22, 2017", "bypasses", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "Bobby Eli", "Nagar Haveli", "Dick Rutan", "Paracelsus", "January 2004", "Manhattan", "the date has been as early as January 3, and as late as February 12", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Teddy Hill", "push the food down the esophagus", "Splodgenessabounds", "to jump - shoot, to dribble ( drive ) past the defender or to pass it to a teammate", "Nancy Birtwhistle", "a lot on the standard speed limit sign, with a yellow background instead of a white one, the words `` speed limit '' omitted and an additional panel stating the type of hazard ahead", "a diastema ( plural diastemata )", "Eddie Murphy", "video game", "Secretaries of State and Defense and the National Security Adviser", "flour and water", "the San Francisco 49ers", "the Gupta Empire", "card verification data ( CSC )", "American blues electric guitar musician T - Bone Walker", "Ray Charles", "Francis Hutcheson", "1937", "at Cairo, Illinois", "Barbara Windsor", "British victims", "Gladys Knight & the Pips", "in the Executive Residence of the White House Complex", "the eighth episode of Arrow's second season", "the courts", "Kanawha River", "athletics", "an isosceles triangle", "1898", "Sir Matthew Arundell", "WFTV", "tennis", "the People vs OJ Simpson", "Austria", "women and breast cancer", "Harry Nicolaides,", "he reached under the counter, grabbed his gun and told the robber to drop the bat and get down on his knees."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6882712498683377}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9473684210526315, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.88, 0.9428571428571428, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.05555555555555555, 1.0, 1.0, 0.0, 0.5, 0.35294117647058826, 0.28571428571428575, 1.0, 0.09523809523809523, 0.5454545454545454, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.923076923076923, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 0.07692307692307693]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-5041", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-9330", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-4715", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1985"], "SR": 0.546875, "CSR": 0.5404829545454546, "retrieved_ids": ["mrqa_squad-train-16493", "mrqa_squad-train-37070", "mrqa_squad-train-62292", "mrqa_squad-train-60671", "mrqa_squad-train-79155", "mrqa_squad-train-5870", "mrqa_squad-train-47408", "mrqa_squad-train-8645", "mrqa_squad-train-49733", "mrqa_squad-train-52541", "mrqa_squad-train-57051", "mrqa_squad-train-52251", "mrqa_squad-train-37600", "mrqa_squad-train-33863", "mrqa_squad-train-73566", "mrqa_squad-train-67761", "mrqa_squad-validation-4572", "mrqa_searchqa-validation-2100", "mrqa_newsqa-validation-2641", "mrqa_squad-validation-3444", "mrqa_naturalquestions-validation-8792", "mrqa_squad-validation-8294", "mrqa_searchqa-validation-12962", "mrqa_naturalquestions-validation-7390", "mrqa_triviaqa-validation-5209", "mrqa_searchqa-validation-5172", "mrqa_naturalquestions-validation-10460", "mrqa_squad-validation-2943", "mrqa_squad-validation-127", "mrqa_squad-validation-7422", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-2232"], "EFR": 0.896551724137931, "Overall": 0.6857663107366772}, {"timecode": 22, "before_eval_results": {"predictions": ["literacy and numeracy", "the bark of mulberry trees", "drama", "1814", "distributive efficiency", "on issues related to the substance of the statement", "Athens", "Continental drift", "Frank Oz", "1975", "775", "Kimberlin Brown", "AD 95 -- 110", "the status line", "on the inner edge of the Nebula Arm, one of the spiral - shaped concentrations of gas and dust", "absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "handheld subscriber equipment", "Weston", "a hexamer in secretory vesicles", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Yuzuru Hanyu", "Qutab Ud - Din - Aibak", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Coton in the Elms", "an ascender", "Wakanda", "1992", "Active absorption", "transceivers", "a place of trade, entertainment, and education", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively", "Robert Hooke", "interstellar medium", "Alicia Vikander", "a male bandmates ( Tavish Crowe )", "the name announcement of Kylie Jenner's first child", "5 liters", "somatic cell nuclear transfer", "Betty", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "June 8, 2009", "head - up display", "presidential representative democratic republic", "Ferm\u00edn Francisco de Lasu\u00e9n", "moral", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "the final capped year under that agreement", "Spanish / Basque", "Laura Jane Haddock", "Atlanta", "February 2002", "the visible part of the human nose", "a coffee house", "Chief Inspector of Prisons", "Cheshire", "#364", "24800", "liberal revolutions of 1848", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Matamoros, Mexico", "security breach at Newark's Liberty International Airport,", "X-Files", "biometrics", "North Dakota"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6562950648888148}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.9166666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.7407407407407407, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.5714285714285715, 0.0, 0.7999999999999999, 0.2857142857142857, 1.0, 0.14285714285714288, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.23076923076923078, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1802", "mrqa_squad-validation-5608", "mrqa_squad-validation-9445", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-5113", "mrqa_hotpotqa-validation-1679", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-7662"], "SR": 0.484375, "CSR": 0.5380434782608696, "EFR": 0.9393939393939394, "Overall": 0.6938468585309618}, {"timecode": 23, "before_eval_results": {"predictions": ["100,000", "an extensive neoclassical centre referred to as Tyneside Classical", "an important tool and object of study in commutative algebra, algebraic number theory and algebraic geometry", "his birthtown, Smiljan", "Persia", "ABC-DuMont", "a long flat bottom in between", "the First World War", "John Constable", "Charlie Harper", "Salvete", "King Macbeth of Scotland", "Everton", "October 3rd", "\"I came, I saw, I conquered\u201d", "Bull Moose Party", "Augusta, GA, USA", "Samwell, New Mexico", "the College of Cardinals", "Cornell University", "Robert Stroud", "Alice in Wonderland", "a lot of the larger group of energy products, which includes bars and gels, and distinct from sports drinks", "Crash", "11", "17", "Achille Lauro", "Quentin Tarantino", "Michael Miles", "Swansea", "Wyatt Earp", "Chuck Hagel", "Hispaniola", "Bangladesh", "an argument", "Sean Maddox", "a king", "Bristol", "a tawny roundle", "Andy Murray", "Independence Day", "an MCA Bonspiel", "Hansa", "Crusades", "King Henry I", "ThunderCats", "Ally Sloper", "the European Council", "Volkswagen", "George IV", "the Kalavinka", "China", "Justice Lawrence John Wargrave", "Thomas Jefferson", "the central plains", "William Adelin", "Barbary pirates", "Sir William Collins", "to hold onto his land", "there was not immediately known whether the captors are still holding Lunsmann's son and nephew.", "\"It's very hard to design complex systems that don't have bugs,\"", "Port In A Storm", "the Lone Star", "Bahadur Shah II"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6267061781609196}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.4, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2758620689655172, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5180", "mrqa_squad-validation-9136", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-5816", "mrqa_triviaqa-validation-4018", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-6138", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-1683", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2371", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-13686"], "SR": 0.578125, "CSR": 0.5397135416666667, "EFR": 0.9629629629629629, "Overall": 0.6988946759259259}, {"timecode": 24, "before_eval_results": {"predictions": ["Napoleon", "The Victorian Alps in the northeast", "skin damage", "three", "European Parliament and the Council of the European Union", "Steve McQueen", "red bull", "piano", "Midtown", "eagle", "shoes", "boxer", "Geneva", "Call for the Dead", "Woodrow Wilson", "Menorca", "Wales", "augusta", "bulldog", "distance", "Edward VI", "silver linseed", "Mercury", "trumpet", "post-modernism", "james bond", "Iain Banks", "Valencia", "gluten", "Jan van Eyck", "stephen", "red-green", "\"Doctor Who\"", "stephen", "World War II", "King Leonidas", "augusta", "Yosemite", "stephen", "king duncan", "about 8 minutes", "radion", "eagle", "poster", "West Point", "stephen", "algebra", "step-by-step solution", "The jet engine", "stephen", "We Interrupt This Week", "Chester", "the Brazilian state of Mato Grosso to its confluence with the Paran\u00e1 River north of Corrientes and Resistencia", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "2005", "Wes Craven", "1698", "President Bill Clinton", "an Airbus A320-214", "\"persistent pain.\"", "246", "hurricane", "The Treasure of the Sierra Madre", "smallpox"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5287236590038314}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.48275862068965514, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2911", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-3769", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1951", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-4073", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-6358", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-2426", "mrqa_hotpotqa-validation-3034", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-1857"], "SR": 0.453125, "CSR": 0.53625, "retrieved_ids": ["mrqa_squad-train-39050", "mrqa_squad-train-74201", "mrqa_squad-train-21910", "mrqa_squad-train-53551", "mrqa_squad-train-37760", "mrqa_squad-train-23898", "mrqa_squad-train-46083", "mrqa_squad-train-1416", "mrqa_squad-train-57246", "mrqa_squad-train-26297", "mrqa_squad-train-37010", "mrqa_squad-train-67200", "mrqa_squad-train-13392", "mrqa_squad-train-62822", "mrqa_squad-train-10574", "mrqa_squad-train-84395", "mrqa_naturalquestions-validation-5938", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-3986", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-4418", "mrqa_triviaqa-validation-2486", "mrqa_newsqa-validation-174", "mrqa_squad-validation-5860", "mrqa_searchqa-validation-16130", "mrqa_squad-validation-1938", "mrqa_squad-validation-27", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-9272", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-4179", "mrqa_squad-validation-7147"], "EFR": 1.0, "Overall": 0.705609375}, {"timecode": 25, "before_eval_results": {"predictions": ["Thoreau", "Thomas Piketty", "1,548", "zoning and building code requirements", "Science and Discovery", "Lausanne", "tuppenny Tube", "Laos", "bullseye", "bluebird", "wine", "300", "1894", "The Colossus of Rhodes", "Neil Morrissey", "edicia", "Billie Holiday", "Gingerbread", "Phil Mickelson", "Jean-Paul Sartre", "Len Deighton", "tARTAN", "Alex Garland", "L. Pasteur", "king minus", "Benjamin Disraeli", "Johannesburg", "Martin Luther King", "Bridgeport", "thomas", "a single point", "jellyfish", "Albert Reynolds", "Newfoundland and Labrador", "Eddie Cochran", "Alessandro Volta", "Andre 3000", "Wanderers", "\"Sunny After afternoon\"", "the Biafra secession", "Anna Mae bullock", "spain", "Cuba", "doves", "Heston Blumenthal", "Harold Godwinson", "Tommy Burns", "Ritchie Valens", "posh", "carWale", "spain", "Gargantua", "Kryptonite", "hair jelly", "if the concentration of a compound exceeds its solubility", "Bob Edwards Show", "Nicholas McCarthy", "Paul John Mueller Jr.", "the underprivileged", "Alicia Keys", "80", "Maldives", "Matt Leinart", "Stephen Hawking"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6323918269230768}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 0.6666666666666666, 0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-3582", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-6724", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2965", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4832", "mrqa_hotpotqa-validation-3714", "mrqa_newsqa-validation-439"], "SR": 0.5625, "CSR": 0.5372596153846154, "EFR": 1.0, "Overall": 0.7058112980769231}, {"timecode": 26, "before_eval_results": {"predictions": ["paid professionals", "Basel", "\"we want to practice Christian love toward them and pray that they convert,\" but also that they are \"our public enemies... and if they could kill us all, they would gladly do so. And so often they do.\"", "Informal rule", "phobias", "raven", "helium", "John Logie Baird", "bristol", "pickwick", "Titanic", "bristol", "tekwondo", "bristol", "Rome", "flolita", "bristol", "jubsbury", "chicago", "oxygen", "lead", "bristol", "Venus", "bristol", "chantal Paradis", "Jupiter", "if\u2013", "chicago", "chicago", "bristol", "Australia", "bristol", "MGM", "chicago", "Portugal", "Vladivostok", "pasteur", "bristol", "phoenicia", "bristol", "Gulf of Aden", "bristol", "bristol", "lithium", "god", "bristol", "ch.1", "heartbeat", "Rio de Janeiro", "peacock", "jubai and Huayu", "chicago", "6ft 1in", "Judith Cynthia Aline Keppel", "10 feet", "eleana Nelson pasteurf 3D Grass", "photographs, film and television", "March 17, 2015", "AbdulMutallab", "blind Majid Movahedi,", "two", "bristol", "jedoublen/jeopardy", "augusta"], "metric_results": {"EM": 0.34375, "QA-F1": 0.35124491869918695}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.14634146341463414, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2368", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-5478", "mrqa_triviaqa-validation-3239", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-4517", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-7030", "mrqa_triviaqa-validation-5278", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-1605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-6046", "mrqa_naturalquestions-validation-1409", "mrqa_hotpotqa-validation-3563", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1535", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-2854", "mrqa_searchqa-validation-1898"], "SR": 0.34375, "CSR": 0.5300925925925926, "EFR": 0.9761904761904762, "Overall": 0.6996159887566138}, {"timecode": 27, "before_eval_results": {"predictions": ["Jim Nantz and Phil Simms", "15th", "60%", "Windows 10", "three", "does not", "Herman Cain", "wipe out\" the United States", "The man ran out of bullets and blew himself up.", "from an older generation", "wildland", "four", "the number of Americans who think things are going very badly has dropped from 40 percent in December to 32 percent now,\"", "clare quilty", "clare quilty", "750", "a missile", "clare clare quilty", "Expedia", "clare quilty", "severe", "jobs", "islam", "Rawalpindi", "Utah", "Sunday", "four", "a residential area", "Johannesburg", "nearly $2 billion", "six", "2002", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "\"It has never been the policy of this president or this administration to torture.\"", "Former Mobile County Circuit Judge Herman Thomas", "Cairo", "Lee Myung-Bak", "kerstin Fritzl", "they don't feelMisty Cummings is not considered suspects,", "bridesmaids", "Melbourne", "into the Southeast,", "Sunday,", "three minutes", "Salt Lake City, Utah", "millionaire's surtax", "an animal tranquilizer,", "bragg, North Carolina", "Hasmat Stanikzai,", "Jaime Andrade", "1994", "dance Your Ass Off", "Santiago Ram\u00f3n y Cajal", "in the present ( 2016 -- 2018, contemporaneous with airing )", "is a fortified complex at the heart of Moscow", "magic Circle", "island of Northumberland", "quiver", "clare quilty", "15,024", "novelist and poet", "mantle", "clare quilty", "marshmallows"], "metric_results": {"EM": 0.359375, "QA-F1": 0.44057267564810665}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.8, 0.5, 0.0, 0.0, 0.0, 0.20689655172413796, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.2857142857142857, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_squad-validation-526", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-1534", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-282", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-1094", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1864", "mrqa_searchqa-validation-13251"], "SR": 0.359375, "CSR": 0.5239955357142857, "retrieved_ids": ["mrqa_squad-train-76856", "mrqa_squad-train-19001", "mrqa_squad-train-32580", "mrqa_squad-train-36482", "mrqa_squad-train-6286", "mrqa_squad-train-48634", "mrqa_squad-train-9744", "mrqa_squad-train-75613", "mrqa_squad-train-56016", "mrqa_squad-train-21652", "mrqa_squad-train-79559", "mrqa_squad-train-3736", "mrqa_squad-train-33054", "mrqa_squad-train-17", "mrqa_squad-train-29640", "mrqa_squad-train-68809", "mrqa_naturalquestions-validation-10311", "mrqa_squad-validation-9912", "mrqa_naturalquestions-validation-8464", "mrqa_searchqa-validation-5631", "mrqa_newsqa-validation-1893", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-1378", "mrqa_squad-validation-3021", "mrqa_triviaqa-validation-7304", "mrqa_searchqa-validation-5755", "mrqa_triviaqa-validation-601", "mrqa_newsqa-validation-3790", "mrqa_squad-validation-7845", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6106", "mrqa_hotpotqa-validation-5808"], "EFR": 0.9512195121951219, "Overall": 0.6934023845818815}, {"timecode": 28, "before_eval_results": {"predictions": ["Peyton Manning", "civil disobedience", "Pittsburgh", "Cress", "molecular clouds in interstellar space", "Stefanie Scott", "the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "Ram Nath Kovind", "Senator Joseph McCarthy's Senate Permanent subcommittee on Investigations", "100", "members of the gay ( LGBT ) community", "Copper ( Cu ), silver ( Ag ), and gold ( Au )", "Wembley Stadium", "more than a million members", "1775", "Continental drift", "Julie Adams", "genetics and the male hormone dihydrotestosterone", "Jonathan Cheban", "Norman Greenbaum", "De Wayne Warren as Jarius `` G - Baby '' Evans", "Thirty years after the Galactic Civil War", "a sequel to the song `` Because of You '' ( 2004 ) but with a `` happy ending ''", "Obi - Wan's Padawan apprentice", "April 15, 2018", "April 17, 1982", "Speaker of the House of Representatives", "London, United Kingdom", "a minority report", "around 4500 BC in the Near East", "two senators", "Club Bijou on Chapel Street", "pre-Columbian times", "central plains", "China ( formerly the Republic of China ), Russia", "a costume party", "the gastrointestinal tract through a series of ducts, and endocrine because they secrete other substances directly into the bloodstream", "Kenny Anderson", "beneath the liver", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "Nathan Hale", "Jesse Frederick James Conaway", "naos", "defense against rain rather than sun", "port of Veracruz", "September 19, 2017", "Long Island in the summer of 1922", "it was first published on November 12, 1976 by Ballantine Books", "Butter Island off North Haven, Maine in the Penobscot Bay", "wintertime", "Craig T. Nelson", "Moira Kelly", "Flanagan and Allen", "Mississippi River", "Mediterranean Sea", "Manor of the More", "Craig William Macneill", "Democratic Unionist Party", "military personnel", "1964", "\"The Orchid Thief\"", "\"Saw the nakedness of his father\"", "Balfour Declaration", "Edgar Allan Poe"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5536622793056617}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 0.2, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1135", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-3145", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-2150", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1570", "mrqa_searchqa-validation-4495", "mrqa_searchqa-validation-12829"], "SR": 0.453125, "CSR": 0.521551724137931, "EFR": 0.9142857142857143, "Overall": 0.6855268626847291}, {"timecode": 29, "before_eval_results": {"predictions": ["Sophocles' play Antigone", "Meuse", "In 1806", "New Delhi", "MacFarlane", "Super Bowl XXXIX", "Hon July Moyo", "many forested parts of the world", "Narendra Modi", "forests and animals", "Andrew Michael Harrison", "White House Executive Chef", "Michael Crawford", "9 February 2018", "the red bone marrow of large bones", "Juice Newton", "Pangaea", "Gina Philips", "epidermis", "Joe Pizzulo and Leeza Miller", "Ming", "201", "Chuck Noland", "Detroit Red Wings ( 6 from 1956 to 1995 )", "Britney Spears", "Waylon Jennings", "Nancy Jean Cartwright", "Beyonc\u00e9 and Bruno Mars", "Philadelphia", "Boston Red Sox", "1996", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "United States customary units", "Joe Spano", "Michael Moriarty", "Rock Island, Illinois", "2002", "September 1959", "Louis Hynes", "Bonnie Lipton", "the Whig Party's colorful Log Cabin Campaign", "the probability of rejecting the null hypothesis given that it is true", "1890", "Rebekah", "the kitchen", "Tagalog or English", "Ernest Rutherford", "Napoleon Bonaparte", "In the 12th century", "Yosemite National Park", "Norman Pritchard", "2014", "florida", "stars and galaxies", "King Henry VI", "October 13, 1980", "motorsport world championship", "Polihale State Park", "Defense of Marriage Act", "the Bronx", "9 million", "\"Tennessee Waltz\"", "abacus", "the throne of Persia"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6599168754040077}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false], "QA-F1": [0.5, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.11764705882352942, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6638", "mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-2382", "mrqa_naturalquestions-validation-3760", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-5106", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-3606", "mrqa_searchqa-validation-2555"], "SR": 0.53125, "CSR": 0.521875, "EFR": 0.9333333333333333, "Overall": 0.6894010416666666}, {"timecode": 30, "UKR": 0.671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-392", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-974", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-13874", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15740", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-1649", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9269", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10181", "mrqa_squad-validation-10268", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1765", "mrqa_squad-validation-1791", "mrqa_squad-validation-1848", "mrqa_squad-validation-1890", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-2019", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2243", "mrqa_squad-validation-2411", "mrqa_squad-validation-2456", "mrqa_squad-validation-247", "mrqa_squad-validation-2545", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2742", "mrqa_squad-validation-305", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3507", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3718", "mrqa_squad-validation-3770", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-386", "mrqa_squad-validation-3863", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3986", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-4046", "mrqa_squad-validation-4054", "mrqa_squad-validation-4175", "mrqa_squad-validation-4213", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4629", "mrqa_squad-validation-4883", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5097", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5588", "mrqa_squad-validation-5692", "mrqa_squad-validation-5724", "mrqa_squad-validation-5781", "mrqa_squad-validation-5818", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-6019", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6353", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6543", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6965", "mrqa_squad-validation-6973", "mrqa_squad-validation-6999", "mrqa_squad-validation-7039", "mrqa_squad-validation-71", "mrqa_squad-validation-7192", "mrqa_squad-validation-7368", "mrqa_squad-validation-7426", "mrqa_squad-validation-7521", "mrqa_squad-validation-7612", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7814", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-829", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9176", "mrqa_squad-validation-9196", "mrqa_squad-validation-942", "mrqa_squad-validation-9445", "mrqa_squad-validation-957", "mrqa_squad-validation-9614", "mrqa_squad-validation-9764", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4465", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7461"], "OKR": 0.78515625, "KG": 0.40078125, "before_eval_results": {"predictions": ["space suit materials", "1992", "at least four", "Genesis", "real estate investment trusts (REITs)", "New Orleans", "carat", "Mission: Impossible.", "dikonos servant", "vi", "Teha'amana", "Galapagos", "Mark Twain", "battle of Chancellorsville", "boxing", "Wii", "Suez Canal Company", "Dave Matthews Band", "heningale", "dentures", "henry's Mom Did It", "kinko's", "a platypus", "light quantum", "skull", "Cherokee Nation", "necropolis", "henry vi", "clams", "henry vi", "henry vi.", "bamboos", "Newton", "Unabomber", "Narnia", "Freud", "henry", "libretti", "henry pohjola", "henry vi", "henry vi", "henry vi", "Cold Brew Iced Coffee Concentrate", "Botswana", "Susan B. Anthony dollar", "Mattel", "Little Red Riding Hood", "bone", "henry", "Slavic", "spine", "henukkah", "in a dry mouth, sun exposure, overclosure of the mouth, smoking, and minor trauma", "Castleford", "usually in May", "henry vi", "Sahara desert", "henry", "You're Next", "Headless Body in Topless Bar", "political correctness", "Hanin Zoabi,", "henry vi Rees, formerly known as Trevor Rees-Jones,", "homicide by undetermined means,"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4889880952380952}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-3969", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-6351", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-3542", "mrqa_searchqa-validation-16452", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-12684", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-838", "mrqa_searchqa-validation-7449", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-16480", "mrqa_searchqa-validation-5586", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-4851", "mrqa_searchqa-validation-382", "mrqa_naturalquestions-validation-2666", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-6237", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3728"], "SR": 0.390625, "CSR": 0.517641129032258, "retrieved_ids": ["mrqa_squad-train-42905", "mrqa_squad-train-23386", "mrqa_squad-train-14787", "mrqa_squad-train-32330", "mrqa_squad-train-54631", "mrqa_squad-train-21051", "mrqa_squad-train-3559", "mrqa_squad-train-64299", "mrqa_squad-train-29922", "mrqa_squad-train-20346", "mrqa_squad-train-1447", "mrqa_squad-train-35216", "mrqa_squad-train-29233", "mrqa_squad-train-52950", "mrqa_squad-train-73113", "mrqa_squad-train-16315", "mrqa_naturalquestions-validation-9330", "mrqa_squad-validation-2000", "mrqa_squad-validation-8904", "mrqa_newsqa-validation-3331", "mrqa_triviaqa-validation-1125", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-2466", "mrqa_triviaqa-validation-4517", "mrqa_searchqa-validation-12996", "mrqa_squad-validation-3994", "mrqa_naturalquestions-validation-9002", "mrqa_squad-validation-3435", "mrqa_triviaqa-validation-601", "mrqa_newsqa-validation-2899", "mrqa_squad-validation-7338", "mrqa_naturalquestions-validation-6211"], "EFR": 1.0, "Overall": 0.6750907258064516}, {"timecode": 31, "before_eval_results": {"predictions": ["the West", "Begter", "2007", "Sheev Palpatine", "the University of Oxford", "July 1, 1923", "to a hot solution of phenolsulfonphthalein in glacial acetic acid", "winter", "Freedom Day", "Assam and Meghalaya", "in front or on top of the brainstem", "Janie Crawford", "Jim Capaldi", "A to B", "the reign of King Beorhtric of Wessex", "the Department of Health and Human Services", "Blind carbon copy to tertiary recipients who receive the message", "the `` round '', the rear leg of the cow", "1957", "Martin Lawrence", "An error", "Andreas Vesalius", "Moscazzano", "Kristy Swanson", "bacteria", "Asuka", "Jay Baruchel", "Oceania", "revolution or orbital revolution", "Houston Astros", "Six Degrees of Separation", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "retina", "in the fascia surrounding skeletal muscle", "Pangaea", "2017", "the Local Fluff of the Local Bubble", "Ricky Nelson", "a special marker called a `` dabber '' or `` dauber ''", "Debbie Gibson", "Death Eaters", "the Mishnah", "the Formless All - pervasive Reality", "the winter", "Algeria", "the King James Bible", "Harlem River", "1998", "R.E.M.", "332", "above the light source and under the sample in an upright microscope", "Georgia Bulldogs", "Illinois", "Northumberland", "Northern Ireland", "Travis County", "Boston Bruins", "Adam Dawes", "Democrats", "Zed", "Mumbai", "a dummy", "Aristotle", "nothing"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6736427238046382}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4444444444444445, 0.11764705882352941, 0.5, 0.0, 1.0, 0.0, 0.13953488372093023, 0.9333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.375, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6428571428571429, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-9791", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4132", "mrqa_triviaqa-validation-3940", "mrqa_hotpotqa-validation-5831", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3518", "mrqa_searchqa-validation-6752"], "SR": 0.546875, "CSR": 0.5185546875, "EFR": 0.8275862068965517, "Overall": 0.6407906788793103}, {"timecode": 32, "before_eval_results": {"predictions": ["magnitude and direction", "2003", "1982", "Albert Lee Ueltschi", "Giotto", "1985", "more than 26,000", "Lakshmibai", "Premier League", "French", "2009", "\"homebrew\" settings", "Bonobo", "Robin David Segal", "Brigitte Nielsen (born Gitte Nielsen; 15 July 1963)", "Shameless", "a stolperstein", "1901", "Carl Zeiss AG", "Olivier Giroud", "Bambi, a Life in the Woods", "Robert \"Bobby\" Germaine", "2004", "IndyCar", "one season", "\"Twice in a Lifetime\"", "the Sun", "Greg Hertz", "Kolkata", "The Walking Dead", "Ted Nugent", "Kara Ross", "Gust Avrakotos", "Maleficent", "Serge Haroche", "Miami-Dade County", "Marty Ingels", "1945", "Edward R. Murrow", "Conservatorio Verdi", "Mindy Kaling", "Tara Lipinski", "beer and soft drinks", "Liga MX", "Donald Duck", "The School Boys", "Lord Chancellor of England", "President Patrick Hillery", "The English Electric Canberra", "Richa Sharma", "48,982", "The Sound of Music", "83 volumes", "Michigan State Spartans", "Frank Langella", "elephant", "a fool", "carbonic acid", "at London's Heathrow airport", "homicide", "maintain an \"aesthetic environment\" and ensure public safety", "Marshal ptain", "tank", "Hannah Montana"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6154246794871795}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.15384615384615385, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-1362", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-4838", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-5051", "mrqa_hotpotqa-validation-1030", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4392", "mrqa_hotpotqa-validation-4655", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3726", "mrqa_searchqa-validation-4641"], "SR": 0.546875, "CSR": 0.5194128787878788, "EFR": 1.0, "Overall": 0.6754450757575758}, {"timecode": 33, "before_eval_results": {"predictions": ["CEPR", "Lehramtstudien", "CTV Television Network", "13\u20133,", "American novelist, playwright, and screenwriter", "July 25 to August 4", "1958", "Norway", "twenty-three", "Crips", "The Crowned Prince of the Philadelphia Mob", "Saturday in May", "Charles Edward Stuart", "historic buildings, arts, and published works", "November 6, 2018", "Batman", "Tennessee", "Jean de Florette", "books, films and other media", "Donnchad mac Crinain", "Europop", "18 January", "Mayor Ed Lee", "Ghana", "Norwegian", "Dutch", "July 28, 1976", "January 23, 1898", "light quadricycles", "Nazareth", "Charlyn Marie \" Chan\" Marshall", "1968", "76,416", "Father Dougal McGuire", "June 17, 2007", "editor of Zeitschrift", "The United States of America (USA)", "The Ryukyuan people", "coaxial", "September 14, 1877", "international producers", "1936", "1952", "Indian", "painter and writer", "Pablo Escobar", "ZZ Top", "Larry Wayne Gatlin", "Russia", "Flex-fuel", "Shenandoah National Park", "King of Cool", "The border between the Cocos Plate and North American Plate, along the Pacific Coast of Mexico", "Barry Bonds", "Owen Vaccaro", "Machu Picchu", "Exile", "The gulf of loneliness and isolation between diner customers", "The U.S. Coast Guard said it has witnessed only normal maritime traffic around Haiti,", "U Win Tin,", "$1.45 billion", "onomatopoeia", "Singapore", "the femur"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6305555555555555}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.8, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.2666666666666667, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3231", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-316", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-3703", "mrqa_naturalquestions-validation-1519", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-742", "mrqa_searchqa-validation-15477"], "SR": 0.53125, "CSR": 0.5197610294117647, "retrieved_ids": ["mrqa_squad-train-35504", "mrqa_squad-train-30675", "mrqa_squad-train-77422", "mrqa_squad-train-12404", "mrqa_squad-train-69457", "mrqa_squad-train-44156", "mrqa_squad-train-80328", "mrqa_squad-train-46228", "mrqa_squad-train-20315", "mrqa_squad-train-41027", "mrqa_squad-train-56416", "mrqa_squad-train-683", "mrqa_squad-train-2203", "mrqa_squad-train-53165", "mrqa_squad-train-72758", "mrqa_squad-train-19181", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5360", "mrqa_hotpotqa-validation-996", "mrqa_searchqa-validation-12829", "mrqa_newsqa-validation-4122", "mrqa_squad-validation-7872", "mrqa_squad-validation-762", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-13900", "mrqa_naturalquestions-validation-4036", "mrqa_squad-validation-8730", "mrqa_squad-validation-3998", "mrqa_naturalquestions-validation-8934", "mrqa_squad-validation-5608", "mrqa_squad-validation-3497", "mrqa_newsqa-validation-831"], "EFR": 0.9666666666666667, "Overall": 0.6688480392156863}, {"timecode": 34, "before_eval_results": {"predictions": ["John Elway", "an upper limit", "over 20 million", "a simple iron boar crest", "Vienna", "the greater risk-adjusted return of value stocks over growth stocks", "the Harpe brothers", "Bill Clinton", "Dirk Werner Nowitzki", "Detroit, Michigan,", "Bury St Edmunds, Suffolk, England", "novelty songs, comedy, and strange or unusual recordings", "Mahoning County", "16 November 1973", "There Is Only the Fight", "Bohemia", "New York", "Don Baker", "400 MW", "Ghanaian", "Household Words", "Gatwick Airport", "Kagoshima Airport", "Minette Walters", "CTV", "comic book series Molly Danger", "2013", "Leslie Edwin Miles", "40 Days and 40 Nights", "James Tinling", "2014", "Louis King", "gull-wing doors", "devious authority figures", "Operation Neptune", "Attack the Block", "House of Commons", "Hessian", "Battle of Chester", "Wayne County, Michigan", "Samoa", "mistress of the Robes", "Eleanor of Aquitaine", "Barack Obama", "August 17, 2017", "Guardians of the Galaxy Vol. 2", "the 40th United States president", "1963", "Bologna Process", "Paris", "Nebraska Cornhuskers", "Salman Rushdie", "the Internal Revenue Service", "the Hongwu Emperor of the Ming Dynasty", "commemorating fealty and filial piety", "Mexico", "Arkansas", "throat abscess", "1979", "his father", "$8.8 million", "Red Heat", "Miriam Makeba", "a tooth"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6993303571428572}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10316", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-1797", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-2316", "mrqa_newsqa-validation-501", "mrqa_searchqa-validation-9394"], "SR": 0.59375, "CSR": 0.521875, "EFR": 1.0, "Overall": 0.6759375000000001}, {"timecode": 35, "before_eval_results": {"predictions": ["the DuMont Television Network", "Mount Kenya", "Albany, New York", "1908", "3 May 1958", "1986", "Ronald Wilson Reagan", "Chiltern Hills", "Ted 2", "Bay of Fundy", "CD Castell\u00f3n", "2001", "Sean Yseult", "country music", "The Hawai\u02bbi State Senate", "Operation Watchtower", "Paul W. S. Anderson", "15 February 1970", "Talib Kweli", "Shooter Jennings", "Cincinnati,", "\"Bad Moon Rising\"", "Johnny Cash", "The American record for the most time in space (381.6 days)", "Atomic Kitten", "Trey Parker and Matt Stone", "Matt Gonzalez", "Helensvale", "1979", "King \u00c6thelred the Unready", "PlayStation 4", "Malta", "1966", "Havana,", "Anne Perkins", "Black Mountain College", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies.", "was also a defender", "\"Chappelle's Show\"", "Prince George's County", "EQT Plaza in Pittsburgh, Pennsylvania", "1891", "Aerol\u00edneas A\u00e9reas", "Gainsborough Trinity Football Club", "Los Angeles,", "October 13, 1980", "water sprite", "India", "Syracuse University", "Women's World Championship", "Orange County", "76,416", "a diffuse system of small concentrations of lymphoid tissue found in various submucosal membrane sites of the body", "mathematical modeling and statistical estimation", "Will", "Caro-Kann", "Albert Reynolds", "George Washington", "U.S. senators", "Current TV", "two", "one bath", "The Lost Boys", "lima beans"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5939138986013985}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.45454545454545453, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.358974358974359, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-257", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-10259", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-1348", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-10491"], "SR": 0.515625, "CSR": 0.5217013888888888, "EFR": 0.967741935483871, "Overall": 0.669451164874552}, {"timecode": 36, "before_eval_results": {"predictions": ["between June and September", "Robert Hawkins", "a music video on his land.", "a bank", "in July for A Country Christmas,", "The Casalesi Camorra clan", "Tulsa, Oklahoma.", "41,280 pounds", "Old Trafford", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's war zone.", "Number 1 album on the Billboard 200 is actually the No. 1 selling album in the country.", "Matthew Perry and Leslie Mann", "Afghanistan", "stabbed Tate, who was 8\u00bd months pregnant, and wrote the word \"pig\" in blood on the door of the home the actress shared with her husband, director Roman Polanski.", "Fourth time lucky in Atlanta in 1996.", "Annie Duke", "was killed by an Israeli airstrike,", "that the legislation will foster racial profiling, arguing that most police officers don't have enough training to look past race while investigating a person's legal status.", "producing rock music with a country influence.", "The Kirchners", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "root out terrorists within its borders.", "violent separatist campaign", "at the ancient Greek site of Olympia", "3,000", "social and political vitality of the nation may depend on closing these racial gaps.", "The Detroit, Michigan, radio station promotion held three years ago was like a class to help women \" learn how to dance and feel sexy,\"", "22", "3-0", "150", "the two remaining crew members from the helicopter,", "South Korea", "more than 30 Latin American and Caribbean nations", "in Yellowstone National Park by grizzly bears,", "only one", "23 million square meters (248 million square feet)", "in Now Zad in Helmand province, Afghanistan.", "Virgin America", "fuel economy and safety while boosting", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "summer", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "3rd District of Utah.", "Republican National Convention", "56,", "The people kill him with the blocks, because the people are angry. They are not hungry, they are angry,\" Mano told CNN's Lisa Desjardins.", "Frank Ricci,", "the man of Steel", "90", "Cash for Clunkers program", "Argentina", "1998", "103", "Carolyn Sue Jones", "Vanilla", "Hercules", "for the world into six major climate regions, based on average annual precipitation, average monthly precipitation, and average monthly temperature", "Gian Carlo Menotti", "NBA's 50th anniversary All-Time Team", "Semites", "sister Maurice Utrillo", "Daisy Miller", "the Nokia Company Detaiils", "Apollo"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5511757058212461}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.5714285714285715, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2666666666666667, 0.125, 0.0, 1.0, 0.24242424242424243, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.8, 0.5, 1.0, 0.47058823529411764, 0.0, 1.0, 1.0, 1.0, 0.125, 0.5, 0.5454545454545454, 0.0, 0.0, 0.15384615384615385, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 0.9565217391304348, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-978", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-2051", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-4038", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4389", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-2623"], "SR": 0.40625, "CSR": 0.5185810810810811, "retrieved_ids": ["mrqa_squad-train-69061", "mrqa_squad-train-34918", "mrqa_squad-train-17785", "mrqa_squad-train-21718", "mrqa_squad-train-60818", "mrqa_squad-train-15536", "mrqa_squad-train-6674", "mrqa_squad-train-10138", "mrqa_squad-train-24136", "mrqa_squad-train-21631", "mrqa_squad-train-44422", "mrqa_squad-train-38784", "mrqa_squad-train-26168", "mrqa_squad-train-51062", "mrqa_squad-train-47620", "mrqa_squad-train-50983", "mrqa_triviaqa-validation-5294", "mrqa_newsqa-validation-250", "mrqa_naturalquestions-validation-2382", "mrqa_naturalquestions-validation-123", "mrqa_searchqa-validation-12440", "mrqa_squad-validation-9863", "mrqa_squad-validation-2943", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-4561", "mrqa_searchqa-validation-13686", "mrqa_naturalquestions-validation-4135", "mrqa_squad-validation-7332", "mrqa_hotpotqa-validation-4322", "mrqa_squad-validation-2463", "mrqa_hotpotqa-validation-3059"], "EFR": 1.0, "Overall": 0.6752787162162163}, {"timecode": 37, "before_eval_results": {"predictions": ["over 760 mm", "the state's attorney", "Abdullah Gul", "Carson", "the Genocide Prevention Task Force.", "off Somalia's coast.", "hand-painted Swedish wooden clogs", "an upper respiratory infection", "two", "tells stories of different women coping with breast cancer in five vignettes.", "Republican Gov. Bobby Jindal", "husband Bill Klein,", "Facebook and Google", "Damon Bankston", "American third seed Venus Williams", "to halt field exercises and training and return to their bases.", "Robert Mugabe", "a female soldier", "half-brother,", "J. Crew,", "$1.4 million", "the Democratic VP candidate", "Al-Aqsa mosque", "\"momentous discovery\"", "a three-story residential building in downtown Nairobi.", "Robert Barnett", "Asian", "Matthew Fisher", "Zimbabwe", "Ben Roethlisberger", "two", "9 percent of Turks polled by the Pew Research Center held favorable views of America, the lowest level among 47 countries surveyed.", "Sergeant. Jason Bendett", "United Arab Emirates", "on websites on the 24th.", "$24.1 million", "a jury", "Salt Lake City, Utah,", "Sunday.", "Mugabe", "13", "One of Osama bin Laden's sons", "for security reasons and not because of their faith.", "\"We tortured (Mohammed al.) Qahtani,\"", "autonomy", "from the Arctic north of Murmansk down to the southern climes of Sochi", "Long Island", "Ma Khin Khin Leh,", "400 years", "Elisabeth", "decommissioned Hanford nuclear site,", "the breast or lower chest of beef or veal", "winter", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Georgia", "bullfight", "the waltz", "1887", "Atlantic Coast Conference", "under the tutelage of his uncle Juan Nepomuceno Guerra", "Pennsylvania", "Rabbit", "Brunswick", "Labrador"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5830071385438234}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.75, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.9523809523809523, 1.0, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2949", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2448", "mrqa_naturalquestions-validation-1823", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-12609"], "SR": 0.4375, "CSR": 0.5164473684210527, "EFR": 0.9722222222222222, "Overall": 0.669296418128655}, {"timecode": 38, "before_eval_results": {"predictions": ["vector quantities", "Transport Workers Union leaders", "season five", "Eleven", "The Mexican military", "Pakistani officials,", "$7.8", "Stratfor", "Madeleine K. Albright", "Obama", "ties,", "the German Foreign Ministry,", "10,000", "IV cafe.", "Red Lines", "in body bags on the roadway near the bus,", "six Pakistan soldiers", "georgia peach", "Islamic militants", "Los Angeles", "last week,", "Sunni Arab and Shiite tribal leaders", "Stratfor's website", "an antihistamine and an epinephrine auto-injector,", "North Korea", "Hong Kong", "Israel will release in exchange for two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "ties,", "President Sheikh Sharif Sheikh Ahmed", "Saturday's Hungarian Grand Prix.", "sanctions 17 entities,", "abuse", "in an artificial coma", "9-1", "Africa", "fear of losing their licenses to fly.", "Robert Mugabe", "FBI.", "his five-set defeat to Del Potro.", "\"it should stay that way.\"", "CNN", "Joe Pantoliano", "the strength of its brand name and the diversity of its product portfolio,", "British Foreign Office", "Monday's suicide blast outside the district courthouse in Peshawar.", "georgia peach's mother", "sculptures", "Pakistan's High Commission in India", "Bryant Purvis,", "pain-relief drugs.", "1871", "Kenneth Kaunda", "Gestalt psychology", "animals phobia", "Treaty of Utrecht", "Massachusetts", "James Harrison", "Ford Island", "Tomorrowland", "Tiger Woods", "greed", "Martin Luther King's", "Sesame Street"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5818786075036075}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.16, 1.0, 0.33333333333333337, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.2, 0.0, 1.0, 0.0, 1.0, 0.6, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-237", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-1060", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-6409", "mrqa_searchqa-validation-13907"], "SR": 0.453125, "CSR": 0.514823717948718, "EFR": 0.9714285714285714, "Overall": 0.6688129578754579}, {"timecode": 39, "before_eval_results": {"predictions": ["Andrew Lortie", "people are paid for each specific, short-term task that they do and don't have conventional contracts of employment", "ovuliferous", "high cooking", "Silver Hatch", "nerves", "Ethiopia", "Red Admiral", "Jets", "Harrier and Harrier Mini", "space travel", "special administrative regions", "Alastair Cook", "Enterprise Rent-A-Car, National Car Rental and Alamo Rent A Car brands.", "Three Little Pigs", "Asia", "glenoid cavity", "success,", "meninges", "English and French", "Charles Brandon", "Munich", "Henry Mancini", "Fred Astaire", "the Caesars", "woe", "Sudan", "Low Countries", "drama", "The Book of Proverbs", "stand-up comedian", "Jamaica", "Tornado", "drama", "s\u00e3o Vicente Island", "pancreas", "puff-the-magic-dragon", "football", "Antoine Lavoisier", "Leon Trotsky", "Gondwana", "public disorder", "Pet Shop Boys", "Matthew Boulton and James Watt", "Algiers", "Marks & Co,", "Aabaptists and the non-sectarians", "St. Colette", "Hebrew", "Jim Davidson", "herpes virus,", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods )", "Garfield Sobers", "in the mountains outside City 17, Resistance fighters Gordon Freeman and Alyx Vance climb from the wreckage of the train they used to escape the city", "Kang and Kodos", "Johannes Vermeer", "O.T. Genasis", "ClimateCare,", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Kevin Kuranyi", "Lost in America", "in the Northern Hemisphere", "Soviet Union", "Republicans"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48334385521885526}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.19999999999999998, 1.0, 0.37037037037037035, 0.5, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-10606", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-2509", "mrqa_searchqa-validation-6304"], "SR": 0.421875, "CSR": 0.5125, "retrieved_ids": ["mrqa_squad-train-10613", "mrqa_squad-train-44662", "mrqa_squad-train-66370", "mrqa_squad-train-40728", "mrqa_squad-train-24041", "mrqa_squad-train-14184", "mrqa_squad-train-24525", "mrqa_squad-train-52956", "mrqa_squad-train-20909", "mrqa_squad-train-39087", "mrqa_squad-train-79709", "mrqa_squad-train-13263", "mrqa_squad-train-37561", "mrqa_squad-train-26481", "mrqa_squad-train-60461", "mrqa_squad-train-53511", "mrqa_squad-validation-9166", "mrqa_searchqa-validation-15995", "mrqa_squad-validation-3130", "mrqa_triviaqa-validation-4457", "mrqa_hotpotqa-validation-2847", "mrqa_searchqa-validation-14446", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-5703", "mrqa_triviaqa-validation-6548", "mrqa_hotpotqa-validation-2887", "mrqa_newsqa-validation-1512", "mrqa_squad-validation-2949", "mrqa_squad-validation-2463", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-227"], "EFR": 1.0, "Overall": 0.6740625}, {"timecode": 40, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1389", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10606", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-1264", "mrqa_searchqa-validation-12829", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16453", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10263", "mrqa_squad-validation-10317", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10369", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-127", "mrqa_squad-validation-1408", "mrqa_squad-validation-1430", "mrqa_squad-validation-1453", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2094", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3124", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3444", "mrqa_squad-validation-3497", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3946", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-4715", "mrqa_squad-validation-491", "mrqa_squad-validation-4918", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5664", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6706", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7147", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7296", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7751", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8154", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-8841", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-568", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-899"], "OKR": 0.873046875, "KG": 0.4796875, "before_eval_results": {"predictions": ["a \"chameleon circuit\"", "Jake LaMotta", "Danish", "Joshua", "pangrams", "Let Die", "belgian", "lancaster", "australia", "Robert Hooke", "gaspard de Coligny", "Napier", "Sony Interactive Entertainment", "king Henry I of England", "green", "1215", "flower", "Robinson Crusoe", "Charles Dickens", "belgian", "Egypt", "sun", "earache", "New York Yankees", "Four Tops", "hudson", "July 20,", "9", "bali", "lilac", "Hilary Swank", "a scarlet tanager", "a dove", "a mouse", "John McCarthy", "Broadway musical", "three", "George lV", "shaft", "the heel of the foot", "daily Mirror", "za", "horse", "australia", "al Abyad", "Machu Picchu", "Paul McCartney", "Madness", "Jane Eyre", "Kansas", "australia", "A marriage officiant", "Ricky Nelson", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "Port Moresby, Papua New Guinea", "a bass", "Security Management", "eight", "Russia", "Rima Fakih", "Malaysia", "Hank Aaron", "Livin", "Visible Hand"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5622395833333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-571", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-1746", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6915", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-774", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-5984", "mrqa_naturalquestions-validation-6903"], "SR": 0.484375, "CSR": 0.5118140243902439, "EFR": 1.0, "Overall": 0.7182221798780487}, {"timecode": 41, "before_eval_results": {"predictions": ["Robert A. Millikan", "art fair", "Los Angeles", "they are \"still trying to absorb the impact of this week's stunning events.\"", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Tim Clark, Matt Kuchar and Bubba Watson", "Philip Markoff,", "Haeftling", "forgery and flying without a valid license,", "at Sea World", "Mafia", "Nat King", "1800s", "convicts caught with phones", "16", "cancer", "$40 and a loaf of bread.", "\"fragrances of fries drifting under Mona Lisa's nose\"", "she's in love,", "France", "President Nicolas Sarkozy and Canada's Prime Minister Stephen Harper", "more than 100", "South Africa's", "Madeleine K. Albright", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "back at work", "be inducted into the Baseball Hall of Fame", "collapsed ConAgra Foods plant in the town of Garner,", "five", "\"wildcat\" strikes,", "The father of Haleigh Cummings,", "Elisabeth's father,", "his club", "they", "$60 billion", "$199", "J.G. Ballard", "Republicans", "he discussed foreplay, sexual conquests and how he picks up women,", "the Airbus A330-200", "United States, NATO member states, Russia and India", "fatally shooting a limo driver", "tie salesman", "\"We have to condition the dogs to the shoes,\"", "all three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "China", "Steve Jobs", "\"Rin Tin: The Life and the Legend\"", "Sri Lanka's", "The station", "the state's attorney", "First Lieutenant Israel Greene", "126 by Wilt Chamberlain", "Brevet Colonel Robert E. Lee", "Telegraph Media Group Limited 2017", "Rabin", "le Marseillaise", "Kristy Lee Cook", "John Samuel Waters,", "Norman Reedus", "denned women", "General Douglas MacArthur", "rice", "at least 18 or 21 years old"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5472599147531213}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.9090909090909091, 0.04761904761904762, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5555555555555556, 0.0, 1.0, 1.0, 0.17391304347826086, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.08333333333333334, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.9090909090909091, 0.5, 1.0, 1.0, 1.0, 0.4, 1.0, 0.33333333333333337, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7880", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-371", "mrqa_naturalquestions-validation-5825", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-2819", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2138", "mrqa_searchqa-validation-1621", "mrqa_searchqa-validation-149", "mrqa_naturalquestions-validation-8617"], "SR": 0.40625, "CSR": 0.5093005952380952, "EFR": 0.9736842105263158, "Overall": 0.7124563361528822}, {"timecode": 42, "before_eval_results": {"predictions": ["ITT", "246", "passion", "The cause of the child's death will be listed as homicide by undetermined means,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Obama", "20", "Oxygen Channel's \"Dance Your Ass Off\"", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls", "15-year-old", "Umar Farouk AbdulMutallab", "London and Buenos Aires", "Democratic National Convention", "in his native Philippines", "Kitty Kelley, biographer of the rich and famous,", "wales", "Democrats and Republicans", "Amanda Knox", "Michael Krane,", "15,000", "about 12 million", "the Gulf", "May 4", "3-0", "Robert Mugabe", "people,", "kill then-Sen. Obama", "10", "165", "when the economy turns unfriendly,", "Ignazio La Russa", "Amir Zaki", "$40 and a loaf of bread.", "Nashville,", "Tulsa, Oklahoma.", "86th minute", "$2 billion", "Russian concerns that the defensive shield could be used for offensive aims.", "1981", "Knox's parents, of course, shattered.", "Los Angeles, California.", "Prague", "more than 100", "Microsoft", "Michael Partain,", "Mitt Romney", "Islamic", "prisoners at the South Dakota State Penitentiary", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates,", "full health-care coverage,", "in the episode `` Cyborg ''", "in Llantrisant, wales", "1973", "Afghanistan", "Martin Howe", "spey", "\"Shake It Off\"", "Cheshire", "Brookhaven", "Transamerica", "\"The Professor's House\"", "The Moonstone", "16"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5411515567765568}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.26666666666666666, 1.0, 0.4, 0.0, 0.2857142857142857, 0.5384615384615384, 0.0, 0.5, 0.4, 0.5, 0.4, 0.25, 0.0, 0.0, 0.4, 1.0, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5714285714285715, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.3, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1429", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-1380", "mrqa_hotpotqa-validation-1900", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-709", "mrqa_naturalquestions-validation-1640"], "SR": 0.421875, "CSR": 0.5072674418604651, "retrieved_ids": ["mrqa_squad-train-69189", "mrqa_squad-train-14776", "mrqa_squad-train-64757", "mrqa_squad-train-57858", "mrqa_squad-train-18861", "mrqa_squad-train-58072", "mrqa_squad-train-69755", "mrqa_squad-train-58552", "mrqa_squad-train-67318", "mrqa_squad-train-28093", "mrqa_squad-train-69974", "mrqa_squad-train-4178", "mrqa_squad-train-10367", "mrqa_squad-train-16087", "mrqa_squad-train-16799", "mrqa_squad-train-52019", "mrqa_searchqa-validation-1857", "mrqa_squad-validation-1863", "mrqa_naturalquestions-validation-276", "mrqa_searchqa-validation-2714", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-1705", "mrqa_searchqa-validation-2568", "mrqa_naturalquestions-validation-8228", "mrqa_searchqa-validation-2141", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-1123", "mrqa_naturalquestions-validation-10461", "mrqa_newsqa-validation-1456", "mrqa_squad-validation-9176", "mrqa_newsqa-validation-3232", "mrqa_naturalquestions-validation-1135"], "EFR": 1.0, "Overall": 0.717312863372093}, {"timecode": 43, "before_eval_results": {"predictions": ["400", "14", "3-0", "the issue to a crowd at the White House,", "a \"procedure on her heart,\"", "Ensenada,", "the punishment for the player", "wings", "Vernon Forrest,", "Mandi Hamlin", "U.S. State Department and British Foreign Office", "five female pastors", "Phoenix, Arizona,", "\"We tortured (Mohammed al ) Qahtani,\"", "The elephant Sanctuary", "Six", "Wednesday.", "the District of Columbia near Takoma Park, Maryland.", "United States", "doctors", "Hamas forces", "the driver", "Michael Jackson", "1,500", "through the weekend,", "three", "\"novel\"", "Aniston, Demi Moore and Alicia Keys", "January", "to hold onto his land", "Cotto", "Two pages -- usually high school juniors who serve Congress as messengers", "Pfc. Bowe Bergdahl", "shark River Park in Monmouth County", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "all buses, subways and trolleys in Philadelphia and on the Frontier line in Bucks and Montgomery counties stopped running at 3 a.m.", "five", "Long troop deployments", "St. Louis, Missouri.", "based on what their cars say about them.", "a number of calls,", "T.I.", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974", "Republican Party,", "almost 9 million", "Asashoryu", "an upper respiratory infection", "Adriano", "\"Zed,\" a Columbian mammoth", "\"Draquila -- Italy Trembles.\"", "the Americas Division of Human Rights Watch,", "1973", "Roanoke is the sixth season of theFX horror anthology television series American Horror Story", "Kelley Johansson", "skull", "Red Sea", "Atlantic Ocean", "Tyler \"Ty\" Mendoza", "Robert Allen Iger", "Salgaocar", "Persuasion", "Abercrombie & Fitch", "a soap opera", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.5, "QA-F1": 0.6211810245353202}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3076923076923077, 0.0, 0.8, 0.14285714285714288, 0.32, 1.0, 1.0, 0.3333333333333333, 0.47058823529411764, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.47058823529411764, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.21052631578947367]}}, "before_error_ids": ["mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-6678", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-802", "mrqa_searchqa-validation-6252", "mrqa_naturalquestions-validation-4915"], "SR": 0.5, "CSR": 0.5071022727272727, "EFR": 1.0, "Overall": 0.7172798295454544}, {"timecode": 44, "before_eval_results": {"predictions": ["transfer and dissipate excess energy", "on Chesapeake Bay, south of Annapolis in Maryland", "Mediterranean Sea", "Alex", "Sailor Meets Sindbad the Sailor", "2001", "August 2015", "neither", "devised by Leonard Nimoy, who portrayed the half - Vulcan character Mr. Spock on the original Star Trek television series", "Rodney Crowell", "Bruce Willis", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "a donor molecule to an acceptor molecule", "Jamie Foxx", "Iowa ( 36.6 % )", "1996", "The uvea", "the President of the United States", "biological taxonomy", "Zeebo", "1977", "more than 6,000", "Philadelphia", "development of electronic computers in the 1950s", "The Archers is the world's longest - running radio soap opera", "1939", "Kristy Swanson", "Jyoti Basu", "A nominating committee composed of rock and roll historians selects names for the `` Performers '' category ( singers, vocal groups, bands, and instrumentalists of all kinds )", "1998", "naturalization", "in the Colony of Virginia", "Arkansas", "December 24, 1836", "the four principal phases of the Moon are new moon, first quarter, full moon, and third quarter ( also known as last quarter )", "American Civil War", "the Executive Residence of the White House Complex", "200 to 500 mg", "Timothy B. Schmit", "January 4, 2016", "Thirty years after the Galactic Civil War, the First Order has risen from the fallen Galactic Empire", "Woody Paige", "Anna Faris", "$75,000", "four", "third", "between the stomach and the large intestine", "USS Chesapeake", "18 - season", "hope that a happy day being marked would recur many more times", "President Lyndon Johnson", "Sarah Palin's", "Al Pacino,", "Passion", "Kinnairdy Castle", "Teenage Mutant Ninja Turtles", "Kona", "his business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court.", "40", "16", "John Deere", "water skiing", "dollop", "(officially named People's Democratic Republic of Algeria)"], "metric_results": {"EM": 0.375, "QA-F1": 0.5320682743791296}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.9333333333333333, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.1714285714285714, 0.0, 0.18181818181818182, 0.4, 1.0, 1.0, 0.09523809523809525, 0.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6, 1.0, 0.0, 1.0, 1.0, 0.0, 0.20000000000000004, 1.0, 0.0, 0.9166666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.3157894736842105, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9445", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-9361", "mrqa_triviaqa-validation-3099", "mrqa_triviaqa-validation-5874", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-880", "mrqa_searchqa-validation-2656", "mrqa_triviaqa-validation-6927"], "SR": 0.375, "CSR": 0.5041666666666667, "EFR": 0.975, "Overall": 0.7116927083333333}, {"timecode": 45, "before_eval_results": {"predictions": ["locomotion", "The first likely description of the disease was in 1841 by Charles Oscar Waters", "Cheryl Campbell", "The Satavahanas", "Michael Moriarty", "Canada", "111", "Virginia", "RAM", "1930s", "Doug Diemoz", "Charlene Holt", "Amanda Leighton", "Thomas Edison", "Hedwig", "O'Meara", "the Atlanta Hawks", "Missi Hale", "2001", "Eddie Murphy", "Asa Taccone", "Theodore Roosevelt", "1940", "parthenogenic", "Lyle Waggoner", "Paradise, Nevada", "Coconut Cove", "the beginning of the third season", "Moloch is the biblical name of a Canaanite god associated with child sacrifice", "Mercedes - Wagen", "10 June 1940", "Bill Pullman", "Great G minor symphony", "his mother's side of the family, the Campbells, led by their grandfather Samuel", "April 2010", "786", "to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Wake County", "National Football League ( NFL )", "generally lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Harry Potter", "Martin Lawrence", "Effy", "the priests and virgins", "Nurhaci", "Muhammad", "a British soldier", "August 22, 1980", "Professor Kantorek", "Yondu Udonta", "the next episode, `` Seeing Red ''", "graphite", "bushfires", "Adrian Edmondson", "Figaro", "Big 12 Conference", "Debbie Reynolds", "Dubai", "the legitimacy of that race.", "Salt Lake City,", "Java", "OPEC", "a jazz saxophonist", "CNN"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5802499359959037}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.3333333333333333, 0.9, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.2666666666666667, 0.0, 0.967741935483871, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-5296", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4457", "mrqa_newsqa-validation-2590"], "SR": 0.453125, "CSR": 0.5030570652173914, "retrieved_ids": ["mrqa_squad-train-82054", "mrqa_squad-train-38339", "mrqa_squad-train-57522", "mrqa_squad-train-41176", "mrqa_squad-train-54225", "mrqa_squad-train-71596", "mrqa_squad-train-8502", "mrqa_squad-train-1971", "mrqa_squad-train-34410", "mrqa_squad-train-59769", "mrqa_squad-train-51252", "mrqa_squad-train-63248", "mrqa_squad-train-32710", "mrqa_squad-train-63187", "mrqa_squad-train-20408", "mrqa_squad-train-40921", "mrqa_searchqa-validation-10856", "mrqa_squad-validation-7147", "mrqa_searchqa-validation-12267", "mrqa_triviaqa-validation-6882", "mrqa_newsqa-validation-2816", "mrqa_triviaqa-validation-1683", "mrqa_newsqa-validation-774", "mrqa_triviaqa-validation-1348", "mrqa_hotpotqa-validation-1843", "mrqa_searchqa-validation-3203", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-3437", "mrqa_searchqa-validation-7517", "mrqa_hotpotqa-validation-3951", "mrqa_triviaqa-validation-834", "mrqa_naturalquestions-validation-5550"], "EFR": 0.9714285714285714, "Overall": 0.7107565023291925}, {"timecode": 46, "before_eval_results": {"predictions": ["the main porch", "Pastoral farming", "Hold On", "the ACU", "Luther Ingram", "Johnny", "Lucius Verus", "Siddharth Arora / Vibhav Roy", "Ray Harroun", "Shiji no yukikai", "Frank Morris", "to establish an electrochemical gradient ( often a proton gradient ) across a membrane", "Copernicus", "a set of related data", "the President pro tempore", "to prevent the flame from being blown out and enhances a thermally induced draft", "electron donors", "T.J. Miller", "Ren\u00e9 Descartes", "2006", "the dealer sets the cards face - down on the table near the player designated to make the cut, typically the player to the dealer's right", "1955", "the town of Acolman, just north of Mexico City", "23 September 1889", "indigenous to many forested parts of the world", "January 2018", "Colon Street", "The higher the vapor pressure of a liquid at a given temperature", "adenine ( A ), uracil ( U )", "1923", "Hugh S. Johnson", "the central nervous system", "Lord Banquo", "harm - joy", "lithium", "al - khimar", "276", "Anthony Hopkins", "the middle of the 15th century", "Paul Henreid", "political ideology", "c. 1000 AD", "Definition of the problems and / or goals", "Missouri River", "2,140 kilometres ( 1,330 mi )", "Anakin Kenobi", "private sector", "the chief lawyer of the United States government", "2018", "C\u03bc and C\u03b4", "August 29, 2017", "red wine", "James Mitchell", "Black Swan", "Lake Wallace", "Paul W. S. Anderson", "1790", "9 percent", "the world's tallest building,", "the commissions as a legitimate forum for prosecution,", "\"unconquerable will of the occupied territories\"", "Vedas", "The Autobiography of Alice B. Toklas", "Bowness"], "metric_results": {"EM": 0.46875, "QA-F1": 0.56696221285435}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4166666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.5, 0.0, 0.3870967741935484, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-4197", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-7285", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-4201", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-1488", "mrqa_triviaqa-validation-5511"], "SR": 0.46875, "CSR": 0.5023271276595744, "EFR": 0.9411764705882353, "Overall": 0.7045600946495619}, {"timecode": 47, "before_eval_results": {"predictions": ["Germany", "Tim Russert", "Amy Gregorio ( Vanessa Ferlito )", "Lisbon Lions", "1978", "two", "September 6, 2019", "Cliff's father", "September 19, 2017", "the Anglo - Norman French waleis", "31 October 1972", "23 September 1889", "Speaker of the House of Representatives", "Pittsburgh", "frontal lobe", "1940s", "increased productivity, trade, and secular economic trends", "around 2 %", "approximately 5 liters", "G -- Games", "the 17th episode in the third season", "the balance sheet is the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year reported on the accompanying income statement", "the New York Yankees", "94 by 50 feet", "Sam Waterston", "the Undying Lands, along with Tol Eress\u00eba and the outliers of Aman", "wisdom", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak )", "bohrium", "Ravi Shastri", "the gated community of Pebble Beach", "the tax on trade in and out of the empire, along with all the gold Mansa Musa had", "electrons", "November 2014", "Lewis Carroll", "Janis Joplin", "the South Pacific Ocean", "T'Pau", "occupation, division, and colonisation of African territory by European powers during the period of New Imperialism, between 1881 and 1914", "Blue laws", "in South America", "Edgar Lungu", "Melanie Martinez", "An empty line", "shout `` Yes '' or `` Bingo ''", "Brian Steele", "to seize power in Munich, Bavaria,", "Gary Grimes", "1998", "Thomas Chisholm", "Tommy James", "nausea", "Dip; Blip; Trouble; Bubble", "\"The best is yet to come.\"", "Chattahoochee", "Patterns of Sexual Behavior", "David X. Cohen", "Argentina", "an ice jam", "Facebook and Google,", "decaffeinated coffee", "One Flew Over the Cuckoo's Nest", "Stephen Hawking", "treats it more than perpetuate misconceptions about the state and its citizens."], "metric_results": {"EM": 0.4375, "QA-F1": 0.5576061299707289}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.13793103448275862, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.8, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.4, 0.08695652173913042, 0.33333333333333337, 0.0, 0.4, 0.15, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-485", "mrqa_triviaqa-validation-4834", "mrqa_triviaqa-validation-2385", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-2819", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14104", "mrqa_newsqa-validation-2608"], "SR": 0.4375, "CSR": 0.5009765625, "EFR": 1.0, "Overall": 0.7160546875}, {"timecode": 48, "before_eval_results": {"predictions": ["Sen. Barack Obama", "Stuttgart", "Three", "Long troop deployments", "well over 1,000 pounds", "allows 10 boys and 10 girls between the age of eight and 11 to create their own mini-societies,", "Dennis Davern,", "in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "people", "a jury", "Two Swedish journalists", "Microsoft", "Ferraris", "2000 and 2004", "From Spain to the Caribbean", "frees up a place", "Michael Krane,", "Russian bombers", "three gunmen outside the facility where aid distribution is coordinated.", "9 a.m.-7 p.m.", "2-1", "200", "it", "a one-shot victory in the Bob Hope Classic", "a relatively neglected topic.", "sodium dichromate,", "nearly $2 billion", "155", "Naples", "racial intolerance.", "Friday,", "\"Twilight\"", "Sheikha Lubna Al Qasimi", "22-year-old", "couple's surrogate,", "El Paso, Texas.", "10", "Retailers who don't speak out against it", "Anil Kapoor.", "Samoa", "authorizing killings and kidnappings by paramilitary death squads.", "E. coli", "Ma Khin Khin Leh,", "\"It was the influence of his Japanese grandmother that first led him on the path to enka.", "\"It's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "1998", "NATO to do more to stop the Afghan opium trade", "London's O2 arena", "The EU naval force", "Cyprus", "The Obama administration", "between $10,000 and $30,000", "1602", "Number 4, Privet Drive, Little Whinging in Surrey, England", "sugarcane", "a dollar bill", "a nylon base material coated with a flexible, 100% waterproof, PVC", "five", "E Street Band", "England", "chippewa", "salt", "Wang Chung", "a specialist US financial institution that provides settlement services to its members in the foreign exchange market (FX )"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5503927865612648}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.45454545454545453, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.5, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 0.1, 0.08695652173913043, 1.0, 0.3, 1.0, 1.0, 1.0, 1.0, 0.32, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-1614", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-572", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2183", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-3970", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-2918", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4399", "mrqa_naturalquestions-validation-3236"], "SR": 0.46875, "CSR": 0.5003188775510203, "retrieved_ids": ["mrqa_squad-train-46581", "mrqa_squad-train-67382", "mrqa_squad-train-72172", "mrqa_squad-train-36352", "mrqa_squad-train-2390", "mrqa_squad-train-16424", "mrqa_squad-train-7648", "mrqa_squad-train-83716", "mrqa_squad-train-16705", "mrqa_squad-train-23163", "mrqa_squad-train-52144", "mrqa_squad-train-56462", "mrqa_squad-train-31065", "mrqa_squad-train-50803", "mrqa_squad-train-70627", "mrqa_squad-train-44647", "mrqa_searchqa-validation-13919", "mrqa_naturalquestions-validation-9979", "mrqa_searchqa-validation-1999", "mrqa_naturalquestions-validation-7095", "mrqa_searchqa-validation-11991", "mrqa_naturalquestions-validation-9737", "mrqa_newsqa-validation-4054", "mrqa_naturalquestions-validation-6340", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-1227", "mrqa_naturalquestions-validation-2794", "mrqa_triviaqa-validation-2926", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-801", "mrqa_triviaqa-validation-2516", "mrqa_hotpotqa-validation-788"], "EFR": 0.9705882352941176, "Overall": 0.7100407975690276}, {"timecode": 49, "before_eval_results": {"predictions": ["Pierre Laval", "Harriet Harman", "edward woodward", "Michaela Tabb", "Orion", "Edward VIII", "Demi Holborn", "falcon", "Stephen Fry", "Libya", "Vadamar", "Robinson", "Roshi", "The Today", "William Shakespeare", "Lancaster", "dying, death and how human beings respond to the inevitability of their mortality and the reality of loss", "Rod Laver", "Texas", "(Classical Music) music (to be performed) in a fiery manner", "Sicily, Italian Sicilia, island of Italy", "hoy", "human rights issues, both thematic and country-specific,", "Brian Deane", "Volkswagen Golf", "Emilia Fox", "October", "i was raised by a toothless, bearded hag,", "Catherine zeta", "South Africa", "Jim Braddock", "Mediterranean", "1819", "ocellae", "the Amorites before the children of Israel", "Hawaii", "vomiting", "Richard Strauss", "albino sperm whale", "Amina Abdallah Alaf al Omari", "penguin", "golf", "purpurea", "Amnesty International", "Spearchucker's full name becomes Oliver Harmon Jones", "bullfighting", "Lesotho", "Variations", "Mauricio Pochettino", "Duke of Edinburgh", "myxoma", "the season - five premiere episode `` Second Opinion ''", "U.S. state of Georgia", "Parker's pregnancy at the time of filming", "Eileen Atkins", "Battle of Chester", "James Gandolfini", "Rebecca Guerrero,", "Angels", "56", "Twenty three", "Berthas", "Eccentricity", "Patty Duke"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5722953216374269}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3157894736842105, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666667, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-5470", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-3752", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-4066", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-2853", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7507", "mrqa_hotpotqa-validation-831", "mrqa_newsqa-validation-4180", "mrqa_newsqa-validation-3793", "mrqa_searchqa-validation-12134", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-4996"], "SR": 0.484375, "CSR": 0.5, "EFR": 0.9090909090909091, "Overall": 0.6976775568181818}, {"timecode": 50, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4431", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-962", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2365", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-893", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5943", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-899"], "OKR": 0.85546875, "KG": 0.4390625, "before_eval_results": {"predictions": ["A Christmas Carol", "james zaks", "Bangladesh", "eagle", "Sunset Boulevard", "South Wales", "Berlin", "Rocky Horror Picture Show", "1925", "Prince Albert", "bill", "Pakistan", "pig", "Popeye", "james boswell", "bull moose", "Genoa", "shierce pierce", "syria", "Jamaica", "Jessica Simpson", "fred stooge", "earthquake", "capua", "Charlie Chan", "chiba", "louis james boswell", "louis pierce", "Anne Boleyn", "playoff basketball", "Thailand", "127 Hours", "james boswell", "fred pierce", "Portsmouth", "louis armstrong", "erebus", "james boswell", "eKIN", "Wolfgang Amadeus Mozart", "Anne-Marie Duff", "Joan Rivers", "salt", "pierce sampras", "phobias", "pears soap", "guitar", "Toby", "Argentina", "Kenny Everett", "Fenn Street School", "1804", "spraying the whole atmosphere as if drawing letters in the air ( `` penciling '' )", "November 3, 2007", "james Bingham", "Marktown", "Bit Instant", "well over 1,000 pounds", "Jet Republic", "Won Sei Hoon, who heads South Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim", "w. c. handy", "lamb of God", "feet", "1978"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5702888257575758}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4901", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-23", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2685", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5281", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-9447", "mrqa_searchqa-validation-3244", "mrqa_searchqa-validation-10670"], "SR": 0.53125, "CSR": 0.5006127450980392, "EFR": 1.0, "Overall": 0.6980912990196078}, {"timecode": 51, "before_eval_results": {"predictions": ["cannes", "blue", "Robin Ellis", "mortadella", "albho", "helium", "the foot", "the natural world and mysticism", "ballando con le stelle", "South Pacific", "Agatha Christie", "Bosnia and Herzegovina", "France", "Sparta", "Morningtown Ride", "squash", "Northwestern University", "Turkey", "robert hargreaves", "China", "diffusion", "David Bowie", "Robben Island", "bukwus", "jocelyn peabody", "a zoom lens", "japan", "robert peabody", "Benny Hill", "Wiz Khalifa", "jocelyn peabody", "Egypt", "the wren", "Eton College", "jocelyn peabody", "Siamese", "August 24", "a highly dangerous Boojum", "can be grouped together to express any nuance of an idea", "Valentine Dyall", "jocelyn peabody", "peabody", "Opus Dei", "the Flying Pickets", "Dry Ice", "Kenya", "jane disraeli", "jocelyn", "jocelyn peabody", "robert peabody", "blood left at crime scenes", "the Islamic Community", "Rachel Kelly Tucker", "Honor\u00e9 Mirabeau", "Big Bad Wolf and Das Festhaus", "Brian Kirk", "Steve Trevor", "President Obama", "from the back of Arsenal dugout then starting to walk down the touchline as the whistle was blown.", "Tom Hanks", "robert Lombardi", "curly wurly", "rosa peabody", "a bed bug"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48020833333333335}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-5154", "mrqa_triviaqa-validation-114", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-4976", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-7596", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-1455", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-3177", "mrqa_hotpotqa-validation-2075", "mrqa_newsqa-validation-318", "mrqa_searchqa-validation-7204", "mrqa_searchqa-validation-2456", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-11960"], "SR": 0.421875, "CSR": 0.4990985576923077, "retrieved_ids": ["mrqa_squad-train-1160", "mrqa_squad-train-52765", "mrqa_squad-train-64442", "mrqa_squad-train-66661", "mrqa_squad-train-83499", "mrqa_squad-train-85923", "mrqa_squad-train-73862", "mrqa_squad-train-31385", "mrqa_squad-train-35361", "mrqa_squad-train-75923", "mrqa_squad-train-31867", "mrqa_squad-train-7490", "mrqa_squad-train-51186", "mrqa_squad-train-39356", "mrqa_squad-train-73623", "mrqa_squad-train-41047", "mrqa_squad-validation-3969", "mrqa_triviaqa-validation-5209", "mrqa_newsqa-validation-2244", "mrqa_triviaqa-validation-7083", "mrqa_newsqa-validation-3484", "mrqa_squad-validation-2943", "mrqa_naturalquestions-validation-2558", "mrqa_newsqa-validation-1895", "mrqa_naturalquestions-validation-5960", "mrqa_newsqa-validation-2024", "mrqa_naturalquestions-validation-6453", "mrqa_triviaqa-validation-2712", "mrqa_searchqa-validation-4258", "mrqa_triviaqa-validation-1348", "mrqa_naturalquestions-validation-81", "mrqa_squad-validation-1708"], "EFR": 0.972972972972973, "Overall": 0.6923830561330562}, {"timecode": 52, "before_eval_results": {"predictions": ["Ringo Starr", "\"Apprendi v. New Jersey\"", "Minnesota's 8th congressional district", "Erreway", "in Sunbury on the outskirts of Melbourne in North Queensland", "George Clooney", "Pamelyn Wanda Ferdin", "Christian Kern", "barry and Loathing in Las Vegas", "$10.5 million", "2017", "12", "2014", "DJ Premier", "Missouri", "Rochdale", "\"50 best cities to live in.\"", "Virginia", "\"The Godfather Part II\"", "two", "Rigoletto", "Scunthorpe", "Yasiin Bey", "motor vehicles", "(n\u00e9e Kildow)", "March, 1904", "o'Neill", "Colonel Gaddafi", "Scottish singer and \"Britain's Got Talent\" winner Jai McDowall", "a wooden roller coaster", "Sofia the First", "Sufism", "pharmaceutical companies", "Roy Spencer", "Magnate", "The Saturdays", "British forces under General Sir William Howe", "New Jersey", "John Joseph Travolta", "ice hockey", "Hong Kong", "2006", "Pacific Place", "science fiction", "sarod", "2009", "Northern Ireland", "1977", "Russian Ark", "Delacorte Press", "the voice of The Beast", "17th Century", "April 4, 2017", "2015", "an ancient optical illusion toy", "a liquid", "Vickers-Armstrong", "food, music, culture and language of Latin America", "gopi Podila", "school, their books burned,", "an employee is insincere simply", "state tree state", "Henry Clay", "Richard Crispin Armitage"], "metric_results": {"EM": 0.484375, "QA-F1": 0.558974358974359}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-5153", "mrqa_hotpotqa-validation-3536", "mrqa_hotpotqa-validation-1351", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-1044", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4939", "mrqa_hotpotqa-validation-4119", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-474", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3422", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-468", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-3078", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-11977"], "SR": 0.484375, "CSR": 0.4988207547169812, "EFR": 1.0, "Overall": 0.6977329009433962}, {"timecode": 53, "before_eval_results": {"predictions": ["Mike Mills", "1998", "30.9%", "Kittie", "American", "People!", "south-east of Adelaide,", "The Vanguard Group", "American", "Bad Boy Records and Arista Records", "to steal the plans for the Death Star, the Galactic Empire's super weapon", "Danish", "York County", "Seventeen", "Wake Island", "Australian Defence Force", "July 26, 1959", "Arthur William Bell III", "Boston", "Erreway", "Tampa Bay Lightning", "CBS", "Boston, Massachusetts", "Marie Stefanik", "Jennifer Taylor", "Estadio Victoria", "9Lives brand cat food", "Black Ravens", "September 10, 1993", "Las Vegas Strip in Paradise, Nevada", "42,972", "9,000", "Michael Seater", "Drunken Master II", "more than 100", "bassline house, Niche or 4x4", "E22", "Allies of World War I, or Entente Powers", "Geraldine Sue Page", "Kristina Ceyton and Kristian Moliere", "Linda McCartney's Sixties: Portrait of an Era", "Philip Billard Municipal Airport", "1964 to 1974", "The BFG", "law firm", "Hamlet", "Bow River and the Elbow River", "Gillian Leigh Anderson", "segues", "British monarchy", "\"Queen In-hyun's Man\"", "American", "Virgil Ogletree", "the # 4 School of Public Health in the country", "Topiary", "Jaime Hawke", "1961", "Luca di Montezemolo", "near the Somali coast", "acid attack by a spurned suitor.", "JWO", "Hummer", "Marky Mark", "cheese"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6881679951992452}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.7692307692307693, 0.8, 1.0, 0.3636363636363636, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.5, 0.0, 0.8571428571428571, 0.8, 0.5714285714285715, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-2025", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-1033", "mrqa_hotpotqa-validation-857", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-6121", "mrqa_triviaqa-validation-115", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1641", "mrqa_searchqa-validation-5501", "mrqa_searchqa-validation-16209"], "SR": 0.515625, "CSR": 0.4991319444444444, "EFR": 1.0, "Overall": 0.6977951388888889}, {"timecode": 54, "before_eval_results": {"predictions": ["her brother, Brian", "E-books", "Callability", "When the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On ''", "Waylon Jennings", "August 2, 1990", "Charlene Holt", "eight episode series", "the courts", "wolves", "in an official document permitting a specific individual to operate one or more types of motorized vehicles", "18", "Jewel Akens", "Division 1", "Irsay", "Abid Ali Neemuchwala", "winter", "Roxette", "the singer and a co-worker decide to `` steal '' a Cadillac", "Jones", "The Union", "the chest", "Authority", "drizzle", "1967", "Virginia", "due to Parker's pregnancy at the time of filming", "lakes or reservoirs at high altitudes", "primarily influenced by West African traditions, with some minor European, and native Taino influences", "from Times Square in New York City west to Lincoln Park in San Francisco", "the 1960s", "IBM", "American singer Elvis Presley", "1958", "1998", "Karen Gillan", "headquarter Khliehriat, West Jaintia Hills district", "A rear - view mirror", "April 29, 2009", "flawed democracy", "September 23, 2017", "William Chatterton Dix", "Kristina Wagner", "Selena Gomez", "Steve Russell", "1881", "an armed conflict without the consent of the U.S. Congress", "bassist Timothy B. Schmit", "Games", "Cetshwayo", "Games", "Cambridge", "downtown", "choroid", "1982", "2015", "epic historical drama", "22", "one", "economic opportunities.", "1998", "Ukrainian Soviet Socialist Republic", "R.M. Johnston", "\"Traumnovelle\" (\"Dream Story)"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6432925106183778}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, true], "QA-F1": [0.5, 0.0, 0.0, 0.8125000000000001, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5294117647058824, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.823529411764706, 0.42857142857142855, 0.0, 0.6, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6086956521739131, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-5785", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-716", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3483", "mrqa_searchqa-validation-354", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-56"], "SR": 0.515625, "CSR": 0.49943181818181814, "retrieved_ids": ["mrqa_squad-train-36078", "mrqa_squad-train-34009", "mrqa_squad-train-70475", "mrqa_squad-train-66892", "mrqa_squad-train-57279", "mrqa_squad-train-76504", "mrqa_squad-train-59331", "mrqa_squad-train-22508", "mrqa_squad-train-55295", "mrqa_squad-train-51561", "mrqa_squad-train-29113", "mrqa_squad-train-9669", "mrqa_squad-train-85987", "mrqa_squad-train-48910", "mrqa_squad-train-56536", "mrqa_squad-train-71880", "mrqa_squad-validation-8594", "mrqa_hotpotqa-validation-5042", "mrqa_squad-validation-4452", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-4851", "mrqa_searchqa-validation-11977", "mrqa_newsqa-validation-1339", "mrqa_naturalquestions-validation-5719", "mrqa_newsqa-validation-139", "mrqa_triviaqa-validation-2693", "mrqa_naturalquestions-validation-4279", "mrqa_searchqa-validation-16659", "mrqa_triviaqa-validation-6046", "mrqa_newsqa-validation-1318", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-7464"], "EFR": 0.8709677419354839, "Overall": 0.6720486620234604}, {"timecode": 55, "before_eval_results": {"predictions": ["the 1960s", "Charlton Heston", "a house edge of between 0.5 % and 1 %", "Doreen Mantle", "Felicity Huffman", "March 18, 2005", "the titular `` fool '', a solitary figure who is not understood by others, but is actually wise", "when the forward reaction proceeds at the same rate as the reverse reaction", "28 July 1914", "Terry Kath", "1922", "2017 season", "Adam Sandler", "2002", "Ethiopia", "English law", "Abid Ali Neemuchwala", "Hodel", "first stand - alone instant messenger", "James Fleet", "from September 15, 2015, to November 17, 2015", "The ulnar nerve", "a Border Collie", "Massachusetts", "the citizens", "The at sign", "The Sun", "60 by West All - Stars", "August 22, 1980", "Jack Nicklaus", "2020", "General George Washington", "7.6 % Per Annum", "9.1 ( M )", "the Deathly Hallows", "1966", "As of January 17, 2018", "2026", "1926", "October 20, 1977", "Cetshwayo", "50 home run club", "al - Mamlakah al - \u02bbArab\u012byah", "Garbi\u00f1e Muguruza", "23 %", "Detroit Tigers", "Rockwell", "in outer space", "Charlotte Hornets", "the lumbar cistern", "February 7, 2018", "shahadah", "Elizabeth Taylor", "Sweden", "U.S. Representative", "\"Queen In-hyun's Man\"", "James Franco", "step up attacks against innocent civilians.\"", "a delegation of American Muslim and Christian leaders", "iTunes", "\"if a man does not keep pace with his companions, perhaps it is because he hears a different drummer", "the Ming dynasty", "1919", "seven"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6816770902708402}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 0.5, 0.923076923076923, 1.0, 0.6, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.8333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5034", "mrqa_triviaqa-validation-6549", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-2622", "mrqa_searchqa-validation-2367", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-12190", "mrqa_newsqa-validation-1458"], "SR": 0.578125, "CSR": 0.5008370535714286, "EFR": 1.0, "Overall": 0.6981361607142857}, {"timecode": 56, "before_eval_results": {"predictions": ["Clifford", "Sweden", "The West Wing", "Adam Smith", "Luxembourg", "El Hiero", "Salvador Domingo Felipe Jacinto", "a clef", "lew of audience and success.", "track cycling", "The Blues Brothers", "onion", "1984", "frottage", "Jane Penhaligon", "Kevin Painter", "Betsy", "NASA's Messenger orbiter", "cutis anserina", "The short-beaked echidna and the duck-billed platypus", "Montr\u00e9al", "Jeffrey Archer", "The Four Tops", "Velazquez", "WED", "Aviva plc", "Charlie Chan", "Apocalypse Now", "taekwondo", "Ishmael", "jubilee line", "Milady de Winter", "delphinium", "the head", "Phileas Fogg", "Chuck Hagel", "haute couture", "zephyr", "300", "motorcycle speedway league", "New Caledonia", "James Garner", "marinated dried fruits", "Jay-Z", "bird", "the Psychiatric Association\u2019s Diagnostic and Statistical Manual of Mental disorders", "George lV", "Margaret Beckett", "Washington Post", "the White Ferns", "U.S.", "1836", "Austria - Hungary", "Sean O' Neal", "reached No. 1 on the United States \"Billboard\" 200 chart", "The New Yorker", "In Pursuit", "Aung San Suu Kyi", "his brother to surrender.", "\"Walk -- Don't Run\"", "Gene Wilder", "South Park", "Prescott", "Springfield"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5895833333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6437", "mrqa_triviaqa-validation-2401", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-3195", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-4810", "mrqa_newsqa-validation-2308", "mrqa_searchqa-validation-13467", "mrqa_searchqa-validation-11576"], "SR": 0.53125, "CSR": 0.5013706140350878, "EFR": 0.9666666666666667, "Overall": 0.6915762061403509}, {"timecode": 57, "before_eval_results": {"predictions": ["CBS", "Lord Nelson", "france", "Utah", "black lights", "lacrosse", "Packers", "france", "Operation Overlord", "george coltrane", "Virginia", "france france", "yachts", "potatoes", "1215", "pullover", "diffusion", "wye", "jack London", "South Carolina", "ellesmere port", "Parsley", "france", "chile", "Cambridge Motorway", "Lynda Baron", "robbie coltrane", "Alcatraz", "90%", "Sven Goran Eriksson", "america", "\"Monaco Historic 2018 Grand Prix\"", "A", "Jordan", "a written record", "Motown", "Sudan", "marbles", "robbie coltrane", "frogs", "france", "Anschluss", "silk", "Irving Berlin", "medical profession", "Leo Tolstoy", "Austria", "oasis", "energy drink", "france", "stamps", "Rah", "2003", "Magnavox Odyssey", "Clark County", "created the American Land- Grant Acts of 1862 and 1890", "france", "\"Mata Zetas,\" or Zeta Killers.", "Japan", "Dr. Maria Siemionow,", "voyeurism", "Voldemort", "apricot", "off Africa"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5737847222222222}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-6233", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5877", "mrqa_naturalquestions-validation-8707", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2021", "mrqa_newsqa-validation-2792", "mrqa_searchqa-validation-16366", "mrqa_searchqa-validation-5198", "mrqa_searchqa-validation-16449"], "SR": 0.515625, "CSR": 0.5016163793103448, "retrieved_ids": ["mrqa_squad-train-54935", "mrqa_squad-train-44194", "mrqa_squad-train-18342", "mrqa_squad-train-61696", "mrqa_squad-train-82072", "mrqa_squad-train-66230", "mrqa_squad-train-7765", "mrqa_squad-train-1706", "mrqa_squad-train-23891", "mrqa_squad-train-25854", "mrqa_squad-train-81033", "mrqa_squad-train-64320", "mrqa_squad-train-79865", "mrqa_squad-train-69936", "mrqa_squad-train-34934", "mrqa_squad-train-74140", "mrqa_searchqa-validation-7475", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-1667", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-872", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-7414", "mrqa_searchqa-validation-13585", "mrqa_hotpotqa-validation-4311", "mrqa_triviaqa-validation-765", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-9508", "mrqa_searchqa-validation-6876"], "EFR": 0.9354838709677419, "Overall": 0.6853888000556173}, {"timecode": 58, "before_eval_results": {"predictions": ["Christian Louboutin", "apples", "Galapagos Islands", "\u201cFor Gallantry\u201d", "West Side Story", "onions", "bratislava", "Mariah Carey", "blancmange", "The Sun", "four inches", "Isaac", "dicksaid", "Philip Larkin", "wynkyn de Worde", "the opossum", "Soviets", "UKIP", "William Wallace", "charlie at the center", "M*A*S*H", "helene hanff", "\"The Great Gate of Kiev\"", "The California condor", "molybdenum", "chine\u017fe", "chine\u017fe", "gives him his job", "united states", "John Huston", "Peterborough United", "cat", "bajan", "aurochs", "us", "james atherley", "chine\u017fe", "Mercury", "capone", "jons Jacob Berzelius", "oscar", "Mary Poppins", "The Mayor of Casterbridge", "Queensland", "Blofeld", "George Eastman", "united states of Football", "Kenya", "george iv", "tuscany", "n Nissan", "Ptolemy", "Toto", "a traditional holiday originating in China, occurring near the summer solstice", "Heather Langenkamp", "Operation Iceberg", "In simple language", "mesac Damas", "Revolutionary Armed Forces of Colombia,", "a preliminary injunction against a Mississippi school district and high school in federal court Tuesday over the April 2 prom.", "The horror!", "pole vaulting", "the Maine", "Coleridge"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5996279761904761}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6428571428571428, 1.0, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-1089", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-3418", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-3481", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-1708", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-4635", "mrqa_naturalquestions-validation-94", "mrqa_hotpotqa-validation-1545", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-384", "mrqa_searchqa-validation-444", "mrqa_searchqa-validation-5746"], "SR": 0.53125, "CSR": 0.5021186440677966, "EFR": 0.9666666666666667, "Overall": 0.6917258121468927}, {"timecode": 59, "before_eval_results": {"predictions": ["Jesus", "Aristotle", "Eliot Cutler", "goalkeeper", "David Weissman", "london tipton", "comedy", "November 29, 1895", "the Goddess of Pop", "Sir Philip Anthony Hopkins", "near Philip Billard Municipal Airport", "the 2011 NCAA Division I FBS football season", "usually last two years", "Walt Disney and Ub Iwerks", "Martin \"Marty\" McCann", "WB Television Network", "Gainsborough Trinity", "turns out to be a terrible date", "$7.3 billion", "best known for his ten seasons with the Charlotte Hornets", "george i", "sixteen", "The Rural Electrification Act of 1936", "2015", "Ron Swanson", "Best Performance by an Actress in a Mini Series", "XXXTentacion", "Dire Straits", "reality", "MGM Grand Garden Special Events Center", "Best Alternative Music Album", "Pieter van Musschenbroek", "1979", "the 70 m and 90 m events", "prime minister", "film", "Bulgarian-Canadian", "KXII", "Thunderball", "Eastern College Athletic Conference", "Indian state of Gujarat", "William Corcoran Eustis", "World Outgames", "Norwood", "Saturday", "Shooter Jennings", "Can't Be Tamed", "Bolton, England", "Stephen Hawking", "Allison Wallace", "Saoirse Ronan", "Nacio Herb Brown", "a region in Greek mythology", "Todd Bridges", "lemon", "dungarees", "Jaipur", "in the southern Gaza city of Rafah,", "to the U.S. Consulate in Rio de Janeiro,", "CNN", "All's Well That ends Well", "ice cream", "davenport", "Captain Cook"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6572924471361972}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2857142857142857, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.9090909090909091, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-1007", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-1491", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-1830", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-2429", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-3404", "mrqa_triviaqa-validation-3147", "mrqa_newsqa-validation-4188", "mrqa_searchqa-validation-6145", "mrqa_searchqa-validation-2444", "mrqa_searchqa-validation-15613"], "SR": 0.515625, "CSR": 0.50234375, "EFR": 0.967741935483871, "Overall": 0.6919858870967742}, {"timecode": 60, "UKR": 0.64453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1496", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2256", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2456", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8282", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1668", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3627", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-468", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5622", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6755", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7112", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7558", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-899"], "OKR": 0.81640625, "KG": 0.4484375, "before_eval_results": {"predictions": ["Batman", "The Constitution of India", "Frank Oz", "786 -- 802", "Patris et Filii et Spiritus Sancti", "19 July 1990", "John Ernest Crawford", "Andy Warhol", "December 19, 1971", "in Paradise, Nevada", "in the British Isles of French and Latin origin", "BC Jean", "the BBC", "22 days", "961", "Jay Baruchel", "December 1886", "U.S. was not officially tied to the Allies by treaty", "in Las Vegas", "judges", "Shannen Doherty", "Greg Norman", "1998", "Coroebus of Elis", "densest giant planet", "Crepuscular animals", "Clarence Williams", "the 1970s in the United States of America", "with his family to Oklahoma", "American Civil War", "around 10 : 30am", "David Ben - Gurion", "RMS Titanic", "Ferelden", "in San Francisco Bay", "Eight full seasons", "a tradition in brass band parades in New Orleans, Louisiana", "Vasoepididymostomy", "the fourth quarter of the preceding year", "James Bolam", "a feminine form of the Hebrew Yohannan", "in Broken Hill and Sydney", "Detective Eddie Thawne", "to Aramaic \u05d0\u05d5\u05e9\u05e2\u05e0\u05d0 ( \u02be\u014dsha\u02bfn\u0101 ) meaning `` save, rescue, savior ''", "sedimentary rock", "Sir Ronald Ross", "NFL Scouting combine", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "post translational modification", "eight hours ( UTC \u2212 08 : 00 )", "Seattle Center", "Goneril", "tomato", "Guru Nanak", "professional footballer", "mixed martial", "James Tinling", "Rima Fakih", "165-room", "David Bowie", "Peach Melba", "December to April", "Godiva Chocolate Company", "Sri Lanka Freedom Party"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5852459733893557}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.5, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 0.3333333333333333, 0.2222222222222222, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-437", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1966", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-9875", "mrqa_triviaqa-validation-1154", "mrqa_hotpotqa-validation-587", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-12527", "mrqa_searchqa-validation-10884"], "SR": 0.453125, "CSR": 0.5015368852459017, "retrieved_ids": ["mrqa_squad-train-81061", "mrqa_squad-train-70848", "mrqa_squad-train-66767", "mrqa_squad-train-38549", "mrqa_squad-train-32185", "mrqa_squad-train-71364", "mrqa_squad-train-64400", "mrqa_squad-train-53972", "mrqa_squad-train-34364", "mrqa_squad-train-7889", "mrqa_squad-train-43063", "mrqa_squad-train-68979", "mrqa_squad-train-61012", "mrqa_squad-train-74062", "mrqa_squad-train-37607", "mrqa_squad-train-32234", "mrqa_triviaqa-validation-3002", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-2951", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-13554", "mrqa_searchqa-validation-3618", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-645", "mrqa_naturalquestions-validation-3186", "mrqa_triviaqa-validation-358", "mrqa_searchqa-validation-7269", "mrqa_triviaqa-validation-2246", "mrqa_naturalquestions-validation-3396", "mrqa_triviaqa-validation-3338", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-10620"], "EFR": 1.0, "Overall": 0.6821823770491803}, {"timecode": 61, "before_eval_results": {"predictions": ["the 1994 season", "Tenochtitlan", "Conrad Lewis", "Bart Millard", "Pangaea or Pangea", "111", "Kiss", "Justice Harlan", "full '' sexual intercourse", "Valene Kane", "fourteen - year - old Georgia Nicholson ( Groome )", "the passing of the year", "November 28, 1973", "T.J. Miller", "excessive wealth", "Malina Weissman", "Pasek & Paul", "the state sector", "741 weeks", "serve as the chief Senate spokespeople for the political parties respectively holding the majority and the minority in the United States Senate, and manage and schedule the legislative and executive business of the Senate", "acidity or basicity of an aqueous solution", "November 1960", "cat", "1999", "the beginning of the American colonies", "the concentration of a compound exceeds its solubility", "February 9, 2018", "animal", "Lloyd Webber and Stilgoe", "Dollree Mapp", "the 15th century", "electrons from electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions, and couples this electron transfer with the transfer of protons ( H ions ) across a membrane", "Nehemiah 1 : 5", "Unwinding of DNA at the origin", "Detroit", "Mickey Mantle", "Shawn", "Gillen Simone Vangsness", "the dress shop", "the First Epistle of John", "1603", "September 25, 1987", "the 1970s and'80s", "1939", "Randy Newman", "1956", "Ravi River", "prokaryotic", "# 4", "an active supporter of the League of Nations", "New York City", "shoes", "Frank Keogh", "horses", "Vanarama National League", "Odysseus", "3 million", "Dr. Maria Siemionow,", "Joe Harn", "his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "Ronald Reagan", "titanium", "Hastings", "(Urien) Urien"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5940273296372087}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false], "QA-F1": [0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.7499999999999999, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.5, 0.0, 0.22222222222222224, 0.2222222222222222, 0.4, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.12903225806451613, 0.0, 0.20000000000000004, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-7356", "mrqa_triviaqa-validation-2177", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3385", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-1246", "mrqa_searchqa-validation-15585", "mrqa_searchqa-validation-13746", "mrqa_triviaqa-validation-3768"], "SR": 0.46875, "CSR": 0.501008064516129, "EFR": 1.0, "Overall": 0.6820766129032257}, {"timecode": 62, "before_eval_results": {"predictions": ["tourist destination in the Overberg District Municipality, Western Cape, South Africa.", "horse racing", "Italy", "honeybee", "arthurianian legend", "linesider", "63 to 144 inches", "dublinian legend", "squash", "Jack London", "arthurian legend", "Scotland", "Edward VIII", "Bugs Bunny", "Malawi", "ambilevous", "arthurian legend", "japan", "wake", "mercury", "Yahoo!", "Klaus Barbie", "a 100% pure and natural sweetener", "Joanne Harris", "The Cave Club", "Munyon", "Moldova", "Chatsworth", "India and Pakistan", "Bull Moose Party", "Mar Pac\u00edfico", "eagle", "Stockholm", "usihnachts-Oratorium BWV 248", "Hercules", "Real Madrid", "Tennessee Whiskey", "Matthew Pinsent", "Iran", "salsa", "Cuba", "arthurian legend of tennis nicknames", "kia", "Robert Stroud", "Cat Stevens", "cuticle", "tyne", "lead", "mulberry", "trumpet", "Cockermouth", "Europe", "October 1941", "spiritual ideas, virtues and the essence of scriptures", "McG", "near Philip Billard Municipal Airport", "gull-wing", "July 8 at London", "sexual harassment", "5,600", "chicken", "a movie heartthrob", "Fitzgerald", "If These Dolls Could Talk"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5963541666666666}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true], "QA-F1": [0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-945", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-5777", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-1366", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-4627", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-5993", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-988", "mrqa_hotpotqa-validation-542", "mrqa_newsqa-validation-3652", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-15132", "mrqa_searchqa-validation-9158"], "SR": 0.5625, "CSR": 0.501984126984127, "EFR": 0.9642857142857143, "Overall": 0.6751289682539683}, {"timecode": 63, "before_eval_results": {"predictions": ["his mother.", "southern city of Naples", "November", "for the rest of the year", "\"It was never our intention to offend anyone,\"", "Bob Bogle,", "\"People of Palestine\"", "his business dealings", "Saturday just hours before he was scheduled to perform at the BET Hip Hop Awards.", "People Against Switching Sides", "South Florida", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "authorities,", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "a review of state government practices completed in 100 days.", "prostate cancer,", "Thirteen", "Bill Haas", "Rima Fakih", "U.S. Department of Agriculture", "37", "advocating against \"compassionate release\" for a terminally ill former Manson family member,", "33-year-old", "Christopher Savoie", "12-hour", "Judge Herman Thomas", "President Obama's race in 2008.", "bikinis", "laundry service", "\"political and religious\"", "twice.", "learn in safer surroundings.", "leaders of more than 30 Latin American and Caribbean nations are meeting in Mexico from Monday to launch a group that will serve as an alternative to the Organization of American States.", "Friday,", "Gavin de Becker", "400 years ago", "U.S. Consulate in Rio de Janeiro,", "Apple employees", "heavy turbulence", "$3 billion", "Nkepile M abuse", "resources", "memories of his mother.", "a pregnant soldier", "Barack Obama,", "Technological Institute of Higher Learning of Monterrey,", "a female soldier,", "\"It was incredible. We've had so much rain, and yet today it was beautiful.", "David Russ,", "wisecracking youngster Arnold Drummond", "Chinese", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "six", "Israel", "algebra", "Chuck Yeager", "M\u00fcnstereifel", "on the River North Esk in Midlothian", "singer", "dinosaurs", "temperature", "Thief", "Fort Kent, Maine"], "metric_results": {"EM": 0.5, "QA-F1": 0.5928942345212725}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.888888888888889, 0.0, 1.0, 0.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.10810810810810811, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.34782608695652173, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.375]}}, "before_error_ids": ["mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3087", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-1827", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-2249", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-9739", "mrqa_searchqa-validation-6571", "mrqa_searchqa-validation-9458", "mrqa_naturalquestions-validation-6670"], "SR": 0.5, "CSR": 0.501953125, "retrieved_ids": ["mrqa_squad-train-25874", "mrqa_squad-train-5801", "mrqa_squad-train-52787", "mrqa_squad-train-2714", "mrqa_squad-train-84488", "mrqa_squad-train-18725", "mrqa_squad-train-43084", "mrqa_squad-train-35439", "mrqa_squad-train-23822", "mrqa_squad-train-61101", "mrqa_squad-train-53139", "mrqa_squad-train-83732", "mrqa_squad-train-40654", "mrqa_squad-train-44127", "mrqa_squad-train-26797", "mrqa_squad-train-72478", "mrqa_triviaqa-validation-945", "mrqa_naturalquestions-validation-7223", "mrqa_squad-validation-5860", "mrqa_naturalquestions-validation-7767", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-655", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-3485", "mrqa_newsqa-validation-2170", "mrqa_hotpotqa-validation-5014", "mrqa_naturalquestions-validation-2476", "mrqa_triviaqa-validation-13", "mrqa_naturalquestions-validation-953", "mrqa_searchqa-validation-4356", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-3707"], "EFR": 1.0, "Overall": 0.682265625}, {"timecode": 64, "before_eval_results": {"predictions": ["Slavic culture", "Canadian beer company", "sweepstakes", "(Alexander) Gardner", "\"The Eve of St.", "Sangria", "Toto", "calcite", "bologna", "potatoes", "Princeton", "the Philippines", "the Knight", "Evian", "unicorns", "Nazareth", "the Andes", "Jim Jarmusch", "Martin Luther", "Miles Davis", "Tennessee", "Audrey Hepburn", "falafel", "Aladdin", "\"The Prairie Wolf\"", "David", "Arthur C. Clarke", "Washington Redskins", "Vietnam War", "Jodie Foster", "at the Gold Coast, Lismore and Coffs Harbour", "Christian Louboutin", "monk seal", "shot", "letters", "skimmed", "Ginger Rogers", "Beijing", "Plumeria", "Lafayette", "Marie Osmond", "\"The Pickwick\"", "buffalo", "comet", "Chuck Yeager", "Newton", "sheep", "\"Inheritance Trilogy\"", "Georgia", "French toast", "The 4th, 5th, 6th, and 8th Amendments", "Elvis Presley", "at the Louvre Museum in Paris", "from the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "catherine bacall", "Jaws", "gold", "18 November [O.S. 6 November] 1860", "\"White Horse\"", "the 1824 Constitution of Mexico", "Kurt Cobain", "Glasgow, Scotland", "Christopher Savoie", "London"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5873868778280542}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7692307692307693, 0.823529411764706, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9479", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-13851", "mrqa_searchqa-validation-214", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-6953", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-16641", "mrqa_searchqa-validation-6692", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-1377", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-13122", "mrqa_searchqa-validation-16755", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-3875", "mrqa_searchqa-validation-13789", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-5831", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1604", "mrqa_hotpotqa-validation-4263", "mrqa_newsqa-validation-2011"], "SR": 0.515625, "CSR": 0.5021634615384616, "EFR": 0.967741935483871, "Overall": 0.6758560794044665}, {"timecode": 65, "before_eval_results": {"predictions": ["Montana", "San Jose Mercury News", "Casino Royale", "the William Tell", "The Apprentice", "Aeschylus", "the College of William and Mary", "Intelligence Quotient", "Stranger in a Strange Land", "a Punjabi harvest dance", "the Mets", "cowherd", "Monty Python and the Holy Grail", "beethoven", "Josef Stalin", "Tawakkul", "Portland", "China", "Absalom", "Castle Rock", "Bollywood", "Marcia Brady", "the Habsburg", "eudaimonia", "a Twinkie", "the altitude", "intolerable", "Richard the Lionheart", "Henry VIII", "Siddur", "Fred Rogers", "Liliuokalani", "the pituitary gland", "the South African Boer War", "the pulp", "Michelle Pfeiffer", "Aswan", "Billy Ray Cyrus", "Vanes", "Vito Schnabel", "Impostor syndrome", "the", "Davy Crockett", "Sagittarius", "the volcano", "copper", "Dubliners", "Jules Verne", "Cuba", "the Taliban", "Soldiers", "Tyler, Ali, and Lydia having fun at Tyler's little sister's birthday party", "Otis Timson", "during prenatal development in the central part of each developing bone", "Exile", "Carmen Miranda", "jocelyn bacall", "Harvey Birdman", "the Chief of the Operations Staff of the Armed Forces High Command", "Erich Maria Remarque", "the nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "ensure that all prescription drugs on the market are FDA approved,", "15-year-old", "Qutabuddin Bakhtiar Kaki"], "metric_results": {"EM": 0.453125, "QA-F1": 0.531077189293784}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.25, 0.0, 0.5, 0.9655172413793104, 0.14814814814814817, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15873", "mrqa_searchqa-validation-9446", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-15917", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-5095", "mrqa_searchqa-validation-894", "mrqa_searchqa-validation-10927", "mrqa_searchqa-validation-9120", "mrqa_searchqa-validation-11135", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-9081", "mrqa_searchqa-validation-16276", "mrqa_searchqa-validation-3734", "mrqa_searchqa-validation-11604", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-15876", "mrqa_searchqa-validation-8840", "mrqa_searchqa-validation-6273", "mrqa_searchqa-validation-1852", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-8703", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-2440", "mrqa_triviaqa-validation-6882", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2982", "mrqa_naturalquestions-validation-10509"], "SR": 0.453125, "CSR": 0.5014204545454546, "EFR": 0.9714285714285714, "Overall": 0.6764448051948052}, {"timecode": 66, "before_eval_results": {"predictions": ["a double hoop", "a mutualistic relationship", "December 20, 1951", "La\u02bbila\u02bbi", "Terry Kath", "comprehend and formulate language", "in the fascia surrounding skeletal muscle", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "1546", "Banquo", "January 1923", "CeCe Drake", "a habitat", "topped with grated cheese", "Geophysicists", "free floating", "the Japanese government", "concert", "30 October 1918", "Austria - Hungary", "Domhnall Gleeson", "13 to 22 June 2012", "T - Bone Walker", "Paul Baumer", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "the Executive Residence of the White House Complex", "Article Two", "March 31, 2018", "Bush", "Yuzuru Hanyu", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "off the southernmost tip of the South American mainland", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "in Rome", "January 1, 2016", "Leonardo da Vinci", "its absolute temperature", "Thawne", "Philippe Petit", "Proposition 103", "2008", "in the cytoplasm", "Julie Adams", "775", "Kenya and Scotland", "Xiu Li Dai and Yongge Dai", "the Miracles", "Americans who served in the armed forces and as civilians", "a period of eight years", "James Fleet", "the year 1 BC", "David Davis", "Dirty Dancing", "mumps", "Delphi Lawrence", "Charlotte Elizabeth Diana", "World Boxing Association", "a one-of-a-kind navy dress", "Pakistani officials,", "Britain", "Jamaica", "copra", "Python", "not guilty of affray"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6551932581016089}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8095238095238095, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9387755102040816, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.47058823529411764, 0.923076923076923, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8181818181818181, 0.6666666666666666, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-4419", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-47", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1259", "mrqa_searchqa-validation-10836", "mrqa_newsqa-validation-37"], "SR": 0.546875, "CSR": 0.5020988805970149, "retrieved_ids": ["mrqa_squad-train-77219", "mrqa_squad-train-48174", "mrqa_squad-train-62184", "mrqa_squad-train-8376", "mrqa_squad-train-51269", "mrqa_squad-train-69030", "mrqa_squad-train-27253", "mrqa_squad-train-41125", "mrqa_squad-train-8702", "mrqa_squad-train-43467", "mrqa_squad-train-53543", "mrqa_squad-train-24512", "mrqa_squad-train-75392", "mrqa_squad-train-24180", "mrqa_squad-train-11165", "mrqa_squad-train-29309", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1039", "mrqa_hotpotqa-validation-3930", "mrqa_naturalquestions-validation-8999", "mrqa_searchqa-validation-7871", "mrqa_naturalquestions-validation-1953", "mrqa_newsqa-validation-2183", "mrqa_naturalquestions-validation-243", "mrqa_newsqa-validation-1021", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-4241", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-1047", "mrqa_newsqa-validation-3652", "mrqa_naturalquestions-validation-5348", "mrqa_newsqa-validation-2179"], "EFR": 0.9655172413793104, "Overall": 0.6753982243952651}, {"timecode": 67, "before_eval_results": {"predictions": ["in Pyeongchang County, Gangwon Province, South Korea", "an apprentice of the fictional Jedi Order in the Star Wars franchise", "in a liquid solution", "April 1917", "London", "Utah", "1969", "by October 1986", "the referee blows the whistle", "legal systems", "parthenogenesis", "Concetta Tomei", "reproductive system", "Federated States of Micronesia", "the Parliament of the United Kingdom", "a stray wandering the streets of Moscow", "Gibraltar", "September 1947", "every year from 6 -- 14 July", "in the bone marrow", "Sophia Akuffo", "who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "player or trade their position to another team for other draft positions, a player or players, or any combination thereof", "Sarah Josepha Hale", "Ingrid Bergman", "Jessica Simpson", "over the specimen", "a book of the Old Testament", "Daren Maxwell Kagasoff", "Steveston Outdoor pool in Richmond, BC", "Ricardo Chavira", "west to Colorado", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "Friedman Billings Ramsey", "Miami Heat of the National Basketball Association ( NBA )", "vasoconstriction", "Toronto Islands", "lighter", "the final episode of the series", "Roger Nichols and Paul Williams", "Konakuppakatil Gopinathan Balakrishnan", "Logan Williams", "a Border Collie", "ancient cult activity as far back as 7th century BCE", "1665", "sugars and amino acids", "Menelaus", "Antarctica", "California", "during Christmas season in the late 1970s", "December 1349", "Ipswich Town Football Club", "post-impressionist", "British Airways", "Genderqueer", "143,372", "YouTube", "five victims by helicopter, one who died, two in critical condition and two in serious condition.", "Joel \"Taz\" DiGregorio", "system of military trials", "Sierra Nevada", "1972", "Ichabod Crane", "Thomas Jefferson"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5734046724348448}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true], "QA-F1": [0.923076923076923, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444444, 0.09523809523809525, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.5, 0.0, 0.3, 0.48275862068965514, 1.0, 0.4444444444444445, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9760", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1915", "mrqa_naturalquestions-validation-1269", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-4558", "mrqa_triviaqa-validation-263", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-4207", "mrqa_searchqa-validation-2955"], "SR": 0.4375, "CSR": 0.5011488970588236, "EFR": 0.9722222222222222, "Overall": 0.6765492238562091}, {"timecode": 68, "before_eval_results": {"predictions": ["Borneo", "nomadic pastoralists", "15%", "Parkinson\\'s Disease", "John Jay", "Warsaw", "\"USSR go home!\"", "2.52-carat", "South Africa", "Clay Aiken", "Muhammad", "Australia", "(James) Namath", "high and dry", "a Doll", "the inquisition", "Cleopatra", "the International Space Station", "Iran", "Gaius Cassius Longinus", "\"The Night They Drove Old Dixie Down,\"", "South Africa", "John Deere", "River Thames", "Oxford", "William Wordsworth", "\"Elphaba\"", "Tuscaloosa", "Cyprus", "Sabino Canyon", "Frasier Crane", "Bob Dylan", "Sicily", "Herbert Hoover", "Zhou Enlai", "mozzarella", "(Lac) Geneva", "(James) Barber", "\"The Mole\"", "HIV/AIDS", "\"How to Get the Best Service\"", "Central City", "liver", "Bern", "bchamel", "Jackie Robinson", "YouTube", "Diane Arbus", "Willa Cather", "\"sustained\"", "At Pimlico", "Masha Skorobogatov", "Kyla Pratt", "Institut polaire fran\u00e7ais Paul - \u00c9mile Victor", "Union Gap", "\"Miracles Do Happen\"", "makossa and bikutsi", "Ding Sheng", "May 5, 2015", "Massapequa, New York", "near Grand Ronde, Oregon.", "\"Five of us for the United States", "Videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings", "acid phosphate"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5592261904761904}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.2666666666666667, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2956", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-13698", "mrqa_searchqa-validation-11405", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-2330", "mrqa_searchqa-validation-6130", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-8156", "mrqa_searchqa-validation-5828", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-14324", "mrqa_searchqa-validation-6816", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-6383", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-450", "mrqa_naturalquestions-validation-6103", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3166", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-893", "mrqa_triviaqa-validation-3820"], "SR": 0.46875, "CSR": 0.5006793478260869, "EFR": 1.0, "Overall": 0.6820108695652174}, {"timecode": 69, "before_eval_results": {"predictions": ["aqueduct", "a quark", "Christopher Reeve", "Bucharest", "Macbeth", "John Jacob Astor", "acting", "South Station", "the Sun Also Rises", "Cherokee", "Ferrari", "banquet", "the High Plains", "Joe Hill", "Job", "Kentucky", "Supernatural", "Jean Foucault", "Montana", "deep brain stimulation", "kiss", "the Amazon", "Oklahoma", "Anne Hathaway", "Model A", "Greece", "Vietnam", "Tintern Abbey", "Canuck", "Cecil Day-Lewis", "Isaac Newton", "the Blue Ridge Mountain", "Frederic Chopin", "Susan B. Anthony", "Dexter", "a desert rat", "Washington Bullets", "Starsky", "the Prisoner of Azkaban", "Knocked Up", "Space Chimps", "Christopher Wren", "a jazz funeral", "Boston", "Han Solo", "Holger", "a proscenium", "Zapata", "a veil", "Goldwater", "Guinness", "Portugal. The Man.", "1983", "Moira Kelly", "Pumas", "red", "Greek", "Academy Award for Best Animated Feature", "1937", "Stephen Ireland", "a group of college students of Pakistani background", "Coptic Christians", "an eye for an eye,\"", "Retina display"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6937500000000001}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-5269", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-6228", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-6956", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-9078", "mrqa_searchqa-validation-2333", "mrqa_searchqa-validation-6185", "mrqa_searchqa-validation-2934", "mrqa_searchqa-validation-5427", "mrqa_triviaqa-validation-3274", "mrqa_triviaqa-validation-7182", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-5477", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-2435"], "SR": 0.59375, "CSR": 0.5020089285714286, "retrieved_ids": ["mrqa_squad-train-29211", "mrqa_squad-train-72410", "mrqa_squad-train-26788", "mrqa_squad-train-82787", "mrqa_squad-train-59001", "mrqa_squad-train-43349", "mrqa_squad-train-49164", "mrqa_squad-train-63200", "mrqa_squad-train-32219", "mrqa_squad-train-5567", "mrqa_squad-train-24817", "mrqa_squad-train-61218", "mrqa_squad-train-31193", "mrqa_squad-train-24369", "mrqa_squad-train-29465", "mrqa_squad-train-74743", "mrqa_squad-validation-2000", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-8189", "mrqa_triviaqa-validation-6746", "mrqa_naturalquestions-validation-10311", "mrqa_searchqa-validation-7449", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-10016", "mrqa_squad-validation-7332", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-637", "mrqa_triviaqa-validation-2856", "mrqa_hotpotqa-validation-3703", "mrqa_triviaqa-validation-5154", "mrqa_naturalquestions-validation-5986", "mrqa_searchqa-validation-450"], "EFR": 1.0, "Overall": 0.6822767857142857}, {"timecode": 70, "UKR": 0.666015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2888", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-39", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-978", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1120", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4890", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-963", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14334", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3809", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-2467", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3832", "mrqa_squad-validation-3852", "mrqa_squad-validation-386", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-3994", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4467", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5493", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6638", "mrqa_squad-validation-6875", "mrqa_squad-validation-6957", "mrqa_squad-validation-7064", "mrqa_squad-validation-739", "mrqa_squad-validation-7549", "mrqa_squad-validation-7688", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-7917", "mrqa_squad-validation-8309", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-893", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-143", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5861", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-595", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917"], "OKR": 0.83203125, "KG": 0.49375, "before_eval_results": {"predictions": ["Kathy Najimy", "2006 -- 07", "( 2016 )", "Mel Tillis", "2026", "Richard Wright", "Andrew Lloyd Webber", "the Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Health or vitality", "Stephen Graham", "6 - 6", "1955", "Kevin McKidd", "Parthenogenesis", "fertilization", "Yente", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "the stems and roots of certain vascular plants", "a Czech word, robota", "skeletal muscle", "Nazi Germany and Fascist Italy", "Gunpei Yokoi", "David Motl", "twelve", "September 9, 2010", "to avoid the inconvenienceiences of a pure barter system", "scrolls dating back to the 12th century", "Buddhism", "Kiss", "the eighth series", "Trace Adkins", "the optic chiasm", "to manage the characteristics of the beer's head", "the United States, the United Kingdom, and their respective allies", "letter series", "James Intveld", "15 February 1998", "Christopher Allen Lloyd", "100,000 writes", "2010", "Bartolomeu Dias", "Isabela Moner", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "potential of hydrogen", "Fall 1998", "the Qianlong Emperor", "Guwahati", "74 languages", "South Africa", "Rufus and Chaka Khan", "eight", "Norway", "Jamaica", "mead", "Tomorrowland", "the Tallahassee City Commission", "John Kavanagh", "National Infrastructure Program", "Maj. Nidal Malik Hasan,", "more than 200 arrests and the recovery of 123 pounds of cocaine and 4.5 pounds of heroin, Tempe, Arizona,", "In Memoriam", "Mercury", "Law & Order: Special Victims Unit", "UNICEF"], "metric_results": {"EM": 0.53125, "QA-F1": 0.629084342944637}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.22222222222222224, 0.18181818181818182, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-527", "mrqa_triviaqa-validation-4646", "mrqa_hotpotqa-validation-3032", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-236", "mrqa_searchqa-validation-9696"], "SR": 0.53125, "CSR": 0.5024207746478873, "EFR": 0.9666666666666667, "Overall": 0.6921768632629107}, {"timecode": 71, "before_eval_results": {"predictions": ["Easter Island", "George Balanchine", "Roussimoff", "the Mesozoic Era", "Austen", "Leonardo DiCaprio", "the Basque", "Sherlock", "Happy Feet", "a guardian angel", "the Army", "Stoke upon Trent", "SVU", "the Caucasus", "June Carter Cash", "Cape Town", "the shoreline", "David Farragut", "1:24 a.m.", "salaried", "the Skull", "Olive Marie Osmond", "Scrabble", "suckers", "the Catholic Church", "London", "Burgenland", "Halliburton", "the laser", "Boston", "Quiz", "Spelling Bee", "Poetry", "the Battle of Fort Donelson", "the Atomic Age", "1964", "sucrose", "Shropshire", "Cuba", "The Prince and the Pauper", "Thomas Paine", "Abraham Lincoln", "(James) North", "Charles I", "the Jemima", "Diane Arbus", "Palitana", "George Bernard Shaw", "Utah", "DNA", "Kublai Khan", "pulmonary heart disease ( cor pulmonale )", "Kimberlin Brown", "Henry Selick", "Caviar", "July 16, 1969", "argentina", "the Sisters of the Community of St Mary the Virgin", "Marc Bolan", "Polish-Jewish", "apartment building in Cologne, Germany,", "Kurdish Regional Government,", "$40", "argentina"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5407394688644689}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7692307692307692, 1.0, 0.0, 0.5714285714285715, 0.0, 0.33333333333333337, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-13029", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-2210", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-5919", "mrqa_searchqa-validation-4309", "mrqa_searchqa-validation-4000", "mrqa_searchqa-validation-12776", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-6284", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-15548", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-9495", "mrqa_searchqa-validation-5088", "mrqa_searchqa-validation-12497", "mrqa_searchqa-validation-6350", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-3746", "mrqa_hotpotqa-validation-3593", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1983", "mrqa_triviaqa-validation-1782"], "SR": 0.4375, "CSR": 0.5015190972222222, "EFR": 0.9444444444444444, "Overall": 0.6875520833333333}, {"timecode": 72, "before_eval_results": {"predictions": ["Austria", "the \"PEN\" Peninsula", "\"I Am Woman\"", "Brazil", "Applebee\\'s", "Colonial America", "Backgammon", "Steely Dan", "Artemis", "Hobart", "Colorado Springs", "Cheap Trick", "poached eggs", "Islam", "Cerberus", "Robert E. Lee", "Tobago", "A Hymn To Him,", "Columbus", "Elijah Muhammad", "Spain", "Federico Fellini", "Fenway Park", "the Hermitage", "The Princess Diaries", "fluoridation", "Herman Melville", "Korea", "John Henry", "Babe Ruth", "Barbara Boxer", "Chicago", "Wallace & Gromit", "sesame", "Adidas", "Jack Nicholson", "ammonia", "the Omaha", "wild dogs", "Paul Gauguin", "Francis Scott Key", "Mexico", "the Peashooter", "Joe Pozzuoli", "$20", "Massachusetts", "ACTIVE", "a box office", "Alfred Hitchcock", "the Basques", "Ambrose Bierce", "The president", "2.45 billion years ago", "Mary Margaret ( Ginnifer Goodwin )", "Brian Friedman", "meteoroid", "Argentina", "Edinburgh", "Campbellsville University", "a French mathematician and physicist", "104 feet long and 95 feet wide at the alcove.", "when he was a teenager", "Brian Smith.", "a Ballon d'Or"], "metric_results": {"EM": 0.609375, "QA-F1": 0.653453947368421}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4210526315789474, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-4160", "mrqa_searchqa-validation-10654", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-1079", "mrqa_searchqa-validation-1805", "mrqa_searchqa-validation-11732", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-10948", "mrqa_searchqa-validation-14818", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-10907", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-2889", "mrqa_triviaqa-validation-6507", "mrqa_triviaqa-validation-5187", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-1962"], "SR": 0.609375, "CSR": 0.5029965753424658, "retrieved_ids": ["mrqa_squad-train-36426", "mrqa_squad-train-63961", "mrqa_squad-train-42024", "mrqa_squad-train-86201", "mrqa_squad-train-37315", "mrqa_squad-train-72203", "mrqa_squad-train-55217", "mrqa_squad-train-10337", "mrqa_squad-train-13370", "mrqa_squad-train-86419", "mrqa_squad-train-29581", "mrqa_squad-train-9746", "mrqa_squad-train-81060", "mrqa_squad-train-2414", "mrqa_squad-train-6383", "mrqa_squad-train-34720", "mrqa_newsqa-validation-3652", "mrqa_triviaqa-validation-3952", "mrqa_hotpotqa-validation-3538", "mrqa_squad-validation-6229", "mrqa_naturalquestions-validation-6103", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-6816", "mrqa_naturalquestions-validation-6194", "mrqa_hotpotqa-validation-5114", "mrqa_newsqa-validation-3782", "mrqa_searchqa-validation-9739", "mrqa_naturalquestions-validation-9897", "mrqa_triviaqa-validation-2063", "mrqa_naturalquestions-validation-7659", "mrqa_triviaqa-validation-570", "mrqa_newsqa-validation-3620"], "EFR": 0.96, "Overall": 0.6909586900684931}, {"timecode": 73, "before_eval_results": {"predictions": ["gondola song", "Harry Sinclair Lewis", "Hilary Swank", "Sunny Leone", "Sacred Wonders", "Israel", "puffin", "Van Morrison", "ferrite", "Stuart Bingham", "Frank Darabont", "proclamation of Neutrality", "Napoleon I", "espresso", "organizational theory and behavior", "Volkswagen", "Bedser", "Oldham", "Netherland", "Jabba the Hutt", "Wimbledon", "Barbara Mandrell", "Taylor", "Baku", "Chechnya", "John Buchan", "green", "Chester", "Hippety Hopper", "dresses", "Mt Kenya", "a pumpkin", "lithuanian Commonwealth", "Sicily", "Switzerland", "Magic Circle", "Julie Andrews", "Pancho Villa", "Nigeria", "Leeds", "Holy Week", "Cologne", "Oliver!", "(Peoples)", "Fidel Castro", "afc Champions League twice,", "Renzo Piano", "impossible objects", "Mexico", "North Carolina", "Friends", "space between a wall mounted faucet and the sink rim", "Tim Rooney", "January to May 2014", "Forrest Gump", "Julianne Moore", "Mel Blanc", "Gary Coleman", "dining scene", "Thaksin Shinawatra,", "Jackie Moon", "Maria Callas", "Desperate Housewives", "intelligent design"], "metric_results": {"EM": 0.546875, "QA-F1": 0.596875}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6526", "mrqa_triviaqa-validation-4074", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-3345", "mrqa_triviaqa-validation-5684", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-7621", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-6862", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-4028", "mrqa_newsqa-validation-2669", "mrqa_searchqa-validation-16053"], "SR": 0.546875, "CSR": 0.503589527027027, "EFR": 1.0, "Overall": 0.6990772804054054}, {"timecode": 74, "before_eval_results": {"predictions": ["smith", "smith", "the navy", "a head-on collision with a 1950 Ford Tutor", "apple", "yellow", "Dreamgirls", "s\u00e8vres", "water", "lewis sarah smith", "allergic rhinitis", "gum syrup", "geologic history", "to find a good shop location", "pertussis disease", "Peter Stuyvesant", "apple", "India", "cecrops", "chiricahua Apache", "heart disease", "blucher", "(Pius XII) Pius XII", "smell", "a number", "george i", "Lincolnshire", "z Zimbabwe", "cork", "orange, lemon, lime, grape and strawberry", "Anwar Sadat", "sue woodall", "Silent Spring", "glastonbury", "zachano Brazzi", "Michael Sheen", "radio opera The Archers", "Tottenham Court Road", "Montmorency", "a condor", "12", "pudding Lane", "pinocchio", "cenozoic", "Jimmy Carter", "Jamie Oliver", "a cable", "smith", "Petula Clark", "new Democracy", "The Blue Boy", "a Border Collie", "Kristy Swanson", "the eighth season", "George Whitefield", "Hermione Baddeley", "Floyd Casey Stadium", "David Bowie,", "economic growth and creating opportunity for our people.", "Robert Kimmitt.", "Mammoth Cave", "recessive", "Charles Dickens", "Virginia Beach, Virginia"], "metric_results": {"EM": 0.4375, "QA-F1": 0.52734375}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-2589", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-3482", "mrqa_triviaqa-validation-6722", "mrqa_triviaqa-validation-3503", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-169", "mrqa_triviaqa-validation-4706", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-4407", "mrqa_triviaqa-validation-2699", "mrqa_triviaqa-validation-7353", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-2090", "mrqa_triviaqa-validation-5738", "mrqa_naturalquestions-validation-8404", "mrqa_newsqa-validation-3008", "mrqa_searchqa-validation-4464", "mrqa_hotpotqa-validation-3787"], "SR": 0.4375, "CSR": 0.5027083333333333, "EFR": 0.9444444444444444, "Overall": 0.6877899305555555}, {"timecode": 75, "before_eval_results": {"predictions": ["new Zealand", "1961", "pasta", "tardis", "Duke Orsino", "king aragon", "Francis Carr", "Budapest", "Gillette", "english", "mediterranean", "Bash", "public square", "wrist", "tavrakis", "in Chicago", "jim bowie", "netherlands", "Gryffindor", "gold hallmarks", "John Buchan", "pyrenees", "17", "sarah", "france", "Elysium", "algebra", "Joe Tracy", "Crete", "france", "Copenhagen", "atria", "jim galsworthy", "killer whale", "Christopher Nolan", "purple rain", "chess", "ireland", "diana vickers", "france", "Damian Green", "kryptos", "bagel", "france", "South Dakota", "(Alexander Dubcek)", "Denver", "Chicago Cubs", "St. Louis", "amsterdam", "Rosetta", "in the basic curriculum", "al - Mamlakah al - \u02bbArab\u012byah", "the nucleus", "August 14, 1848", "1892", "American", "1,500", "Shanghai", "Iran", "bone", "Washington, D.C.", "sedimentary", "golf"], "metric_results": {"EM": 0.5, "QA-F1": 0.5546875}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5653", "mrqa_triviaqa-validation-2012", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-6528", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-3005", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-156", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-6463", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-3745", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-366", "mrqa_hotpotqa-validation-4720", "mrqa_newsqa-validation-2495", "mrqa_searchqa-validation-10770"], "SR": 0.5, "CSR": 0.502672697368421, "retrieved_ids": ["mrqa_squad-train-39849", "mrqa_squad-train-8266", "mrqa_squad-train-83932", "mrqa_squad-train-50586", "mrqa_squad-train-55568", "mrqa_squad-train-38128", "mrqa_squad-train-84544", "mrqa_squad-train-46979", "mrqa_squad-train-46221", "mrqa_squad-train-9305", "mrqa_squad-train-31545", "mrqa_squad-train-16596", "mrqa_squad-train-44036", "mrqa_squad-train-47305", "mrqa_squad-train-75988", "mrqa_squad-train-8210", "mrqa_searchqa-validation-6228", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-7356", "mrqa_searchqa-validation-3542", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2460", "mrqa_naturalquestions-validation-1850", "mrqa_searchqa-validation-7596", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-4351", "mrqa_newsqa-validation-1570", "mrqa_naturalquestions-validation-7080", "mrqa_searchqa-validation-3525", "mrqa_naturalquestions-validation-10009", "mrqa_newsqa-validation-1175", "mrqa_hotpotqa-validation-350"], "EFR": 1.0, "Overall": 0.6988939144736842}, {"timecode": 76, "before_eval_results": {"predictions": ["English author Rudyard Kipling", "Andrew Garfield", "California, Utah and Arizona", "study that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "William Chatterton Dix", "1924", "The fifth season", "Alabama", "Scheria", "Sanchez Navarro", "Thomas Jefferson", "August 2, 1990", "S\u00e9rgio Mendes", "Julie Adams", "Ian Hart", "Philadelphia", "a Native American nation from the Great Plains", "in capillaries, alveoli, glomeruli, outer layer of skin and other tissues where rapid diffusion is required", "ARPANET", "April 1979", "Tbilisi", "a security feature for `` card not present '' payment card transactions instituted to reduce the incidence of credit card fraud", "Atticus Finch", "50", "Liam Cunningham", "2013", "ummat al - Islamiyah", "4", "in the United Kingdom ( with the exception of Scotland since August 1, 2016 )", "2017 season", "W. Edwards Deming", "Saphira hatches", "usernames, passwords, commands and data", "Galveston hurricane", "1972", "Ajay Tyagi", "fr\u00e9d\u00e9ric Bazille", "Paul Revere", "Julius Caesar", "Thespis", "1927", "Zeus", "two degrees", "April 10, 2018", "Lee County, Florida, United States", "annually in Rockefeller Center, in Midtown Manhattan", "Kevin Spacey", "Fa Ze YouTubers", "20th Century Fox", "the French", "hyperinflation", "Lingerie", "the Victorian woman", "Octavian", "Karolina Dean", "around four hundred", "Caesars Entertainment Corporation", "North Korea intends to launch a long-range missile in the near future,", "Lisbon, Portugal", "Marcus Schrenker,", "Gandalf", "Quartet", "ask for help", "Charice"], "metric_results": {"EM": 0.515625, "QA-F1": 0.578311600084698}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.6086956521739131, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-1907", "mrqa_hotpotqa-validation-4503", "mrqa_newsqa-validation-1354", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-1590"], "SR": 0.515625, "CSR": 0.5028409090909092, "EFR": 1.0, "Overall": 0.6989275568181819}, {"timecode": 77, "before_eval_results": {"predictions": ["Lyndon Johnson", "Istanbul", "Cana of Galilee", "\"Take It\"", "\"Le Nozze di Figaro\"", "Valerie Bertinelli", "\"I Get That A Lot\"", "Murtaza", "the Randolph Caldecott Medal", "Karl Rove", "Russians", "Ireland", "\"The Library, Westminster School, London, S. W. The Oxford and Cambridge Club, c/o Messrs.", "Portland", "Florida Keys", "Doctor Dolittle", "fish", "transmission", "hot air balloons", "vacuum tubes", "The Bridges of Madison County", "colombia", "ghee", "LOUIS XIV", "ice cream", "Louis XIV", "hyaena hyaena hallmark hyena", "Alien", "John F. Kennedy", "Indira Gandhi", "the pacarana", "Stephen Decatur", "Patti LaBelle", "the Franklin", "Molly Brown", "container", "hurricanes", "The Wall Street Journal", "fragging", "Tinactin", "Virgin Atlantic", "perrier", "Eastwick", "Richard III", "trout", "India", "Minnesota", "San Francisco", "\"Put it All Out There\"", "milk foam", "the pistol", "South Africa", "Nicole DuPort", "habitat", "deep purple", "Piero da Vinci", "Futurist", "July 25 to August 4", "1899", "Trey Parker and Matt Stone", "1.2 million", "president Luca di Montezemolo", "Roger Federer", "Alessandro Allori"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6133184523809524}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-692", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-1044", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-16112", "mrqa_searchqa-validation-2938", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-15029", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-15498", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-13399", "mrqa_searchqa-validation-5989", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-6955", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11923", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-2858", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-7164", "mrqa_triviaqa-validation-3041", "mrqa_triviaqa-validation-3098", "mrqa_hotpotqa-validation-4168", "mrqa_newsqa-validation-1364"], "SR": 0.5625, "CSR": 0.5036057692307692, "EFR": 1.0, "Overall": 0.6990805288461538}, {"timecode": 78, "before_eval_results": {"predictions": ["(1546-1601)", "Little Miss Sunshine", "Philadelphia", "Peter Rabbit", "Tommy Franks", "Ur", "Jonny Quest", "Burundi and Rwanda", "Fort Sumter", "Love Story", "Captains Courageous", "Bryan Adams", "Moses", "Mechanical", "Chaucer", "Toronto Blue Jays", "the yeoman", "Dr. Isaac Asimov", "Sayonara", "the Orient Express", "Inferno", "Sir Walter Scott", "a rick", "Louisiana", "Maltese Falcon", "MacArthur", "a fluorochemical finish", "the human breast", "PG", "Occipital", "a spoonful", "Little Red Riding Hood", "Hold On", "New Zealand", "ice", "San Francisco", "the yeoman", "\"Chelsea Morning\"", "a comb", "Venice", "Paraguay", "Hoffmann", "debts", "the Cowardly Lion", "El Supremo", "Foot Locker", "Princess Leia", "artichoke", "a yeoman", "Hammurabi", "alkaline", "the ninth", "Ruthe Berl\u00e9", "on the two tablets", "Mt Kenya", "Elvis Presley", "Boston Legal", "Whitney Houston", "Channel 4", "Mark Donohue", "two soldiers and two civilians from the Defense and State departments", "a share in the royalties for the tune.", "Arizona", "3D computer-animated comedy"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6875600961538462}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.15384615384615383, 0.7499999999999999, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10495", "mrqa_searchqa-validation-4423", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-6426", "mrqa_searchqa-validation-10152", "mrqa_searchqa-validation-12278", "mrqa_searchqa-validation-15144", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-11076", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-15756", "mrqa_searchqa-validation-13115", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-10310", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-4688", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-2489", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-2151"], "SR": 0.59375, "CSR": 0.504746835443038, "retrieved_ids": ["mrqa_squad-train-59564", "mrqa_squad-train-44392", "mrqa_squad-train-43923", "mrqa_squad-train-11783", "mrqa_squad-train-63269", "mrqa_squad-train-3299", "mrqa_squad-train-50895", "mrqa_squad-train-85954", "mrqa_squad-train-53657", "mrqa_squad-train-50710", "mrqa_squad-train-61441", "mrqa_squad-train-128", "mrqa_squad-train-15111", "mrqa_squad-train-15855", "mrqa_squad-train-59346", "mrqa_squad-train-32422", "mrqa_newsqa-validation-501", "mrqa_naturalquestions-validation-8005", "mrqa_newsqa-validation-3043", "mrqa_hotpotqa-validation-5831", "mrqa_newsqa-validation-245", "mrqa_naturalquestions-validation-5925", "mrqa_hotpotqa-validation-3790", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-3518", "mrqa_triviaqa-validation-285", "mrqa_searchqa-validation-12497", "mrqa_searchqa-validation-1182", "mrqa_triviaqa-validation-5478", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-3926", "mrqa_naturalquestions-validation-440"], "EFR": 0.9615384615384616, "Overall": 0.6916164343962998}, {"timecode": 79, "before_eval_results": {"predictions": ["a Tattoos", "Banquo", "Detroit Rock City", "life", "y", "Ford", "Joseph Campbell", "cantankerous", "Faith Hill", "Novel", "a broom sweeps clean", "Edinburgh", "engineering", "Cyprus", "a savanna mosaic", "a tandoori", "a floatplane", "piano", "Sure", "oyster", "Gilbert Grape", "Mrs. Barbara Bush", "a 2003 American epic war drama film written and directed by Anthony Minghella", "The Jungle Book", "eggs", "the Atsugi NAF, Japan", "The Sadler\\'s Wells Ballet", "Pakistan", "The 1st Special Forces Operational Detachment- Delta", "Aaron Burr", "Johns Hopkins University", "Jason", "The Mississippi River", "Damascus", "Oahu", "We Are Devo!", "Biology", "stuffing", "Reading", "George Eliot", "the Cotton Bowl", "Shiloh", "The Charlie McCarthy Show", "Women\\'s Studies", "apples", "aromatic Cedar", "Almond Joy", "The Children", "Sam Houston", "Caesar salads", "cable cars", "July 14, 1969", "on permanent display at the Louvre Museum in Paris", "1923", "Sebastian Beach", "Casualty", "The Boar", "Johnny Cash and Jennings", "Sarajevo", "Annie Ida Jenny No\u00eb Haesendonck", "Mother\\'s Day", "Italian Serie A title", "The son of Gabon's former president", "Wildcats"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5875744047619048}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.75, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-9910", "mrqa_searchqa-validation-15449", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-12299", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-15892", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-3152", "mrqa_searchqa-validation-2799", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-15134", "mrqa_searchqa-validation-8338", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-3790", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-13472", "mrqa_searchqa-validation-787", "mrqa_searchqa-validation-1853", "mrqa_searchqa-validation-9372", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-6870", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-3155", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3923"], "SR": 0.46875, "CSR": 0.504296875, "EFR": 1.0, "Overall": 0.69921875}, {"timecode": 80, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3137", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8631", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-8803", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7052", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.841796875, "KG": 0.48203125, "before_eval_results": {"predictions": ["india", "a partridge", "pep", "Bavarian", "george iv", "azerbaijan", "peter", "Sisyphus", "Italy", "a coffee house", "Cambodia", "Moldova", "Check Me", "Ethiopia", "Frank McCourt", "Gremlins (Dante)", "Arkansas", "Texas", "Norway", "\"Archer\"", "William Blake", "paul paul smith", "Federer", "Charlie Chan", "Christiaan Huygens", "Great British Bake Off", "World War I", "shekel", "george Sand", "Michael Caine", "Professor Brian Cox", "brabham", "Knutsford", "Coronation Street", "McDonnell Douglas", "tyne", "Missouri", "Emma Chambers", "Buckinghamshire", "emothereevlad\u0131r\u00e7\u00fck\u00fcnucununk", "cat", "David Lynch", "nine", "One Direction", "Groucho Marx", "brazil", "Kate Winslet", "india", "August 1925", "Sweeney Todd", "Rio Grande", "If waivers are requested outside the playing season, or before November 1", "12 February 1999", "David Joseph Madden", "\"The Braes of Balquhither\"", "Mary Astor", "al-Qaeda", "natural gas", "CNN's best ten golf movies ever made", "Madonna", "Hawaii", "Monaco", "P.D. James", "MacFarlane"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6979166666666666}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-3969", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-909", "mrqa_triviaqa-validation-2921", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-2733", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-9986", "mrqa_hotpotqa-validation-2718", "mrqa_newsqa-validation-4110"], "SR": 0.671875, "CSR": 0.5063657407407407, "EFR": 0.8571428571428571, "Overall": 0.6777017195767195}, {"timecode": 81, "before_eval_results": {"predictions": ["Pebble Beach", "a punctuation mark written before the first letter of an interrogative sentence or clause to indicate that a question follows", "Rochester", "John Dalton", "Alamodome and city of San Antonio", "rearview mirror", "one - mile - wide ( 1.6 km )", "Churchill", "BC Jean and Toby Gad", "UNESCO / ILO Recommendation concerning the Status of Teachers", "September 2017", "Universal Pictures and Focus Features", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "2017", "2017", "Tbilisi, Georgia", "April 1917", "1900", "Frankie Muniz", "Geothermal gradient", "10 : 30am", "dorsal portion of the frontal lobe", "planned invasion of the United Kingdom", "potential of hydrogen", "volcanic activity", "held that `` a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Egypt", "As of January 17, 2018, 201 episodes", "pia mater", "members of the gay ( LGBT ) community", "Burbank, California", "1986", "2018", "rapid destruction of the donor red blood cells by host antibodies", "1623", "Rudyard Kipling", "March 16, 2018", "Fusajiro Yamauchi", "the rez", "2013", "the breast or lower chest", "joseph granny mavrakis", "data services ( Flex Data Services )", "2018", "Saint Peter", "1963", "August 19, 2016", "Madison", "George Washington", "ABC", "Brevet Colonel Robert E. Lee", "glockenspiel", "Alaska", "jane galton", "Elbow", "Dundalk", "Division II", "the Airbus A330-200", "46.88 knots.", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old,", "porto", "gravity", "Hercule Poirot", "Yemeni port city of Aden"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5646221766250394}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.5454545454545454, 0.0, 1.0, 0.2857142857142857, 0.4, 0.7499999999999999, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.5555555555555556, 0.8333333333333333, 1.0, 1.0, 0.7710843373493976, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.17391304347826086, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-9521", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-9756", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-2319", "mrqa_triviaqa-validation-4135", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-5829", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-4136", "mrqa_searchqa-validation-7915", "mrqa_searchqa-validation-2328", "mrqa_newsqa-validation-4144"], "SR": 0.421875, "CSR": 0.5053353658536586, "retrieved_ids": ["mrqa_squad-train-56993", "mrqa_squad-train-36310", "mrqa_squad-train-74158", "mrqa_squad-train-80135", "mrqa_squad-train-53476", "mrqa_squad-train-10865", "mrqa_squad-train-62724", "mrqa_squad-train-45914", "mrqa_squad-train-28737", "mrqa_squad-train-64675", "mrqa_squad-train-55278", "mrqa_squad-train-73537", "mrqa_squad-train-81010", "mrqa_squad-train-76801", "mrqa_squad-train-77935", "mrqa_squad-train-59197", "mrqa_naturalquestions-validation-5583", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-13396", "mrqa_naturalquestions-validation-9818", "mrqa_triviaqa-validation-2536", "mrqa_naturalquestions-validation-1372", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-334", "mrqa_hotpotqa-validation-2333", "mrqa_triviaqa-validation-1708", "mrqa_hotpotqa-validation-5531", "mrqa_searchqa-validation-16103", "mrqa_newsqa-validation-340", "mrqa_searchqa-validation-5911", "mrqa_naturalquestions-validation-779", "mrqa_searchqa-validation-13003"], "EFR": 0.9459459459459459, "Overall": 0.6952562623599209}, {"timecode": 82, "before_eval_results": {"predictions": ["Jon Stewart", "king henry i", "Ross Kemp", "jumanji", "Kirk Douglas", "William Shakespeare", "Christmas", "violets", "Rod Stewart", "Gerald Ford", "doppel", "pembrokeshire coast park", "mavrakisia", "South Africa", "sows", "The Persistence of Memory", "orangutan", "The Time Machine", "Uranus", "Tacitus", "Lady Gaga", "Mecca", "cirrus cloud", "voronya", "myxomatosis", "joseph bacall", "Philippines", "xerophyte", "joseph Gilbert", "The King and I", "The Last King of Scotland", "jaws", "Pearson PLC", "John Steinbeck", "Bulletin", "violin", "Ross Bagdasarian", "Mark Hamill", "Sam Smith", "Burma", "peasant", "cryonics", "j\u00f8rn Utzon", "Another Day in Paradise", "decorate", "vatican city", "Department of Justice", "australia", "wakefulness", "J. S. Bach", "Corfu", "Walter Brennan", "a solitary figure who is not understood by others, but is actually wise", "Continental drift", "Major Charles White Whittlesey", "January 11, 2016", "White Knights of the Ku Klux Klan", "6,000", "fill a million sandbags and place 700,000 around our city,\"", "Judge Herman Thomas", "the processor", "the Obereks", "@font-face", "1945 to 1951"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6368303571428571}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-6126", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-6450", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-5492", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4711", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-2044"], "SR": 0.578125, "CSR": 0.5062123493975903, "EFR": 0.9629629629629629, "Overall": 0.6988350624721107}, {"timecode": 83, "before_eval_results": {"predictions": ["Libya", "Syriza", "aves", "wrigley", "PJ Harvey", "blur", "one", "Charles Taylor", "Palm sunday", "d'\u00e9tat", "The Wicker Man", "lungs", "chess", "a serpention", "Peter Nichols", "bear grylls", "Count Basie", "john Glenn", "Plato", "amundsen", "dutch", "Pensacola, Florida", "india", "Michael Hordern", "Gerald Durrell", "ishmael", "francis", "climate", "a trenches exhibition", "madonna", "etruscan", "vivistic", "wednesday", "Bulls Eye", "south africa", "a pair of welding boots", "Helen Gurley Brown", "vivbes", "Jungle Book", "jimannam", "Massachusetts", "blurlin", "Hamlet", "Heather Stanning", "peter", "USS macon", "madonna", "blur", "australian", "madonna", "bindweed", "1996", "a few central Mexican recipes", "16 June", "Squam Lake", "\"Big Hero 6\"", "1946", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "The Impeccable", "ruling Justicialist Party,", "the Ming dynasty", "Prince Albert", "a crossword clue", "al Qaeda."], "metric_results": {"EM": 0.5, "QA-F1": 0.5604910714285714}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-3264", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-925", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-352", "mrqa_triviaqa-validation-2857", "mrqa_triviaqa-validation-7713", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-4848", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-5596", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4407", "mrqa_newsqa-validation-3703", "mrqa_searchqa-validation-581", "mrqa_searchqa-validation-6285"], "SR": 0.5, "CSR": 0.5061383928571428, "EFR": 0.96875, "Overall": 0.6999776785714286}, {"timecode": 84, "before_eval_results": {"predictions": ["ganges", "david Hilbert", "Halifax", "porto", "Q", "Franklin Delano Roosevelt", "Buncefield Depot", "helium", "cappuccino", "bacall", "lord asquith", "cimarron", "david fincher", "florida", "lord leveson", "Jupiter Mining Corporation", "david hartman", "nouakchott", "helen", "verona", "once every two weeks", "crystal gayle", "Noah", "jimzebel", "budge", "queen Victoria and Prince Albert", "douglas", "Dick Whittington", "caelian hill", "rowing", "ou walt Disney", "gin", "supertramp", "leicestershire", "halogens", "Jackie Kennedy", "blue and blue", "calcium carbonate", "tinware, copper", "cuba", "lorraine", "Nicola Adams", "lewis city", "Andes", "ellay topley", "carry on Cleo", "davidsucks", "dysmenorrhea", "kenya", "jill esmond", "reneHiguita", "approximately 26,000 years", "Travis Tritt and Marty Stuart", "Thorleif Haug", "perdita", "Michael Ebenazer Kwadjo Omari Owuo, Jr.", "Dele Alli", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "one count of attempted murder", "Ma Khin Khin Leh,", "Wii", "Germaine Greer", "and Esau", "Surrey"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4788938492063492}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.5714285714285715, 1.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3183", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7646", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-3125", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-2381", "mrqa_triviaqa-validation-7129", "mrqa_triviaqa-validation-4587", "mrqa_naturalquestions-validation-2509", "mrqa_hotpotqa-validation-3085", "mrqa_hotpotqa-validation-875", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-729", "mrqa_newsqa-validation-2718", "mrqa_searchqa-validation-4541", "mrqa_searchqa-validation-14852"], "SR": 0.421875, "CSR": 0.5051470588235294, "retrieved_ids": ["mrqa_squad-train-55641", "mrqa_squad-train-10516", "mrqa_squad-train-72834", "mrqa_squad-train-35044", "mrqa_squad-train-27498", "mrqa_squad-train-15662", "mrqa_squad-train-65107", "mrqa_squad-train-85910", "mrqa_squad-train-69753", "mrqa_squad-train-86170", "mrqa_squad-train-26647", "mrqa_squad-train-68895", "mrqa_squad-train-22956", "mrqa_squad-train-30739", "mrqa_squad-train-40915", "mrqa_squad-train-22213", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-3564", "mrqa_newsqa-validation-2222", "mrqa_hotpotqa-validation-3177", "mrqa_squad-validation-4150", "mrqa_searchqa-validation-9394", "mrqa_triviaqa-validation-2811", "mrqa_squad-validation-739", "mrqa_newsqa-validation-1674", "mrqa_searchqa-validation-10927", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-15055", "mrqa_newsqa-validation-4030", "mrqa_searchqa-validation-3451", "mrqa_squad-validation-335", "mrqa_searchqa-validation-11096"], "EFR": 0.972972972972973, "Overall": 0.7006240063593004}, {"timecode": 85, "before_eval_results": {"predictions": ["eagle", "teacher", "Shaft", "a conic curve", "robert ramin", "region of SW Asia", "Spanish", "pull", "rudyard kepling", "eat porridge", "The Life and Opinions of Tristram Shandy", "vincenzo Nibali", "long ton", "god of Earth", "pram", "lexis", "c Cyprus", "sheep", "laos", "The Toilet Lid Lock", "Andes Mountains", "georgia sand", "10", "rollover", "shepherd name", "shoulder", "c. 800 AD", "legs", "brighton", "Saturday Night", "afterlife", "on the first Monday of September", "1982", "bea", "Danish", "priests or the priesthood", "p Pablo Escobar", "south africa", "Microsoft", "Bolivia", "Napoleon Bonaparte", "secretary", "Apocalypse Now", "fred gumm", "Amnesty International", "black man in black", "colonial", "ireland", "fred perry", "50", "russia", "before the first year begins", "2,579", "American country music group The Nitty Gritty Dirt Band", "1919", "Apatosaurus", "freddi's \"Aida\" at the Teatro Metastasio", "Virgin America", "he and the others were trained for more than a year in Pakistan by Lashkar-e-Tayyiba, a banned Islamic militant group.", "police", "Daredevil", "George Washington Carver", "the giant panda", "California, Texas and Florida,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5574379105090312}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.13793103448275862, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2922", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-3394", "mrqa_triviaqa-validation-4316", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-4290", "mrqa_triviaqa-validation-193", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-3319", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-3109", "mrqa_triviaqa-validation-6065", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-3040", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-3672", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-2820", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-672", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-34", "mrqa_hotpotqa-validation-4899", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-2338"], "SR": 0.484375, "CSR": 0.504905523255814, "EFR": 0.9090909090909091, "Overall": 0.6877992864693445}, {"timecode": 86, "before_eval_results": {"predictions": ["police car sits outside the Westroads Mall in Omaha, Nebraska,", "money or other discreet aid for the effort if it could be made available,", "41,", "Adidas", "to promote the attempts but simply to oversee them in a fair and independent manner and ratify successful efforts.", "suicide bombing", "iTunes service", "sedona", "South Africa's", "shot in the head", "at a house party in Crandon, Wisconsin,", "kenya Cole", "$17,000", "Eleven", "gun", "urged NATO to take a more active role in countering the spread of the", "school-age girls", "Bagosora", "\"The Lost Symbol,\"", "Haiti", "2,000", "German authorities", "his brother to surrender.", "Roy Foster", "Mogadishu", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "16", "portrait of Michael Jackson", "fighting charges of Nazi war crimes", "Big Brother", "London, England", "a new gene", "revolution of values", "\"Number Ones\"", "friends and fans.", "U.S. Marines training base", "an empty water bottle down the touchline", "Samuel Herr,", "forged credit cards and identity theft", "the two were embedded with the rebels while working on a story about the region.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "\"Dr. No.\"", "five female pastors", "Facebook and Google,", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "NATO's International Security Assistance Force", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England.", "U.S. military helicopter", "mental health", "plastination", "fined", "Thomas Edison", "Randy", "Thomas Lennon", "1947", "new york city", "mariette", "Boston Celtics", "Australian", "Northwestern Hawaiian Islands", "florida", "bagels", "the Seine", "Mary Tyler Moore Show"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5165281583833012}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 0.5555555555555556, 1.0, 1.0, 0.10526315789473684, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.7499999999999999, 1.0, 0.19512195121951217, 0.09090909090909091, 0.04761904761904762, 0.0, 0.5, 1.0, 0.0, 1.0, 0.4799999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-982", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-227", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-3913", "mrqa_naturalquestions-validation-10724", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-4625", "mrqa_searchqa-validation-14248"], "SR": 0.421875, "CSR": 0.5039511494252873, "EFR": 1.0, "Overall": 0.7057902298850574}, {"timecode": 87, "before_eval_results": {"predictions": ["four", "yellow", "pertussis", "Kawasaki", "Harrison Ford", "Reservoir", "the equator", "Wimbledon Lawn Tennis Championship", "\"Sugar Baby Love\"", "1981", "Bernardo Bertolucci", "\u201cThe Seven Year Itch\u201d", "dieppe", "Nile River", "spinach", "opera Libretti", "Nicky Morgan", "Midsomer Murders", "Muriel", "Abraham", "Aquaman", "the American Civil War", "Christian Louboutin", "St Pauls", "the wren", "orange", "shingles", "Queen Mother's coffin", "indonesian rupiah", "lisping Violet-Elizabeth Bott", "Charles II", "Illinois", "Danelaw", "Landlord\u2019s Game", "true", "Christine Keeler", "Silver Hatch", "magic", "Guatemala", "clogs", "\"Bad Mac\"", "lolita", "Edwina Currie", "Baton Rouge", "Warsaw", "2010", "Carole King", "drizzle", "Casualty", "brimdon", "sleepless in seattle", "Telma Hopkins, Joyce Vincent Wilson and her sister Samantha Vincent on backing vocals", "1624", "Milira", "Wiltshire, in the south west of England", "Austrian Volksbanks", "French Second Republic", "The son of Gabon's former president", "The group, the Kurdish militant group in Turkey,", "Sonia Sotomayor", "an abacus", "the Maine", "the Marquis de Lafayette", "The Osmonds"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6132254464285715}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2178", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-1851", "mrqa_triviaqa-validation-1047", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-2756", "mrqa_triviaqa-validation-985", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-5775", "mrqa_naturalquestions-validation-2862", "mrqa_hotpotqa-validation-3917", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-720"], "SR": 0.578125, "CSR": 0.5047940340909092, "retrieved_ids": ["mrqa_squad-train-35716", "mrqa_squad-train-47442", "mrqa_squad-train-45121", "mrqa_squad-train-51858", "mrqa_squad-train-80077", "mrqa_squad-train-62139", "mrqa_squad-train-41327", "mrqa_squad-train-81727", "mrqa_squad-train-13190", "mrqa_squad-train-39568", "mrqa_squad-train-55041", "mrqa_squad-train-20096", "mrqa_squad-train-17168", "mrqa_squad-train-39402", "mrqa_squad-train-21805", "mrqa_squad-train-51094", "mrqa_naturalquestions-validation-1770", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-318", "mrqa_naturalquestions-validation-437", "mrqa_newsqa-validation-37", "mrqa_searchqa-validation-3244", "mrqa_searchqa-validation-10836", "mrqa_triviaqa-validation-2655", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-4134", "mrqa_searchqa-validation-15686", "mrqa_newsqa-validation-2582", "mrqa_searchqa-validation-244", "mrqa_naturalquestions-validation-485", "mrqa_triviaqa-validation-4407"], "EFR": 0.9629629629629629, "Overall": 0.6985513994107744}, {"timecode": 88, "before_eval_results": {"predictions": ["a chicken", "a luau feast", "Pat Paulsen", "paddington bear", "Antarctica", "casino", "the Mensheviks", "Prada", "Eternity Men Eau de Toilette", "a hatchet", "\"The Aston Martin won't start, so (he) drives a '95 Ford Focus\"", "baboons", "Chicken Little", "Bach", "Bangkok", "Eli Whitney", "John Smith", "Vasco Nez", "A Bug\\'s Life", "boys", "quiver", "the joker", "Richard Nixon", "Benito Mussolini", "a sheepshank", "Robert Burns", "Ebony", "Jack Nicklaus", "brass", "Las Vegas", "fiber", "poppy", "a portrait", "Lord of the Flies", "Jeopardy", "Nickelback", "succotash", "jack London", "Falklands", "acetone", "fudge", "adultery", "frankfurter", "Roanoke", "Blackbeard", "Danielle Davenport", "Borden", "SO2", "urban", "a dachshund", "Robert frost", "Virginia Dare", "Ole Einar Bj\u00f8rndalen", "the first quarter of the 19th century", "George Washington", "puerto Rico", "dav go!", "1987", "Jacobite", "Leinster", "a tenement in the Mumbai suburb of Chembur,", "Monday's", "an initial report outlining its findings and recommendations in about 100 days.", "t.S. Eliot"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6182291666666666}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-10484", "mrqa_searchqa-validation-16628", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-14289", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-3438", "mrqa_searchqa-validation-5949", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-12850", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-14294", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-13827", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-2110", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-5235", "mrqa_searchqa-validation-5816", "mrqa_triviaqa-validation-3013", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-220", "mrqa_newsqa-validation-3631", "mrqa_triviaqa-validation-813"], "SR": 0.5625, "CSR": 0.5054424157303371, "EFR": 1.0, "Overall": 0.7060884831460674}, {"timecode": 89, "before_eval_results": {"predictions": ["Bill Bryson", "Pink Panther", "Jordan", "Scandinavia", "sartre", "Motown", "Bernadotte", "Venus", "riyadh", "Margot fonteyn", "Kay Adams", "a plutocracy", "dominoes", "ringway", "Radio 4 Extra", "a handbags", "violin", "u2", "Barcelona", "australian", "auk", "weirs", "Southampton", "soy", "George Best", "round of fools", "antoine de caunes", "Red Rock West", "x", "zagreb", "handley page", "Marine One", "zachary taylor", "Hitler", "all Hallows", "Shaft", "brazil", "Louis Le Vau", "Scotland", "Tripoli", "jubilee", "Abbey Theatre", "Maine", "willow", "b4425", "Denver", "June 14th", "Mel Blanc", "Lily Allen", "terrorism", "oats", "Wisconsin", "The Crossing", "Whig candidates", "more than 230", "Serie B", "Mark O'Connor", "Colombia.", "a federal judge in Mississippi", "\"The workers should be dealt (with) with compassion and should not be pushed so hard that they resort to whatever that had happened in Nodia\"", "insects", "the Untouchables", "curge", "Tyne Daly"], "metric_results": {"EM": 0.625, "QA-F1": 0.6839407568238214}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.5, 1.0, 1.0, 1.0, 1.0, 0.06451612903225806, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-5922", "mrqa_triviaqa-validation-3265", "mrqa_triviaqa-validation-6744", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-484", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-7107", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1265", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-4552", "mrqa_hotpotqa-validation-87", "mrqa_newsqa-validation-3566", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-9329"], "SR": 0.625, "CSR": 0.5067708333333334, "EFR": 0.9583333333333334, "Overall": 0.6980208333333333}, {"timecode": 90, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.853515625, "KG": 0.48046875, "before_eval_results": {"predictions": ["meteoroid", "colleen McCullough", "lion king", "Cyprus", "f. Lee Bailey", "sprite", "Uzbekistan", "dove", "the giraffe", "fairgrounds", "united states", "Venus", "top do was to dress and act as cool", "united states", "colleen McCullough", "Egypt", "fred viviata", "Drew Carey", "three Mile Island", "s Sicily", "united states", "enigma", "brussels", "arrows", "quatermass experiment", "pasta harvest", "Frogmore Estate or Gardens", "the Emmy Awards", "c Caucasus", "88", "cold Comfort farm", "winter", "iceland", "dav Hilbert", "mediterranean", "Declaration of Independence", "marlon Brando", "fish", "Octavian", "Greenham Common", "Robert Schumann", "pimlico", "Grace Slick", "michael caine", "the university of tain", "Robert Boyle", "1929", "The Lone Gunmen", "sue", "daily herald", "prufie tempah", "Brian Steele", "a 1920 play R.U.R.", "Philadelphia, which is Greek for brotherly love", "the Cherokee River", "Benedict of Nursia", "Wichita", "Jewish tradition", "Friday,", "put him in \"solitary confinement.", "a dove", "acting", "Annie Proulx", "\"Nude, Green Leaves and Bust\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.6777935606060606}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-5508", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-5925", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-4815", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-3038", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-1398", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4925", "mrqa_naturalquestions-validation-3609", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-508"], "SR": 0.609375, "CSR": 0.5078983516483517, "retrieved_ids": ["mrqa_squad-train-61451", "mrqa_squad-train-14288", "mrqa_squad-train-15677", "mrqa_squad-train-66915", "mrqa_squad-train-24062", "mrqa_squad-train-34964", "mrqa_squad-train-85879", "mrqa_squad-train-75392", "mrqa_squad-train-27636", "mrqa_squad-train-75100", "mrqa_squad-train-70165", "mrqa_squad-train-35880", "mrqa_squad-train-35034", "mrqa_squad-train-10643", "mrqa_squad-train-76063", "mrqa_squad-train-12911", "mrqa_newsqa-validation-2404", "mrqa_hotpotqa-validation-45", "mrqa_newsqa-validation-3513", "mrqa_naturalquestions-validation-1770", "mrqa_squad-validation-7332", "mrqa_triviaqa-validation-4315", "mrqa_searchqa-validation-7204", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1021", "mrqa_searchqa-validation-10836", "mrqa_hotpotqa-validation-2840", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-6366", "mrqa_searchqa-validation-9394", "mrqa_naturalquestions-validation-9756", "mrqa_newsqa-validation-1512"], "EFR": 0.96, "Overall": 0.7099859203296703}, {"timecode": 91, "before_eval_results": {"predictions": ["Christianity Today", "2011", "John McClane", "Marika Green", "Princeton", "online", "Milan", "writer", "Elton John", "Newcastle upon Tyne, England", "Lev Ivanovich Yashin", "Blackwood Partners Management Corporation", "1958", "2007", "robot Overlords", "1776", "Ashland is home to Scribner-Fellows State Forest.", "public", "1987", "Austria", "Ron Cowen and Daniel Lipman", "\"Lucky Deeds\" & the t.v. series Boystown", "stadium in the north central United States, located in the Downtown East neighborhood of Minneapolis, Minnesota", "New York", "January 30, 1930", "Dr. Alberto Taquini", "John Gotti", "Michael Bisping", "Santiago del Estero Province", "Harrison Ford", "C. H. Greenblatt", "Taoiseach of Ireland", "Dissection", "The Killer", "seacoast region", "Ken Rutherford", "Dorothy", "2017", "people working in film and the performing arts", "June 2, 2008", "The 8th Habit", "one", "London", "australian", "Ready Player One", "World Rowing Championships", "1911", "15,023", "America East Conference", "the highland regions of Scotland", "Phelan Beale", "March 2018", "Bay of Plenty, Taupo and Wellington", "62", "flybe", "perry michael", "Oliver Goldsmith", "Pakistani officials.", "James Whitehouse,", "Kenyan forces", "Ford", "Jason Bourne", "Kiwanis", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5951636904761906}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.3333333333333333, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4275", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-4208", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-4040", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-2628", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-12", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-3250", "mrqa_naturalquestions-validation-1636", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-61", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2233", "mrqa_searchqa-validation-9994", "mrqa_newsqa-validation-719"], "SR": 0.515625, "CSR": 0.5079823369565217, "EFR": 1.0, "Overall": 0.7180027173913043}, {"timecode": 92, "before_eval_results": {"predictions": ["Margery Williams", "Hanoi", "Terry Richardson", "one of the youngest publicly documented people to be identified as transgender", "The Australian women's national soccer team", "Odense Boldklub", "SpongeBob SquarePants 4-D", "Oldham County", "those subjects or skills that in classical antiquity were considered essential for a free person", "The Wright brothers", "a research university with high research activity", "O.T. Genasis", "science fiction drama", "Speedway World Championship", "Citric acid", "About 200", "a moth", "Gerald Hatten Buss", "Delacorte Press", "close range combat", "twice", "Eli Roth", "Adelaide", "Lincoln Riley", "December 13, 1920", "Richard B. Riddick", "John McClane", "rural", "Orchard Central", "Art of Dying", "The Book of Judges", "Ant Timpson, Ted Geoghegan and Tim League", "Manhattan federal court", "John Ford", "classical", "Marvel Comics", "40,000", "crafting and voting on legislation", "Magic Johnson", "Nevada", "Yasir Hussain", "Victoria", "Nancy Dow", "Long Island Rail Road", "Volcano Bay", "25 December 2009", "the Hanna-Barbera show \"Birdman and the Galaxy Trio.\"", "Jango Fett", "Die Frau ohne Schatten", "Minnesota's 8th congressional district", "NBA 2K16", "Little Mo", "Reverse - Flash", "India", "1896", "beetles", "australian", "fan", "Kearny, New Jersey.", "problems processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "noodles", "Tennessee Williams", "Illinois", "sheila Kaye-Smith"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6666890948963318}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.33333333333333326, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.4, 0.9090909090909091, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-1339", "mrqa_hotpotqa-validation-4211", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-41", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-1445", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-4979", "mrqa_hotpotqa-validation-4735", "mrqa_naturalquestions-validation-4919", "mrqa_triviaqa-validation-6079", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-8642", "mrqa_triviaqa-validation-4848"], "SR": 0.5625, "CSR": 0.5085685483870968, "EFR": 1.0, "Overall": 0.7181199596774194}, {"timecode": 93, "before_eval_results": {"predictions": ["YIVO", "Archbishop of Canterbury", "Samuel Beckett", "January 28, 2016", "The Primate of All Ireland", "close range", "Iran", "Kate Millett", "Timothy Matthew Howard", "\"Lucky\"", "Do Kyung-soo", "John Hunt", "Kongo", "William Finn", "Sam Raimi", "The final of 2011 AFC Asian Cup", "east-west", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains", "Marine Corps Base Hawaii", "March 2016", "Sergei Prokofiev", "Walter R\u00f6hrl", "his left hand", "Vladimir Menshov", "biggest commercial success is the erotic thriller \"Ch Chloe\"", "Denmark and Norway", "Love and Theft", "C. W. Grafton", "Jean-Claude Van Damme", "My Love from the Star", "14,597", "Robert Noyce", "Robert Parker Parrott", "Afghanistan", "Operation Aqueduct", "guitar feedback", "Flushed Away", "the George Washington Bridge", "Reginald Engelbach", "Van Diemen's Land", "Tampa", "Sergeant First Class", "140 million", "SpongeBob SquarePants 4-D", "LA Galaxy", "Argentinian", "Neotropical realm", "ninth most influential person in Maine", "1998", "The More", "John F. Kennedy Jr.", "com TLD", "Andrew Lloyd Webber", "John Bull", "NASCAR", "The Avengers", "Rick Wakeman", "mental health", "Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist-Leninist)", "hardship", "Berlin", "William Golding", "The Odd Couple", "Americans who served in the armed forces and as civilians during World War II"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7064460712898213}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.25, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.5, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-386", "mrqa_hotpotqa-validation-3505", "mrqa_hotpotqa-validation-5676", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-1420", "mrqa_hotpotqa-validation-2085", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-4776", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-4905", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4235", "mrqa_naturalquestions-validation-4844", "mrqa_newsqa-validation-964", "mrqa_newsqa-validation-1063"], "SR": 0.640625, "CSR": 0.5099734042553192, "retrieved_ids": ["mrqa_squad-train-58316", "mrqa_squad-train-45484", "mrqa_squad-train-28796", "mrqa_squad-train-9425", "mrqa_squad-train-68305", "mrqa_squad-train-86262", "mrqa_squad-train-76776", "mrqa_squad-train-63144", "mrqa_squad-train-35987", "mrqa_squad-train-37798", "mrqa_squad-train-55826", "mrqa_squad-train-7104", "mrqa_squad-train-9733", "mrqa_squad-train-18257", "mrqa_squad-train-63244", "mrqa_squad-train-2078", "mrqa_naturalquestions-validation-366", "mrqa_triviaqa-validation-5560", "mrqa_hotpotqa-validation-3790", "mrqa_naturalquestions-validation-5378", "mrqa_searchqa-validation-16908", "mrqa_hotpotqa-validation-2044", "mrqa_searchqa-validation-7086", "mrqa_triviaqa-validation-3825", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-429", "mrqa_naturalquestions-validation-9979", "mrqa_triviaqa-validation-3263", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3063", "mrqa_naturalquestions-validation-8700", "mrqa_triviaqa-validation-7704"], "EFR": 1.0, "Overall": 0.7184009308510639}, {"timecode": 94, "before_eval_results": {"predictions": ["Nutbush, TN", "kilo", "6", "golf", "Einstein", "Manchester", "city of sarah", "Bleak house", "Vienna", "Harry S. Truman", "Seattle", "to make wrinkles", "Amy Tan", "The Great Gatsby", "Charlie Chan", "1664", "Finding forrester", "lord", "Iain Duncan Smith", "engraver", "winston", "Jim Peters", "nitrogen", "oldpatricktoe-end", "Delilah", "Infante", "cuckoo", "PPTH", "The Wicker Man", "yellow", "Prince Edward Island", "vii", "Guardian", "John Huston", "The Passenger Pigeon", "Anne Frank", "manchego", "Mexico", "pi\u00f1a colada", "fauntleroy", "kachhi biryani", "Petula Clark", "Jo Moore", "Flo Rida", "The Comedy of Errors", "beer", "chemical origins of life", "Russia", "glucose", "dolma", "kempton racecourse", "Cress", "Vesta", "Season 5 premiere, `` weight Loss ''", "Tiffany & Company", "2010 to 2012", "Nathan Bedford Forrest", "Friday,", "Six", "that things are going well for them personally.", "tanning", "Louisiana State University", "Heidi Montag", "the Bactrian"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6419270833333333}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.25]}}, "before_error_ids": ["mrqa_triviaqa-validation-4669", "mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-7529", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-233", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-4091", "mrqa_hotpotqa-validation-2141", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-8477", "mrqa_searchqa-validation-5376", "mrqa_naturalquestions-validation-8046"], "SR": 0.609375, "CSR": 0.5110197368421052, "EFR": 1.0, "Overall": 0.718610197368421}, {"timecode": 95, "before_eval_results": {"predictions": ["the Korean War started", "$1.5 million", "Fernando Caceres", "37", "opposition parties", "German", "people give the United States abysmal approval ratings.", "Secretary of State", "my wife's first name", "undress from the waist up", "U.S. senators", "marking", "Alexey pajitnov,", "Spc. Megan Lynn Touma,", "regulators in the agency's Colorado office received improper gifts from energy industry representatives and engaged in illegal drug use and inappropriate sexual relations with them.", "west African nation", "Leo Frank,", "Sri Lanka,", "Johannesburg", "last year's", "enormous suffering and massive displacement,\"", "heavy floss", "it is not something that has gotten lost,\"", "Quetta, the capital of Balochistan province,", "walk", "an independent homeland", "bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "longest domestic relay in the games' history,", "Kim Jong Un", "Arthur E. Morgan III,", "billboards with an image of the burning World Trade Center", "Adidas,", "Too many glass shards left by beer drinkers in the city center,", "\"E! News\"", "Cologne, Germany,", "she was lured from a dorm and assaulted in a bathroom stall.", "70,000", "The station was getting continuing inquiries,", "northwestern province of Antioquia,", "very little in public about the scandal,", "large accumulations of ice in places such as the north Georgia mountains,", "East Java", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "billions of dollars", "between June 20 and July 20,", "Rod Blagojevich,", "attacked L.K. Chaudhary,", "Majid Movahedi,", "$1.45 billion", "Isabella, Emma, Olivia, Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia.", "billions of dollars", "Windows Media Video ( WMV )", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Joe Pizzulo and Leeza Miller", "Alan Turing", "paisley", "thumbelina", "three", "The Apple iPod+HP", "Lithuanian", "Wayne's World", "Krakauer", "Jake Barnes", "Rob Reiner"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6322924624395213}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true], "QA-F1": [0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.33333333333333337, 1.0, 0.0, 0.9047619047619047, 0.8333333333333334, 1.0, 0.4, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.8181818181818182, 1.0, 0.33333333333333337, 0.0, 0.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1018", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-3916", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1314", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-318", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-10142"], "SR": 0.484375, "CSR": 0.5107421875, "EFR": 0.9696969696969697, "Overall": 0.712494081439394}, {"timecode": 96, "before_eval_results": {"predictions": ["\"we don't have hard evidence\" on this.", "poppy production", "security breach", "urged NATO to take a more active role in countering the spread of the", "the situation of America wielding a big stick for the last eight years.\"", "Noriko Savoie", "Brad Blauser,", "Tuesday afternoon.", "Iowa,", "Dr. Jennifer Arnold and husband Bill Klein,", "Chinese", "Mayor Michael Bloomberg", "more than 1.2 million", "the estate with its 18th-century sights, sounds, and scents.", "\"Seventy-three percent of those questioned in a CNN/Opinion Research Corporation survey released Monday", "$250,000 for Rivers' charity: God's Love We Deliver.", "Flint, Michigan", "FARC rebels.", "Mexico", "Larry King", "Alberto Espinoza Barron,", "spiral into economic disaster.", "four", "Mawise Gumba", "burned over 65 percent of his body", "Brian Smith.", "2-1", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "Hank Moody", "April 2010.", "\"Nothing But Love\" comeback tour,", "Mandi Hamlin", "people look at the content of the speech, not just the delivery.", "Yemen,", "her boyfriend, Dodi Fayed,", "\"He is a very special member of our family.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "the company has not yet managed to sell the concept to a buyer.", "Manny Pacquiao", "\"You take a bite of cheesecake and you think'should I be doing this?'", "\"sincere gratitude for all the help the Angels have provided.\"", "President Thabo Mbeki", "Bright Automotive, a small carmaker from Anderson, Indiana,", "Jeffrey Jamaleldine", "did not get them to agree on a solution.", "Haiti,", "Salt Lake City,", "his business dealings", "hardship for terminally ill patients and their caregivers,", "nearly 28 years of rule.", "Hollywood", "V \u00d7 2", "Thon Maker", "a section of the Torah ( Five Books of Moses )", "Syria", "Benedictine", "The straight Dope", "sexy Star", "August 31, 1944", "Dutch", "a dragon", "Marcus Garvey", "a zodiac", "obsessive-compulsive"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6407599778693529}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.2222222222222222, 0.125, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.6153846153846153, 0.0, 0.9166666666666666, 0.0, 1.0, 0.1, 0.0, 0.2, 0.4444444444444445, 0.6666666666666666, 0.11111111111111112, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3578", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-905", "mrqa_newsqa-validation-1138", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-6184", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-1912", "mrqa_searchqa-validation-2594", "mrqa_searchqa-validation-9869"], "SR": 0.515625, "CSR": 0.5107925257731959, "retrieved_ids": ["mrqa_squad-train-21010", "mrqa_squad-train-34584", "mrqa_squad-train-15322", "mrqa_squad-train-43837", "mrqa_squad-train-52648", "mrqa_squad-train-21047", "mrqa_squad-train-26091", "mrqa_squad-train-39127", "mrqa_squad-train-72629", "mrqa_squad-train-43517", "mrqa_squad-train-72177", "mrqa_squad-train-81713", "mrqa_squad-train-4521", "mrqa_squad-train-48071", "mrqa_squad-train-19122", "mrqa_squad-train-46021", "mrqa_naturalquestions-validation-9917", "mrqa_searchqa-validation-14480", "mrqa_triviaqa-validation-7361", "mrqa_newsqa-validation-1290", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-4007", "mrqa_searchqa-validation-11135", "mrqa_hotpotqa-validation-585", "mrqa_triviaqa-validation-6018", "mrqa_searchqa-validation-6285", "mrqa_squad-validation-3946", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-5051", "mrqa_newsqa-validation-3847", "mrqa_triviaqa-validation-4750", "mrqa_searchqa-validation-6816"], "EFR": 1.0, "Overall": 0.7185647551546392}, {"timecode": 97, "before_eval_results": {"predictions": ["Johannes Gutenberg", "2018", "Exodus and Deuteronomy", "John Adams and Benjamin Franklin, as well as Jefferson's notes of changes made by Congress", "about the level of the third lumbar vertebra, or L3, at birth", "his father King Dasharatha", "Pakistan", "the red - bed country of its watershed", "the Western Bloc ( the United States, its NATO allies and others )", "Rashida Jones", "cut off close by the hip, and under the left shoulder", "sunny throughout the year", "Peter Andrew Beardsley MBE", "Season two", "Terry Reid", "1260 cubic centimeters ( cm )", "May 3, 2005", "43", "the Rashidun Caliphs", "British Columbia, Canada", "Book of Revelation", "Pyeongchang County, Gangwon Province, South Korea", "a diffuse interstellar medium ( ISM )", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings", "1943", "Tokyo for the 2020 Summer Olympics", "a giant landslide at the head of Lituya Bay in Alaska", "San Jose, California", "wagen VIII Maus", "July 2, 1776", "accomplish the objectives of the organization", "Domhnall Gleeson", "Simon Callow", "Florida", "Laura Jane Haddock", "May 2016", "Bacon", "1994", "Matthew Broderick", "senators", "origins of replication, in the genome", "the fourth quarter of the preceding year", "April 2016", "Michael Schumacher", "Massachusetts", "altitude", "2008", "post translational modification", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "a writ of certiorari", "Jules Shear", "south america", "island", "the neck", "Romeo Montague", "De La Soul", "Delilah Rene", "July", "says conviction of Peru's ex-president is a warning to those who deny human rights", "$80,000 a year", "the Missouri", "jade", "Frank Sinatra", "Long troop deployments"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6661779453185703}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.5, 0.0, 0.15384615384615383, 0.0, 1.0, 0.923076923076923, 0.3636363636363636, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.39999999999999997, 0.375, 1.0, 1.0, 0.6153846153846153, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.6666666666666666, 0.9600000000000001, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-6687", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-9772", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-2425", "mrqa_triviaqa-validation-7592", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1987", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2892"], "SR": 0.546875, "CSR": 0.5111607142857143, "EFR": 0.9310344827586207, "Overall": 0.704845289408867}, {"timecode": 98, "before_eval_results": {"predictions": ["beads", "Deimos", "Lana Turner", "a Polaroid picture", "manhattan", "Eris", "the owl", "Dr. Quinn", "fat", "poison ivy", "Miguel Montero", "a bicycle", "The Mount", "the Liberian", "rockabilly", "the Royal Court", "AARP", "Arturo Toscanini", "Bangladesh", "Saturn", "Nancy Pelosi", "a song written by Rob Parissi and recorded by the band Wild Cherry", "Dr. Pepper", "misery", "a anglerfish", "coal mining", "Iowa", "kidnapping", "Pope John Paul II", "a photocopier", "Syria", "Margaretta D'Arcy", "a plies", "the Bean Sidhe", "Japan", "Zephyr", "a ballistic missile submarine", "Ambrose Bierce", "(Walt) Whitman", "the frequency", "Macbeth", "the Colorado River", "a Vice President", "Tommy Franks", "Botswana", "Mousehunt", "the Dow Jones", "(William) Marrion Branham", "Vietnam", "a tuba", "a Croque Madam", "Kyla Pratt", "Wisconsin", "June 11, 2002", "top cat", "horripilation", "Adrian Cronauer", "1 August 1971", "Australian", "Dorothy Tangney", "Jonas", "Madhav Kumar Nepal", "his father's", "About 200"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6566220238095238}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-8500", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-8280", "mrqa_searchqa-validation-3023", "mrqa_searchqa-validation-1733", "mrqa_searchqa-validation-15155", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-14445", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-10204", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13824", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-1625", "mrqa_searchqa-validation-2884", "mrqa_naturalquestions-validation-937", "mrqa_triviaqa-validation-312", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-1854"], "SR": 0.609375, "CSR": 0.5121527777777778, "EFR": 1.0, "Overall": 0.7188368055555555}, {"timecode": 99, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1078", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3329", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3905", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8120", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-70", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12568", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-2884", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4848", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.83984375, "KG": 0.51875, "before_eval_results": {"predictions": ["Pensacola", "guppy!", "Minnesota", "Rosetta stone", "Japanese", "biscets", "Lord Bill Astor", "peripheral", "(Henry) Wadsworth", "Canton", "Hormel Foods", "syllable", "Theodore", "Huguenot", "Roger Williams", "Bohr", "the sun", "a Norwegian sea captain", "Abominable", "Surf's Up", "Scorpio", "a cat", "Finding Nemo", "the International Space Station", "Shakira", "Candice", "a shark", "Ireland", "George J. Mitchell", "(Henry) Wadsworth", "Gauguin", "James I", "bamboo", "cereal", "Crete", "Frank Sinatra", "(William) Custer", "barney stinson", "March 18", "Marlee Matlin", "Ben- Hur: A Tale of the Christ", "(Cash) Nomo", "Dan Rather", "KLM", "food combining", "a tutor", "elephants", "Arkansas", "a Bank of America", "piccolo", "a tuba", "Jason Marsden", "1998", "Garfield Sobers", "senegal", "Jimmy Carter", "blue", "Detroit, Michigan", "ethno-nationalist", "ARY", "Sri Lanka,", "Omar Bongo,", "commission, led by former U.S. Attorney Patrick Collins,", "the Islamic prophet Muhammad"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6750744047619047}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-4116", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-3515", "mrqa_searchqa-validation-3417", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-9926", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-81", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-3988", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-1713", "mrqa_searchqa-validation-2630", "mrqa_searchqa-validation-7727", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-3439", "mrqa_hotpotqa-validation-4325", "mrqa_hotpotqa-validation-4869", "mrqa_naturalquestions-validation-6637"], "SR": 0.578125, "CSR": 0.5128125, "retrieved_ids": ["mrqa_squad-train-17071", "mrqa_squad-train-61282", "mrqa_squad-train-24110", "mrqa_squad-train-64936", "mrqa_squad-train-48576", "mrqa_squad-train-52980", "mrqa_squad-train-15515", "mrqa_squad-train-55928", "mrqa_squad-train-82927", "mrqa_squad-train-61455", "mrqa_squad-train-9554", "mrqa_squad-train-10263", "mrqa_squad-train-29768", "mrqa_squad-train-52649", "mrqa_squad-train-69122", "mrqa_squad-train-42240", "mrqa_naturalquestions-validation-9079", "mrqa_triviaqa-validation-3079", "mrqa_newsqa-validation-3556", "mrqa_searchqa-validation-792", "mrqa_newsqa-validation-2476", "mrqa_naturalquestions-validation-4919", "mrqa_searchqa-validation-8477", "mrqa_triviaqa-validation-6507", "mrqa_triviaqa-validation-6260", "mrqa_searchqa-validation-11570", "mrqa_hotpotqa-validation-4211", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1916", "mrqa_triviaqa-validation-4931", "mrqa_squad-validation-6494"], "EFR": 1.0, "Overall": 0.718421875}]}