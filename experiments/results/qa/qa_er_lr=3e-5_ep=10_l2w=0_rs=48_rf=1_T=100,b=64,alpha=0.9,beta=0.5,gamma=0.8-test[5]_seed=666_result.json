{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=48_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=666', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=48_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=666/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=1, replay_size=48, save_ckpt_freq=25, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=8, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=48_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=666_result.json', stream_id=5, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 9840, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a plug valve", "1550", "French Louisiana west of the Mississippi River", "2012", "carbon dioxide", "the Lisbon Treaty", "all colors", "in the chloroplasts of C4 plants", "An attorney", "democracy", "The Greens", "third", "Enthusiastic teachers", "expositions", "no French regular army troops were stationed in North America", "estates of the Holy Roman Empire", "Stromatoveris", "2011", "Louis Pasteur", "the owner", "Time Lord", "mosaic floors", "economic", "1893", "environmental factors like light color and intensity", "Gandhi", "deforestation", "Middle Rhine Valley", "pump this into the mesoglea", "low-light conditions", "No Child Left Behind", "one way streets", "\u20ac25,000 per year", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "unbalanced torque", "Ulaanbaatar", "power", "very weak", "Judith Merril", "Gender pay gap", "the Ilkhanate", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "University of Chicago campus", "3D printing technology", "1957", "2000", "a certain number of teacher's salaries are paid by the State", "the Dutch Republic", "San Jose Marriott", "April 20", "the Commission", "evacuate the cylinder", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "Hurricane Beryl", "temperature and light", "terra nullius", "growth", "human", "the \u2018combs\u2019", "1978", "non-Catholics", "Sanders", "vice president", "700 employees"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8576388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10143", "mrqa_squad-validation-8841", "mrqa_squad-validation-2145", "mrqa_squad-validation-739", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-7449", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-9764", "mrqa_squad-validation-7051"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["1986", "Peridinin", "standardized", "50% to 60%", "Stromatoveris", "lower incomes", "Fort Duquesne", "Katharina von Bora", "Miller", "women", "The Love Boat", "Frank Marx", "architect or engineer", "$2 million", "superintendent of New York City schools", "Santa Clara, California", "Kingdom of Prussia", "the same league as the Asian Economic Tigers", "Lebanon and Hamas in Palestine", "Aristotle and Archimedes", "in the chloroplasts of C4 plants", "Outlaws", "increased blood flow into tissue", "Edgar Scherick", "14th to the 19th century", "Gibraltar and the \u00c5land islands", "the Evangelical Lutheran Church", "oxygen", "the BBC National Orchestra of Wales", "Thanksgiving", "the founding of new Protestant churches", "it is impossible to determine what the acceleration of the rope will be", "Venus", "those who proceed to secondary school or vocational training", "zoning and building code requirements", "the Ikh Zasag", "Central Bridge", "the Holy Roman Empire, the Duchy of Prussia, the Channel Islands, and Ireland", "King James Bible", "1935", "seven", "Grumman", "1191", "Maciot de Bethencourt", "Euclid", "case law by the Court of Justice", "long, slender tentacles", "mesoglea", "1970s", "white", "misguided", "2014", "Reconstruction and the Gilded Age", "European Parliament and the Council of the European Union", "members in good standing with the college", "Manakin Episcopal Church", "John Michael Rysbrack", "due to ongoing tectonic subsidence", "the release of her eponymous debut album the following year", "a form of business network", "the House of Representatives moved early into their House wing in 1807", "The euro is the result of the European Union's project for economic and monetary union", "Djokovic went on to win his fifth Australian Open title", "the \"colorful\" mercenary group fought for Padua, Florence & other Italian city-states"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7794263028638029}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.15384615384615385, 0.18181818181818182, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7332", "mrqa_squad-validation-6031", "mrqa_squad-validation-8459", "mrqa_squad-validation-9594", "mrqa_squad-validation-10339", "mrqa_squad-validation-10321", "mrqa_squad-validation-3021", "mrqa_squad-validation-2328", "mrqa_squad-validation-3946", "mrqa_squad-validation-1906", "mrqa_squad-validation-5588", "mrqa_squad-validation-9166", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-7062", "mrqa_searchqa-validation-2579"], "SR": 0.734375, "CSR": 0.78125, "retrieved_ids": ["mrqa_squad-train-80836", "mrqa_squad-train-9169", "mrqa_squad-train-78742", "mrqa_squad-train-28716", "mrqa_squad-train-2009", "mrqa_squad-train-45746", "mrqa_squad-train-74947", "mrqa_squad-train-52575", "mrqa_squad-train-48798", "mrqa_squad-train-54078", "mrqa_squad-train-41273", "mrqa_squad-train-45929", "mrqa_squad-train-31923", "mrqa_squad-train-18349", "mrqa_squad-train-41171", "mrqa_squad-train-43346", "mrqa_squad-train-54835", "mrqa_squad-train-38271", "mrqa_squad-train-21281", "mrqa_squad-train-12861", "mrqa_squad-train-53659", "mrqa_squad-train-26302", "mrqa_squad-train-19583", "mrqa_squad-train-86351", "mrqa_squad-validation-4452", "mrqa_squad-validation-8841", "mrqa_squad-validation-739", "mrqa_squad-validation-7449", "mrqa_squad-validation-9764", "mrqa_squad-validation-10143", "mrqa_squad-validation-2145", "mrqa_squad-validation-7051", "mrqa_squad-validation-3019", "mrqa_squad-validation-7364", "mrqa_squad-validation-9173"], "EFR": 1.0, "Overall": 0.890625}, {"timecode": 2, "before_eval_results": {"predictions": ["Lek", "prohibited emigration", "The Private Education Student Financial Assistance", "higher paid", "Labor", "time and storage", "special efforts", "rhetoric", "the British", "a year", "Genghis Khan", "a supervisory church body", "35", "a cubic interpolation formula", "King Sigismund III Vasa", "1835", "the exploitation of the valuable assets and supplies of the nation that was conquered", "poor management", "five", "liquid oxygen", "Gosforth Park", "Metropolitan Police Authority", "18 February 1546", "1996", "1.7 billion years ago", "Mike Carey", "coal", "31 October", "Stanford University", "1976", "LOVE Radio", "ambiguity", "Khasar", "Sky Digital", "99.4", "about a third", "the issue of laity having a voice and vote", "1995", "Endosymbiotic gene transfer", "avionics, telecommunications, and computers", "linebacker", "feed water", "Sir Edward Poynter", "oxygen", "August 1967", "Velamen parallelum", "human rights abuses and war crimes", "three", "Lowry Digital", "worst-case time complexity", "2010", "33", "Buffalo Lookout", "the term originated in Missouri, during the Kirtland period of Latter Day Saint history, circa 1834", "The User State Migration Tool", "1773", "Cadmium", "October 6, 2017", "11 p.m. to 3 a.m", "Haliaeetus", "Sir Henry Bartle Frere", "James Zeebo", "through the weekend", "Ewan McGregor"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8379493464052288}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7357", "mrqa_squad-validation-1672", "mrqa_squad-validation-335", "mrqa_squad-validation-10217", "mrqa_squad-validation-2538", "mrqa_squad-validation-6171", "mrqa_squad-validation-9876", "mrqa_squad-validation-9717", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-6211", "mrqa_newsqa-validation-174"], "SR": 0.8125, "CSR": 0.7916666666666666, "retrieved_ids": ["mrqa_squad-train-1439", "mrqa_squad-train-81118", "mrqa_squad-train-83905", "mrqa_squad-train-84506", "mrqa_squad-train-1986", "mrqa_squad-train-25082", "mrqa_squad-train-9003", "mrqa_squad-train-18617", "mrqa_squad-train-52200", "mrqa_squad-train-57093", "mrqa_squad-train-4282", "mrqa_squad-train-49165", "mrqa_squad-train-65144", "mrqa_squad-train-34966", "mrqa_squad-train-13924", "mrqa_squad-train-76985", "mrqa_squad-train-1090", "mrqa_squad-train-37267", "mrqa_squad-train-56193", "mrqa_squad-train-10928", "mrqa_squad-train-56846", "mrqa_squad-train-80607", "mrqa_squad-train-79225", "mrqa_squad-train-71367", "mrqa_naturalquestions-validation-1187", "mrqa_squad-validation-9173", "mrqa_squad-validation-3019", "mrqa_squad-validation-4452", "mrqa_squad-validation-10321", "mrqa_squad-validation-3946", "mrqa_squad-validation-5588", "mrqa_squad-validation-739", "mrqa_squad-validation-7449", "mrqa_squad-validation-9166", "mrqa_squad-validation-9764", "mrqa_squad-validation-8841", "mrqa_searchqa-validation-2579", "mrqa_squad-validation-10339", "mrqa_squad-validation-7051", "mrqa_squad-validation-6031", "mrqa_squad-validation-2145", "mrqa_squad-validation-1906", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-7062", "mrqa_squad-validation-7332", "mrqa_squad-validation-3021", "mrqa_squad-validation-2328", "mrqa_squad-validation-8459"], "EFR": 1.0, "Overall": 0.8958333333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["female", "1884", "Sayyid Abul Ala Maududi", "anyone with knowledge or skills in the wider community setting", "James E. Webb", "the Lutheran and Reformed states in Germany and Scandinavia", "the phycoerytherin", "a losing proposition", "swimming-plates", "10 July 1856", "130 million cubic foot", "teleforce", "Heinrich Himmler", "34\u201319", "Baptism", "Decision problems", "without markings", "1957", "The Day of the Doctor", "Muhammad Khan", "Sun Life Stadium", "the Council", "February 9, 1953", "biennial", "sea gooseberry", "1961", "Trio Tribe", "Dai Setsen", "the Late Medieval Catholic Church", "January 1979", "phagocytic", "Rankine cycle", "$2.2 billion", "Seine", "Newton's Law of Gravitation", "15 February 1546", "Marquis de la Jonqui\u00e8re", "BBC Dead Ringers", "Kenyans for Kenya", "Fresno", "Israel", "the Presiding Officer", "an intuitive understanding", "default emission factors", "Inherited wealth", "Michael P. Millardi", "Goldman Sachs", "Microfluidics", "aproveitando espaos, cama suspensa, armario", "Great faces, great places", "a young male who assisted the priest at a Mass", "the children were nestled all snug in their beds", "the Great Temple at Abu Simbel", "the Leyden jar", "a list of the subjects that candidates", "the last two of these had libretti by Gaetano Rossi, whom Meyerbeer", "the post office said his \"Droll Stories\", part of \"La Comedie Humaine\", was too alarming to mail", "70%", "the First Macy's Thanksgiving Day Parade", "making* this First Annual Clearing Sale one that will long be.... Bed Spreads, very large size, fine", "the British", "early 1960s", "April 1917", "poor hygiene"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6838575487012987}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.4, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-1863", "mrqa_squad-validation-3270", "mrqa_squad-validation-8595", "mrqa_squad-validation-7525", "mrqa_squad-validation-2595", "mrqa_squad-validation-10369", "mrqa_squad-validation-3863", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-10156"], "SR": 0.640625, "CSR": 0.75390625, "retrieved_ids": ["mrqa_squad-train-77942", "mrqa_squad-train-71199", "mrqa_squad-train-887", "mrqa_squad-train-45782", "mrqa_squad-train-3814", "mrqa_squad-train-62113", "mrqa_squad-train-68672", "mrqa_squad-train-55243", "mrqa_squad-train-50683", "mrqa_squad-train-79147", "mrqa_squad-train-10528", "mrqa_squad-train-51700", "mrqa_squad-train-15073", "mrqa_squad-train-57385", "mrqa_squad-train-55496", "mrqa_squad-train-6129", "mrqa_squad-train-53113", "mrqa_squad-train-55269", "mrqa_squad-train-11842", "mrqa_squad-train-21772", "mrqa_squad-train-9658", "mrqa_squad-train-11730", "mrqa_squad-train-59055", "mrqa_squad-train-33555", "mrqa_naturalquestions-validation-191", "mrqa_squad-validation-6031", "mrqa_naturalquestions-validation-6211", "mrqa_squad-validation-10217", "mrqa_squad-validation-9764", "mrqa_squad-validation-7449", "mrqa_squad-validation-3021", "mrqa_squad-validation-7364", "mrqa_squad-validation-8459", "mrqa_squad-validation-10321", "mrqa_squad-validation-3019", "mrqa_squad-validation-8841", "mrqa_squad-validation-3946", "mrqa_squad-validation-1672", "mrqa_squad-validation-9717", "mrqa_squad-validation-6171", "mrqa_squad-validation-2145", "mrqa_squad-validation-10339", "mrqa_squad-validation-9173", "mrqa_newsqa-validation-174", "mrqa_squad-validation-739", "mrqa_squad-validation-7332", "mrqa_squad-validation-5588", "mrqa_squad-validation-9594"], "EFR": 1.0, "Overall": 0.876953125}, {"timecode": 4, "before_eval_results": {"predictions": ["boom-and-bust cycles", "The Prince of P\u0142ock", "hormones", "1840", "occupational stress", "in the parts of the internal canal network under the comb rows", "in no way", "Tesla Electric Company", "African-American", "Thomson", "1905", "Nun komm, der Heiden Heiland", "John Fox", "all health care settings", "cut in half", "the study of rocks", "colonies", "lower wages", "geophysical", "Huguenots", "social power and wealth", "2,900 kilometres", "Elie Metchnikoff", "an algorithm", "Immediately after Decision Time", "Confucian propriety and ancestor veneration", "25-minute", "eight", "elude host immune responses", "Pusey Library", "inequality", "designs", "cytokines", "requiring his arrest", "wide sidewalks", "other members", "Air Force", "an occupancy permit", "reactive allotrope of oxygen", "Nederrijn", "multi-cultural", "pump water out of the mesoglea", "Zeebo", "Australia", "a judicial officer", "mathematical model", "Henry Purcell", "Ram Nath Kovind", "Transvaginal ultrasonography", "Cheap trick", "Sandy Knox and Billy Stritch", "Hudson Bay", "Bart Cummings", "bow bridge", "1991", "Nicole Gale Anderson", "1", "sedimentary", "Mrs. Wolowitz", "plate tectonics", "Colombia", "Yolande of Brienne", "Conway", "UNESCO"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7369791666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7338", "mrqa_squad-validation-2943", "mrqa_squad-validation-8093", "mrqa_squad-validation-6957", "mrqa_squad-validation-3497", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-4002", "mrqa_triviaqa-validation-5855", "mrqa_hotpotqa-validation-4815", "mrqa_newsqa-validation-2042", "mrqa_searchqa-validation-172"], "SR": 0.703125, "CSR": 0.74375, "retrieved_ids": ["mrqa_squad-train-25791", "mrqa_squad-train-21826", "mrqa_squad-train-44587", "mrqa_squad-train-21967", "mrqa_squad-train-85581", "mrqa_squad-train-20742", "mrqa_squad-train-26620", "mrqa_squad-train-82997", "mrqa_squad-train-28116", "mrqa_squad-train-26008", "mrqa_squad-train-82499", "mrqa_squad-train-47364", "mrqa_squad-train-35758", "mrqa_squad-train-78749", "mrqa_squad-train-63879", "mrqa_squad-train-80644", "mrqa_squad-train-73483", "mrqa_squad-train-41302", "mrqa_squad-train-30763", "mrqa_squad-train-32798", "mrqa_squad-train-85250", "mrqa_squad-train-9764", "mrqa_squad-train-57816", "mrqa_squad-train-66178", "mrqa_squad-validation-335", "mrqa_searchqa-validation-2568", "mrqa_squad-validation-6031", "mrqa_squad-validation-1672", "mrqa_squad-validation-3946", "mrqa_squad-validation-1906", "mrqa_squad-validation-739", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-9764", "mrqa_squad-validation-10369", "mrqa_squad-validation-7332", "mrqa_squad-validation-10217", "mrqa_searchqa-validation-3451", "mrqa_squad-validation-3270", "mrqa_squad-validation-9876", "mrqa_searchqa-validation-13232", "mrqa_squad-validation-2538", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-8841", "mrqa_squad-validation-7525"], "EFR": 1.0, "Overall": 0.871875}, {"timecode": 5, "before_eval_results": {"predictions": ["former flooded terraces", "20th century", "1974", "ABC", "dictatorial", "Ben Johnston", "quantum", "Book of Exodus", "Synthetic aperture radar", "Mission Impossible", "patients' prescriptions and patient safety issues", "No, that's no good", "1697", "3 January 1521", "magma", "a \"principal hostile country\"", "Jan Hus", "Newton", "Croatia", "2011", "Swynnerton Plan", "machine gun", "Theatre Museum", "August 10, 1948", "they are distinct or equal classes", "the 2004 Treaty establishing a Constitution for Europe", "Serge Chermayeff", "Thomas Edison", "Mnemiopsis", "the flail of God", "Woodward Park", "Melbourne Cricket Ground", "Wednesdays", "most common", "concentration gradient", "tears and urine", "six", "plants and algae", "Republic Day", "1913", "Yuzuru Hanyu", "Konakuppakatil Gopinathan Balakrishnan", "1942", "March 2016", "Texas, Oklahoma, and the surrounding Great Plains", "a balance sheet", "Mayor Hudnut", "1995", "William the Conqueror", "1922", "an anembryonic gestation", "Bemis Heights", "9pm ET ( UTC - 5 )", "twice", "Joe Pizzulo and Leeza Miller", "Lituya Bay in Alaska", "Sarah", "routing protocols", "The euro", "Ultraviolet Ultraviolet", "2000", "KCNA", "a papermaker headquartered on Peachtree Street", "Sierra Repertory Theatre"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6799288799968148}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3076923076923077, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5818", "mrqa_squad-validation-1827", "mrqa_squad-validation-1566", "mrqa_squad-validation-10388", "mrqa_squad-validation-3770", "mrqa_squad-validation-1780", "mrqa_squad-validation-3985", "mrqa_squad-validation-4572", "mrqa_squad-validation-8904", "mrqa_squad-validation-6439", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-2749", "mrqa_newsqa-validation-2404", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15169"], "SR": 0.609375, "CSR": 0.7213541666666667, "retrieved_ids": ["mrqa_squad-train-59538", "mrqa_squad-train-77394", "mrqa_squad-train-80635", "mrqa_squad-train-22381", "mrqa_squad-train-25702", "mrqa_squad-train-57607", "mrqa_squad-train-27708", "mrqa_squad-train-39205", "mrqa_squad-train-65108", "mrqa_squad-train-75753", "mrqa_squad-train-46388", "mrqa_squad-train-72970", "mrqa_squad-train-32583", "mrqa_squad-train-11997", "mrqa_squad-train-25460", "mrqa_squad-train-61867", "mrqa_squad-train-65441", "mrqa_squad-train-54332", "mrqa_squad-train-44193", "mrqa_squad-train-48595", "mrqa_squad-train-84168", "mrqa_squad-train-57263", "mrqa_squad-train-53353", "mrqa_squad-train-26977", "mrqa_searchqa-validation-15194", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_squad-validation-7357", "mrqa_squad-validation-6031", "mrqa_squad-validation-9173", "mrqa_naturalquestions-validation-8116", "mrqa_squad-validation-1863", "mrqa_naturalquestions-validation-2476", "mrqa_searchqa-validation-14194", "mrqa_squad-validation-6957", "mrqa_searchqa-validation-2579", "mrqa_squad-validation-9166", "mrqa_searchqa-validation-13232", "mrqa_squad-validation-6171", "mrqa_searchqa-validation-5631", "mrqa_naturalquestions-validation-4002", "mrqa_squad-validation-4452", "mrqa_squad-validation-739", "mrqa_newsqa-validation-174", "mrqa_squad-validation-10339", "mrqa_searchqa-validation-4367", "mrqa_triviaqa-validation-5855", "mrqa_squad-validation-8093"], "EFR": 0.96, "Overall": 0.8406770833333334}, {"timecode": 6, "before_eval_results": {"predictions": ["four", "2 million", "Gender pay gap", "Pliocene", "relationship between teachers and children", "LeGrande", "After the sixth sermon", "Jason Bourne", "7.8%", "10,000", "University of Chicago College Bowl Team", "organized labor", "Santa Clara Marriott", "oxygen chambers", "two", "two catechisms", "Cologne", "1991", "Silk Road", "Surveyor 3 unmanned lunar probe", "145", "growth and investment", "the centers were computer service bureaus, offering batch processing services.", "Vampire bats", "antiforms", "still be standing", "weight", "Mongolians to refer to their country as \"Genghis Khan's Mongolia", "oil was priced in dollars, oil producers' real income decreased", "Beyonc\u00e9 and Bruno Mars", "a university or college", "More than 1 million", "pseudorandom number generators", "Japan", "Coriolis effect", "Mickey Rourke", "May 2016", "Nicklaus", "Superstition Mountains", "Panama Canal Authority", "silk, hair / fur", "The King", "two", "Sebastian Vettel", "April 10, 2018", "Gorakhpur", "How I Met Your Mother", "elected", "December 15, 2016", "Abraham Gottlob Werner", "Jourdan Miller", "1991", "Mandy '' Moore", "Denmark", "Broken Hill and Sydney", "159", "the United States", "Judiththia Aline Keppel", "medellin", "Crown Holdings", "Expedia", "Large Orbiting Telescope or Large Space Telescope", "Zed", "us to step up"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6947307288299935}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 0.07692307692307693, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.25, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.23529411764705882, 0.5714285714285715, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7445", "mrqa_squad-validation-6965", "mrqa_squad-validation-5435", "mrqa_squad-validation-7422", "mrqa_squad-validation-4000", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_squad-validation-6228", "mrqa_squad-validation-3718", "mrqa_squad-validation-9161", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-6106", "mrqa_hotpotqa-validation-1471", "mrqa_searchqa-validation-10372"], "SR": 0.59375, "CSR": 0.703125, "retrieved_ids": ["mrqa_squad-train-52292", "mrqa_squad-train-61458", "mrqa_squad-train-76410", "mrqa_squad-train-56078", "mrqa_squad-train-58492", "mrqa_squad-train-25117", "mrqa_squad-train-27074", "mrqa_squad-train-13265", "mrqa_squad-train-10044", "mrqa_squad-train-51240", "mrqa_squad-train-37171", "mrqa_squad-train-55442", "mrqa_squad-train-42858", "mrqa_squad-train-75664", "mrqa_squad-train-44791", "mrqa_squad-train-68134", "mrqa_squad-train-11344", "mrqa_squad-train-313", "mrqa_squad-train-29428", "mrqa_squad-train-33710", "mrqa_squad-train-72966", "mrqa_squad-train-16138", "mrqa_squad-train-8614", "mrqa_squad-train-50122", "mrqa_searchqa-validation-14194", "mrqa_squad-validation-1566", "mrqa_squad-validation-2595", "mrqa_searchqa-validation-2579", "mrqa_squad-validation-3019", "mrqa_naturalquestions-validation-6461", "mrqa_searchqa-validation-15169", "mrqa_squad-validation-9166", "mrqa_squad-validation-7525", "mrqa_squad-validation-1827", "mrqa_newsqa-validation-174", "mrqa_squad-validation-2943", "mrqa_naturalquestions-validation-9240", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-2476", "mrqa_squad-validation-1906", "mrqa_naturalquestions-validation-3672", "mrqa_squad-validation-9173", "mrqa_triviaqa-validation-5855", "mrqa_searchqa-validation-9536", "mrqa_naturalquestions-validation-7080", "mrqa_searchqa-validation-14371", "mrqa_naturalquestions-validation-1911", "mrqa_squad-validation-3985"], "EFR": 0.9615384615384616, "Overall": 0.8323317307692308}, {"timecode": 7, "before_eval_results": {"predictions": ["Director", "travel literature, cartography, geography, and scientific education", "oxygen chambers", "McManus", "Two", "1066", "2008", "Mojave Desert", "Operating System Principles", "St. Lawrence and Mississippi watersheds", "27", "4000", "Rhine Gorge", "stromal thylakoids", "highest", "impact process effects", "Asian, African and Caribbean", "Warner Bros. Presents", "pharmacists", "high-voltage", "4:51", "Kabaty Forest", "the seal of the Federal Communications Commission", "strong sedimentation", "European Commission", "SAP Center", "respiration", "352", "eliminate the accusing law", "October 6, 2004", "\"The Day of the Doctor\"", "Pakistan", "November 1999", "September 6, 2019", "Mandarin", "During the fourth season", "four", "Nick Kroll", "the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Billy Gibbons", "an apprentice of the fictional Wars Order", "the brain", "31", "1970s", "the U.S. State Department", "Art Carney", "accomplish the objectives of the organization", "the female spends extra time basking to keep her eggs warm", "December 1922", "Hurricane Charley", "September 2017", "3 September", "southern USA", "Terrell Owens", "March 14", "five points", "Dolph Lundgren", "Hampton Court Palace", "Sela Ann Ward", "Alice Horton", "the ARRL Letter for January 5  Jan 5, 2012", "Benjamin Britten", "isosceles", "NOW Magazine"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6778521825396826}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.9714285714285714, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-762", "mrqa_squad-validation-4629", "mrqa_squad-validation-8819", "mrqa_squad-validation-1938", "mrqa_squad-validation-6409", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16130", "mrqa_triviaqa-validation-6548"], "SR": 0.609375, "CSR": 0.69140625, "retrieved_ids": ["mrqa_squad-train-9778", "mrqa_squad-train-61905", "mrqa_squad-train-49266", "mrqa_squad-train-440", "mrqa_squad-train-69084", "mrqa_squad-train-26238", "mrqa_squad-train-86200", "mrqa_squad-train-37346", "mrqa_squad-train-49152", "mrqa_squad-train-8950", "mrqa_squad-train-66789", "mrqa_squad-train-11246", "mrqa_squad-train-7200", "mrqa_squad-train-315", "mrqa_squad-train-13320", "mrqa_squad-train-79020", "mrqa_squad-train-71077", "mrqa_squad-train-54430", "mrqa_squad-train-42753", "mrqa_squad-train-9757", "mrqa_squad-train-73315", "mrqa_squad-train-3288", "mrqa_squad-train-9895", "mrqa_squad-train-28745", "mrqa_naturalquestions-validation-9737", "mrqa_hotpotqa-validation-4815", "mrqa_naturalquestions-validation-2102", "mrqa_squad-validation-7332", "mrqa_naturalquestions-validation-6211", "mrqa_squad-validation-6957", "mrqa_triviaqa-validation-5855", "mrqa_squad-validation-10388", "mrqa_squad-validation-739", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-2476", "mrqa_squad-validation-7525", "mrqa_searchqa-validation-10372", "mrqa_squad-validation-7364", "mrqa_naturalquestions-validation-3558", "mrqa_squad-validation-8093", "mrqa_squad-validation-1780", "mrqa_squad-validation-4452", "mrqa_squad-validation-3998", "mrqa_squad-validation-3497", "mrqa_squad-validation-9717", "mrqa_hotpotqa-validation-1471", "mrqa_squad-validation-1863"], "EFR": 1.0, "Overall": 0.845703125}, {"timecode": 8, "before_eval_results": {"predictions": ["cytokines", "William Pitt", "North Carolina and New Mexico", "p-adic norm", "Hassan al Banna", "Gottfried Fritschel", "Eighth Doctor", "ular plastoglobulus", "pound-force", "Ming", "Dorothy and Michael Hintze", "The Small Catechism", "36", "Giovanni Battista Foggini", "April 20", "biomass", "a violation of criminal law that does not infringe the rights of others", "K MJ-TV", "Foreign Protestants Naturalization Act", "southern and central parts", "10%", "not", "Metro Trains Melbourne", "BBC 1", "$2 million", "Vince Lombardi Trophy", "Galileo", "in linked groups or chains", "over", "The Tiber", "1885", "James Madison", "Ryan Pinkston", "federal republic", "chyle", "2007", "foreign investors", "Comancheria", "8ft", "Bartolomeu Dias", "William Wyler", "1961", "March 1930", "Julie Adams", "Thomas Jefferson", "February 2017", "January 1, 2016", "United States customary units", "Millerlite", "Sunday night", "Billy Hill", "Mara", "Malina Weissman", "September 6, 2019", "1773", "A lacteal", "April 26, 2005", "Jennifer Eccles", "Albert", "a coma", "Croatia", "Drew Kesse", "teen", "a group of 20 similar cars making an annual road trip"], "metric_results": {"EM": 0.625, "QA-F1": 0.6723958333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-8958", "mrqa_squad-validation-7876", "mrqa_squad-validation-8786", "mrqa_squad-validation-5724", "mrqa_squad-validation-6706", "mrqa_squad-validation-4715", "mrqa_squad-validation-2975", "mrqa_squad-validation-8769", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-8339", "mrqa_hotpotqa-validation-2800", "mrqa_newsqa-validation-3043", "mrqa_searchqa-validation-2141", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-3476"], "SR": 0.625, "CSR": 0.6840277777777778, "retrieved_ids": ["mrqa_squad-train-67972", "mrqa_squad-train-52989", "mrqa_squad-train-26239", "mrqa_squad-train-25971", "mrqa_squad-train-63084", "mrqa_squad-train-56678", "mrqa_squad-train-33235", "mrqa_squad-train-16111", "mrqa_squad-train-33648", "mrqa_squad-train-63723", "mrqa_squad-train-42485", "mrqa_squad-train-48344", "mrqa_squad-train-42514", "mrqa_squad-train-41970", "mrqa_squad-train-38396", "mrqa_squad-train-54260", "mrqa_squad-train-16524", "mrqa_squad-train-16053", "mrqa_squad-train-10101", "mrqa_squad-train-39251", "mrqa_squad-train-64491", "mrqa_squad-train-6113", "mrqa_squad-train-13341", "mrqa_squad-train-6796", "mrqa_squad-validation-3863", "mrqa_searchqa-validation-13232", "mrqa_squad-validation-1672", "mrqa_newsqa-validation-2112", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-6998", "mrqa_squad-validation-10369", "mrqa_naturalquestions-validation-3672", "mrqa_newsqa-validation-2404", "mrqa_squad-validation-1566", "mrqa_naturalquestions-validation-4002", "mrqa_newsqa-validation-174", "mrqa_naturalquestions-validation-6461", "mrqa_squad-validation-10217", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-10460", "mrqa_searchqa-validation-8711", "mrqa_squad-validation-3270", "mrqa_squad-validation-6965", "mrqa_naturalquestions-validation-6524", "mrqa_squad-validation-1938"], "EFR": 1.0, "Overall": 0.8420138888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["to emphasize academics over athletics", "3,600", "nine", "individual states and territories", "30%\u201350%", "one of his wife's ladies-in-waiting", "liquid phase", "their greatest common divisor is one", "Europe", "the cell membrane", "The Master is the Doctor's archenemy", "Laverne & Shirley", "carbohydrates", "his butchery", "Jean Ribault", "March 2011", "Continental Edison Company in France", "2010", "more equality in the income distribution", "X is no more difficult than Y", "38", "1887", "1469", "a \"world classic of epoch-making oratory.\"", "up to half", "one octave lower than the four lower strings", "WD-40", "Gretchen Wilson", "Georgie Porgie", "Vodka", "William Shaksper", "The Fray", "Venus", "Helen Hayes", "Canberra", "the Awl", "Alexander Graham Bell", "Anna Pavlova", "a person who computes premium rates, dividends, risks, etc.", "Al Campanis", "Beanie and Cecil", "the Chicago Cubs", "an urban street-tough school", "the goat", "Stephen Collins", "the INTJ \"Mastermind\" Personality Type", "Linda Wagner", "the White Nile", "Beverly and Elliot Mantle", "the chimney in an attempt to get a longer burn time also contributes to creosote buildup", "Andrew Jackson", "Madonna", "the painters fauves", "a sailfish", "a bead Barrette", "Laurie", "between 1859 and 1869", "James Hutton", "Nita", "Stuart Neame", "Total Nonstop Action Wrestling", "a four-piece band from Portland, Oregon", "the Bronx", "Antonio Maria Costa"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6269243777056277}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7700", "mrqa_squad-validation-1240", "mrqa_squad-validation-1748", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-7463", "mrqa_triviaqa-validation-6421", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-2179"], "SR": 0.578125, "CSR": 0.6734375, "retrieved_ids": ["mrqa_squad-train-56793", "mrqa_squad-train-80176", "mrqa_squad-train-54770", "mrqa_squad-train-7615", "mrqa_squad-train-33844", "mrqa_squad-train-71558", "mrqa_squad-train-59221", "mrqa_squad-train-40390", "mrqa_squad-train-51557", "mrqa_squad-train-33051", "mrqa_squad-train-7581", "mrqa_squad-train-37925", "mrqa_squad-train-81619", "mrqa_squad-train-81716", "mrqa_squad-train-33535", "mrqa_squad-train-74347", "mrqa_squad-train-32371", "mrqa_squad-train-35604", "mrqa_squad-train-71096", "mrqa_squad-train-42411", "mrqa_squad-train-64030", "mrqa_squad-train-72689", "mrqa_squad-train-45227", "mrqa_squad-train-34915", "mrqa_squad-validation-9176", "mrqa_squad-validation-5435", "mrqa_naturalquestions-validation-2476", "mrqa_newsqa-validation-3043", "mrqa_squad-validation-8958", "mrqa_naturalquestions-validation-10156", "mrqa_squad-validation-9161", "mrqa_squad-validation-2328", "mrqa_naturalquestions-validation-9715", "mrqa_squad-validation-3718", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-7896", "mrqa_triviaqa-validation-5855", "mrqa_hotpotqa-validation-4815", "mrqa_naturalquestions-validation-3442", "mrqa_squad-validation-6031", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-844", "mrqa_squad-validation-7525", "mrqa_squad-validation-4838"], "EFR": 1.0, "Overall": 0.83671875}, {"timecode": 10, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8454", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-3245", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-5035", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-10000", "mrqa_squad-validation-10115", "mrqa_squad-validation-10136", "mrqa_squad-validation-1017", "mrqa_squad-validation-10181", "mrqa_squad-validation-10184", "mrqa_squad-validation-10217", "mrqa_squad-validation-10263", "mrqa_squad-validation-10281", "mrqa_squad-validation-10290", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-10361", "mrqa_squad-validation-10369", "mrqa_squad-validation-1038", "mrqa_squad-validation-10410", "mrqa_squad-validation-10454", "mrqa_squad-validation-10496", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1181", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1226", "mrqa_squad-validation-1240", "mrqa_squad-validation-1254", "mrqa_squad-validation-1269", "mrqa_squad-validation-1371", "mrqa_squad-validation-1499", "mrqa_squad-validation-1521", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1651", "mrqa_squad-validation-166", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1748", "mrqa_squad-validation-1780", "mrqa_squad-validation-1787", "mrqa_squad-validation-1848", "mrqa_squad-validation-1863", "mrqa_squad-validation-1892", "mrqa_squad-validation-1924", "mrqa_squad-validation-1938", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-1998", "mrqa_squad-validation-2019", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-2108", "mrqa_squad-validation-2145", "mrqa_squad-validation-2209", "mrqa_squad-validation-2233", "mrqa_squad-validation-2241", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-247", "mrqa_squad-validation-2521", "mrqa_squad-validation-2545", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2642", "mrqa_squad-validation-27", "mrqa_squad-validation-2751", "mrqa_squad-validation-2820", "mrqa_squad-validation-2885", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-3039", "mrqa_squad-validation-305", "mrqa_squad-validation-3076", "mrqa_squad-validation-3144", "mrqa_squad-validation-3164", "mrqa_squad-validation-317", "mrqa_squad-validation-3184", "mrqa_squad-validation-322", "mrqa_squad-validation-3230", "mrqa_squad-validation-3270", "mrqa_squad-validation-334", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3376", "mrqa_squad-validation-3380", "mrqa_squad-validation-3392", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3497", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3687", "mrqa_squad-validation-3703", "mrqa_squad-validation-3718", "mrqa_squad-validation-374", "mrqa_squad-validation-3769", "mrqa_squad-validation-3770", "mrqa_squad-validation-381", "mrqa_squad-validation-3824", "mrqa_squad-validation-3829", "mrqa_squad-validation-3842", "mrqa_squad-validation-3848", "mrqa_squad-validation-3852", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3917", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3985", "mrqa_squad-validation-3986", "mrqa_squad-validation-3998", "mrqa_squad-validation-4000", "mrqa_squad-validation-4009", "mrqa_squad-validation-402", "mrqa_squad-validation-4031", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4181", "mrqa_squad-validation-4187", "mrqa_squad-validation-4213", "mrqa_squad-validation-4291", "mrqa_squad-validation-4312", "mrqa_squad-validation-4348", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4452", "mrqa_squad-validation-4467", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-451", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4539", "mrqa_squad-validation-4557", "mrqa_squad-validation-4557", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4838", "mrqa_squad-validation-491", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5019", "mrqa_squad-validation-5064", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-516", "mrqa_squad-validation-5262", "mrqa_squad-validation-5396", "mrqa_squad-validation-5436", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5493", "mrqa_squad-validation-5527", "mrqa_squad-validation-5546", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5602", "mrqa_squad-validation-5631", "mrqa_squad-validation-5664", "mrqa_squad-validation-5677", "mrqa_squad-validation-57", "mrqa_squad-validation-5726", "mrqa_squad-validation-5750", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5806", "mrqa_squad-validation-5818", "mrqa_squad-validation-5852", "mrqa_squad-validation-5860", "mrqa_squad-validation-5865", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6031", "mrqa_squad-validation-6066", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6176", "mrqa_squad-validation-6206", "mrqa_squad-validation-6222", "mrqa_squad-validation-6229", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6319", "mrqa_squad-validation-6330", "mrqa_squad-validation-6347", "mrqa_squad-validation-6353", "mrqa_squad-validation-6355", "mrqa_squad-validation-6409", "mrqa_squad-validation-6439", "mrqa_squad-validation-6502", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6611", "mrqa_squad-validation-6649", "mrqa_squad-validation-6664", "mrqa_squad-validation-6694", "mrqa_squad-validation-6790", "mrqa_squad-validation-6815", "mrqa_squad-validation-6838", "mrqa_squad-validation-6875", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6898", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7036", "mrqa_squad-validation-7039", "mrqa_squad-validation-7064", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7228", "mrqa_squad-validation-7260", "mrqa_squad-validation-7261", "mrqa_squad-validation-7297", "mrqa_squad-validation-7332", "mrqa_squad-validation-7338", "mrqa_squad-validation-7357", "mrqa_squad-validation-7364", "mrqa_squad-validation-7368", "mrqa_squad-validation-7380", "mrqa_squad-validation-739", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7445", "mrqa_squad-validation-7457", "mrqa_squad-validation-7470", "mrqa_squad-validation-7492", "mrqa_squad-validation-7503", "mrqa_squad-validation-7525", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-762", "mrqa_squad-validation-7693", "mrqa_squad-validation-7700", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7775", "mrqa_squad-validation-7781", "mrqa_squad-validation-7785", "mrqa_squad-validation-779", "mrqa_squad-validation-7863", "mrqa_squad-validation-7871", "mrqa_squad-validation-7917", "mrqa_squad-validation-7943", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8016", "mrqa_squad-validation-8043", "mrqa_squad-validation-8093", "mrqa_squad-validation-8125", "mrqa_squad-validation-8154", "mrqa_squad-validation-8177", "mrqa_squad-validation-8184", "mrqa_squad-validation-8192", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8365", "mrqa_squad-validation-8414", "mrqa_squad-validation-8449", "mrqa_squad-validation-8459", "mrqa_squad-validation-8471", "mrqa_squad-validation-8484", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8568", "mrqa_squad-validation-8585", "mrqa_squad-validation-8661", "mrqa_squad-validation-8670", "mrqa_squad-validation-8670", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-8841", "mrqa_squad-validation-888", "mrqa_squad-validation-8904", "mrqa_squad-validation-8925", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8958", "mrqa_squad-validation-8985", "mrqa_squad-validation-908", "mrqa_squad-validation-9095", "mrqa_squad-validation-9161", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9403", "mrqa_squad-validation-9405", "mrqa_squad-validation-9446", "mrqa_squad-validation-9464", "mrqa_squad-validation-9556", "mrqa_squad-validation-957", "mrqa_squad-validation-9594", "mrqa_squad-validation-9615", "mrqa_squad-validation-9669", "mrqa_squad-validation-9716", "mrqa_squad-validation-9717", "mrqa_squad-validation-9764", "mrqa_squad-validation-9814", "mrqa_squad-validation-9816", "mrqa_squad-validation-9876", "mrqa_squad-validation-9907", "mrqa_squad-validation-9928", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7463"], "OKR": 0.951171875, "KG": 0.4421875, "before_eval_results": {"predictions": ["Northern Europe and the Mid-Atlantic", "$2 million", "causing fish stocks to collapse", "Chris Keates", "its many castles and vineyards", "Cinerama Productions/Palomar theatrical library", "Antigone", "3.5 million", "Denver Broncos", "1997", "A \u2192 G deamination", "since 2001", "A", "1767", "Narrow alleys", "another problem", "economic", "John and Benjamin Green", "1530", "installed electrical arc light based illumination systems", "two", "the poor", "Irish Hospitals' Sweepstakes", "Pearl Jam", "Grey's Anatomy", "(Prince) Paula Burch", "Bruce Springsteen", "Wounded Knee", "Maria Callas", "Henry Moore", "Red Sox", "Charlotte", "one", "Narcissus", "Fred Williamson", "the Orange River", "(William) James Needles", "the Holy Grail", "Mellon Collie and the Infinite Sadness", "Marsha Hunt", "Ludwig Van Beethoven", "Lake Victoria", "(Prince) tides", "Dr Pepper", "Fram", "Sarah Orne Jewett", "Guns N' Roses", "Francis Bacon", "polar", "You Bet Your Life", "China", "Nova Scotia", "Kenny G", "C. S. Lewis", "Franklin Pierce", "Pearl Harbor", "Michael Schumacher", "a four - page pamphlet", "snooker", "Glasgow", "Paul W. S. Anderson", "Hugh Dowding", "NATO fighters", "Congress"], "metric_results": {"EM": 0.578125, "QA-F1": 0.64140625}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8990", "mrqa_squad-validation-5887", "mrqa_squad-validation-6655", "mrqa_squad-validation-9959", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3530", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-11388", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-8760", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-12552", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-6909", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-6181", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6896"], "SR": 0.578125, "CSR": 0.6647727272727273, "retrieved_ids": ["mrqa_squad-train-78907", "mrqa_squad-train-27405", "mrqa_squad-train-31921", "mrqa_squad-train-12856", "mrqa_squad-train-32464", "mrqa_squad-train-51366", "mrqa_squad-train-20843", "mrqa_squad-train-70932", "mrqa_squad-train-79884", "mrqa_squad-train-70520", "mrqa_squad-train-42461", "mrqa_squad-train-37605", "mrqa_squad-train-11176", "mrqa_squad-train-53759", "mrqa_squad-train-59405", "mrqa_squad-train-66023", "mrqa_squad-train-29812", "mrqa_squad-train-53479", "mrqa_squad-train-49688", "mrqa_squad-train-69483", "mrqa_squad-train-1888", "mrqa_squad-train-32288", "mrqa_squad-train-26554", "mrqa_squad-train-14156", "mrqa_naturalquestions-validation-9737", "mrqa_squad-validation-10217", "mrqa_newsqa-validation-2112", "mrqa_squad-validation-5724", "mrqa_naturalquestions-validation-3332", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-3613", "mrqa_squad-validation-335", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-174", "mrqa_squad-validation-6957", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-4367", "mrqa_squad-validation-4000", "mrqa_squad-validation-2145", "mrqa_searchqa-validation-10372", "mrqa_squad-validation-4838", "mrqa_searchqa-validation-14480", "mrqa_squad-validation-6171", "mrqa_triviaqa-validation-5855", "mrqa_squad-validation-6439", "mrqa_squad-validation-6409", "mrqa_naturalquestions-validation-8983", "mrqa_squad-validation-3021"], "EFR": 1.0, "Overall": 0.7694389204545455}, {"timecode": 11, "before_eval_results": {"predictions": ["the Horn of Africa", "Grumman", "to civil disobedients", "1700", "St. Johns River", "the AS-205 mission was canceled", "The President of the Council and a Commissioner can sit in on ECB meetings, but don't have voting rights", "Ismailiyah, Egypt", "phycobilisomes", "lupus erythematosus", "December 1878", "bars, caf\u00e9s and clubs", "PNU and ODM camps", "T(n) = O(n2)", "Bill Clinton", "qu", "International Crops Research Institute for the Semi-Arid Tropics", "straight line", "Scandinavia", "autoimmune", "January 26, 1996", "his advocacy of young earth creationism and intelligent design", "Seoul", "2005", "December 24, 1973", "May 21, 2000", "100 metres", "January 2016", "seven", "Samuel Beckett's", "Eilean Donan", "Sonic Mania", "Homeland", "Carson City", "The Three Caesars' Alliance", "President Barack Obama", "Brian A. Miller", "Washington, D.C.", "December 13, 2015", "Front Row", "before 1638", "Vixen", "Revolution Studios", "Mach number", "1990", "Richard L. Thompson", "Gangsta's Paradise", "A41", "Bette Davis, Olivia de Havilland, Joseph Cotten, Agnes Moorehead and Mary Astor", "five times", "Indiana", "Esteban Ocon", "ABC", "Jean Baptiste Point Du Sable", "National Lottery", "2018", "Emma Watson", "Robert Lucas", "Billy Wilder", "2-1", "forward deck space", "the Sudan", "a pillar of salt", "Yahya Khan"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6619318181818181}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.4, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9912", "mrqa_squad-validation-6759", "mrqa_squad-validation-3954", "mrqa_squad-validation-4150", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-10161", "mrqa_triviaqa-validation-1378", "mrqa_newsqa-validation-1700", "mrqa_searchqa-validation-11991", "mrqa_naturalquestions-validation-3485"], "SR": 0.59375, "CSR": 0.6588541666666667, "retrieved_ids": ["mrqa_squad-train-18011", "mrqa_squad-train-79890", "mrqa_squad-train-28939", "mrqa_squad-train-27695", "mrqa_squad-train-43952", "mrqa_squad-train-68655", "mrqa_squad-train-77728", "mrqa_squad-train-5853", "mrqa_squad-train-5507", "mrqa_squad-train-21333", "mrqa_squad-train-35320", "mrqa_squad-train-27146", "mrqa_squad-train-35233", "mrqa_squad-train-86040", "mrqa_squad-train-59323", "mrqa_squad-train-21044", "mrqa_squad-train-19522", "mrqa_squad-train-15441", "mrqa_squad-train-57422", "mrqa_squad-train-13723", "mrqa_squad-train-33181", "mrqa_squad-train-84411", "mrqa_squad-train-64035", "mrqa_squad-train-34240", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-4071", "mrqa_searchqa-validation-11137", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-1415", "mrqa_searchqa-validation-2866", "mrqa_naturalquestions-validation-7080", "mrqa_hotpotqa-validation-929", "mrqa_squad-validation-2943", "mrqa_squad-validation-7357", "mrqa_squad-validation-8459", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-9753", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-5702", "mrqa_searchqa-validation-3960", "mrqa_squad-validation-1672", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-3451", "mrqa_squad-validation-3863", "mrqa_squad-validation-2538", "mrqa_squad-validation-6706", "mrqa_squad-validation-4715"], "EFR": 0.9615384615384616, "Overall": 0.7605629006410257}, {"timecode": 12, "before_eval_results": {"predictions": ["Sky Q Silver set top boxes with a Wi-Fi or Power-line connection", "water pump", "Tesla coil", "1946", "21 to 11", "Convening the Parliamentary Bureau", "Japan and Latin America", "sent missionaries, backed by a fund to financially reward converts to Catholicism", "Arizona Cardinals", "842 pounds", "1540s", "John Fox", "the American Indians in the colony of Georgia", "orbit the Moon", "poison", "quickly", "a system of many biological structures and processes within an organism that protects against disease", "March 1896", "The Lightning thief", "James `` Jamie '' Dornan", "W. Edwards Deming", "May", "biochemistry", "Titanic earned $657.4 million in North America and $1.528 billion in other countries", "During the last Ice Age", "Accounting Standards Board", "Ole Einar Bj\u00f8rndalen", "General George Washington", "following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Djokovic", "Longline", "1952", "dome", "1997", "Procol Harum", "Sheev Palpatine", "Dan Rooney", "punk rock", "the septum", "The White House Executive chef", "vaskania", "the church at Philippi", "10 May 1940", "Brenda", "bohrium", "prejudice in favour of or against one thing, person, or group compared with another", "cartilage", "wake me up", "they rapidly became independent of one another", "Walt Disney", "Owen Vaccaro", "Walter Brennan", "1872", "Mike Alstott", "1992", "King Richard II", "between Austria and Switzerland", "Chuck vs. First Class", "Dana Mulder", "Kooyong Classic", "Monday", "Swing Low, Sweet Chariot", "blinking his left eye", "the Puget Sound"], "metric_results": {"EM": 0.5, "QA-F1": 0.6456375466853408}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, true, true, false, true], "QA-F1": [0.7058823529411764, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.88, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.13333333333333336, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.13333333333333336, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-978", "mrqa_squad-validation-9458", "mrqa_squad-validation-3130", "mrqa_squad-validation-3811", "mrqa_squad-validation-9863", "mrqa_squad-validation-8164", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-2813", "mrqa_triviaqa-validation-4886", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-5292", "mrqa_newsqa-validation-1360", "mrqa_searchqa-validation-9822"], "SR": 0.5, "CSR": 0.6466346153846154, "retrieved_ids": ["mrqa_squad-train-15347", "mrqa_squad-train-28233", "mrqa_squad-train-71114", "mrqa_squad-train-77800", "mrqa_squad-train-68940", "mrqa_squad-train-349", "mrqa_squad-train-62778", "mrqa_squad-train-48920", "mrqa_squad-train-33741", "mrqa_squad-train-68963", "mrqa_squad-train-77916", "mrqa_squad-train-24620", "mrqa_squad-train-75265", "mrqa_squad-train-39196", "mrqa_squad-train-79058", "mrqa_squad-train-62403", "mrqa_squad-train-25833", "mrqa_squad-train-4860", "mrqa_squad-train-13864", "mrqa_squad-train-38183", "mrqa_squad-train-86540", "mrqa_squad-train-53792", "mrqa_squad-train-13040", "mrqa_squad-train-15093", "mrqa_naturalquestions-validation-7896", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-2761", "mrqa_squad-validation-9912", "mrqa_hotpotqa-validation-62", "mrqa_naturalquestions-validation-3392", "mrqa_squad-validation-6409", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-686", "mrqa_naturalquestions-validation-4071", "mrqa_searchqa-validation-7517", "mrqa_squad-validation-5724", "mrqa_squad-validation-4715", "mrqa_hotpotqa-validation-1471", "mrqa_searchqa-validation-6463", "mrqa_naturalquestions-validation-3442", "mrqa_hotpotqa-validation-3304", "mrqa_naturalquestions-validation-1378", "mrqa_squad-validation-7700", "mrqa_squad-validation-7357", "mrqa_squad-validation-4150", "mrqa_naturalquestions-validation-10161", "mrqa_searchqa-validation-14480", "mrqa_naturalquestions-validation-7080"], "EFR": 0.96875, "Overall": 0.7595612980769231}, {"timecode": 13, "before_eval_results": {"predictions": ["Protestantism", "Extension", "Riverside", "The conservation of momentum", "Hamburg merchants and traders", "Department of Justice", "water flow through the body cavity", "67.9", "Fort Duquesne", "Sports Programs, Inc.", "quality rental units", "Pittsburgh Steelers", "Edward Teller", "the geographical area it covers as well as the frequency of meeting", "to stay", "Andrew Lortie", "Animals are divided by body plan into vertebrates and invertebrates", "the First Order has risen from the fallen Galactic Empire and seeks to eliminate the New Republic", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "eight years", "The Vulcan salute", "Longline fishing", "various locations in Redford's adopted home state of Utah", "Stephen A. Douglas", "April 2011", "around 1940", "Paradise, Nevada", "Herman Hollerith", "Dr. Rajendra Prasad", "Ron Harper", "hairpin turn", "over two days", "the IB Diploma Program and the IB Career - related Program for students aged 15 to 18, the IB Middle Years Program, designed for Students aged 11 to 14, and theIB Primary Years Program for children aged 3 to 12", "when the cell is undergoing the metaphase of cell division", "it activates a relay which will handle the higher current load", "Donald Trump", "Liam Cunningham", "spectroscopic notation", "Betty", "The Pardoner's Tale", "A rotation", "Sauron's", "Gustav Bauer", "2002", "Mohammad Reza Pahlavi", "on the southeastern coast of the Commonwealth of Virginia in the United States", "two tectonic plates", "Jourdan Miller", "10,605", "1983", "1773", "Jesse McCartney", "73", "on Mars Hill, 150 miles ( 240 km ) to the northeast", "1920", "Catherine Zeta-Jones", "Michael Crawford", "247,597", "10,000", "those missing", "Mormon Tabernacle Choir", "carbon dioxide", "Pickwick", "Sunshine State"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6009502923976608}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.1, 1.0, 1.0, 0.10526315789473684, 1.0, 0.19999999999999998, 1.0, 0.0, 0.8, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-259", "mrqa_squad-validation-4435", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-585", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-8699", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-1166"], "SR": 0.515625, "CSR": 0.6372767857142857, "retrieved_ids": ["mrqa_squad-train-63053", "mrqa_squad-train-39660", "mrqa_squad-train-33361", "mrqa_squad-train-40100", "mrqa_squad-train-73058", "mrqa_squad-train-82016", "mrqa_squad-train-80197", "mrqa_squad-train-84342", "mrqa_squad-train-32685", "mrqa_squad-train-78606", "mrqa_squad-train-1281", "mrqa_squad-train-44173", "mrqa_squad-train-52563", "mrqa_squad-train-6155", "mrqa_squad-train-64322", "mrqa_squad-train-44723", "mrqa_squad-train-66526", "mrqa_squad-train-61259", "mrqa_squad-train-35217", "mrqa_squad-train-19060", "mrqa_squad-train-21264", "mrqa_squad-train-67233", "mrqa_squad-train-16303", "mrqa_squad-train-84602", "mrqa_squad-validation-10369", "mrqa_naturalquestions-validation-1863", "mrqa_newsqa-validation-1538", "mrqa_searchqa-validation-16130", "mrqa_squad-validation-1938", "mrqa_squad-validation-2975", "mrqa_naturalquestions-validation-8277", "mrqa_squad-validation-5588", "mrqa_squad-validation-9717", "mrqa_newsqa-validation-1360", "mrqa_searchqa-validation-10372", "mrqa_searchqa-validation-8760", "mrqa_squad-validation-4629", "mrqa_naturalquestions-validation-2476", "mrqa_searchqa-validation-123", "mrqa_squad-validation-4452", "mrqa_hotpotqa-validation-3253", "mrqa_searchqa-validation-14569", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-4102", "mrqa_searchqa-validation-3613", "mrqa_hotpotqa-validation-1471", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-9240"], "EFR": 1.0, "Overall": 0.7639397321428572}, {"timecode": 14, "before_eval_results": {"predictions": ["a Tulku", "the Quaternary", "King George's War", "Brad Nortman", "Behind the Sofa", "BBC Dead Ringers", "1206", "Louis Pasteur", "The Brain of Morbius", "the oceans and seas", "two fumbles, and intercepted four passes of his own fumbles.", "a mainline Protestant Methodist denomination", "Albert Einstein", "Vince Lombardi Trophy", "death in body and soul, if only as highwaymen and murderers", "Candice Susan Swanepoel", "AT&T", "Australian", "German", "Chris Anderson", "just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "1949", "Red", "Australia", "Humphrey Goodman", "Jena Malone", "John M. Dowd", "twelfth", "Republican", "New York", "Missouri Hatchet", "a tragedy", "Cricket fighting", "14th Street", "guitar", "Ed Asner", "2012", "New Orleans, Louisiana", "Robert \"Bobby\" Germaine, Sr.", "May 4, 1924", "Australian", "1966", "2012", "1926", "Texas's 27th congressional district", "Benjamin \"Benny\" Ciaramello", "mother goddess", "VAQ-135", "1892", "Ludwig van Beethoven", "Sam Phillips", "Manchester United", "Saudi Arabian", "1942", "October 6, 2017", "at a given temperature", "wolf", "Ganges", "January 24, 2006", "repression and dire economic circumstances", "a pager", "Baltimore", "April 6, 1917", "about an intersection with U.S. Route 340 ( US 340 ) near Front Royal, and the southern terminus is at an interchange with US 250 near Interstate 64 ( I - 64 ) in Rockfish Gap,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6717112737787281}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6341463414634146]}}, "before_error_ids": ["mrqa_squad-validation-8229", "mrqa_squad-validation-802", "mrqa_squad-validation-218", "mrqa_squad-validation-2383", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-10613", "mrqa_newsqa-validation-1879", "mrqa_naturalquestions-validation-1813"], "SR": 0.5625, "CSR": 0.6322916666666667, "retrieved_ids": ["mrqa_squad-train-7574", "mrqa_squad-train-64755", "mrqa_squad-train-34271", "mrqa_squad-train-13429", "mrqa_squad-train-6353", "mrqa_squad-train-54939", "mrqa_squad-train-25669", "mrqa_squad-train-19189", "mrqa_squad-train-63971", "mrqa_squad-train-34351", "mrqa_squad-train-65950", "mrqa_squad-train-12463", "mrqa_squad-train-80269", "mrqa_squad-train-56252", "mrqa_squad-train-14623", "mrqa_squad-train-62897", "mrqa_squad-train-48619", "mrqa_squad-train-34801", "mrqa_squad-train-8669", "mrqa_squad-train-12807", "mrqa_squad-train-27469", "mrqa_squad-train-76722", "mrqa_squad-train-21332", "mrqa_squad-train-40045", "mrqa_naturalquestions-validation-9737", "mrqa_squad-validation-2595", "mrqa_squad-validation-3946", "mrqa_squad-validation-6706", "mrqa_squad-validation-9717", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-9597", "mrqa_searchqa-validation-13232", "mrqa_squad-validation-8459", "mrqa_naturalquestions-validation-1415", "mrqa_searchqa-validation-11388", "mrqa_squad-validation-3497", "mrqa_triviaqa-validation-4844", "mrqa_hotpotqa-validation-4815", "mrqa_naturalquestions-validation-7095", "mrqa_hotpotqa-validation-3253", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-9979", "mrqa_searchqa-validation-15169", "mrqa_squad-validation-3130", "mrqa_squad-validation-5588", "mrqa_searchqa-validation-13600"], "EFR": 1.0, "Overall": 0.7629427083333333}, {"timecode": 15, "before_eval_results": {"predictions": ["1937", "Educational Institute of Scotland and the Scottish Secondary Teachers' Association", "June 4, 2014", "Journey's End", "a stronger, tech-oriented economy in the Bay Area and an emerging Greater Sacramento region", "John Houghton", "heterokontophyte", "NP-complete", "Chinggis", "128,843", "a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections", "56.2%", "11 points", "Chaplain to the Forces", "KlingStubbins", "Edward Albert Heimberger", "Named in honour of Louis Mountbatten, 1st Earl MountBatten of Burma", "Alcorn State", "You're Next", "The Light in the Piazza", "Philadelphia, Pennsylvania", "9", "A41", "Royce da 5'9\" (Bad) and Eminem (Evil)", "\"Pimp My Ride\"", "1998", "casting, job opportunities, and career advice", "Mary Harron", "Flashback", "Eenasul Fateh", "Chicago", "Australia", "2014", "Battle of Singapore", "Lismore", "rural", "teenage actor or teen actor", "Summerlin, Clark County, Nevada", "Lester Ben \"Benny\" Binion", "winner (Hangul: \uc704\ub108)", "water sprite", "Noel Gallagher", "\"Pour le M\u00e9rite\"", "Trey Parker and Matt Stone", "\"Riot Act\"", "Aqua", "various registries.", "four operas", "Christy Walton", "Lt. Gen. Ulysses S. Grant", "Hechingen", "\"N.I.B.\"", "manager", "8,211", "Kristin Beth Baxter", "nucleus", "a Bristol Box Kite", "1961", "a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented", "South Dakota State Penitentiary", "Douglas Fir", "drink wine or kiss a fool", "a striking blow to due process and the rule of law", "Philip Markoff"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6896005036630036}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.6153846153846153, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.4, 0.0, 0.5, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2094", "mrqa_squad-validation-2835", "mrqa_squad-validation-4298", "mrqa_squad-validation-257", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-2", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2558", "mrqa_triviaqa-validation-7461", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-13595", "mrqa_searchqa-validation-1757"], "SR": 0.578125, "CSR": 0.62890625, "retrieved_ids": ["mrqa_squad-train-43545", "mrqa_squad-train-22121", "mrqa_squad-train-40422", "mrqa_squad-train-49710", "mrqa_squad-train-52853", "mrqa_squad-train-44170", "mrqa_squad-train-58579", "mrqa_squad-train-17920", "mrqa_squad-train-47782", "mrqa_squad-train-61840", "mrqa_squad-train-80702", "mrqa_squad-train-16176", "mrqa_squad-train-19514", "mrqa_squad-train-61530", "mrqa_squad-train-38868", "mrqa_squad-train-32605", "mrqa_squad-train-15014", "mrqa_squad-train-54847", "mrqa_squad-train-73044", "mrqa_squad-train-26562", "mrqa_squad-train-86375", "mrqa_squad-train-36329", "mrqa_squad-train-62081", "mrqa_squad-train-64990", "mrqa_searchqa-validation-15770", "mrqa_hotpotqa-validation-3253", "mrqa_squad-validation-8459", "mrqa_hotpotqa-validation-5386", "mrqa_squad-validation-3021", "mrqa_newsqa-validation-2179", "mrqa_squad-validation-7700", "mrqa_naturalquestions-validation-10122", "mrqa_squad-validation-8841", "mrqa_newsqa-validation-1671", "mrqa_squad-validation-8904", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-1471", "mrqa_squad-validation-2932", "mrqa_hotpotqa-validation-1704", "mrqa_squad-validation-7051", "mrqa_naturalquestions-validation-4547", "mrqa_searchqa-validation-15202", "mrqa_squad-validation-3954", "mrqa_searchqa-validation-2866", "mrqa_naturalquestions-validation-473", "mrqa_squad-validation-1827", "mrqa_squad-validation-978", "mrqa_newsqa-validation-1538"], "EFR": 1.0, "Overall": 0.7622656250000001}, {"timecode": 16, "before_eval_results": {"predictions": ["Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Puritan", "James Wolfe", "March 1974", "2003", "Frederick II the Great", "Lower taxes, increased economic development, unification of the community, better public spending and effective administration by a more central authority", "Armenians vassal-states of Sassoun and Taron", "redistributive taxation", "Seattle Seahawks", "paid professionals", "a polynomial-time reduction", "revelry", "Krishna Rajaram", "a woman of faith", "British charities for aid to Gaza", "1-0", "Choi", "second-degree aggravated battery", "former Massachusetts governor", "one day", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews", "be silent", "200", "2,000", "several weeks", "auction off one of the earliest versions of the Magna Carta later this year", "it pulls the scab and it cracks", "Prince", "Caylee", "10 below", "women", "Manmohan Singh", "jazz", "1983", "cancer", "Al-Shabaab", "Camorra", "CNN affiliate KVBC", "Appathurai", "Eintracht Frankfurt", "poppy production", "The Bronx County District Attorneys Office", "1,073", "Arthur E. Morgan III", "an army major assigned to a guard unit protecting Mexican President Felipe Calderon", "Chinese nationals", "Pakistan", "the Bainbridge", "the chief executive officer", "18", "\"Draquila -- Italy Trembles.\"", "India", "Mumbai", "Miami Heat", "a combination of genetics and the male hormone dihydrotestosterone", "Senegal", "Windermere", "Field of Dreams", "Bill Paxton", "a star", "American Airlines LAS-PHL", "Sex Pistols", "an ambitious Jewish boy growing up in a poor neighborhood"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6094866071428572}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false], "QA-F1": [0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8294", "mrqa_squad-validation-7296", "mrqa_squad-validation-1136", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-2900", "mrqa_triviaqa-validation-4966", "mrqa_hotpotqa-validation-5742", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-4356"], "SR": 0.53125, "CSR": 0.6231617647058824, "retrieved_ids": ["mrqa_squad-train-55440", "mrqa_squad-train-18038", "mrqa_squad-train-82915", "mrqa_squad-train-53097", "mrqa_squad-train-79150", "mrqa_squad-train-35857", "mrqa_squad-train-51122", "mrqa_squad-train-84531", "mrqa_squad-train-791", "mrqa_squad-train-55357", "mrqa_squad-train-24375", "mrqa_squad-train-460", "mrqa_squad-train-62394", "mrqa_squad-train-32780", "mrqa_squad-train-63605", "mrqa_squad-train-16758", "mrqa_squad-train-7343", "mrqa_squad-train-13176", "mrqa_squad-train-41498", "mrqa_squad-train-46973", "mrqa_squad-train-85593", "mrqa_squad-train-82007", "mrqa_squad-train-11227", "mrqa_squad-train-12736", "mrqa_hotpotqa-validation-1736", "mrqa_squad-validation-1240", "mrqa_naturalquestions-validation-8792", "mrqa_squad-validation-6439", "mrqa_newsqa-validation-174", "mrqa_squad-validation-259", "mrqa_naturalquestions-validation-7080", "mrqa_squad-validation-6655", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-844", "mrqa_squad-validation-4298", "mrqa_squad-validation-9176", "mrqa_squad-validation-4000", "mrqa_squad-validation-4715", "mrqa_naturalquestions-validation-6426", "mrqa_hotpotqa-validation-2452", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-7269", "mrqa_newsqa-validation-2404", "mrqa_naturalquestions-validation-3442", "mrqa_squad-validation-6171", "mrqa_hotpotqa-validation-2198", "mrqa_naturalquestions-validation-5780"], "EFR": 1.0, "Overall": 0.7611167279411765}, {"timecode": 17, "before_eval_results": {"predictions": ["higher economic inequality", "private individuals, private organizations or religious groups", "high schools", "a glass case suspended from the lid", "phagocytic", "2000", "five", "no overall increase in weight", "Leukocytes", "3D printing technology", "Ong Khan", "colonel in the Rwandan army", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "Wigan Athletic", "Russian air company Vertikal-T", "Graeme Smith", "228", "the commissions", "her father's", "St. Francis De Sales Catholic Church", "The Tinkler", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "helicopters and unmanned aerial vehicles", "seven deaths", "African National Congress", "Somalia", "bikinis", "23", "Adam Yahiye Gadahn,", "Governor Sanford", "150", "weren't taking it well.", "the equator", "Chinese President Hu Jintao", "183", "to alert patients of possible tendon ruptures and tendonitis", "Too many glass shards left by beer drinkers in the city center", "Cirque du Soleil", "\"Goldstone Report\"", "11th year in a row", "stole four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday", "fastest circumnavigation of the globe in a powerboat", "Guinea, Myanmar, Sudan and Venezuela", "Austin Wuennenberg", "Diversity", "forcibly injecting them with psychotropic drugs", "Oaxacan", "buckling under pressure from the ruling party.", "a polo", "more than 100", "bribing other wrestlers to lose bouts", "Alfredo Astiz,", "MacFarlane", "convert single - stranded genomic RNA into double - stranded cDNA", "Harrison Ford", "Andes", "Peter Robert Auty", "1884", "\"Candid Microphone\"", "Marilyn Monroe", "Smashed Guitars", "Walt Disney Night", "Mexita"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5110721146566735}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.4, 1.0, 1.0, 1.0, 0.25, 0.2666666666666667, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9411764705882353, 1.0, 0.0, 0.0, 1.0, 0.14285714285714288, 0.2857142857142857, 0.0, 1.0, 1.0, 0.25, 0.2857142857142857, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4864864864864865, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7136", "mrqa_squad-validation-2000", "mrqa_squad-validation-7845", "mrqa_squad-validation-3435", "mrqa_squad-validation-6477", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1123", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-112", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1649"], "SR": 0.421875, "CSR": 0.6119791666666667, "retrieved_ids": ["mrqa_squad-train-40717", "mrqa_squad-train-32450", "mrqa_squad-train-72534", "mrqa_squad-train-8136", "mrqa_squad-train-6438", "mrqa_squad-train-66188", "mrqa_squad-train-10602", "mrqa_squad-train-55199", "mrqa_squad-train-42878", "mrqa_squad-train-61427", "mrqa_squad-train-37459", "mrqa_squad-train-254", "mrqa_squad-train-18128", "mrqa_squad-train-39463", "mrqa_squad-train-17920", "mrqa_squad-train-51164", "mrqa_squad-train-49135", "mrqa_squad-train-46517", "mrqa_squad-train-36636", "mrqa_squad-train-81468", "mrqa_squad-train-42104", "mrqa_squad-train-80870", "mrqa_squad-train-65744", "mrqa_squad-train-70117", "mrqa_naturalquestions-validation-4192", "mrqa_hotpotqa-validation-3253", "mrqa_squad-validation-2932", "mrqa_newsqa-validation-1538", "mrqa_naturalquestions-validation-10039", "mrqa_squad-validation-6965", "mrqa_searchqa-validation-7269", "mrqa_newsqa-validation-2404", "mrqa_naturalquestions-validation-10411", "mrqa_squad-validation-6957", "mrqa_searchqa-validation-14655", "mrqa_naturalquestions-validation-8189", "mrqa_squad-validation-2094", "mrqa_hotpotqa-validation-929", "mrqa_squad-validation-8841", "mrqa_squad-validation-1240", "mrqa_squad-validation-5435", "mrqa_hotpotqa-validation-1534", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2205", "mrqa_squad-validation-10321", "mrqa_hotpotqa-validation-3721", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-13600"], "EFR": 1.0, "Overall": 0.7588802083333335}, {"timecode": 18, "before_eval_results": {"predictions": ["Jason Bourne", "in the condenser", "1999", "mesoglea", "a body of treaties and legislation,", "liquid", "socially", "Mark Twain", "amylopectin starch granules", "Tower District", "to disrupt the inauguration,", "flying glass and rocks", "at a construction site in the heart of Los Angeles.", "overthrow the socialist government of Salvador Allende in Chile", "The school's first \"micro-yachtsman\"", "a rally at the State House next week", "2,000", "Michael Schumacher", "Ventures", "seven", "hanging a noose", "resigned", "\"I'm just getting started.\"", "21", "to release the women without feeling that its legal system has been slighted,", "hand-painted Swedish wooden clogs", "Daniel Radcliffe", "Muslim", "five", "Mom.", "$10 billion", "Six members of Zoe's Ark", "Galveston, Texas,", "9-week-old", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Lucky Dube", "\"Wicked\"", "James Newell Osterberg", "At least 40", "NATO", "Lindsey Vonn", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "the National Park Service", "International Polo Club Palm Beach in Florida.", "Isabella, Emma, Olivia, Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia.", "HPV (human papillomavirus)", "unemployed or underemployed", "At least 88", "creation of an Islamic emirate in Gaza", "an \"unnamed international terror group\"", "Nicole", "Mario Balotelli", "Taher Nunu", "26 March 2015", "January 2017", "-27000", "black Cold Shoulder", "Waylon Albright", "people working in film and the performing arts", "Reese's", "school holidays", "Marlborough,", "1968", "The Krypto Report"], "metric_results": {"EM": 0.5, "QA-F1": 0.5583333333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07692307692307693, 0.4, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.2564102564102564, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2471", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-5209", "mrqa_hotpotqa-validation-2986", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-3428"], "SR": 0.5, "CSR": 0.6060855263157895, "retrieved_ids": ["mrqa_squad-train-49017", "mrqa_squad-train-54230", "mrqa_squad-train-33219", "mrqa_squad-train-27727", "mrqa_squad-train-5661", "mrqa_squad-train-70826", "mrqa_squad-train-32827", "mrqa_squad-train-74955", "mrqa_squad-train-32473", "mrqa_squad-train-32132", "mrqa_squad-train-46219", "mrqa_squad-train-22538", "mrqa_squad-train-71340", "mrqa_squad-train-23051", "mrqa_squad-train-56861", "mrqa_squad-train-73578", "mrqa_squad-train-15521", "mrqa_squad-train-72794", "mrqa_squad-train-18861", "mrqa_squad-train-72017", "mrqa_squad-train-8719", "mrqa_squad-train-23291", "mrqa_squad-train-28055", "mrqa_squad-train-22645", "mrqa_newsqa-validation-3986", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-4924", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-8711", "mrqa_naturalquestions-validation-6106", "mrqa_hotpotqa-validation-1576", "mrqa_squad-validation-3718", "mrqa_squad-validation-6171", "mrqa_squad-validation-4572", "mrqa_naturalquestions-validation-4674", "mrqa_searchqa-validation-15169", "mrqa_hotpotqa-validation-5328", "mrqa_squad-validation-2943", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6211", "mrqa_hotpotqa-validation-1011", "mrqa_newsqa-validation-445", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-3469", "mrqa_searchqa-validation-5149", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-15770"], "EFR": 1.0, "Overall": 0.7577014802631579}, {"timecode": 19, "before_eval_results": {"predictions": ["blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "an Executive Committee", "New Orleans", "the death of Elisabeth Sladen", "The annual NFL Experience", "English and Swahili", "61%", "plastoglobulus", "three", "Turkey", "Wombat", "KENNY", "gestation", "Broncos Place", "the Hope Diamond", "Gin rummy", "Pilate", "enamel", "Japanese beef", "\"help yourself to happiness\"", "Battle of Hastings", "the Caspian Sea", "the office", "the army", "Gannett", "\"Won't Get Fooled Again\"", "\"Don Juan De Marco\"", "Fes", "the 1998 World Cup final", "Interlaken", "Mystic Pizza", "Princeton", "Mandy", "the 100 largest libraries in the United States", "Malay Peninsulamakes", "Herman Wouk", "Prince Albert", "the mouth of a fool", "poets", "Napoleon Bonaparte", "Stock Dinosaurs", "unassisted triple play", "thermodynamics", "Derek Smalls", "Dalits", "Harry Houdini", "Mary Steenburgen", "Double Vision", "Sporcle", "Lust for Life", "Camembert", "James Ross Clemens", "a hole", "1991", "to universalize the topic of the song into something everyone could relate to and ascribe personal meaning to in their own way", "Cuban cigars", "\"Thrilla in Manila\"", "North America, Australia, and India", "841", "Ike", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "Harlem River", "the modern state system", "the city of Cairo, Illinois"], "metric_results": {"EM": 0.453125, "QA-F1": 0.592773666901574}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.13953488372093023, 1.0, 0.5, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-16558", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-2769", "mrqa_newsqa-validation-3214", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3413"], "SR": 0.453125, "CSR": 0.5984375, "retrieved_ids": ["mrqa_squad-train-18847", "mrqa_squad-train-42618", "mrqa_squad-train-57563", "mrqa_squad-train-24402", "mrqa_squad-train-14110", "mrqa_squad-train-72294", "mrqa_squad-train-8017", "mrqa_squad-train-4148", "mrqa_squad-train-40347", "mrqa_squad-train-6550", "mrqa_squad-train-11772", "mrqa_squad-train-6180", "mrqa_squad-train-7148", "mrqa_squad-train-31503", "mrqa_squad-train-42330", "mrqa_squad-train-4744", "mrqa_squad-train-65246", "mrqa_squad-train-86514", "mrqa_squad-train-48847", "mrqa_squad-train-54492", "mrqa_squad-train-66685", "mrqa_squad-train-44106", "mrqa_squad-train-72049", "mrqa_squad-train-64854", "mrqa_triviaqa-validation-5500", "mrqa_naturalquestions-validation-1415", "mrqa_squad-validation-2943", "mrqa_squad-validation-8093", "mrqa_squad-validation-8595", "mrqa_naturalquestions-validation-3558", "mrqa_searchqa-validation-4356", "mrqa_squad-validation-5435", "mrqa_searchqa-validation-16627", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-129", "mrqa_hotpotqa-validation-2887", "mrqa_naturalquestions-validation-9239", "mrqa_newsqa-validation-2179", "mrqa_hotpotqa-validation-2198", "mrqa_triviaqa-validation-6421", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-1378", "mrqa_newsqa-validation-1360", "mrqa_searchqa-validation-2568", "mrqa_squad-validation-6965", "mrqa_squad-validation-9458"], "EFR": 0.9714285714285714, "Overall": 0.7504575892857143}, {"timecode": 20, "UKR": 0.779296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-1252", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1967", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-3519", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-782", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-920", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11270", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2607", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4469", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-971", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10143", "mrqa_squad-validation-10168", "mrqa_squad-validation-10241", "mrqa_squad-validation-10266", "mrqa_squad-validation-10370", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1141", "mrqa_squad-validation-115", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1254", "mrqa_squad-validation-127", "mrqa_squad-validation-1288", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1747", "mrqa_squad-validation-1765", "mrqa_squad-validation-1827", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-218", "mrqa_squad-validation-22", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2379", "mrqa_squad-validation-2383", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-2538", "mrqa_squad-validation-2545", "mrqa_squad-validation-257", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2886", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-305", "mrqa_squad-validation-3052", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3567", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3864", "mrqa_squad-validation-3917", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3975", "mrqa_squad-validation-3986", "mrqa_squad-validation-3994", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4187", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4883", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5097", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-5237", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5604", "mrqa_squad-validation-5677", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5859", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6347", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6594", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7034", "mrqa_squad-validation-7039", "mrqa_squad-validation-7051", "mrqa_squad-validation-71", "mrqa_squad-validation-7125", "mrqa_squad-validation-7136", "mrqa_squad-validation-7192", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7449", "mrqa_squad-validation-7521", "mrqa_squad-validation-7576", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7708", "mrqa_squad-validation-7751", "mrqa_squad-validation-7814", "mrqa_squad-validation-7863", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7881", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8471", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8670", "mrqa_squad-validation-8710", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9095", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9367", "mrqa_squad-validation-9405", "mrqa_squad-validation-942", "mrqa_squad-validation-9594", "mrqa_squad-validation-9614", "mrqa_squad-validation-9669", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-4881", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-7496"], "OKR": 0.88671875, "KG": 0.48203125, "before_eval_results": {"predictions": ["the main contractor", "widespread education", "300 men", "an attack on New France's capital, Quebec", "two-thirds", "Decompression sickness", "1979", "Parliament Square, High Street and George IV Bridge", "1959", "the Superdome", "grizzly bear", "Dracula", "Sid Vicious", "nitrous oxide", "Tchaikovsky", "Frederic Remington", "Laguna Diamante", "Arkansas", "a programming language model organized around objects", "10", "the Uniform Code of Military Justice", "Horace Greeley", "ER", "Lotus", "Vitamix", "The Princess", "Arkansas", "Mao Zedong", "Doner", "a genie", "Wells Fargo", "Sundance", "a house of prayer", "amber", "Holly Golightly", "Umbria", "a Roth IRA", "Quentin Tarantino", "Palatine Hill", "Kentucky", "axiom", "Daylight Saving Time", "Miniskirt", "the Airplane", "Scooter Libby", "a flood", "Lovett", "Equatorial Guinea", "Bob Crane", "Bowling ball", "Walter Reed", "Aerobic", "Anaheim", "Steve Hale", "the study and analysis of the distribution and determinants of health and disease conditions in defined populations", "Belgium", "Jack Frost", "137th", "Merck & Co.", "Microsoft", "the Russian cosmonaut", "France", "Hagrid", "Phil Mickelson"], "metric_results": {"EM": 0.5625, "QA-F1": 0.678125}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4918", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-16826", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-12996", "mrqa_searchqa-validation-839", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-3811", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-9853", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-4992", "mrqa_hotpotqa-validation-3157", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-4118", "mrqa_triviaqa-validation-436"], "SR": 0.5625, "CSR": 0.5967261904761905, "retrieved_ids": ["mrqa_squad-train-48854", "mrqa_squad-train-82137", "mrqa_squad-train-33063", "mrqa_squad-train-86250", "mrqa_squad-train-40312", "mrqa_squad-train-32126", "mrqa_squad-train-47139", "mrqa_squad-train-73015", "mrqa_squad-train-45595", "mrqa_squad-train-30879", "mrqa_squad-train-14952", "mrqa_squad-train-27839", "mrqa_squad-train-4068", "mrqa_squad-train-21717", "mrqa_squad-train-67749", "mrqa_squad-train-82173", "mrqa_squad-train-13262", "mrqa_squad-train-7796", "mrqa_squad-train-8710", "mrqa_squad-train-49097", "mrqa_squad-train-40181", "mrqa_squad-train-82632", "mrqa_squad-train-35120", "mrqa_squad-train-70567", "mrqa_naturalquestions-validation-3066", "mrqa_hotpotqa-validation-1534", "mrqa_naturalquestions-validation-81", "mrqa_squad-validation-978", "mrqa_newsqa-validation-1012", "mrqa_naturalquestions-validation-6046", "mrqa_hotpotqa-validation-5328", "mrqa_newsqa-validation-1330", "mrqa_hotpotqa-validation-3395", "mrqa_squad-validation-4435", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-15169", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-8699", "mrqa_squad-validation-3770", "mrqa_squad-validation-5818", "mrqa_searchqa-validation-5172", "mrqa_squad-validation-7700", "mrqa_newsqa-validation-895", "mrqa_squad-validation-739", "mrqa_triviaqa-validation-6896", "mrqa_searchqa-validation-7269"], "EFR": 1.0, "Overall": 0.7489546130952381}, {"timecode": 21, "before_eval_results": {"predictions": ["a lesson plan", "laws of physics", "1893", "Welsh", "pastors and teachers", "criminal investigations", "a monthly subscription", "10,000 BC", "novella", "President of the United States", "above the light source and under the sample in an upright microscope", "November 3, 2007", "1939", "April 1917", "1959", "Antonio Banderas", "September 19 - 22, 2017", "tolled ( quota ) highways", "affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "John Freeman", "Arunachal Pradesh", "Dick Rutan", "Paracelsus", "January 2004", "members of the gay ( LGBT ) community", "late January or early February", "they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Jerry Lee Lewis", "push the food down the esophagus", "Splodgenessabounds", "Triple threat", "Edd Kimber, Joanne Wheatley, John Whaite, Frances Quinn, Nancy Birtwhistle, Nadiya Hussain, Candice Brown and Sophia Faldo", "warning sign", "A diastema ( plural diastemata )", "Eddie Murphy", "The Walking Dead", "Secretaries of State and Defense", "flour and water", "the National Football League ( NFL )", "the Golden Age of India", "card verification code ( CVC )", "T - Bone Walker", "Ray Charles", "Francis Hutcheson", "1937", "at Cairo, Illinois", "Barbara Windsor", "British victims", "Gladys Knight & the Pips", "Executive Residence of the White House Complex", "the eighth episode of Arrow's second season", "judges", "Kanawha River", "athletics", "isosceles", "1898", "Sir Matthew Arundell", "WFTV", "tennis", "Las Vegas", "Austria", "women and breast cancer", "Harry Nicolaides", "\"He says, 'Don't call police, don't shoot me, I have no money, I has no food in my house"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6678911239435334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6428571428571429, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.10909090909090909, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5000000000000001, 0.9863013698630138, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.15384615384615385, 1.0, 1.0, 0.0, 0.7499999999999999, 0.35294117647058826, 0.0, 0.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-6087", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-4715", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1985"], "SR": 0.578125, "CSR": 0.5958806818181819, "retrieved_ids": ["mrqa_squad-train-7046", "mrqa_squad-train-9744", "mrqa_squad-train-38995", "mrqa_squad-train-37821", "mrqa_squad-train-62183", "mrqa_squad-train-14271", "mrqa_squad-train-11457", "mrqa_squad-train-62055", "mrqa_squad-train-58947", "mrqa_squad-train-59376", "mrqa_squad-train-83233", "mrqa_squad-train-23376", "mrqa_squad-train-63179", "mrqa_squad-train-6916", "mrqa_squad-train-18772", "mrqa_squad-train-61052", "mrqa_squad-train-51420", "mrqa_squad-train-7649", "mrqa_squad-train-39594", "mrqa_squad-train-50761", "mrqa_squad-train-85120", "mrqa_squad-train-10428", "mrqa_squad-train-82863", "mrqa_squad-train-41366", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-1218", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-7080", "mrqa_hotpotqa-validation-5742", "mrqa_triviaqa-validation-7463", "mrqa_hotpotqa-validation-1742", "mrqa_searchqa-validation-1053", "mrqa_newsqa-validation-1330", "mrqa_naturalquestions-validation-4193", "mrqa_hotpotqa-validation-1471", "mrqa_hotpotqa-validation-1650", "mrqa_newsqa-validation-872", "mrqa_naturalquestions-validation-8699", "mrqa_squad-validation-7700", "mrqa_naturalquestions-validation-8339", "mrqa_searchqa-validation-4701", "mrqa_squad-validation-9876", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-3559", "mrqa_searchqa-validation-1156", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-5702", "mrqa_squad-validation-2595"], "EFR": 0.8888888888888888, "Overall": 0.7265632891414142}, {"timecode": 22, "before_eval_results": {"predictions": ["literacy and numeracy", "bark of mulberry trees", "drama series", "1806", "\"distributive efficiency\"", "on issues related to the substance of the statement", "in 23 cities", "Continental drift", "Frank Oz", "1975", "775", "Kimberlin Brown", "in Ephesus in AD 95 -- 110", "the status line", "the disk, about 26,000 light - years from the Galactic Center", "permanently absorbed the superhuman powers and the psyche of Carol Danvers", "Dr. Joel S. Engel of Bell Labs", "Dyrham Park", "PC2", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Javier Fern\u00e1ndez", "a lightning strike", "a landmark decision by the United States Supreme Court on US labor law and constitutional law", "Coton in the Elms", "a stem", "Wakanda and the Savage Land", "1992", "two theories", "MFSK", "Mansa Musa's control", "Edward Kenway", "Robert Hooke", "molecular clouds in interstellar space", "Alicia Vikander", "her neighbour", "the name announcement of Kylie Jenner's first child", "5 liters", "somatic cell nuclear transfer", "Veronica", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "June 8, 2009", "head - up display", "presidential representative democratic republic", "Ferm\u00edn Francisco de Lasu\u00e9n", "a moral tale", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "2009", "of Spanish / Basque origin", "Laura Jane Haddock", "Atlanta", "2002", "the nasal septum", "a coffee house", "Chief Inspector", "Cheshire", "#364", "24800 mi", "liberal revolutions", "punishment", "Matamoros, Mexico", "a terminal for hours while authorities rescreened thousands of passengers.", "The X-Files", "authentication", "North Dakota"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6696404818008079}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6956521739130436, 0.8181818181818181, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.2, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.7999999999999999, 0.2857142857142857, 1.0, 0.14285714285714288, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1802", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-7124", "mrqa_triviaqa-validation-4496", "mrqa_hotpotqa-validation-5437", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-7662"], "SR": 0.53125, "CSR": 0.5930706521739131, "retrieved_ids": ["mrqa_squad-train-76952", "mrqa_squad-train-13138", "mrqa_squad-train-38768", "mrqa_squad-train-21506", "mrqa_squad-train-1651", "mrqa_squad-train-70064", "mrqa_squad-train-3127", "mrqa_squad-train-19747", "mrqa_squad-train-64137", "mrqa_squad-train-31233", "mrqa_squad-train-17662", "mrqa_squad-train-58890", "mrqa_squad-train-84315", "mrqa_squad-train-10610", "mrqa_squad-train-10022", "mrqa_squad-train-32744", "mrqa_squad-train-64313", "mrqa_squad-train-45668", "mrqa_squad-train-21103", "mrqa_squad-train-42142", "mrqa_squad-train-66534", "mrqa_squad-train-20025", "mrqa_squad-train-23933", "mrqa_squad-train-45144", "mrqa_squad-validation-5435", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-801", "mrqa_squad-validation-3946", "mrqa_hotpotqa-validation-4102", "mrqa_newsqa-validation-3088", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-3413", "mrqa_squad-validation-8093", "mrqa_newsqa-validation-3331", "mrqa_searchqa-validation-3451", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7080", "mrqa_hotpotqa-validation-3090", "mrqa_naturalquestions-validation-3412", "mrqa_squad-validation-335", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6166", "mrqa_squad-validation-3998", "mrqa_hotpotqa-validation-230", "mrqa_searchqa-validation-16378", "mrqa_naturalquestions-validation-4740", "mrqa_squad-validation-8819"], "EFR": 0.9333333333333333, "Overall": 0.7348901721014494}, {"timecode": 23, "before_eval_results": {"predictions": ["around 100,000", "Tyneside Classical", "ring theory", "his birthtown, Smiljan", "Persia", "ABC-DuMont", "bathtub curve", "the First World War", "John Constable", "Charlie Harper", "the Nominative", "Duncan I", "Everton", "October", "cogito ergo sum", "Bull Moose Party", "Augusta", "Demi Moore", "the College of Cardinals", "Cornell", "Robert Stroud", "Alice in Wonderland", "caffeine", "The Blind Side", "11", "17 pink \"double-word\" squares", "Achille Lauro", "Quentin Tarantino", "Bert Jones", "New York", "Wyatt", "Chuck Hagel", "Haiti", "India", "argument", "Claire Goose", "sixteen pieces", "the English", "tinctures", "Andy Murray", "Independence Day", "\"I've never had an eight-ender before,\"", "Hanseatic League", "Crusades", "Geoffrey Plantagenet", "Thundercats", "comic", "The European Council", "Volkswagen", "George IV", "the Kalavinka", "the United States", "Edward Seton", "Thomas Jefferson", "central plains", "William Adelin", "Barbary pirates", "Sir William Collins", "to hold onto his land -- a year after the country's political rivals pledged to govern jointly -- fears he will eventually lose to politics and violence.", "island stronghold of the Islamic militant group Abu Sayyaf", "neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "port", "the Lone Star", "Bahadur Shah Zafar II"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6133928571428571}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3571428571428571, 0.0, 0.9333333333333333, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6205", "mrqa_squad-validation-9136", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-7212", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2325", "mrqa_naturalquestions-validation-1782", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2371", "mrqa_searchqa-validation-13686"], "SR": 0.53125, "CSR": 0.5904947916666667, "retrieved_ids": ["mrqa_squad-train-27196", "mrqa_squad-train-25385", "mrqa_squad-train-41716", "mrqa_squad-train-7172", "mrqa_squad-train-18488", "mrqa_squad-train-71625", "mrqa_squad-train-35876", "mrqa_squad-train-86093", "mrqa_squad-train-49747", "mrqa_squad-train-66509", "mrqa_squad-train-82469", "mrqa_squad-train-29522", "mrqa_squad-train-82561", "mrqa_squad-train-82860", "mrqa_squad-train-4561", "mrqa_squad-train-51986", "mrqa_squad-train-2384", "mrqa_squad-train-20306", "mrqa_squad-train-41340", "mrqa_squad-train-727", "mrqa_squad-train-23623", "mrqa_squad-train-42277", "mrqa_squad-train-57980", "mrqa_squad-train-61682", "mrqa_squad-validation-2094", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-11395", "mrqa_newsqa-validation-394", "mrqa_searchqa-validation-11406", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-779", "mrqa_squad-validation-10339", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6787", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3525", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-8277", "mrqa_squad-validation-2835", "mrqa_squad-validation-10321", "mrqa_squad-validation-1906", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-10460", "mrqa_squad-validation-4572", "mrqa_hotpotqa-validation-5627", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-140"], "EFR": 1.0, "Overall": 0.7477083333333334}, {"timecode": 24, "before_eval_results": {"predictions": ["Napoleon", "The Victorian Alps in the northeast", "skin damage", "three", "European Parliament and the Council of the European Union", "Steve McQueen", "Olivier Dahan", "piano", "Midtown", "bogey", "shoes of the finest leather", "boxer", "Paris", "The Spy Who Came in from the Cold, The Looking Glass War and The Secret Pilgrim", "Woodrow Wilson", "Menorca", "Wales", "meadowbank Thistle", "bulldog", "distance selling", "Edward VI", "linseed", "Mercury", "trumpet", "architecture", "Shooter McGavin", "Iain Banks", "Valencia", "gluten", "Jan van Eyck", "Brenda", "dalton", "Tom Baker", "Rita Hayworth", "World War II", "the Persian Army", "september", "Yosemite", "the Sandstone Trail", "duncan", "8 minutes", "uranium", "pretty Betsy", "a bra", "West Point", "Saint Cecilia", "algebra", "Johnsonkip", "Whittle", "eagle", "Loose ends", "Chester", "through Brazil, Bolivia, Paraguay and Argentina", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "2005", "Wes Craven", "1698", "Bill Clinton", "Airbus A320-214", "broken pelvis", "246", "hurricane", "The Treasure of the Sierra", "smallpox"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6205939848585322}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.07692307692307691, 0.48275862068965514, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2911", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-5981", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1951", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-2151", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-2426", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-1857"], "SR": 0.546875, "CSR": 0.58875, "retrieved_ids": ["mrqa_squad-train-22278", "mrqa_squad-train-74032", "mrqa_squad-train-63502", "mrqa_squad-train-21092", "mrqa_squad-train-71323", "mrqa_squad-train-29621", "mrqa_squad-train-56758", "mrqa_squad-train-79258", "mrqa_squad-train-10517", "mrqa_squad-train-77966", "mrqa_squad-train-26695", "mrqa_squad-train-44713", "mrqa_squad-train-57102", "mrqa_squad-train-44549", "mrqa_squad-train-37728", "mrqa_squad-train-54033", "mrqa_squad-train-40751", "mrqa_squad-train-60382", "mrqa_squad-train-73535", "mrqa_squad-train-50560", "mrqa_squad-train-51020", "mrqa_squad-train-43863", "mrqa_squad-train-38302", "mrqa_squad-train-10935", "mrqa_triviaqa-validation-5044", "mrqa_squad-validation-3998", "mrqa_searchqa-validation-3597", "mrqa_newsqa-validation-3088", "mrqa_naturalquestions-validation-7310", "mrqa_triviaqa-validation-2902", "mrqa_naturalquestions-validation-10411", "mrqa_searchqa-validation-5928", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-2309", "mrqa_squad-validation-4572", "mrqa_newsqa-validation-2179", "mrqa_searchqa-validation-2714", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-4674", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1175", "mrqa_hotpotqa-validation-4418", "mrqa_naturalquestions-validation-844", "mrqa_squad-validation-1136", "mrqa_squad-validation-10143", "mrqa_squad-validation-8093", "mrqa_squad-validation-2835", "mrqa_naturalquestions-validation-10311"], "EFR": 1.0, "Overall": 0.747359375}, {"timecode": 25, "before_eval_results": {"predictions": ["Thoreau", "Thomas Piketty", "1,548", "zoning and building code requirements", "Science and Discovery", "New York", "key Central line", "Vietnam", "a non-speaking character", "bluebird", "wherry", "300", "1894", "The pyramids", "Neil Morrissey", "jodie Foster", "Billie Holiday", "The National Council for the Unmarried Mother and her Child", "Phil Mickelson", "jon pertina flessibile Acquisto", "Len Deighton", "tARTAN", "Alex Garland", "G. Ramon", "Dionysus", "Benjamin Disraeli", "Johannesburg", "Martin Luther King", "Bridgeport", "thomas Jeremiah Greengrass", "a single point", "blue", "Albert Reynolds", "Newfoundland", "Eddie Cochran", "Alessandro Giuseppe Antonio Anastasio Volta", "OutKast", "Wanderers", "taxman", "Biafra secession", "Tina Turner", "Flint", "Cuba", "dove", "Heston Blumenthal", "Harold II", "jack Johnson", "Ritchie Valens", "posh", "Honda", "Bristol", "Gargantua", "Krypton", "Cyanea capillata", "if the concentration of a compound exceeds its solubility", "The Bob Edwards Show", "Nicholas John \" Nick\" McCarthy", "Paul John Manafort Jr.", "his salary", "Jennifer Aniston, Demi Moore and Alicia Keys", "80", "Maldives", "Matt Leinart", "Stephen Hawking"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6640024038461538}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.923076923076923, 0.0, 1.0, 0.25, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-3418", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-4715", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2965", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-3714", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-439"], "SR": 0.59375, "CSR": 0.5889423076923077, "retrieved_ids": ["mrqa_squad-train-11530", "mrqa_squad-train-80867", "mrqa_squad-train-56599", "mrqa_squad-train-51884", "mrqa_squad-train-9717", "mrqa_squad-train-48791", "mrqa_squad-train-19770", "mrqa_squad-train-50750", "mrqa_squad-train-61013", "mrqa_squad-train-77041", "mrqa_squad-train-63120", "mrqa_squad-train-59209", "mrqa_squad-train-51779", "mrqa_squad-train-13500", "mrqa_squad-train-42840", "mrqa_squad-train-75378", "mrqa_squad-train-48327", "mrqa_squad-train-30409", "mrqa_squad-train-55730", "mrqa_squad-train-29149", "mrqa_squad-train-62679", "mrqa_squad-train-34344", "mrqa_squad-train-14469", "mrqa_squad-train-33585", "mrqa_squad-validation-7338", "mrqa_naturalquestions-validation-9691", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-16908", "mrqa_newsqa-validation-1514", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-6439", "mrqa_naturalquestions-validation-10037", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-9183", "mrqa_newsqa-validation-2429", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-585", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-2735", "mrqa_naturalquestions-validation-4740", "mrqa_searchqa-validation-15194", "mrqa_squad-validation-10388", "mrqa_squad-validation-10369", "mrqa_naturalquestions-validation-3686", "mrqa_newsqa-validation-2068"], "EFR": 1.0, "Overall": 0.7473978365384615}, {"timecode": 26, "before_eval_results": {"predictions": ["paid professionals", "centre of Basel", "\"we want to practice Christian love toward them and pray that they convert,\"", "Informal rule", "anxiety disorders", "raven", "helium", "John Logie Baird", "city of Venice", "Pickwick", "the Titanic", "Benjamin Britten", "taekwondo", "Morocco", "Rome", "lola", "skull", "Mike Gatting", "bury", "oxygen", "jubnium", "Andy D'urso", "Venus", "wigan Athletic", "French", "Jupiter", "Triumph and Disaster", "the Beatles", "city of chicago", "city", "Australia", "cerebrospinal fluid", "Charlie Chaplin", "Cambridge", "the Netherlands", "Vladivostok", "boiling", "beetles", "phoenicia", "Norwegian", "the south", "beard", "lichfield", "lithium", "salema", "America", "chancery", "tem\u00b7per\u00b7a", "Brazil", "peacock", "mainland China", "china", "6ft 1in", "Judith Cynthia Aline Keppel", "94 by 50 feet", "Philip Rivers", "photographs, film and television", "March 17, 2015", "AbdulMutallab", "an eye for an eye", "three", "provincial hymn", "Dennis Miller", "May"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6178385416666667}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.5, 0.375, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9196", "mrqa_squad-validation-2368", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-7030", "mrqa_triviaqa-validation-5278", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-1605", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-6046", "mrqa_hotpotqa-validation-3563", "mrqa_searchqa-validation-8872"], "SR": 0.59375, "CSR": 0.5891203703703703, "retrieved_ids": ["mrqa_squad-train-5292", "mrqa_squad-train-71801", "mrqa_squad-train-47147", "mrqa_squad-train-47577", "mrqa_squad-train-74170", "mrqa_squad-train-45529", "mrqa_squad-train-70057", "mrqa_squad-train-29871", "mrqa_squad-train-48686", "mrqa_squad-train-10132", "mrqa_squad-train-78317", "mrqa_squad-train-38922", "mrqa_squad-train-34477", "mrqa_squad-train-53041", "mrqa_squad-train-67965", "mrqa_squad-train-1715", "mrqa_squad-train-44849", "mrqa_squad-train-45933", "mrqa_squad-train-11502", "mrqa_squad-train-57842", "mrqa_squad-train-71255", "mrqa_squad-train-77421", "mrqa_squad-train-35537", "mrqa_squad-train-51542", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-3721", "mrqa_squad-validation-9717", "mrqa_hotpotqa-validation-5529", "mrqa_naturalquestions-validation-2462", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-6463", "mrqa_triviaqa-validation-2199", "mrqa_naturalquestions-validation-5938", "mrqa_squad-validation-3770", "mrqa_searchqa-validation-11388", "mrqa_hotpotqa-validation-2522", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-386", "mrqa_searchqa-validation-12611", "mrqa_newsqa-validation-1805", "mrqa_squad-validation-7845", "mrqa_searchqa-validation-3613", "mrqa_squad-validation-1906", "mrqa_searchqa-validation-123", "mrqa_triviaqa-validation-6558", "mrqa_newsqa-validation-1456", "mrqa_squad-validation-7296", "mrqa_newsqa-validation-394"], "EFR": 1.0, "Overall": 0.747433449074074}, {"timecode": 27, "before_eval_results": {"predictions": ["CBS", "15th", "60%", "CBS Sports", "two", "not", "the National Restaurant Association", "to \"wipe out\" the United States if provoked.", "blew himself up.", "Pittsburgh", "\"project work\"", "paintings", "that things are going well for them personally.", "Sonia Sotomayor", "Mandi Hamlin", "750", "fire", "Lana Clarkson", "Expedia", "Mildred", "severe", "jobs", "Swat Valley", "Rawalpindi", "Santaquin City, Utah,", "Sunday,", "four", "East Java", "Johannesburg", "nearly $2 billion", "Six members of Zoe's Ark were arrested last week as they tried to put the children on a plane to France,", "2002", "the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "\"It has never been the policy of this president or this administration to torture.\"", "Herman Thomas", "Cairo", "the underprivileged.", "Elisabeth", "that they don't feelMisty Cummings, was the last person known to have seen Haleigh the night she disappeared from the family's rented mobile home.", "wore out of either heavy flannel or wool -- fabrics that would not be transparent when wet -- and covered the entire body from neck to toe.", "Melbourne", "into the Southeast", "Sunday", "$273 million", "Salt Lake City, Utah,", "millionaire's surtax", "an animal tranquilizer", "Section 60", "NATO's International Security Assistance Force", "Jaime Andrade", "1994,", "dance", "Santiago Ram\u00f3n y Cajal", "Pittsburgh", "the official residence of the President of the Russian Federation", "danish", "jethro Tull", "Sutherland Brothers", "Lin-Manuel Miranda", "15,024", "novelist and poet", "mantle", "beatbox", "marshmallows"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5814448572261073}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4166666666666667, 0.2857142857142857, 0.9, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0909090909090909, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_squad-validation-526", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-3559", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-5756", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-4411", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1864", "mrqa_searchqa-validation-13251"], "SR": 0.453125, "CSR": 0.5842633928571428, "retrieved_ids": ["mrqa_squad-train-79887", "mrqa_squad-train-55425", "mrqa_squad-train-4621", "mrqa_squad-train-6805", "mrqa_squad-train-81975", "mrqa_squad-train-8561", "mrqa_squad-train-71103", "mrqa_squad-train-28688", "mrqa_squad-train-62320", "mrqa_squad-train-11415", "mrqa_squad-train-42767", "mrqa_squad-train-43877", "mrqa_squad-train-6644", "mrqa_squad-train-16924", "mrqa_squad-train-35939", "mrqa_squad-train-80652", "mrqa_squad-train-82572", "mrqa_squad-train-7107", "mrqa_squad-train-50056", "mrqa_squad-train-13337", "mrqa_squad-train-21283", "mrqa_squad-train-65940", "mrqa_squad-train-65595", "mrqa_squad-train-38591", "mrqa_naturalquestions-validation-3485", "mrqa_squad-validation-1827", "mrqa_searchqa-validation-14655", "mrqa_naturalquestions-validation-7351", "mrqa_squad-validation-802", "mrqa_squad-validation-1566", "mrqa_naturalquestions-validation-1682", "mrqa_triviaqa-validation-7463", "mrqa_naturalquestions-validation-2476", "mrqa_squad-validation-2145", "mrqa_naturalquestions-validation-5721", "mrqa_triviaqa-validation-1605", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-3613", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-423", "mrqa_hotpotqa-validation-2198", "mrqa_naturalquestions-validation-8441", "mrqa_searchqa-validation-1053", "mrqa_squad-validation-2094", "mrqa_squad-validation-7332", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-13686"], "EFR": 0.9714285714285714, "Overall": 0.7407477678571428}, {"timecode": 28, "before_eval_results": {"predictions": ["Denver's Executive Vice President of Football Operations and General Manager", "illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins", "Pittsburgh", "Cress", "molecular clouds in interstellar space", "Jamie Elman as Rancis Fluggerbutter", "the highway between the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "Ram Nath Kovind", "Senator Joseph McCarthy", "200 lakh rupees ''", "members of the gay ( LGBT ) community", "Copper ( Cu ), silver ( Ag ), and gold ( Au )", "Wembley Stadium", "royal society", "1776", "Continental drift", "Julie Adams", "a combination of genetics and the male hormone dihydrotestosterone", "Jonathan Cheban", "Norman Greenbaum", "De Wayne Warren as Jarius `` G", "Thirty years after the Galactic Civil War,", "about restoring someone's faith in love and family relationships", "Obi - Wan Kenobi", "April 15, 2018", "April 17, 1982", "the Speaker of the House of Representatives", "London, United Kingdom", "a minority report", "the Near East", "Article One of the United States Constitution", "Club Bijou on Chapel Street", "about 1500 BC", "in the central plains", "China ( formerly the Republic of China ), Russia (formerly the Soviet Union ), France, the United Kingdom, and the United States", "costume party", "directly into the bloodstream", "Kenny Anderson", "beneath the liver", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "Nathan Hale", "Jesse Frederick James Conaway", "the naos", "near temples", "the port of Nueva Espa\u00f1a to the Spanish coast", "September 19, 2017", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "it was first published on November 12, 1976 by Ballantine Books", "Butter Island off North Haven, Maine in the Penobscot Bay", "wintertime", "Tony Rydinger", "Moira Kelly", "Flanagan and Allen", "islands of Miquelon and Saint Pierre", "Mediterranean Sea", "the Manor of More", "Jeffrey Perry", "the Northern Ireland Assembly for Fermanagh and South Tyrone", "Michael Partain,", "1960", "\"The Orchid thief\"", "took an outer cloak", "\"the establishment in Palestine of a national home for the Jewish people,\"", "Edgar Allan Poe"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6061613376411171}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.47058823529411764, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.7777777777777778, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8181818181818181, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-386", "mrqa_squad-validation-6848", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1135", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-3145", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-5358", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1570", "mrqa_searchqa-validation-4495", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-12829"], "SR": 0.484375, "CSR": 0.5808189655172413, "retrieved_ids": ["mrqa_squad-train-41499", "mrqa_squad-train-4592", "mrqa_squad-train-10200", "mrqa_squad-train-5959", "mrqa_squad-train-36599", "mrqa_squad-train-61784", "mrqa_squad-train-70048", "mrqa_squad-train-86443", "mrqa_squad-train-57022", "mrqa_squad-train-46743", "mrqa_squad-train-7331", "mrqa_squad-train-71970", "mrqa_squad-train-82865", "mrqa_squad-train-10805", "mrqa_squad-train-7814", "mrqa_squad-train-14667", "mrqa_squad-train-76397", "mrqa_squad-train-77847", "mrqa_squad-train-42698", "mrqa_squad-train-15172", "mrqa_squad-train-30093", "mrqa_squad-train-58374", "mrqa_squad-train-26357", "mrqa_squad-train-80776", "mrqa_triviaqa-validation-7220", "mrqa_searchqa-validation-3451", "mrqa_squad-validation-4452", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-2016", "mrqa_squad-validation-3863", "mrqa_newsqa-validation-1985", "mrqa_triviaqa-validation-3263", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-2143", "mrqa_searchqa-validation-3525", "mrqa_naturalquestions-validation-7464", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-7401", "mrqa_squad-validation-9876", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-8983", "mrqa_newsqa-validation-1759", "mrqa_squad-validation-5887", "mrqa_triviaqa-validation-3864", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-3463", "mrqa_squad-validation-2911"], "EFR": 0.9393939393939394, "Overall": 0.7336519559822361}, {"timecode": 29, "before_eval_results": {"predictions": ["Sophocles' play Antigone", "Meuse", "1806", "New Delhi", "MacFarlane", "Super Bowl LII", "Hon July Moyo", "many forested parts of the world", "Narendra Modi", "forests and animals", "Aaron Harrison", "The White House Executive chef", "Michael Crawford", "9 February 2018", "the red bone marrow of large bones", "Juice Newton", "Pangaea", "Jonathan Breck", "dermis", "S\u00e9rgio Mendes", "Ming", "201", "Chuck Noland", "Montreal Canadiens", "Britney Spears", "Waylon Jennings", "Nancy Jean Cartwright", "Coldplay", "Ephesus ( Revelation 2 : 1 - 7 )", "the New York Yankees", "1996", "when energy from light is absorbed by proteins", "United States customary units", "Joe Spano", "Michael Moriarty", "Rock Island, Illinois", "2002 -- 03", "September 1959", "Klaus Baudelaire", "Bonnie Lipton", "`` Tip and Ty ''", "0.05 ( 5 % )", "posthumously in 1890", "Rebekah", "the kitchen", "rizal", "Ernest Rutherford", "Napoleon", "the 12th century", "the slopes of Mt. Hood in Oregon", "Norman Pritchard", "2014", "polecat", "the Big Bang", "King Henry VI", "October 13, 1980", "250cc world championship", "Polihale State Park", "the Defense of Marriage Act", "Bronx", "9 million", "florida", "abacus", "Cyrus the Younger"], "metric_results": {"EM": 0.46875, "QA-F1": 0.669789601250199}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, false], "QA-F1": [0.5, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 0.33333333333333337, 1.0, 1.0, 0.608695652173913, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.14285714285714288, 0.6666666666666666, 0.08, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6638", "mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3760", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-5106", "mrqa_hotpotqa-validation-1489", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-3606", "mrqa_searchqa-validation-5573", "mrqa_searchqa-validation-2555"], "SR": 0.46875, "CSR": 0.5770833333333334, "retrieved_ids": ["mrqa_squad-train-48573", "mrqa_squad-train-29138", "mrqa_squad-train-52975", "mrqa_squad-train-40017", "mrqa_squad-train-67363", "mrqa_squad-train-48000", "mrqa_squad-train-38728", "mrqa_squad-train-60901", "mrqa_squad-train-45178", "mrqa_squad-train-52907", "mrqa_squad-train-28618", "mrqa_squad-train-41437", "mrqa_squad-train-37448", "mrqa_squad-train-54787", "mrqa_squad-train-21968", "mrqa_squad-train-9457", "mrqa_squad-train-56080", "mrqa_squad-train-37117", "mrqa_squad-train-82253", "mrqa_squad-train-55360", "mrqa_squad-train-67916", "mrqa_squad-train-44839", "mrqa_squad-train-48074", "mrqa_squad-train-19566", "mrqa_triviaqa-validation-6757", "mrqa_searchqa-validation-10372", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-2265", "mrqa_searchqa-validation-3618", "mrqa_naturalquestions-validation-4865", "mrqa_searchqa-validation-11137", "mrqa_newsqa-validation-216", "mrqa_naturalquestions-validation-2124", "mrqa_searchqa-validation-2052", "mrqa_hotpotqa-validation-5742", "mrqa_naturalquestions-validation-8356", "mrqa_newsqa-validation-2591", "mrqa_triviaqa-validation-4992", "mrqa_squad-validation-6848", "mrqa_searchqa-validation-16076", "mrqa_hotpotqa-validation-1471", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-3998", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-7330", "mrqa_naturalquestions-validation-2452", "mrqa_triviaqa-validation-254"], "EFR": 0.9705882352941176, "Overall": 0.7391436887254902}, {"timecode": 30, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-392", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-974", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-13874", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15740", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-1649", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9269", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10181", "mrqa_squad-validation-10268", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1765", "mrqa_squad-validation-1791", "mrqa_squad-validation-1848", "mrqa_squad-validation-1890", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-2019", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2243", "mrqa_squad-validation-2411", "mrqa_squad-validation-2456", "mrqa_squad-validation-247", "mrqa_squad-validation-2545", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2742", "mrqa_squad-validation-305", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3507", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3718", "mrqa_squad-validation-3770", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-386", "mrqa_squad-validation-3863", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3986", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-4046", "mrqa_squad-validation-4054", "mrqa_squad-validation-4175", "mrqa_squad-validation-4213", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4629", "mrqa_squad-validation-4883", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5097", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5588", "mrqa_squad-validation-5692", "mrqa_squad-validation-5724", "mrqa_squad-validation-5781", "mrqa_squad-validation-5818", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-6019", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6353", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6543", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6965", "mrqa_squad-validation-6973", "mrqa_squad-validation-6999", "mrqa_squad-validation-7039", "mrqa_squad-validation-71", "mrqa_squad-validation-7192", "mrqa_squad-validation-7368", "mrqa_squad-validation-7426", "mrqa_squad-validation-7521", "mrqa_squad-validation-7612", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7814", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-829", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9176", "mrqa_squad-validation-9196", "mrqa_squad-validation-942", "mrqa_squad-validation-9445", "mrqa_squad-validation-957", "mrqa_squad-validation-9614", "mrqa_squad-validation-9764", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4465", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7461"], "OKR": 0.89453125, "KG": 0.478125, "before_eval_results": {"predictions": ["flammable cabin and space suit materials", "1992", "four", "Genesis", "REITs", "the bayou", "the carat", "Mission: Impossible", "presbyter", "Edinburgh", "Teha'amana", "the Baha de Darwin,", "Mark Twain", "Battle of Chancellorsville", "boxing", "Wii", "the Suez Canal", "Dave Matthews Band", "a nightingale", "a shift in your remaining teeth", "The One Where Jason's Mom Did It", "Kinko's", "a platypus", "photon", "the skull", "Cherokee", "nekropolis", "Eleanor Roosevelt", "oyster", "\"The World's Leading\" fortune", "Johnny Cash", "bamboo", "Isaac Newton", "the Unabomber", "Narnia", "Freud", "Burma Ruby stone", "librettos", "pohjola", "Who's Afraid of Virginia Woolf", "Medium", "beefalo", "an American sitcom", "Botswana", "Susan B. Anthony dollar", "Mattel", "Little Red Riding Hood", "spint", "melanin", "Slavic", "nerves", "fried in oil", "inflammation such as Candida albicans and bacteria such as Staph. aureus", "Castleford", "usually in May", "henry florida", "Sahara desert", "Sherlock Holmes", "\"The Guest\"", "Headless Body in Topless Bar", "political correctness", "Hanin Zoabi", "Princess Diana", "homicide"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6383928571428572}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-3542", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-12684", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-7004", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-16480", "mrqa_searchqa-validation-5586", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-382", "mrqa_naturalquestions-validation-2666", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-6237", "mrqa_hotpotqa-validation-1239"], "SR": 0.59375, "CSR": 0.5776209677419355, "retrieved_ids": ["mrqa_squad-train-62145", "mrqa_squad-train-85443", "mrqa_squad-train-77108", "mrqa_squad-train-18346", "mrqa_squad-train-14008", "mrqa_squad-train-75468", "mrqa_squad-train-60900", "mrqa_squad-train-69435", "mrqa_squad-train-2914", "mrqa_squad-train-71613", "mrqa_squad-train-42501", "mrqa_squad-train-76980", "mrqa_squad-train-44776", "mrqa_squad-train-6147", "mrqa_squad-train-48398", "mrqa_squad-train-45797", "mrqa_squad-train-6633", "mrqa_squad-train-27861", "mrqa_squad-train-31099", "mrqa_squad-train-67785", "mrqa_squad-train-21068", "mrqa_squad-train-73054", "mrqa_squad-train-36513", "mrqa_squad-train-61538", "mrqa_searchqa-validation-15995", "mrqa_naturalquestions-validation-473", "mrqa_searchqa-validation-16908", "mrqa_triviaqa-validation-1314", "mrqa_newsqa-validation-2735", "mrqa_triviaqa-validation-7401", "mrqa_newsqa-validation-1456", "mrqa_squad-validation-3863", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-3288", "mrqa_squad-validation-4452", "mrqa_naturalquestions-validation-10460", "mrqa_hotpotqa-validation-2205", "mrqa_searchqa-validation-6234", "mrqa_squad-validation-3435", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-4674", "mrqa_hotpotqa-validation-1534", "mrqa_squad-validation-10339", "mrqa_newsqa-validation-1426", "mrqa_triviaqa-validation-5387", "mrqa_newsqa-validation-2491", "mrqa_naturalquestions-validation-9330", "mrqa_hotpotqa-validation-2198"], "EFR": 1.0, "Overall": 0.747867943548387}, {"timecode": 31, "before_eval_results": {"predictions": ["the West", "Begter", "beginning in 2016", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor )", "University of Oxford", "July 1, 1923", "a pH indicator, a color marker, and a dye", "winter", "Freedom Day 27 April 2000", "the present Indian constitutive state of Meghalaya ( formerly Assam )", "the five major divisions of the brain", "Janie Crawford, an African - American woman in her early forties", "Elliot Scheiner, and RobJac", "the straight - line distance from A to B", "786", "Office of Inspector General", "Blind carbon copy to tertiary recipients who receive the message", "round, bottom round, and top round", "1957", "Aldis Hodge", "An error does not count as a hit", "Andreas Vesalius", "Moscazzano", "Kristy Swanson", "bacteria", "Asuka", "Jay Baruchel", "Oceania", "revolution or orbital revolution", "Houston Astros", "Six Degrees of Separation", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "the retina", "in the fascia surrounding skeletal muscle", "Pangaea", "2017", "near the inner rim of the Orion Arm", "Ricky Nelson", "a special marker called a `` dabber '' or `` dauber ''", "Debbie Gibson", "Death Eaters", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE )", "a column - like or oval ( egg - shaped ) symbol of Shiva", "the winter", "Algeria", "the King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "Harlem River", "1998", "R.E.M.", "332", "above the light source and under the sample in an upright microscope", "Georgia Bulldogs football team", "Illinois", "flodden", "Ireland", "Travis County", "Boston Bruins", "Adam Dawes", "Republican", "zed", "in a tenement in the Mumbai suburb of Chembur,", "a dummy", "Aristotle", "nothing gained"], "metric_results": {"EM": 0.5, "QA-F1": 0.6259151999960824}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.5, 0.5, 1.0, 0.0, 0.33333333333333337, 0.15384615384615385, 0.3636363636363636, 0.0, 0.0, 0.6666666666666666, 0.2, 0.5, 0.0, 1.0, 0.0, 0.05128205128205128, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6428571428571429, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.923076923076923, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-9791", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-5599", "mrqa_triviaqa-validation-6051", "mrqa_hotpotqa-validation-5831", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3518"], "SR": 0.5, "CSR": 0.5751953125, "retrieved_ids": ["mrqa_squad-train-41584", "mrqa_squad-train-30375", "mrqa_squad-train-80022", "mrqa_squad-train-82446", "mrqa_squad-train-50978", "mrqa_squad-train-5174", "mrqa_squad-train-65039", "mrqa_squad-train-9148", "mrqa_squad-train-22603", "mrqa_squad-train-28954", "mrqa_squad-train-73235", "mrqa_squad-train-46065", "mrqa_squad-train-5108", "mrqa_squad-train-48599", "mrqa_squad-train-59750", "mrqa_squad-train-51007", "mrqa_squad-train-59054", "mrqa_squad-train-54041", "mrqa_squad-train-71006", "mrqa_squad-train-19552", "mrqa_squad-train-36107", "mrqa_squad-train-18357", "mrqa_squad-train-1068", "mrqa_squad-train-24418", "mrqa_squad-validation-1780", "mrqa_newsqa-validation-593", "mrqa_naturalquestions-validation-276", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-3263", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-2568", "mrqa_naturalquestions-validation-10412", "mrqa_hotpotqa-validation-1576", "mrqa_naturalquestions-validation-3686", "mrqa_squad-validation-7051", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-81", "mrqa_triviaqa-validation-2000", "mrqa_naturalquestions-validation-588", "mrqa_triviaqa-validation-2735", "mrqa_squad-validation-7364", "mrqa_searchqa-validation-2141", "mrqa_naturalquestions-validation-9002", "mrqa_hotpotqa-validation-3157", "mrqa_hotpotqa-validation-511", "mrqa_squad-validation-2094", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-8760"], "EFR": 0.875, "Overall": 0.7223828125}, {"timecode": 32, "before_eval_results": {"predictions": ["magnitude and direction", "2003", "1982", "National Aviation Hall of Fame class of 2001", "Giotto", "1985", "26,000", "Lakshmibai", "Championship", "French", "2009", "a role-playing game or wargame campaign", "Bonobo", "Robin David Segal", "Greg Gorman and Helmut Newton", "Shameless", "stolperstein", "1901", "Carl Zeiss AG", "Premier League club Arsenal", "Bambi, a Life in the Woods", "Robert \"Bobby\" Germaine", "2004", "DTM", "one", "Love Child", "the Sun", "Greg Hertz", "1984 in Kolkata", "The Walking Dead", "Ted Nugent", "jewelry designer", "Gust Avrakotos", "Maleficent", "Coll\u00e8ge de France", "Miami-Dade County", "Marty Ingels", "1945", "Edward R. Murrow", "Conservatorio Verdi", "Mindy Kaling", "June 10, 1982", "beer and soft drinks", "Liga MX", "Donald Duck", "The School Boys", "Lord Chancellor of England", "Taoiseach", "English Electric Canberra", "Richa Sharma", "48,982", "The Sound of Music", "53", "Big Ten Conference Champions Stanford Cardinal", "Frank Langella", "an elephant", "a cuckoo", "Hydrochloric acid", "Stansted", "homicide", "maintain an \"aesthetic environment\" and ensure public safety", "Marshal ptain", "a tank", "Hannah Montana"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7214543269230769}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-4870", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-1030", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4655", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-3926", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3726", "mrqa_searchqa-validation-4641"], "SR": 0.640625, "CSR": 0.5771780303030303, "retrieved_ids": ["mrqa_squad-train-9034", "mrqa_squad-train-12297", "mrqa_squad-train-78627", "mrqa_squad-train-69785", "mrqa_squad-train-36631", "mrqa_squad-train-7619", "mrqa_squad-train-71555", "mrqa_squad-train-80509", "mrqa_squad-train-48198", "mrqa_squad-train-68035", "mrqa_squad-train-41357", "mrqa_squad-train-5445", "mrqa_squad-train-7832", "mrqa_squad-train-47370", "mrqa_squad-train-16500", "mrqa_squad-train-15705", "mrqa_squad-train-52891", "mrqa_squad-train-14817", "mrqa_squad-train-10545", "mrqa_squad-train-37822", "mrqa_squad-train-33105", "mrqa_squad-train-75529", "mrqa_squad-train-10674", "mrqa_squad-train-46392", "mrqa_searchqa-validation-14569", "mrqa_newsqa-validation-1032", "mrqa_naturalquestions-validation-4667", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-9390", "mrqa_naturalquestions-validation-9791", "mrqa_squad-validation-739", "mrqa_naturalquestions-validation-9737", "mrqa_searchqa-validation-123", "mrqa_triviaqa-validation-7212", "mrqa_squad-validation-1037", "mrqa_newsqa-validation-445", "mrqa_squad-validation-2595", "mrqa_naturalquestions-validation-3392", "mrqa_searchqa-validation-8665", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-9436", "mrqa_newsqa-validation-3476", "mrqa_searchqa-validation-14371", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1078", "mrqa_searchqa-validation-13600", "mrqa_naturalquestions-validation-10684"], "EFR": 1.0, "Overall": 0.7477793560606061}, {"timecode": 33, "before_eval_results": {"predictions": ["CEPR", "Lehramtstudien", "CTV Television Network", "13\u20133", "American", "July 25 to August 4", "1958", "Norway", "twenty-three", "Crips", "The Crowned Prince of the Philadelphia Mob", "Kentucky Derby", "Charles Edward Stuart", "historic buildings, arts, and published works", "August 9, 2017", "Batman", "eastern Tennessee,", "G\u00e9rard Depardieu", "books, films and other media", "King Duncan", "Europop", "1835", "Mayor Ed Lee", "Ghana", "Norwegian", "Dutch", "1976", "January 23, 1898", "Motorised quadricycle", "30.9%", "Charlyn Marie \" Chan\" Marshall", "1968", "76,416", "Father Dougal McGuire", "June 17, 2007", "Karl Haushofer", "The United States of America", "The Ryukyuan people (\u7409\u7403\u6c11\u65cf, Ry\u016bky\u016b minzoku, Okinawan: \"Ruuchuu minzuku\")", "coaxial", "September 14, 1877, Triebendorf \u2013 December 6, 1933, Duchcov", "international producers", "1961", "1952", "Indian", "one child, Lisa Brennan-Jobs", "Pablo Escobar", "ZZ Top", "Larry Wayne Gatlin", "Russian Empire", "Hydrogen", "Blue Ridge Parkway", "King of Cool", "subduction zone", "Barry Bonds", "Owen Vaccaro", "Machu Picchu", "Exile", "a downtown Subway late at night", "normal maritime traffic", "Ma Khin Khin Leh,", "$1.45 billion", "onomatopoeia", "Singapore", "the femur"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7208333333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-3703", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-1673", "mrqa_searchqa-validation-15477"], "SR": 0.640625, "CSR": 0.5790441176470589, "retrieved_ids": ["mrqa_squad-train-53535", "mrqa_squad-train-12422", "mrqa_squad-train-6470", "mrqa_squad-train-65012", "mrqa_squad-train-12558", "mrqa_squad-train-18439", "mrqa_squad-train-85666", "mrqa_squad-train-76947", "mrqa_squad-train-51", "mrqa_squad-train-32575", "mrqa_squad-train-44788", "mrqa_squad-train-22370", "mrqa_squad-train-49381", "mrqa_squad-train-54538", "mrqa_squad-train-83471", "mrqa_squad-train-57455", "mrqa_squad-train-31286", "mrqa_squad-train-43702", "mrqa_squad-train-37364", "mrqa_squad-train-43371", "mrqa_squad-train-79341", "mrqa_squad-train-62451", "mrqa_squad-train-9121", "mrqa_squad-train-30137", "mrqa_squad-validation-1765", "mrqa_searchqa-validation-4367", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5485", "mrqa_searchqa-validation-3525", "mrqa_naturalquestions-validation-779", "mrqa_squad-validation-739", "mrqa_triviaqa-validation-6896", "mrqa_searchqa-validation-4393", "mrqa_squad-validation-2538", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-7598", "mrqa_triviaqa-validation-4844", "mrqa_naturalquestions-validation-3186", "mrqa_newsqa-validation-4030", "mrqa_searchqa-validation-9730", "mrqa_naturalquestions-validation-5348", "mrqa_newsqa-validation-439", "mrqa_squad-validation-6439", "mrqa_naturalquestions-validation-10614", "mrqa_triviaqa-validation-2975"], "EFR": 0.9565217391304348, "Overall": 0.7394569213554987}, {"timecode": 34, "before_eval_results": {"predictions": ["Manning", "Pushing against an object on a frictional surface", "20 million", "A simple iron boar crest", "Vienna", "period dependent", "the Harpe brothers", "Bill Clinton", "Dirk Werner Nowitzki", "Detroit, Michigan,", "Bury St Edmunds, Suffolk, England", "novelty songs, comedy, and strange or unusual recordings", "Mahoning County", "16 November 1973", "An Analysis of the Alinsky Model", "Hungary", "New York", "The Indianapolis Times", "400 MW", "Mauritian", "\"Household Words\"", "Gatwick Airport", "Tokyo's Narita International Airport", "Minette Walters", "CTV", "Firestorm", "2013", "Les Miles", "40 Days and 40 Nights", "James Tinling", "2014", "Louis King", "gull-wing doors", "Terry Malloy", "Operation Neptune", "Attack the Block", "House of Commons", "Hessians", "Battle of Chester", "Wayne County, Michigan", "Samoa", "mistress of the Robes", "Eleanor of Aquitaine", "Barack Obama", "August 17, 2017", "Guardians of the Galaxy Vol. 2", "President of the United States", "1963", "Bologna Process", "Paris", "Nebraska Cornhuskers", "Salman Rushdie", "Internal Revenue Service", "the Hongwu Emperor of the Ming Dynasty", "commemorating fealty and filial piety", "Chihuahua", "Arkansas", "throat", "1979", "father,", "$8.8 million", "Red Heat", "Miriam Makeba", "tooth"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7376860119047619}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-384", "mrqa_squad-validation-10316", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-4143", "mrqa_triviaqa-validation-2316", "mrqa_newsqa-validation-501", "mrqa_searchqa-validation-9394"], "SR": 0.65625, "CSR": 0.58125, "retrieved_ids": ["mrqa_squad-train-86412", "mrqa_squad-train-21557", "mrqa_squad-train-73695", "mrqa_squad-train-35646", "mrqa_squad-train-8687", "mrqa_squad-train-5113", "mrqa_squad-train-34899", "mrqa_squad-train-10010", "mrqa_squad-train-6761", "mrqa_squad-train-19870", "mrqa_squad-train-71801", "mrqa_squad-train-1791", "mrqa_squad-train-6535", "mrqa_squad-train-74872", "mrqa_squad-train-17791", "mrqa_squad-train-23490", "mrqa_squad-train-47588", "mrqa_squad-train-79905", "mrqa_squad-train-30841", "mrqa_squad-train-32350", "mrqa_squad-train-80066", "mrqa_squad-train-80995", "mrqa_squad-train-13091", "mrqa_squad-train-6777", "mrqa_squad-validation-4838", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-6998", "mrqa_newsqa-validation-1671", "mrqa_squad-validation-10339", "mrqa_searchqa-validation-14655", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-2404", "mrqa_naturalquestions-validation-1336", "mrqa_searchqa-validation-14569", "mrqa_naturalquestions-validation-6106", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-442", "mrqa_naturalquestions-validation-10490", "mrqa_squad-validation-2328", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-3919", "mrqa_triviaqa-validation-7414", "mrqa_naturalquestions-validation-7067", "mrqa_hotpotqa-validation-5328", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-12996", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-2495"], "EFR": 1.0, "Overall": 0.74859375}, {"timecode": 35, "before_eval_results": {"predictions": ["DuMont Television Network", "Mount Kenya", "Albany", "1952", "3 May 1958", "1986 to 2013", "Ronald Wilson Reagan", "Chiltern Hills", "\"Ted\"", "Bay of Fundy", "CD Castell\u00f3n", "2001", "Sean Yseult", "country", "she was a member of the Hawaii House of Representatives from 1990-96.", "Operation Watchtower", "Paul W. S. Anderson", "15 February 1970", "Talib Kweli", "Shooter Jennings", "downtown Cincinnati,", "Bad Moon Rising", "Kris Kristofferson", "381.6 days", "Atomic Kitten", "Matt Stone", "Matthew Edward Gonzalez", "The Gold Coast", "1898", "\u00c6thelred the Unready", "PlayStation 4", "Malta", "1966", "Key West", "Europe", "Black Mountain College", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies.", "Fabio Cannavaro", "\"Chappelle's Show\"", "Prince George's County", "Pittsburgh, Pennsylvania", "1891", "Austral L\u00edneas A\u00e9reas", "Gainsborough Trinity", "Los Angeles", "October 13, 1980", "a water sprite", "India", "Syracuse University", "FIFA Women's World Cup", "Orange County", "76,416", "found in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Quantitative psychological research", "Brian Steele", "Deep Blue", "Albert Reynolds", "George W. Bush", "U.S. senators", "California-based Current TV", "two", "bath", "The Lost Boys", "succotash"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6935257782997144}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9787234042553191, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-257", "mrqa_naturalquestions-validation-553", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-2595", "mrqa_searchqa-validation-16021"], "SR": 0.59375, "CSR": 0.5815972222222222, "retrieved_ids": ["mrqa_squad-train-33814", "mrqa_squad-train-31731", "mrqa_squad-train-56045", "mrqa_squad-train-14276", "mrqa_squad-train-49317", "mrqa_squad-train-83012", "mrqa_squad-train-77921", "mrqa_squad-train-38650", "mrqa_squad-train-12804", "mrqa_squad-train-32595", "mrqa_squad-train-30048", "mrqa_squad-train-26372", "mrqa_squad-train-7455", "mrqa_squad-train-26410", "mrqa_squad-train-62553", "mrqa_squad-train-45233", "mrqa_squad-train-36570", "mrqa_squad-train-35166", "mrqa_squad-train-75587", "mrqa_squad-train-12842", "mrqa_squad-train-56138", "mrqa_squad-train-62156", "mrqa_squad-train-53241", "mrqa_squad-train-35838", "mrqa_triviaqa-validation-3563", "mrqa_newsqa-validation-1412", "mrqa_triviaqa-validation-4844", "mrqa_squad-validation-5588", "mrqa_naturalquestions-validation-4309", "mrqa_squad-validation-8819", "mrqa_squad-validation-1780", "mrqa_triviaqa-validation-4411", "mrqa_squad-validation-386", "mrqa_triviaqa-validation-834", "mrqa_squad-validation-384", "mrqa_searchqa-validation-2971", "mrqa_hotpotqa-validation-1867", "mrqa_searchqa-validation-271", "mrqa_newsqa-validation-469", "mrqa_hotpotqa-validation-2522", "mrqa_newsqa-validation-442", "mrqa_triviaqa-validation-6421", "mrqa_squad-validation-2328", "mrqa_squad-validation-4918", "mrqa_naturalquestions-validation-7351", "mrqa_squad-validation-8229", "mrqa_hotpotqa-validation-788", "mrqa_naturalquestions-validation-7767"], "EFR": 0.9615384615384616, "Overall": 0.7409708867521367}, {"timecode": 36, "before_eval_results": {"predictions": ["between June and September", "emergency plans", "\"We Found Love\"", "bank", "July", "The Casalesi Camorra clan", "Tulsa, Oklahoma", "41,280", "Old Trafford", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's war zone.", "Number Ones", "Zac Efron", "Kabul", "stabbed Tate, who was 8\u00bd months pregnant, and wrote the word \"pig\" in blood on the door of the home the actress shared with her husband, director Roman Polanski.", "Jeannie Longo-Ciprelli", "Annie Duke", "to kill Jewish children anywhere in the world was utterly chilling and beyond any kind of civilised, humanitarian norm.", "Omar Jadwat", "producing rock music with a country influence", "The Kirchners", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "root out terrorists within its borders", "violent separatist campaign", "ancient Greek site of Olympia", "3,000", "closing these racial gaps", "Behar.", "22", "3-2", "150", "helicopters and boats", "U.N. agencies", "more than 30", "The man who was killed had been part of a hunting party of three men,", "one", "23 million square meters (248 million square feet)", "eastern Afghan province of Logar", "Virgin America", "fuel economy and safety", "American Civil Liberties Union", "summer", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "Jason", "mental health and recovery", "56", "\"Gruesome photos\"", "Frank Ricci,", "the Ku Klux Klan", "90", "Cash for Clunkers", "Argentina", "1997", "103", "Carolyn Sue Jones", "vanilla", "Hercules", "for his company\u2019s annual report", "Gian Carlo Menotti", "National Basketball Association", "Semites", "Donald Art Company", "Daisy Miller", "Detaiils", "Apollo"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6371827900360509}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 1.0, 0.0, 0.24242424242424243, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.8, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.2666666666666667, 0.0, 0.15384615384615385, 0.1818181818181818, 1.0, 0.8, 1.0, 1.0, 0.9565217391304348, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-978", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-533", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4389", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-2623"], "SR": 0.515625, "CSR": 0.5798141891891893, "retrieved_ids": ["mrqa_squad-train-80622", "mrqa_squad-train-6794", "mrqa_squad-train-30457", "mrqa_squad-train-51282", "mrqa_squad-train-41222", "mrqa_squad-train-43678", "mrqa_squad-train-71839", "mrqa_squad-train-74089", "mrqa_squad-train-13331", "mrqa_squad-train-41035", "mrqa_squad-train-39466", "mrqa_squad-train-7449", "mrqa_squad-train-70581", "mrqa_squad-train-71006", "mrqa_squad-train-10316", "mrqa_squad-train-8225", "mrqa_squad-train-57839", "mrqa_squad-train-8465", "mrqa_squad-train-53073", "mrqa_squad-train-30120", "mrqa_squad-train-75424", "mrqa_squad-train-56061", "mrqa_squad-train-44345", "mrqa_squad-train-34186", "mrqa_hotpotqa-validation-4102", "mrqa_naturalquestions-validation-6787", "mrqa_hotpotqa-validation-577", "mrqa_naturalquestions-validation-2299", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-16558", "mrqa_searchqa-validation-4319", "mrqa_squad-validation-9876", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-4552", "mrqa_searchqa-validation-8705", "mrqa_naturalquestions-validation-9330", "mrqa_hotpotqa-validation-2848", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-1415", "mrqa_hotpotqa-validation-3469", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-9753", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-4367", "mrqa_triviaqa-validation-6558", "mrqa_naturalquestions-validation-4073", "mrqa_hotpotqa-validation-3930", "mrqa_searchqa-validation-15202"], "EFR": 1.0, "Overall": 0.7483065878378379}, {"timecode": 37, "before_eval_results": {"predictions": ["five", "state's attorney", "Abdullah Gul,", "Ed McMahon,", "They are co-chair of the Genocide Prevention Task Force.", "off east  Africa", "hand-painted Swedish wooden clogs", "upper respiratory infection", "seven", "tells stories of different women coping with breast cancer in five vignettes.", "Gov. Bobby Jindal", "Bill Klein,", "Twitter", "Alwin Landry", "Venus Williams", "they learned of the death from TV news coverage,", "Robert Mugabe", "Holley Wimunc,", "surrender", "a $158 green skirt and $298 bead and rhinestone cardigan", "$530 million", "(or woman)", "Al-Aqsa mosque", "\"momentous discovery\"", "downtown Nairobi.", "Robert Barnett,", "Group 2", "Matthew Fisher", "Zimbabwean", "Ben Roethlisberger", "two younger ones ( Teiji and Takeo) for the Japanese navy and army.", "Pew Research Center", "John Auer,", "Brazil", "Stratfor, a global intelligence company,", "$12.7 million", "a jury", "Salt Lake City, Utah,", "in Karlsruhe.", "Robert Mugabe", "13.", "Osama bin Laden's sons", "for security reasons and not because of their faith.", "His treatment met the legal definition of torture.", "autonomy", "the Arctic north of Murmansk down to the southern climes of Sochi", "Long Island", "Ma Khin Khin Leh", "400 years ago", "Kerstin Fritzl,", "Washington State's decommissioned Hanford nuclear site,", "the breast or lower chest of beef or veal", "winter", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Georgia", "bullfight", "Viennese", "1887", "Atlantic Coast Conference", "under the tutelage of his uncle Juan Nepomuceno Guerra", "Pennsylvania", "Rabbit", "Mulligan stew", "Labrador"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5736687383286647}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.75, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8235294117647058, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.2222222222222222, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-2297", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-2905", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-9856", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-12609"], "SR": 0.453125, "CSR": 0.5764802631578947, "retrieved_ids": ["mrqa_squad-train-32022", "mrqa_squad-train-1794", "mrqa_squad-train-36364", "mrqa_squad-train-23307", "mrqa_squad-train-55001", "mrqa_squad-train-76417", "mrqa_squad-train-2718", "mrqa_squad-train-53486", "mrqa_squad-train-61362", "mrqa_squad-train-53920", "mrqa_squad-train-15097", "mrqa_squad-train-30345", "mrqa_squad-train-15108", "mrqa_squad-train-16861", "mrqa_squad-train-12086", "mrqa_squad-train-75936", "mrqa_squad-train-73602", "mrqa_squad-train-83995", "mrqa_squad-train-1587", "mrqa_squad-train-40447", "mrqa_squad-train-23171", "mrqa_squad-train-36218", "mrqa_squad-train-50095", "mrqa_squad-train-85608", "mrqa_hotpotqa-validation-3020", "mrqa_naturalquestions-validation-8159", "mrqa_newsqa-validation-2074", "mrqa_naturalquestions-validation-3442", "mrqa_squad-validation-4629", "mrqa_triviaqa-validation-2325", "mrqa_triviaqa-validation-2486", "mrqa_squad-validation-1906", "mrqa_searchqa-validation-9185", "mrqa_squad-validation-1802", "mrqa_hotpotqa-validation-4418", "mrqa_naturalquestions-validation-7095", "mrqa_squad-validation-8819", "mrqa_newsqa-validation-4032", "mrqa_squad-validation-8769", "mrqa_triviaqa-validation-2856", "mrqa_squad-validation-6171", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-15033", "mrqa_squad-validation-3954", "mrqa_squad-validation-259", "mrqa_searchqa-validation-15194", "mrqa_squad-validation-802", "mrqa_searchqa-validation-13247"], "EFR": 1.0, "Overall": 0.7476398026315789}, {"timecode": 38, "before_eval_results": {"predictions": ["vector quantities", "Transport Workers Union leaders", "March 24,", "Eleven people died and 36 were wounded", "Mexican military", "Pakistani officials,", "$7.8 million", "Stratfor's", "Madeleine K. Albright", "Roland Martin", "ties,", "German Foreign Ministry,", "10,000 refugees,", "IV cafe.", "Red Lines", "body bags on the roadway near the bus,", "40 militants and six Pakistan soldiers dead,", "beautiful fields, the glistening Pacific and the town of San Simeon,", "Islamic", "Los Angeles", "last week", "Sunni Arab and Shiite tribal leaders,", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "antihistamine and an epinephrine auto-injector for emergencies,", "North Korea", "Hong Kong", "two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "dropped", "ties,", "Ahmed,", "Saturday's Hungarian Grand Prix.", "power-sharing talks to take place in the next few weeks.", "mistreated students at the school.", "in an artificial coma", "5-0", "Africa", "fear of losing their licenses to fly.", "Zimbabwe's main opposition party", "FBI.", "first grand Slam,", "\"it should stay that way.\"", "CNN", "Obama and McCain camps", "strength of its brand name and the diversity of its product portfolio,", "U.S. State Department and British Foreign Office", "Monday's", "Gavin X. Pacheco,", "sculptures", "Pakistan's High Commission in India", "A member of the group dubbed the \"Jena 6\"", "pain-relief drugs.", "1871", "Frederick Chiluba, Levy Mwanawasa, Rupiah Banda, Michael Sata, and current President Edgar Lungu", "Gestalt psychology", "animals", "Treaty of Utrecht", "Massachusetts", "James Harrison", "Ford Island", "Tomorrowland", "Tiger Woods", "Wall Street", "George Washington", "Sesame Street"], "metric_results": {"EM": 0.515625, "QA-F1": 0.621233193889444}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2666666666666667, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-1060", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-6409", "mrqa_searchqa-validation-2728", "mrqa_searchqa-validation-13907"], "SR": 0.515625, "CSR": 0.5749198717948718, "retrieved_ids": ["mrqa_squad-train-11155", "mrqa_squad-train-8127", "mrqa_squad-train-81205", "mrqa_squad-train-77610", "mrqa_squad-train-33909", "mrqa_squad-train-59007", "mrqa_squad-train-72325", "mrqa_squad-train-22959", "mrqa_squad-train-1740", "mrqa_squad-train-78448", "mrqa_squad-train-44772", "mrqa_squad-train-8966", "mrqa_squad-train-28662", "mrqa_squad-train-77141", "mrqa_squad-train-58892", "mrqa_squad-train-24461", "mrqa_squad-train-11586", "mrqa_squad-train-73771", "mrqa_squad-train-24372", "mrqa_squad-train-38819", "mrqa_squad-train-62587", "mrqa_squad-train-42761", "mrqa_squad-train-27812", "mrqa_squad-train-84306", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-1798", "mrqa_squad-validation-2932", "mrqa_newsqa-validation-3893", "mrqa_hotpotqa-validation-2567", "mrqa_naturalquestions-validation-5739", "mrqa_squad-validation-7422", "mrqa_naturalquestions-validation-2813", "mrqa_searchqa-validation-5631", "mrqa_naturalquestions-validation-1682", "mrqa_newsqa-validation-463", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-2923", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-5721", "mrqa_hotpotqa-validation-1742", "mrqa_triviaqa-validation-1156", "mrqa_naturalquestions-validation-1813", "mrqa_triviaqa-validation-6237", "mrqa_naturalquestions-validation-6787", "mrqa_newsqa-validation-2886", "mrqa_hotpotqa-validation-4842", "mrqa_newsqa-validation-2068"], "EFR": 1.0, "Overall": 0.7473277243589743}, {"timecode": 39, "before_eval_results": {"predictions": ["Andrew Lortie", "to make something that is failing or weak", "cone", "high cooking", "Silver Hatch", "peripheral nerves", "Ethiopia", "Butterflies", "The Jets", "the Osprey Mini", "a person trained for travelling in space", "special administrative regions", "Alastair Cook", "Enterprise", "The Three Little Pigs", "china", "hip joint", "\"Noah,\"", "meninges", "English and French", "Guildford Dudley", "Munich", "Henry Mancini", "Fred Astaire", "the five hills", "a fortune-telling song", "Sudan", "Flanders", "\"S Sierra One from Sierra Oscar\"", "The Book of Proverbs", "stand-up comedian", "Jamaica", "Tornado", "drama", "S\u00e3o Jorge Island", "the pancreas", "puff-the-magic-dragon", "football", "Antoine Lavoisier", "\u00e9migr\u00e9", "supercontinents", "public disorder", "Pet Shop Boys", "Matthew Boulton", "Algiers", "Frank Doel,", "Aabaptists", "St. Agatha", "Hebrew", "John Virgo", "herpes virus", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods )", "Garfield Sobers", "In the mountains outside City 17", "Kodos", "Johannes Vermeer", "O.T. Genasis", "Climatecare,", "five companies to stop selling unapproved pain-relief drugs.", "Kevin Kuranyi", "Lost in America", "the equinox", "Soviet Union", "Republicans"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4864346590909091}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-970", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-86", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-1064", "mrqa_searchqa-validation-6304"], "SR": 0.4375, "CSR": 0.571484375, "retrieved_ids": ["mrqa_squad-train-37901", "mrqa_squad-train-49151", "mrqa_squad-train-52006", "mrqa_squad-train-83699", "mrqa_squad-train-64581", "mrqa_squad-train-51652", "mrqa_squad-train-43947", "mrqa_squad-train-12308", "mrqa_squad-train-49016", "mrqa_squad-train-17422", "mrqa_squad-train-82973", "mrqa_squad-train-18472", "mrqa_squad-train-36227", "mrqa_squad-train-30055", "mrqa_squad-train-51332", "mrqa_squad-train-44858", "mrqa_squad-train-73606", "mrqa_squad-train-4712", "mrqa_squad-train-67610", "mrqa_squad-train-10487", "mrqa_squad-train-27103", "mrqa_squad-train-6079", "mrqa_squad-train-72158", "mrqa_squad-train-15666", "mrqa_hotpotqa-validation-906", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-6087", "mrqa_squad-validation-7457", "mrqa_newsqa-validation-169", "mrqa_naturalquestions-validation-4547", "mrqa_hotpotqa-validation-3395", "mrqa_newsqa-validation-2900", "mrqa_searchqa-validation-2728", "mrqa_newsqa-validation-3232", "mrqa_hotpotqa-validation-4879", "mrqa_triviaqa-validation-6652", "mrqa_naturalquestions-validation-801", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-339", "mrqa_triviaqa-validation-5141", "mrqa_squad-validation-4435", "mrqa_searchqa-validation-8711", "mrqa_naturalquestions-validation-10040", "mrqa_squad-validation-3770", "mrqa_newsqa-validation-631", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-4132"], "EFR": 1.0, "Overall": 0.746640625}, {"timecode": 40, "UKR": 0.794921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1389", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10606", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-1264", "mrqa_searchqa-validation-12829", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16453", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10263", "mrqa_squad-validation-10317", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10369", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-127", "mrqa_squad-validation-1408", "mrqa_squad-validation-1430", "mrqa_squad-validation-1453", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2094", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3124", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3444", "mrqa_squad-validation-3497", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3946", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-4715", "mrqa_squad-validation-491", "mrqa_squad-validation-4918", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5664", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6706", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7147", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7296", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7751", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8154", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-8841", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-568", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-899"], "OKR": 0.90234375, "KG": 0.47734375, "before_eval_results": {"predictions": ["chameleon circuit", "Jake LaMotta", "danish", "Joshua", "gwyneth paltrow", "Live and Let Die", "Ruth Elizabeth", "lancaster", "gama", "Robert Hooke", "gaspard", "Napier", "Sony Interactive Entertainment", "Henry I", "green", "1215", "delphiniums", "Robinson Crusoe", "Charles Dickens", "belgian", "Egypt", "a neutron", "grommets", "New York Yankees", "Four Tops", "hudson", "July 20, 1969.", "10.8", "london", "lilac", "Hilary Swank", "scarlet", "dove", "a mole", "mandarins", "Hitler", "two", "london", "Shaft", "hindfoot", "The Daily Mirror", "Zak", "horse", "bangladesh", "gulhas", "Machu Picchu", "london philharmonic Orchestra", "Madness", "Jane Eyre", "Victoria", "belgian", "solemniser", "Ricky Nelson", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "Port Moresby, Papua New Guinea", "bass", "Security Management", "eight", "Russia", "was like a class to help women \" learn how to dance and feel sexy,\"", "Malaya", "Hank Aaron", "Tommy", "the Octopus"], "metric_results": {"EM": 0.40625, "QA-F1": 0.48467261904761905}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.7000000000000001, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-56", "mrqa_triviaqa-validation-2632", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6915", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-6885", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-774", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-5984", "mrqa_naturalquestions-validation-6903"], "SR": 0.40625, "CSR": 0.5674542682926829, "retrieved_ids": ["mrqa_squad-train-54927", "mrqa_squad-train-45672", "mrqa_squad-train-65106", "mrqa_squad-train-32312", "mrqa_squad-train-9273", "mrqa_squad-train-76170", "mrqa_squad-train-10514", "mrqa_squad-train-23434", "mrqa_squad-train-15307", "mrqa_squad-train-12271", "mrqa_squad-train-75370", "mrqa_squad-train-24967", "mrqa_squad-train-20282", "mrqa_squad-train-20269", "mrqa_squad-train-20488", "mrqa_squad-train-28984", "mrqa_squad-train-56421", "mrqa_squad-train-74655", "mrqa_squad-train-22887", "mrqa_squad-train-76691", "mrqa_squad-train-39327", "mrqa_squad-train-47138", "mrqa_squad-train-35243", "mrqa_squad-train-19073", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-8239", "mrqa_hotpotqa-validation-5114", "mrqa_naturalquestions-validation-4073", "mrqa_hotpotqa-validation-1123", "mrqa_triviaqa-validation-5044", "mrqa_naturalquestions-validation-4036", "mrqa_squad-validation-259", "mrqa_naturalquestions-validation-3598", "mrqa_newsqa-validation-1544", "mrqa_naturalquestions-validation-8027", "mrqa_squad-validation-10143", "mrqa_hotpotqa-validation-3395", "mrqa_squad-validation-6031", "mrqa_triviaqa-validation-1951", "mrqa_naturalquestions-validation-2666", "mrqa_searchqa-validation-6234", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-2299", "mrqa_squad-validation-3497", "mrqa_newsqa-validation-3827", "mrqa_hotpotqa-validation-4451", "mrqa_triviaqa-validation-2303", "mrqa_searchqa-validation-6445"], "EFR": 1.0, "Overall": 0.7484127286585366}, {"timecode": 41, "before_eval_results": {"predictions": ["Arthur H. Compton", "art fair", "Los Angeles", "they would not be making any further comments, citing the investigation.", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Tim Clark, Matt Kuchar and Bubba Watson", "Philip Markoff,", "Haeftling", "forgery and flying without a valid license,", "Sea World in San Antonio", "Mafia", "Nat King Cole,", "45th anniversary.", "convicts caught with phones", "16", "cancer", "$40 and a loaf of bread.", "McDonald\\'s' burgers and fries", "she's in love,", "France", "President Obama and Britain's Prince Charles", "more than 100.", "South Africa's", "Madeleine K. Albright", "air support.", "back at work", "will be inducted into the Baseball Hall of Fame in July.", "\"The explosion, reported about 11:30 a.m. Tuesday, caused sections of the roof to collapse.", "five", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "The father of Haleigh Cummings,", "Elisabeth's father,", "his club", "they", "$60 billion", "$249", "J.G. Ballard,", "Sen. Debbie Stabenow", "for bragging about his sex life", "Airbus A330-200", "United States, NATO member states, Russia and India", "adultery", "ties,", "dogs who walk on ice in Alaska.", "all three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "China", "Steve Jobs", "\"Rin Tin Tin Tin: The Life and the Legend\"", "Sri Lanka's", "The station", "state's attorney", "Colonel Robert E. Lee", "126", "Brevet Colonel Robert E. Lee", "Telegraph Media Group Limited 2017", "prime minister Yitzhak Rabin", "le Marseillaise", "Kristy Lee Cook", "John Samuel Waters Jr.", "Norman Mark Reedus", "\"The Paris of the Second Empire\"", "Douglas MacArthur", "rice", "the shooter must be at least 18 or 21 years old ( or have a legal guardian present )"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5412696084982269}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.26666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 0.3333333333333333, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.10526315789473684, 1.0, 0.8571428571428571, 0.0, 0.0, 0.25, 0.4444444444444445, 1.0, 1.0, 0.923076923076923, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7880", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-371", "mrqa_naturalquestions-validation-2418", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-2819", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2138", "mrqa_searchqa-validation-1621", "mrqa_searchqa-validation-149", "mrqa_naturalquestions-validation-8617"], "SR": 0.40625, "CSR": 0.5636160714285714, "retrieved_ids": ["mrqa_squad-train-50229", "mrqa_squad-train-11820", "mrqa_squad-train-83941", "mrqa_squad-train-35674", "mrqa_squad-train-56412", "mrqa_squad-train-76350", "mrqa_squad-train-56175", "mrqa_squad-train-52160", "mrqa_squad-train-49023", "mrqa_squad-train-79659", "mrqa_squad-train-5989", "mrqa_squad-train-18478", "mrqa_squad-train-19280", "mrqa_squad-train-62287", "mrqa_squad-train-47725", "mrqa_squad-train-78910", "mrqa_squad-train-22523", "mrqa_squad-train-85596", "mrqa_squad-train-49094", "mrqa_squad-train-31901", "mrqa_squad-train-42368", "mrqa_squad-train-17906", "mrqa_squad-train-22587", "mrqa_squad-train-53002", "mrqa_squad-validation-3130", "mrqa_hotpotqa-validation-1123", "mrqa_naturalquestions-validation-9753", "mrqa_newsqa-validation-3484", "mrqa_triviaqa-validation-5418", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-6407", "mrqa_newsqa-validation-3036", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-10037", "mrqa_hotpotqa-validation-2800", "mrqa_naturalquestions-validation-10194", "mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-3263", "mrqa_naturalquestions-validation-5312", "mrqa_triviaqa-validation-3563", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-989", "mrqa_newsqa-validation-1415", "mrqa_naturalquestions-validation-4103", "mrqa_newsqa-validation-1456", "mrqa_hotpotqa-validation-4079", "mrqa_naturalquestions-validation-4326", "mrqa_newsqa-validation-3825"], "EFR": 1.0, "Overall": 0.7476450892857143}, {"timecode": 42, "before_eval_results": {"predictions": ["ITT", "246", "beloved and admired", "The cause of the child's death will be listed as homicide by undetermined means,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Ali Larijani", "20", "\"Dance Your Ass Off.\"", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "15", "AbdulMutallab,", "London", "Democratic National Convention", "his native Philippines", "Kitty Kelley, biographer of the rich and famous,", "France", "The local Republican Party", "Amanda Knox's aunt", "Michael Krane,", "15,000", "17", "the Gulf", "May 4", "3-0", "Morgan Tsvangirai.", "three men with suicide vests who were plotting to carry out the attacks,", "kill then-Sen. Obama", "10", "165", "last few months,", "Ignazio La Russa", "Amir Zaki", "a loaf of bread.", "a 2,700-acre sanctuary in rural Tennessee.", "Tulsa, Oklahoma.", "2-1", "nearly $2 billion", "Russian concerns that the defensive shield could be used for offensive aims.", "1981", "\"They are, of course, shattered.", "London", "historical, inspiring, creative, romantic and beautiful.", "more than 100", "Microsoft.", "Michael Partain,", "Mitt Romney", "Islamic militants", "prisoners at the South Dakota State Penitentiary", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "has not grant full health-care coverage,", "season five", "in Llantrisant, Wales", "From 1940 until 1973", "Afghanistan", "gunga Din", "angler", "\"Shake It Off\"", "Cheshire", "Selden", "Transamerica", "\"The Professor's House\"", "Pirates of The Caribbean", "16"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5079737517916865}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.26666666666666666, 1.0, 0.0, 0.0, 0.4, 0.5384615384615384, 0.0, 1.0, 1.0, 0.5, 0.5, 0.25, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.2608695652173913, 0.0, 0.3333333333333333, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1429", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-143", "mrqa_triviaqa-validation-1380", "mrqa_triviaqa-validation-6580", "mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-5848", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-709", "mrqa_naturalquestions-validation-1640"], "SR": 0.40625, "CSR": 0.5599563953488372, "retrieved_ids": ["mrqa_squad-train-5830", "mrqa_squad-train-39204", "mrqa_squad-train-66641", "mrqa_squad-train-5073", "mrqa_squad-train-50205", "mrqa_squad-train-77042", "mrqa_squad-train-80170", "mrqa_squad-train-50589", "mrqa_squad-train-22142", "mrqa_squad-train-84167", "mrqa_squad-train-84701", "mrqa_squad-train-72191", "mrqa_squad-train-8438", "mrqa_squad-train-32805", "mrqa_squad-train-74482", "mrqa_squad-train-49468", "mrqa_squad-train-69151", "mrqa_squad-train-75016", "mrqa_squad-train-55579", "mrqa_squad-train-24520", "mrqa_squad-train-47860", "mrqa_squad-train-27084", "mrqa_squad-train-31812", "mrqa_squad-train-68680", "mrqa_newsqa-validation-1461", "mrqa_naturalquestions-validation-7554", "mrqa_hotpotqa-validation-260", "mrqa_newsqa-validation-1855", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-9235", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1022", "mrqa_naturalquestions-validation-10461", "mrqa_newsqa-validation-2112", "mrqa_naturalquestions-validation-10194", "mrqa_hotpotqa-validation-2377", "mrqa_triviaqa-validation-3660", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-1135", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2244", "mrqa_searchqa-validation-13520", "mrqa_naturalquestions-validation-5583", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-4451", "mrqa_naturalquestions-validation-6998"], "EFR": 1.0, "Overall": 0.7469131540697675}, {"timecode": 43, "before_eval_results": {"predictions": ["400", "14", "3-0", "\"Everybody gets sick, and everybody has someone in the family that gets sick.\"", "open heart surgery", "school in the Oaxacan countryside of southern", "the punishment for the player", "wings", "Vernon Forrest,", "Mandi Hamlin", "U.S. State Department and British Foreign Office", "Pastor Paula White", "Phoenix, Arizona, police", "\"We tortured (Mohammed al-) Qahtani,\"", "elephants,", "Six", "Wednesday.", "along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "Soviet republic of Georgia,", "nurse who tried to treat Jackson's insomnia with natural remedies", "Sheikh Abu al-Nour al-Maqdessi,", "a senior at Stetson University studying computer science.", "Michael Jackson", "1,500", "through the weekend,", "three", "\"To be one of his four children and know that is there for the world to see,", "Aniston, Demi Moore and Alicia Keys", "January", "to hold onto his land", "Miguel Cotto", "Two pages -- usually high school juniors who serve Congress as messengers", "Bowe Bergdahl", "shark River Park in Monmouth County", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "shutting down buses, subways and trolleys that carry almost a million people daily.", "five", "Long troop deployments", "St. Louis, Missouri.", "\"seems to be more often a reward car.", "a number of calls, and those calls were intriguing, and we're chasing those down now.", "Clifford Harris,", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Republican", "almost 9 million", "Asashoryu", "upper respiratory infection", "Adriano", "\"Zed,\"", "prime minister's handling of the L'Aquila earthquake,", "death squad killings", "1973", "American Horror Story : Roanoke", "Scarlett Johansson", "the skull", "Red Sea", "Narragansett Bay", "Tyler \"Ty\" Mendoza", "Robert A. Iger", "Salgaocar", "Persuasion", "Abercrombie & Fitch", "a soap opera", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6337194603641971}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.8, 0.14285714285714288, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.21052631578947367]}}, "before_error_ids": ["mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1086", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1989", "mrqa_triviaqa-validation-3275", "mrqa_hotpotqa-validation-802", "mrqa_searchqa-validation-6252", "mrqa_naturalquestions-validation-4915"], "SR": 0.578125, "CSR": 0.5603693181818181, "retrieved_ids": ["mrqa_squad-train-39361", "mrqa_squad-train-24539", "mrqa_squad-train-20404", "mrqa_squad-train-14294", "mrqa_squad-train-86417", "mrqa_squad-train-17697", "mrqa_squad-train-70247", "mrqa_squad-train-64441", "mrqa_squad-train-74740", "mrqa_squad-train-32914", "mrqa_squad-train-21106", "mrqa_squad-train-3060", "mrqa_squad-train-38328", "mrqa_squad-train-19971", "mrqa_squad-train-13175", "mrqa_squad-train-46235", "mrqa_squad-train-12812", "mrqa_squad-train-48206", "mrqa_squad-train-13467", "mrqa_squad-train-43728", "mrqa_squad-train-56026", "mrqa_squad-train-16138", "mrqa_squad-train-8527", "mrqa_squad-train-67426", "mrqa_hotpotqa-validation-1030", "mrqa_naturalquestions-validation-143", "mrqa_searchqa-validation-15496", "mrqa_triviaqa-validation-2655", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4017", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-8907", "mrqa_hotpotqa-validation-4712", "mrqa_naturalquestions-validation-5721", "mrqa_triviaqa-validation-2199", "mrqa_naturalquestions-validation-9818", "mrqa_hotpotqa-validation-5114", "mrqa_searchqa-validation-10856", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-6748", "mrqa_newsqa-validation-2509", "mrqa_naturalquestions-validation-4762", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-1569"], "EFR": 1.0, "Overall": 0.7469957386363637}, {"timecode": 44, "before_eval_results": {"predictions": ["transfer and dissipate excess energy", "south of Annapolis in Maryland", "the Mediterranean Sea", "Alex Ryan", "Wimpy's", "2001", "the fall of 2015", "Archie proposes marriage to Betty instead of to Veronica", "devised by Leonard Nimoy, who portrayed the half - Vulcan character Mr. Spock on the original Star Trek television series", "Rodney Crowell", "Jason Momoa", "north of the Equator", "a donor molecule to an acceptor molecule", "DJ Twist and the Fly Girls", "Iowa ( 36.6 % )", "1996", "uvea", "Director of National Intelligence", "biological taxonomy", "Zeebo", "1977", "Department of Health and Human Services", "France", "electronic computers", "a contemporary drama in a rural setting", "1939", "Kristy Swanson", "Jyotirindra Basu", "25 years after the release of their first record", "2018", "the Naturalization Act of 1790", "Colony of Virginia", "Arkansas", "1843", "at slightly different times when viewed from different points on Earth", "during the American Civil War", "Executive Residence of the White House Complex", "200 to 500 mg up to 7 mg", "Timothy B. Schmit", "March 2, 2016", "Thirty years after the Galactic Civil War,", "Woody Paige", "Bonnie Plunkett ( Allison Janney )", "$75,000", "four", "in `` Blood is the New Black ''", "between the stomach and the large intestine", "USS Chesapeake", "18 - season", "to offer the hope that a happy day being marked would recur many more times", "President Lyndon Johnson", "Sarah Palin", "Frank Ford Coppola", "pomegranate", "Kinnairdy Castle", "Teenage Mutant Ninja Turtles", "Kona", "his business dealings for possible securities violations", "40", "16", "John Deere", "snowboarding", "dollop", "Algiers"], "metric_results": {"EM": 0.5, "QA-F1": 0.6362306096681096}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.8333333333333333, 0.0, 1.0, 0.0, 1.0, 0.5, 0.19999999999999998, 1.0, 1.0, 1.0, 0.5454545454545454, 0.5714285714285715, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.09523809523809525, 1.0, 0.28571428571428575, 0.5, 1.0, 0.0, 0.060606060606060615, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.888888888888889, 0.20000000000000004, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9445", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-824", "mrqa_triviaqa-validation-7072", "mrqa_triviaqa-validation-5874", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-880", "mrqa_searchqa-validation-2656"], "SR": 0.5, "CSR": 0.5590277777777778, "retrieved_ids": ["mrqa_squad-train-33724", "mrqa_squad-train-35345", "mrqa_squad-train-1827", "mrqa_squad-train-57983", "mrqa_squad-train-38909", "mrqa_squad-train-75465", "mrqa_squad-train-85539", "mrqa_squad-train-85376", "mrqa_squad-train-5728", "mrqa_squad-train-2164", "mrqa_squad-train-66293", "mrqa_squad-train-64937", "mrqa_squad-train-22259", "mrqa_squad-train-23158", "mrqa_squad-train-19171", "mrqa_squad-train-306", "mrqa_squad-train-15038", "mrqa_squad-train-77412", "mrqa_squad-train-57008", "mrqa_squad-train-62497", "mrqa_squad-train-25969", "mrqa_squad-train-744", "mrqa_squad-train-7397", "mrqa_squad-train-65146", "mrqa_hotpotqa-validation-558", "mrqa_squad-validation-3019", "mrqa_newsqa-validation-2284", "mrqa_searchqa-validation-13919", "mrqa_triviaqa-validation-575", "mrqa_squad-validation-8819", "mrqa_naturalquestions-validation-2016", "mrqa_triviaqa-validation-2669", "mrqa_newsqa-validation-1060", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-55", "mrqa_naturalquestions-validation-7212", "mrqa_hotpotqa-validation-1996", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-4193", "mrqa_triviaqa-validation-4966", "mrqa_naturalquestions-validation-1782", "mrqa_triviaqa-validation-7705", "mrqa_naturalquestions-validation-191", "mrqa_triviaqa-validation-2262", "mrqa_naturalquestions-validation-143", "mrqa_searchqa-validation-2579", "mrqa_newsqa-validation-3189", "mrqa_naturalquestions-validation-10199"], "EFR": 0.96875, "Overall": 0.7404774305555556}, {"timecode": 45, "before_eval_results": {"predictions": ["locomotion", "The genetic basis was discovered in 1993 by an international collaborative effort led by the Hereditary Disease Foundation", "Cheryl Campbell", "Satavahanas", "Michael Moriarty", "Canada", "111", "Virginia", "modern random - access memory ( RAM )", "1940", "Doug Diemoz", "Charlene Holt", "David Kaye", "Fred Ott", "Hedwig", "O'Meara", "Washingtonizards", "Missi Hale", "2001", "Eddie Murphy", "Portugal", "Theodore Roosevelt", "1940", "parthenogenic", "Lyle Waggoner", "in Paradise, Nevada", "Coconut Cove", "May 19, 2009", "a Canaanite god associated with child sacrifice", "mid-size four - wheel drive", "10 June 1940", "Bill Pullman", "Great G minor symphony", "using a baby as bait, allowing a child to go through a torturous treatment to gain information, and allowing Dean to become a Dracula", "August 2, 1990", "786 -- 802", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Franklin and Wake counties in the U.S. state of North Carolina", "Justin Timberlake", "lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Remus Lupin", "Martin Lawrence", "Naomi", "Numa Pompilius", "Nurhaci", "Muhammad", "Americans", "August 22, 1980", "Sergeant Himmelstoss", "Yondu Udonta", "the next episode, `` Seeing Red ''", "luster", "bushfires", "Alexei Sayle", "Figaro", "Big 12 Conference", "Debbie Reynolds", "Dubai", "legitimacy of that race.", "Salt Lake City,", "(Java Man)", "OPEC", "bird", "Bill Clinton"], "metric_results": {"EM": 0.5, "QA-F1": 0.6331030020703934}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.9600000000000001, 0.4347826086956522, 1.0, 0.9333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3339", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2590"], "SR": 0.5, "CSR": 0.5577445652173914, "retrieved_ids": ["mrqa_squad-train-54527", "mrqa_squad-train-15901", "mrqa_squad-train-28978", "mrqa_squad-train-38579", "mrqa_squad-train-41122", "mrqa_squad-train-15420", "mrqa_squad-train-69627", "mrqa_squad-train-13313", "mrqa_squad-train-31328", "mrqa_squad-train-63314", "mrqa_squad-train-24307", "mrqa_squad-train-10168", "mrqa_squad-train-75899", "mrqa_squad-train-42317", "mrqa_squad-train-73189", "mrqa_squad-train-27714", "mrqa_squad-train-83706", "mrqa_squad-train-56360", "mrqa_squad-train-36998", "mrqa_squad-train-50491", "mrqa_squad-train-51830", "mrqa_squad-train-9933", "mrqa_squad-train-10016", "mrqa_squad-train-8697", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-1165", "mrqa_hotpotqa-validation-2905", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-6015", "mrqa_hotpotqa-validation-3703", "mrqa_searchqa-validation-8711", "mrqa_newsqa-validation-2582", "mrqa_triviaqa-validation-2063", "mrqa_newsqa-validation-3827", "mrqa_naturalquestions-validation-9691", "mrqa_squad-validation-7332", "mrqa_squad-validation-6638", "mrqa_newsqa-validation-2503", "mrqa_hotpotqa-validation-5437", "mrqa_newsqa-validation-4122", "mrqa_hotpotqa-validation-5358", "mrqa_squad-validation-6848", "mrqa_newsqa-validation-1339", "mrqa_naturalquestions-validation-10194", "mrqa_searchqa-validation-3613", "mrqa_squad-validation-3497", "mrqa_searchqa-validation-839", "mrqa_naturalquestions-validation-8063"], "EFR": 0.96875, "Overall": 0.7402207880434784}, {"timecode": 46, "before_eval_results": {"predictions": ["the main porch", "horticulture", "The Nitty Gritty Dirt Band", "the ACU", "Luther Ingram", "Taron Egerton", "Spencer Treat Clark", "Siddharth Arora / Vibhav Roy", "Ray Harroun", "hyakujo", "Clarence Anglin", "to establish an electrochemical gradient ( often a proton gradient ) across a membrane", "Copernicus", "a set of related data", "President pro tempore", "capillary action", "electron donors", "T.J. Miller", "Ren\u00e9 Descartes", "2006 -- 06 season", "the dealer sets the cards face - down on the table near the player designated to make the cut, typically the player to the dealer's right", "1955", "the town of Acolman, just north of Mexico City", "23 September 1889", "indigenous to many forested parts of the world", "January 2018", "Colon Street", "The higher the vapor pressure of a liquid at a given temperature", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "1923", "Hugh S. Johnson", "axons", "Lord Banquo", "joy", "lithium", "al - khimar", "291 episodes", "Anthony Hopkins", "the middle of the 15th century", "Ingrid Bergman", "social ideology", "c. 1000 AD", "Identification of alternative plans / policies", "Missouri River", "Venezuela and the remainder in Colombia", "Anakin Skywalker", "private sector to artisan and agricultural production / trade", "concerned with all legal affairs", "2018", "44", "September 19, 2017", "Beaujolais Nouveau", "Edward Woodward", "Black Swan", "Lake Wallace", "Paul W. S. Anderson", "1828\u20131866", "9", "the world's tallest building", "the Bush administration's controversial system of military trials", "V", "the Vedas", "Gertrude Stein", "Ilkley"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6709753787878787}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4166666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4929", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-538", "mrqa_hotpotqa-validation-1605", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-4201"], "SR": 0.578125, "CSR": 0.5581781914893618, "retrieved_ids": ["mrqa_squad-train-64643", "mrqa_squad-train-28724", "mrqa_squad-train-48076", "mrqa_squad-train-2906", "mrqa_squad-train-13149", "mrqa_squad-train-63982", "mrqa_squad-train-23313", "mrqa_squad-train-47550", "mrqa_squad-train-63771", "mrqa_squad-train-47640", "mrqa_squad-train-1757", "mrqa_squad-train-12121", "mrqa_squad-train-58162", "mrqa_squad-train-81150", "mrqa_squad-train-52017", "mrqa_squad-train-37444", "mrqa_squad-train-55416", "mrqa_squad-train-4324", "mrqa_squad-train-24613", "mrqa_squad-train-29599", "mrqa_squad-train-1121", "mrqa_squad-train-32657", "mrqa_squad-train-9636", "mrqa_squad-train-16475", "mrqa_naturalquestions-validation-2196", "mrqa_triviaqa-validation-3954", "mrqa_searchqa-validation-217", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-8841", "mrqa_newsqa-validation-593", "mrqa_naturalquestions-validation-10559", "mrqa_triviaqa-validation-3208", "mrqa_squad-validation-8958", "mrqa_hotpotqa-validation-5703", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-2199", "mrqa_searchqa-validation-13251", "mrqa_naturalquestions-validation-3319", "mrqa_squad-validation-5588", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-3633", "mrqa_naturalquestions-validation-2558", "mrqa_searchqa-validation-9822", "mrqa_naturalquestions-validation-4071", "mrqa_newsqa-validation-1175", "mrqa_hotpotqa-validation-2585", "mrqa_newsqa-validation-2905", "mrqa_squad-validation-589"], "EFR": 0.9259259259259259, "Overall": 0.7317426984830575}, {"timecode": 47, "before_eval_results": {"predictions": ["Germany", "Tim Russert", "Amanda Gregorio ( Vanessa Ferlito )", "Celtic", "1978", "two", "September 6, 2019", "Earle Hyman", "September 19, 2017", "Anglo - Norman French waleis", "31 October 1972", "23 September 1889", "the House of Representatives", "Pittsburgh", "frontal lobe", "the 1940s", "increased productivity, trade, and secular economic trends", "2 %", "approximately 5 liters, with females generally having less blood volume than males", "G -- Games", "the 17th episode in the third season", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year reported on the accompanying income statement", "New York Yankees", "94 by 50", "Sam Waterston", "the realm of the Valar in Aman", "knowledge, piety, and fear of the Lord", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak )", "bohrium", "Ravi Shastri", "the main road through the gated community of Pebble Beach", "a place of trade, entertainment, and education", "electrons", "November 2014", "Lewis Carroll", "Janis Joplin", "the South Pacific Ocean", "T'Pau", "European powers", "Blue laws in the United States vary by state.", "in South America", "Edgar Lungu", "Melanie Martinez", "HTTP / 1.1 200 OK", "The number is called, players check to see if it appears on their tickets", "Will", "the centre of Munich", "Gary Grimes", "1998", "Thomas Chisholm", "Tommy James and the Shondells", "nausea", "credit issues with government debt", "\"The best is yet to come.\"", "Chattahoochee", "Patterns of Sexual Behavior", "The Simpsons", "Argentina", "ice jam", "Facebook and Google,", "decaffeinated coffee", "One Flew Over the Cuckoo's Nest", "Stephen Hawking", "$420,000 tax break previously granted to MTV's most popular program -- the highly viewed and just as highly derided reality series featuring Snooki, The Situation and other often inebriated free-range Narcists."], "metric_results": {"EM": 0.4375, "QA-F1": 0.5536006391365569}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.4444444444444445, 0.2857142857142857, 0.8, 1.0, 1.0, 0.125, 1.0, 0.0, 0.4, 0.08695652173913042, 0.33333333333333337, 0.0, 0.4, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-7614", "mrqa_triviaqa-validation-4834", "mrqa_triviaqa-validation-2385", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-2819", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14104", "mrqa_newsqa-validation-2608"], "SR": 0.4375, "CSR": 0.5556640625, "retrieved_ids": ["mrqa_squad-train-70407", "mrqa_squad-train-79604", "mrqa_squad-train-23748", "mrqa_squad-train-11123", "mrqa_squad-train-79429", "mrqa_squad-train-52881", "mrqa_squad-train-11384", "mrqa_squad-train-84661", "mrqa_squad-train-59010", "mrqa_squad-train-37768", "mrqa_squad-train-48835", "mrqa_squad-train-32578", "mrqa_squad-train-10236", "mrqa_squad-train-39012", "mrqa_squad-train-37007", "mrqa_squad-train-54609", "mrqa_squad-train-45183", "mrqa_squad-train-69854", "mrqa_squad-train-35922", "mrqa_squad-train-7716", "mrqa_squad-train-83295", "mrqa_squad-train-64421", "mrqa_squad-train-40948", "mrqa_squad-train-68867", "mrqa_naturalquestions-validation-2067", "mrqa_triviaqa-validation-7401", "mrqa_naturalquestions-validation-868", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-2463", "mrqa_naturalquestions-validation-3922", "mrqa_triviaqa-validation-2325", "mrqa_naturalquestions-validation-2956", "mrqa_newsqa-validation-2590", "mrqa_naturalquestions-validation-720", "mrqa_searchqa-validation-10806", "mrqa_triviaqa-validation-56", "mrqa_newsqa-validation-1442", "mrqa_naturalquestions-validation-7849", "mrqa_searchqa-validation-2052", "mrqa_triviaqa-validation-2316", "mrqa_newsqa-validation-3803", "mrqa_naturalquestions-validation-6201", "mrqa_newsqa-validation-4143", "mrqa_naturalquestions-validation-3598", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-8062", "mrqa_newsqa-validation-2902"], "EFR": 1.0, "Overall": 0.7460546875}, {"timecode": 48, "before_eval_results": {"predictions": ["Democratic VP candidate", "Stuttgart", "Three", "Long troop deployments", "over 1,000 pounds", "\"The show allows 10 boys and 10 girls between the age of eight and 11 to create their own mini-societies, organizing everything from what they eat to how they should entertain themselves.", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "the shoreline of the city of Quebradillas.", "Eleven", "police", "The journalists who were found guilty in Ethiopia of supporting terrorism", "Microsoft.", "Ferraris, a Lamborghini and an Acura NSX", "eight", "1831", "frees up a place", "Michael Krane,", "Russian bombers", "Three aid workers", "10 a.m.", "0-0", "63", "\"It's just going to become part of the fabric of the fashion imagery of pop culture,", "a one-shot victory", "test scores and graduation rates", "a cancer-causing toxic chemical.", "\"It was perfect work, ready to go for the stimulus package,\"", "US Airways Flight 1549", "the southern city of Naples", "racial intolerance", "Friday", "\"Twilight\"", "Robert Kimmitt.", "22", "\"We really want to be parents, and that is the goal here through surrogacy and adoption.", "El Paso, Texas.", "10", "Retailers who don't speak out against it", "Anil Kapoor.", "Samoa", "authorizing killings and kidnappings by paramilitary death squads.", "E. coli", "Ma Khin Khin Leh,", "\"It shot up the Japanese singles chart, reaching No 4, the highest ever position for a first time enka release.", "\"The train ride up there is spectacular.", "1998.", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "London's O2 arena", "The EU naval force", "Cyprus", "The Obama administration", "between $10,000 and $30,000", "March 1602", "Number 4, Privet Drive, Little Whinging in Surrey, England", "fructose", "gold certificates", "a rain hat", "five", "E Street Band", "England", "chippewa", "salt", "Everybody Have Fun Tonight", "the foreign exchange market (FX )"], "metric_results": {"EM": 0.5, "QA-F1": 0.5842275432900432}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.25, 0.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.3636363636363636, 0.25, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.09090909090909091, 0.0, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.32, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-3228", "mrqa_newsqa-validation-1614", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2183", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-3970", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-2918", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-12129"], "SR": 0.5, "CSR": 0.5545280612244898, "retrieved_ids": ["mrqa_squad-train-66189", "mrqa_squad-train-22987", "mrqa_squad-train-16149", "mrqa_squad-train-35027", "mrqa_squad-train-81149", "mrqa_squad-train-63744", "mrqa_squad-train-2261", "mrqa_squad-train-56682", "mrqa_squad-train-66094", "mrqa_squad-train-52590", "mrqa_squad-train-2138", "mrqa_squad-train-68039", "mrqa_squad-train-52132", "mrqa_squad-train-46104", "mrqa_squad-train-52973", "mrqa_squad-train-42523", "mrqa_squad-train-32430", "mrqa_squad-train-74430", "mrqa_squad-train-61566", "mrqa_squad-train-48077", "mrqa_squad-train-66910", "mrqa_squad-train-86361", "mrqa_squad-train-81972", "mrqa_squad-train-24127", "mrqa_naturalquestions-validation-801", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-2546", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-8699", "mrqa_hotpotqa-validation-906", "mrqa_newsqa-validation-593", "mrqa_hotpotqa-validation-4879", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-4863", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-1744", "mrqa_triviaqa-validation-5632", "mrqa_squad-validation-9876", "mrqa_naturalquestions-validation-9240", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-1318", "mrqa_searchqa-validation-14104", "mrqa_naturalquestions-validation-2503", "mrqa_triviaqa-validation-2694", "mrqa_searchqa-validation-839", "mrqa_hotpotqa-validation-246"], "EFR": 0.96875, "Overall": 0.739577487244898}, {"timecode": 49, "before_eval_results": {"predictions": ["Pierre Laval", "Beckett", "\ufffd\u00bf\u00bdGrammar and History of the English Language\u00ef\u00bf+ and thousands of quotations from such authors as John Dryden, William Shakespeare and John Milton", "Michaela Tabb", "Alpha Orionis", "Edward VIII", "maddy Bell", "falcon", "Stephen Fry", "Libya", "Vadamar", "Robinson", "ascetics", "The Daily Mirror", "William Shakespeare", "Handley Page", "dying", "Rod Laver", "Texas", "motion marking molto allegro", "Strait of Messina", "ahoy", "the Security Council and the General Assembly whenever there is a threat of serious and large-scale violations of human rights", "Brian Deane", "Volkswagen Golf", "Emilia Fox", "October", "PETER FRAMPTON LYRICS", "catherine zeta", "South Africa", "Jim Braddock", "mediterranean", "1840", "bony fish", "Gibeon", "islands in the south east almost at the equator", "vomiting", "Hugo von Hofmannsthal", "sperm", "Croatian", "penguin", "golf", "purpurea", "Amnesty International", "Spearchucker", "her skills", "the Kingdom of Lesotho", "The first performance of Elgar\u2019s \u2018Enigma\u2019 Variations", "Mauricio Pochettino", "Duke of Edinburgh", "myxoma", "the season - five premiere episode `` Second Opinion ''", "Georgia", "Parker's pregnancy at the time of filming", "Dame Eileen June Atkins", "Battle of Chester", "James Gandolfini", "Bill Stanton", "Los Angeles Angels", "56,", "Twenty three", "the 16.5-inch (420-mm) howitzer", "parabola", "Patty Duke"], "metric_results": {"EM": 0.53125, "QA-F1": 0.58671875}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.3, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-3752", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-4066", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-2853", "mrqa_naturalquestions-validation-7507", "mrqa_searchqa-validation-12134", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-4996"], "SR": 0.53125, "CSR": 0.5540625, "retrieved_ids": ["mrqa_squad-train-42359", "mrqa_squad-train-72028", "mrqa_squad-train-25639", "mrqa_squad-train-36705", "mrqa_squad-train-58822", "mrqa_squad-train-41985", "mrqa_squad-train-19577", "mrqa_squad-train-37824", "mrqa_squad-train-48792", "mrqa_squad-train-49955", "mrqa_squad-train-11913", "mrqa_squad-train-84341", "mrqa_squad-train-41784", "mrqa_squad-train-59466", "mrqa_squad-train-65199", "mrqa_squad-train-63008", "mrqa_squad-train-57116", "mrqa_squad-train-70776", "mrqa_squad-train-43211", "mrqa_squad-train-74219", "mrqa_squad-train-67277", "mrqa_squad-train-12139", "mrqa_squad-train-68503", "mrqa_squad-train-67867", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-7690", "mrqa_searchqa-validation-8760", "mrqa_triviaqa-validation-4784", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-2761", "mrqa_naturalquestions-validation-8530", "mrqa_triviaqa-validation-765", "mrqa_naturalquestions-validation-232", "mrqa_newsqa-validation-3502", "mrqa_hotpotqa-validation-5792", "mrqa_triviaqa-validation-6421", "mrqa_naturalquestions-validation-1912", "mrqa_squad-validation-9717", "mrqa_naturalquestions-validation-4137", "mrqa_newsqa-validation-3557", "mrqa_triviaqa-validation-7705", "mrqa_squad-validation-4918", "mrqa_triviaqa-validation-1403", "mrqa_hotpotqa-validation-4712", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-13554", "mrqa_newsqa-validation-1550", "mrqa_naturalquestions-validation-5926"], "EFR": 0.9, "Overall": 0.725734375}, {"timecode": 50, "UKR": 0.798828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4431", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-962", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2365", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-893", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5943", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-899"], "OKR": 0.892578125, "KG": 0.49375, "before_eval_results": {"predictions": ["A Christmas Carol", "Robert De Niro", "bangladesh", "eagle", "SUNSET BOULEVARD", "neath", "Berlin", "The Rocky Horror Picture Show", "1925", "Prince Albert", "bill", "Pakistan", "pig", "pappy", "ely", "Bull Moose Party", "Genoa", "sarah ferguson", "The Spectator", "Jamaica", "Jessica Simpson", "fred fred stooge", "earthquake", "campania", "Charlie Chan", "china", "clairelouise28", "louis XVIII", "Jane Seymour", "playoff basketball", "Thailand", "strainspotting, Slumdog Millionaire", "james boswell", "Lew Hoad", "Portsmouth", "sarah armstrong", "Zeus", "widow", "virtual mirror", "Wolfgang Amadeus Mozart", "Anne-Marie Duff", "Joan Rivers", "salt or sugar", "fifth", "phobias", "pears soap", "guitar", "Toby", "Argentina", "kenny Everett", "fenn Street School", "1804", "An alternative is to cool all the atmosphere by spraying the whole atmosphere as if drawing letters in the air ( `` penciling '' )", "November 3, 2007", "John Bingham", "Marktown", "Bit Instant", "well over 1,000 pounds).", "Jet Republic, one of Europe's most experienced providers of carbon offsets,", "for admitting they learned of the death from TV news coverage,", "W.C. Handy", "the Lamb of God", "legs", "1978"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6404723748473748}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.5, 1.0, 0.0, 1.0, 0.3076923076923077, 0.5714285714285715, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4901", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-23", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2685", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5281", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-3244", "mrqa_searchqa-validation-10670"], "SR": 0.578125, "CSR": 0.5545343137254901, "retrieved_ids": ["mrqa_squad-train-32223", "mrqa_squad-train-8637", "mrqa_squad-train-73430", "mrqa_squad-train-53710", "mrqa_squad-train-13950", "mrqa_squad-train-76249", "mrqa_squad-train-18131", "mrqa_squad-train-63295", "mrqa_squad-train-25005", "mrqa_squad-train-13655", "mrqa_squad-train-39319", "mrqa_squad-train-73716", "mrqa_squad-train-35278", "mrqa_squad-train-23053", "mrqa_squad-train-74264", "mrqa_squad-train-54539", "mrqa_squad-train-10803", "mrqa_squad-train-77625", "mrqa_squad-train-39525", "mrqa_squad-train-44742", "mrqa_squad-train-40896", "mrqa_squad-train-39346", "mrqa_squad-train-11960", "mrqa_squad-train-53997", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-1003", "mrqa_triviaqa-validation-6548", "mrqa_hotpotqa-validation-577", "mrqa_searchqa-validation-1156", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-2138", "mrqa_newsqa-validation-4118", "mrqa_searchqa-validation-6181", "mrqa_newsqa-validation-3961", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-1798", "mrqa_triviaqa-validation-1463", "mrqa_squad-validation-1827", "mrqa_triviaqa-validation-570", "mrqa_searchqa-validation-12363", "mrqa_hotpotqa-validation-5604", "mrqa_newsqa-validation-1673", "mrqa_triviaqa-validation-1402", "mrqa_searchqa-validation-1649", "mrqa_triviaqa-validation-1380", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-1165", "mrqa_squad-validation-2911"], "EFR": 0.9259259259259259, "Overall": 0.7331232979302832}, {"timecode": 51, "before_eval_results": {"predictions": ["bacall", "blue", "Robin Ellis", "O-s-c-a-are", "binder", "hydrogen", "between the 'Tarsal' bones of the hind-foot and the 'Phalanges' bones in the toe", "priests or the priesthood", "dancing with the Stars", "South Pacific", "stanley", "Bosnia and Herzegovina", "France", "Sparta", "morningtown ride", "squash", "Northwestern University", "Turkey", "barry stanley", "China", "diffusion", "David Bowie", "Robben Island", "bukwus", "frankincense", "medium", "Libyan Desert", "Rocky Marciano", "zsa zsa Gabor", "Tinie Tempah", "Ruth Ellis", "Egypt", "sparrow", "Eton College", "annelies", "tabby", "july", "a highly dangerous Boojum", "bacall", "Valentine Dyall", "stanley Lawson", "Portugal", "Opus Dei", "the Flying Pickets", "Dry Ice", "Kenya", "Benjamin Disraeli, 1st Earl of Beaconsfield", "bakers van", "thrasher", "reanne Evans", "blood", "ummat al - Islamiyah", "Rachel Kelly Tucker", "Honor\u00e9 Mirabeau", "Big Bad Wolf", "Daniel Radcliffe", "Chris Pine", "President Obama", "Croatia", "Tom Hanks", "Curly Lambeau", "the candy bar", "Rosa Parks", "a bed bug"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6020833333333333}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-4257", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-5154", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-6925", "mrqa_naturalquestions-validation-1455", "mrqa_hotpotqa-validation-2075", "mrqa_searchqa-validation-2456", "mrqa_searchqa-validation-11960"], "SR": 0.53125, "CSR": 0.5540865384615384, "retrieved_ids": ["mrqa_squad-train-63089", "mrqa_squad-train-73376", "mrqa_squad-train-44920", "mrqa_squad-train-68634", "mrqa_squad-train-21558", "mrqa_squad-train-58748", "mrqa_squad-train-9179", "mrqa_squad-train-84760", "mrqa_squad-train-44717", "mrqa_squad-train-17635", "mrqa_squad-train-23501", "mrqa_squad-train-18029", "mrqa_squad-train-38681", "mrqa_squad-train-26380", "mrqa_squad-train-10384", "mrqa_squad-train-23992", "mrqa_squad-train-20061", "mrqa_squad-train-51138", "mrqa_squad-train-60302", "mrqa_squad-train-24226", "mrqa_squad-train-37303", "mrqa_squad-train-43926", "mrqa_squad-train-41217", "mrqa_squad-train-48519", "mrqa_searchqa-validation-5038", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-4036", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-2476", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-5291", "mrqa_squad-validation-9136", "mrqa_naturalquestions-validation-5360", "mrqa_newsqa-validation-1136", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-4269", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-720", "mrqa_newsqa-validation-11", "mrqa_naturalquestions-validation-8189", "mrqa_newsqa-validation-2112", "mrqa_triviaqa-validation-4922", "mrqa_hotpotqa-validation-3742", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-4624"], "EFR": 0.9666666666666667, "Overall": 0.741181891025641}, {"timecode": 52, "before_eval_results": {"predictions": ["Ringo Starr", "Apprendi v. New Jersey", "8th", "Erreway", "North Queensland", "George Clooney, Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "Pamelyn Wanda Ferdin", "Christian Kern", "The Social Network", "$10.5 million", "2017", "Dutch", "2014", "Musical", "Missouri", "Rochdale, North West England", "50 best cities to live in.\"", "Virginia", "The Godfather Part II", "two", "Rigoletto", "Scunthorpe", "Talib Kweli", "motor", "American", "1 September 1864", "Eugene O'Neill", "Colonel Gaddafi", "Sony Music and Syco Music", "figure-eight variety", "Sofia the First", "Sufism", "$700 million", "Frank Wentz", "a great man", "The Saturdays", "New York and New Jersey campaign", "Piedmont", "John Travolta", "ice hockey", "Hong Kong", "2006", "Pacific Place", "science fiction", "sarod", "2009", "Northern Ireland", "1999", "Russian Ark", "Delacorte Press", "the voice of The Beast", "17th Century", "April 12, 2017", "Revenge of the Wars", "an ancient optical illusion toy", "halogens", "vickers-Armstrong", "food, music, culture and language of Latin America", "Amy", "school,", "lip service", "Maine is the northernmost state in the New England region of the northeastern United States", "Henry Clay", "Richard Crispin Armitage"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6922475961538461}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-3536", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1351", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-4119", "mrqa_hotpotqa-validation-2802", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3422", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-468", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-11977"], "SR": 0.640625, "CSR": 0.5557193396226415, "retrieved_ids": ["mrqa_squad-train-18430", "mrqa_squad-train-36103", "mrqa_squad-train-73156", "mrqa_squad-train-82344", "mrqa_squad-train-14705", "mrqa_squad-train-27698", "mrqa_squad-train-6523", "mrqa_squad-train-43899", "mrqa_squad-train-36495", "mrqa_squad-train-27842", "mrqa_squad-train-46562", "mrqa_squad-train-77686", "mrqa_squad-train-49331", "mrqa_squad-train-14391", "mrqa_squad-train-58243", "mrqa_squad-train-26702", "mrqa_squad-train-39880", "mrqa_squad-train-78855", "mrqa_squad-train-8254", "mrqa_squad-train-16994", "mrqa_squad-train-60147", "mrqa_squad-train-63385", "mrqa_squad-train-81559", "mrqa_squad-train-50757", "mrqa_newsqa-validation-1330", "mrqa_squad-validation-4572", "mrqa_hotpotqa-validation-1736", "mrqa_naturalquestions-validation-8896", "mrqa_newsqa-validation-3767", "mrqa_searchqa-validation-3618", "mrqa_newsqa-validation-2816", "mrqa_squad-validation-9176", "mrqa_newsqa-validation-1392", "mrqa_squad-validation-4000", "mrqa_naturalquestions-validation-2503", "mrqa_triviaqa-validation-102", "mrqa_naturalquestions-validation-9639", "mrqa_hotpotqa-validation-2075", "mrqa_squad-validation-3946", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-1012", "mrqa_squad-validation-2000", "mrqa_searchqa-validation-14194", "mrqa_hotpotqa-validation-5237", "mrqa_naturalquestions-validation-6772"], "EFR": 1.0, "Overall": 0.7481751179245283}, {"timecode": 53, "before_eval_results": {"predictions": ["Mike Mills", "1998", "30.9%", "Kittie", "American", "People!", "Wolseley", "The Vanguard Group", "American", "Ready to Die", "to steal the plans for the Death Star, the Galactic Empire's super weapon", "Danish", "York County", "Seventeen", "Wake Island", "Australian Defence Force", "June 11, 1973", "Arthur William Bell III", "Boston", "Erreway", "Tampa Bay Lightning", "CBS", "Boston, Massachusetts", "Elise Marie Stefanik", "Jennifer Taylor", "Estadio Victoria", "9Lives brand cat food", "Black Ravens", "2005", "Flamingo Hotel in Las Vegas", "42,972", "9,000", "Michael Seater", "Drunken Master II", "more than 100 countries", "bassline", "E22", "Allies of World War I", "Geraldine Sue Page", "Kristina Ceyton and Kristian Moliere", "\"Linda McCartney's Life in Photography\"", "near Philip Billard Municipal Airport", "1964 to 1974", "Big Fucking German", "law", "Hamlet", "Bow River and the Elbow River", "Gillian Anderson", "American rock band formed in Princeton, New Jersey in 1987", "a united Ireland", "\"Queen In-hyun's Man\"", "American musical group founded by Marcus Bowens and Jermaine Fuller, with the later addition of J.J. O' Neal and Dougy Williams", "Virgil Ogletree", "4", "Topiary", "charlie", "2010", "Luca di Montezemolo", "near the Somali coast", "blind,", "deck boss", "AMC Eagle Wagon", "Marky Mark", "cheese"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6524553571428571}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.8, 0.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-2025", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1033", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2482", "mrqa_triviaqa-validation-6121", "mrqa_triviaqa-validation-115", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1641", "mrqa_searchqa-validation-5501", "mrqa_searchqa-validation-3970", "mrqa_searchqa-validation-16209"], "SR": 0.546875, "CSR": 0.5555555555555556, "retrieved_ids": ["mrqa_squad-train-52614", "mrqa_squad-train-33496", "mrqa_squad-train-43436", "mrqa_squad-train-15377", "mrqa_squad-train-17265", "mrqa_squad-train-26569", "mrqa_squad-train-71226", "mrqa_squad-train-51027", "mrqa_squad-train-43267", "mrqa_squad-train-29936", "mrqa_squad-train-58523", "mrqa_squad-train-17966", "mrqa_squad-train-57211", "mrqa_squad-train-45852", "mrqa_squad-train-47511", "mrqa_squad-train-83729", "mrqa_squad-train-46146", "mrqa_squad-train-51131", "mrqa_squad-train-39687", "mrqa_squad-train-27562", "mrqa_squad-train-73791", "mrqa_squad-train-22326", "mrqa_squad-train-85313", "mrqa_squad-train-6807", "mrqa_naturalquestions-validation-10156", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2042", "mrqa_triviaqa-validation-3418", "mrqa_searchqa-validation-12129", "mrqa_naturalquestions-validation-5531", "mrqa_newsqa-validation-2506", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-4192", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-1985", "mrqa_squad-validation-10388", "mrqa_newsqa-validation-1086", "mrqa_searchqa-validation-9148", "mrqa_hotpotqa-validation-5627", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1094", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-3323", "mrqa_searchqa-validation-3811", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-2495", "mrqa_naturalquestions-validation-7554", "mrqa_searchqa-validation-15877"], "EFR": 1.0, "Overall": 0.7481423611111111}, {"timecode": 54, "before_eval_results": {"predictions": ["her brother, Brian", "live events", "callable bonds", "the original timeline is eventually restored", "Waylon Jennings", "August 2, 1990", "Charlene Holt", "eight episode series", "the courts", "wolves", "an official document permitting a specific individual to operate one or more types of motorized vehicles", "18", "Jewel Akens", "Division 1", "Irsay", "Abid Ali Neemuchwala", "winter", "Roxette", "a Cadillac", "Jones'then - wife, Peggy Lipton", "Union", "the chest, back, shoulders, torso and / or legs", "Authority", "drizzle, rain, sleet, snow", "1967", "Virginia", "Parker's pregnancy", "lakes or reservoirs", "merengue", "from Times Square in New York City west to Lincoln Park in San Francisco", "the 1960s", "IBM", "American singer Elvis Presley", "American author Elizabeth George Speare", "1998", "Karen Gillan", "part of the present Indian constitutive state of Meghalaya ( formerly Assam ), which includes the present districts of East Jaintia Hills district,", "A rear - view mirror", "April 29, 2009", "flawed democracy", "2026", "William Chatterton Dix", "Donna Mills", "Selena Gomez", "Steve Russell", "1881", "to check the president's power to commit the United States to an armed conflict without the consent of the U.S. Congress", "bassist Timothy B. Schmit", "Games played", "Cetshwayo", "Games", "Cambridge", "Oklahoma City", "choroid", "1982", "2015", "film", "22", "one", "riders a 19th-century experience on cruises complete with the carnival-like sounds of the steam-whistle calliope.", "Ray Walston", "the Ukrainian Soviet Socialist Republic", "Napoleon", "\"Traumnovelle\" (\"Dream Story\")"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6926065111618607}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true], "QA-F1": [0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.4, 1.0, 1.0, 0.1818181818181818, 0.24000000000000002, 1.0, 0.6, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6440677966101694, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.875, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5785", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-3", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2996", "mrqa_newsqa-validation-2246", "mrqa_searchqa-validation-354", "mrqa_searchqa-validation-7780"], "SR": 0.578125, "CSR": 0.555965909090909, "retrieved_ids": ["mrqa_squad-train-66595", "mrqa_squad-train-35875", "mrqa_squad-train-21784", "mrqa_squad-train-35827", "mrqa_squad-train-9210", "mrqa_squad-train-9612", "mrqa_squad-train-2078", "mrqa_squad-train-84809", "mrqa_squad-train-75708", "mrqa_squad-train-86099", "mrqa_squad-train-60003", "mrqa_squad-train-511", "mrqa_squad-train-20415", "mrqa_squad-train-77426", "mrqa_squad-train-25029", "mrqa_squad-train-76450", "mrqa_squad-train-12467", "mrqa_squad-train-56763", "mrqa_squad-train-29260", "mrqa_squad-train-19899", "mrqa_squad-train-58236", "mrqa_squad-train-1773", "mrqa_squad-train-57354", "mrqa_squad-train-68705", "mrqa_newsqa-validation-2660", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-5968", "mrqa_searchqa-validation-5586", "mrqa_naturalquestions-validation-4132", "mrqa_squad-validation-3270", "mrqa_squad-validation-8294", "mrqa_triviaqa-validation-5477", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-3558", "mrqa_triviaqa-validation-3752", "mrqa_naturalquestions-validation-186", "mrqa_newsqa-validation-4170", "mrqa_hotpotqa-validation-4311", "mrqa_newsqa-validation-169", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-15770", "mrqa_hotpotqa-validation-1239", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-953", "mrqa_triviaqa-validation-4901", "mrqa_triviaqa-validation-4494", "mrqa_newsqa-validation-2984"], "EFR": 0.8518518518518519, "Overall": 0.7185948021885522}, {"timecode": 55, "before_eval_results": {"predictions": ["the 1960s", "Charlton Heston", "house edge of between 0.5 % and 1 %", "Doreen Mantle", "Felicity Huffman", "March 18, 2005", "a solitary figure who is not understood by others, but is actually wise", "when the forward reaction proceeds at the same rate as the reverse reaction", "28 July 1914", "Terry Kath", "1922", "2017 season", "Sylvester Stallone", "2004", "Ethiopia and Liberia", "English law", "Abid Ali Neemuchwala", "Hodel", "first stand - alone instant messenger", "James Fleet", "from September 15, 2015, to November 17, 2015", "ulnar nerve", "Border Collie", "Massachusetts", "citizens", "commercial at", "the star at the center of the Solar System", "60", "August 22, 1980", "Jack Nicklaus", "2020", "General George Washington", "7.6 % Per Annum'( compounded annually )", "14 : 46 JST ( 05 : 46 UTC )", "Part 2", "1966", "As of January 17, 2018, 201 episodes", "2026", "1926", "October 20, 1977", "Cetshwayo", "50 home runs", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "Garbi\u00f1e Muguruza", "23 % of GDP", "Detroit Tigers", "Rockwell", "in outer space", "Charlotte Hornets", "the lumbar cistern", "February 7, 2018", "muezzin", "Elizabeth Taylor", "Sweden", "fourth", "\"Queen In-hyun's Man\"", "James Franco", "step up attacks against innocent civilians.\"", "American Muslim and Christian leaders", "iTunes,", "drummer", "the Manchus", "the 1919 Black Sox Scandal", "seven"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7233676046176046}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.4444444444444445, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-10163", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5034", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-2622", "mrqa_searchqa-validation-12190", "mrqa_newsqa-validation-1458"], "SR": 0.609375, "CSR": 0.5569196428571428, "retrieved_ids": ["mrqa_squad-train-45510", "mrqa_squad-train-37914", "mrqa_squad-train-29419", "mrqa_squad-train-16155", "mrqa_squad-train-57120", "mrqa_squad-train-84303", "mrqa_squad-train-30281", "mrqa_squad-train-21553", "mrqa_squad-train-57929", "mrqa_squad-train-35581", "mrqa_squad-train-81000", "mrqa_squad-train-73330", "mrqa_squad-train-33044", "mrqa_squad-train-29730", "mrqa_squad-train-85760", "mrqa_squad-train-47058", "mrqa_squad-train-70063", "mrqa_squad-train-29320", "mrqa_squad-train-65043", "mrqa_squad-train-69263", "mrqa_squad-train-27807", "mrqa_squad-train-39015", "mrqa_squad-train-37494", "mrqa_squad-train-82047", "mrqa_newsqa-validation-1483", "mrqa_naturalquestions-validation-2794", "mrqa_triviaqa-validation-6380", "mrqa_newsqa-validation-1514", "mrqa_triviaqa-validation-5387", "mrqa_squad-validation-802", "mrqa_triviaqa-validation-3456", "mrqa_naturalquestions-validation-3485", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4844", "mrqa_searchqa-validation-2141", "mrqa_hotpotqa-validation-2800", "mrqa_squad-validation-3954", "mrqa_naturalquestions-validation-9715", "mrqa_searchqa-validation-15033", "mrqa_triviaqa-validation-2996", "mrqa_squad-validation-7845", "mrqa_searchqa-validation-12552", "mrqa_triviaqa-validation-7401", "mrqa_newsqa-validation-3733", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-2006"], "EFR": 1.0, "Overall": 0.7484151785714286}, {"timecode": 56, "before_eval_results": {"predictions": ["stanley", "Sweden", "White House drama", "Adam Smith", "Luxembourg", "El Hiero", "Salvador Domingo Felipe Jacinto", "stave", "tyne", "road race", "The Blues Brothers", "onion", "1984", "Frottage", "Penhaligon", "Kevin Painter", "food", "Messenger orbiter", "cutis anserina", "duck-billed platypus", "Montr\u00e9al", "Jeffrey Archer", "Four Tops", "Velazquez", "Restless Leg Syndrome", "Aviva plc", "Charlie Chan", "Apocalypse Now", "taekwondo", "Ishmael", "jubilee line", "d'Artagnan", "flower", "head", "fogg", "Chuck Hagel", "haute", "zephyr", "300", "motorcycle", "France", "James Garner", "marinated dried fruits", "Jay-Z", "bird", "sexual arousal to pubescent children", "George IV", "Margaret Beckett", "bozeman Daily Chronicle", "White Ferns", "United States", "1836", "Austria - Hungary", "Sean O' Neal", "140 million", "The New Yorker", "In Pursuit", "Aung San Suu Kyi", "his brother to surrender.", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "Gene Wilder", "South Park", "Tucson", "The Simpsons"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6802083333333333}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6437", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-6930", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-4810", "mrqa_newsqa-validation-2308", "mrqa_searchqa-validation-13467", "mrqa_searchqa-validation-11576"], "SR": 0.609375, "CSR": 0.5578399122807017, "retrieved_ids": ["mrqa_squad-train-32025", "mrqa_squad-train-50991", "mrqa_squad-train-76901", "mrqa_squad-train-30483", "mrqa_squad-train-53944", "mrqa_squad-train-71873", "mrqa_squad-train-4269", "mrqa_squad-train-43561", "mrqa_squad-train-76287", "mrqa_squad-train-26295", "mrqa_squad-train-6564", "mrqa_squad-train-85447", "mrqa_squad-train-66176", "mrqa_squad-train-18612", "mrqa_squad-train-86540", "mrqa_squad-train-22649", "mrqa_squad-train-20192", "mrqa_squad-train-52327", "mrqa_squad-train-86165", "mrqa_squad-train-63682", "mrqa_squad-train-83920", "mrqa_squad-train-3516", "mrqa_squad-train-34085", "mrqa_squad-train-70153", "mrqa_triviaqa-validation-4457", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-2385", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-81", "mrqa_naturalquestions-validation-10037", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-468", "mrqa_naturalquestions-validation-1884", "mrqa_hotpotqa-validation-4676", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-2568", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3523", "mrqa_squad-validation-3497", "mrqa_squad-validation-978", "mrqa_newsqa-validation-2506", "mrqa_searchqa-validation-6181", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2671", "mrqa_searchqa-validation-1857", "mrqa_naturalquestions-validation-2832", "mrqa_triviaqa-validation-6469"], "EFR": 0.96, "Overall": 0.7405992324561403}, {"timecode": 57, "before_eval_results": {"predictions": ["CBS", "lord Nelson", "l", "Utah", "black light", "lacrosse", "Packers", "denmark", "Operation Overlord", "james boswell", "Virginia", "james boswell", "yachts", "eggplant", "1215", "pullover", "diffusion", "wye", "jack London", "South Carolina", "ellesmere port", "lion", "japan", "chile", "japan", "Lynda Baron", "Robert Guerrero", "alcatraz", "90%", "president Sven Goran Eriksson", "james boswell", "july", "star", "Jordan", "a system of recording important things", "Motown", "Sudan", "salvaged glass", "hawks", "army", "edith", "anschluss", "tabinet", "Irving Berlin", "medical", "Leo Tolstoy", "Austria", "oasis", "energy drinks", "Naples", "planes", "first adopted by the university's science club in 1886", "2003", "Magnavox Odyssey", "Clark County", "created the American Land- Grant universities and colleges", "fifth level", "a vigilante group whose goal is the eradication of the Zetas cartel from the state of Veracruz,", "Japan", "Dr. Maria Siemionow,", "the altitude", "Rowling", "apricot", "the Red Sea"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5852120535714286}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.125, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-1395", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-6233", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5877", "mrqa_naturalquestions-validation-8707", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2021", "mrqa_newsqa-validation-2792", "mrqa_searchqa-validation-5198"], "SR": 0.515625, "CSR": 0.5571120689655172, "retrieved_ids": ["mrqa_squad-train-22611", "mrqa_squad-train-32141", "mrqa_squad-train-25322", "mrqa_squad-train-17239", "mrqa_squad-train-48840", "mrqa_squad-train-65948", "mrqa_squad-train-50893", "mrqa_squad-train-45321", "mrqa_squad-train-83373", "mrqa_squad-train-47950", "mrqa_squad-train-51641", "mrqa_squad-train-5154", "mrqa_squad-train-5261", "mrqa_squad-train-42870", "mrqa_squad-train-24567", "mrqa_squad-train-66674", "mrqa_squad-train-3209", "mrqa_squad-train-16554", "mrqa_squad-train-27997", "mrqa_squad-train-2884", "mrqa_squad-train-49304", "mrqa_squad-train-70107", "mrqa_squad-train-81942", "mrqa_squad-train-74062", "mrqa_searchqa-validation-6304", "mrqa_squad-validation-8229", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-2595", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3347", "mrqa_newsqa-validation-2886", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-2206", "mrqa_hotpotqa-validation-10", "mrqa_newsqa-validation-1557", "mrqa_naturalquestions-validation-2023", "mrqa_squad-validation-2383", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-230", "mrqa_squad-validation-1748", "mrqa_naturalquestions-validation-10199", "mrqa_searchqa-validation-2456", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-1496", "mrqa_naturalquestions-validation-1439", "mrqa_squad-validation-4150"], "EFR": 0.9032258064516129, "Overall": 0.7290988250834259}, {"timecode": 58, "before_eval_results": {"predictions": ["Christian Louboutin", "apples", "Galapagos Islands", "for Gallantry", "Tonight (Ensemble)", "onions", "bratislava", "Mariah Carey", "blancmange", "daily Herald", "four", "Isaac", "dicks Francis", "larkin", "thomas caxton", "opossum", "Soviets", "UKIP", "king Edward", "charlie", "mASH", "hanff", "capone mortuis", "condor", "molybdenum", "France", "laos", "sports agent", "Puerto Rico", "John Huston", "posh", "Turkish Van", "bajan", "aurochs", "red bull", "michael Churchill", "de Gaulle", "Mercury", "the Kamikaze", "michael dalton", "fagott", "Mary Poppins", "furmity tent", "Queensland", "Blofeld", "Kodak", "211 associations", "Kenya", "George IV", "tuscany", "n Nissan", "Ptolemy", "Toto", "commemorating fealty and filial piety", "Heather Elizabeth Langenkamp", "Operation Iceberg", "quarterly", "Rambosk", "Revolutionary Armed Forces of Colombia,", "a preliminary injunction", "the final exclamation", "pole vaulting", "Maine", "Coleridge"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6266826923076922}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-1089", "mrqa_triviaqa-validation-1383", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-3453", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-1832", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-2487", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1708", "mrqa_triviaqa-validation-1637", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-4635", "mrqa_hotpotqa-validation-2639", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-384", "mrqa_searchqa-validation-10238", "mrqa_searchqa-validation-444", "mrqa_searchqa-validation-5746"], "SR": 0.515625, "CSR": 0.5564088983050848, "retrieved_ids": ["mrqa_squad-train-35015", "mrqa_squad-train-86383", "mrqa_squad-train-30857", "mrqa_squad-train-22928", "mrqa_squad-train-85067", "mrqa_squad-train-1618", "mrqa_squad-train-28059", "mrqa_squad-train-51290", "mrqa_squad-train-29532", "mrqa_squad-train-72847", "mrqa_squad-train-3126", "mrqa_squad-train-1401", "mrqa_squad-train-17328", "mrqa_squad-train-43464", "mrqa_squad-train-85374", "mrqa_squad-train-12600", "mrqa_squad-train-3907", "mrqa_squad-train-43361", "mrqa_squad-train-56801", "mrqa_squad-train-21792", "mrqa_squad-train-79328", "mrqa_squad-train-12639", "mrqa_squad-train-5714", "mrqa_squad-train-66609", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-2369", "mrqa_triviaqa-validation-1395", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-4886", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-421", "mrqa_squad-validation-2975", "mrqa_hotpotqa-validation-2262", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-3170", "mrqa_searchqa-validation-6445", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-5179", "mrqa_squad-validation-9166", "mrqa_naturalquestions-validation-4137", "mrqa_hotpotqa-validation-2138", "mrqa_newsqa-validation-3463", "mrqa_naturalquestions-validation-10259", "mrqa_triviaqa-validation-2478", "mrqa_hotpotqa-validation-4399", "mrqa_naturalquestions-validation-3893", "mrqa_newsqa-validation-2580"], "EFR": 1.0, "Overall": 0.748313029661017}, {"timecode": 59, "before_eval_results": {"predictions": ["Jesus", "Strato of Lampsacus", "Eliot Cutler", "goalkeeper", "David Weissman", "george tipton", "comedy", "November 29, 1895", "the Goddess of Pop", "Sir Philip Anthony Hopkins", "near Philip Billard Municipal Airport", "first bowl win since the 1992 John Hancock Bowl", "two years", "Walt Disney and Ub Iwerks at the Walt Disney Studios in 1928", "Martin McCann", "WB", "gainsborough Trinity Football Club is a football club based in Gainsborough, Lincolnshire, England.", "turned out to be a terrible date", "$7.3 billion", "best known for his ten seasons with the Charlotte Hornets", "George I", "sixteen", "The Rural Electrification Act of 1936", "2015", "Ron Swanson", "Golden Globe Award for Best Actress \u2013 Motion Picture Comedy or Musical", "Jahseh Dwayne Onfroy", "Dire Straits", "American reality television series", "MGM Grand Garden Special Events Center", "Best Alternative Music Album", "Pieter van Musschenbroek", "1979", "70 m and 90 m", "prime minister", "video game", "Bulgarian-Canadian", "KXII", "James Bond", "Eastern College Athletic Conference", "Indian", "William Corcoran Eustis", "The 1st World Outgames", "Adelaide", "Saturday", "Shooter Jennings", "Can't Be Tamed", "Bolton, England", "Stephen Hawking", "Samantha Rockwell", "Saoirse Ronan", "Debbie Reynolds, Gene Kelly and Donald O'Connor", "Scheria", "Todd Bridges", "lemon", "english", "Jaipur", "in the southern Gaza city of Rafah,", "U.S. Consulate in Rio de Janeiro", "CNN", "All's Well That Ending Well", "ice cream", "Rock Island, Illinois", "Captain James Cook"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7515275956682206}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.625, 0.8, 0.5, 1.0, 0.7692307692307692, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8000000000000002, 0.0, 0.8, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2372", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-1007", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-2256", "mrqa_hotpotqa-validation-679", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-2429", "mrqa_naturalquestions-validation-5600", "mrqa_triviaqa-validation-3147", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-6145", "mrqa_searchqa-validation-2444", "mrqa_searchqa-validation-15613"], "SR": 0.609375, "CSR": 0.5572916666666667, "retrieved_ids": ["mrqa_squad-train-23845", "mrqa_squad-train-36710", "mrqa_squad-train-82682", "mrqa_squad-train-34265", "mrqa_squad-train-26052", "mrqa_squad-train-12110", "mrqa_squad-train-13535", "mrqa_squad-train-19810", "mrqa_squad-train-31867", "mrqa_squad-train-24807", "mrqa_squad-train-59095", "mrqa_squad-train-34428", "mrqa_squad-train-81774", "mrqa_squad-train-95", "mrqa_squad-train-64982", "mrqa_squad-train-871", "mrqa_squad-train-9951", "mrqa_squad-train-21932", "mrqa_squad-train-80183", "mrqa_squad-train-4004", "mrqa_squad-train-74776", "mrqa_squad-train-54934", "mrqa_squad-train-38430", "mrqa_squad-train-47221", "mrqa_naturalquestions-validation-10377", "mrqa_searchqa-validation-9536", "mrqa_naturalquestions-validation-5360", "mrqa_triviaqa-validation-5981", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1634", "mrqa_triviaqa-validation-1714", "mrqa_hotpotqa-validation-1906", "mrqa_newsqa-validation-4041", "mrqa_searchqa-validation-16378", "mrqa_triviaqa-validation-5613", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-7767", "mrqa_triviaqa-validation-4317", "mrqa_naturalquestions-validation-1863", "mrqa_squad-validation-7332", "mrqa_squad-validation-5435", "mrqa_triviaqa-validation-6368", "mrqa_newsqa-validation-1762", "mrqa_naturalquestions-validation-809", "mrqa_triviaqa-validation-6833", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-129", "mrqa_newsqa-validation-2471"], "EFR": 1.0, "Overall": 0.7484895833333334}, {"timecode": 60, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1496", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2256", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2456", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8282", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1668", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3627", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-468", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5622", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6755", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7112", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7558", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-899"], "OKR": 0.90234375, "KG": 0.48671875, "before_eval_results": {"predictions": ["Batman", "Part XI of the Indian constitution", "Frank Oz", "786 -- 802", "Patris et Filii et Spiritus Sancti", "19 July 1990", "John Ernest Crawford", "Andy Warhol", "September 1972", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "in France", "BC Jean and Toby Gad", "the BBC", "57 days", "961", "Jay Baruchel", "December 1886", "American foreign policy was largely determined by President Woodrow Wilson,", "at the state and national governmental level", "The courts", "Lori Rom", "Tiger Woods", "2014", "Coroebus of Elis", "the fourth - largest planet by diameter, the third-most - massive planet, and the densest giant planet", "Crepuscular animals", "Hank Williams", "due to not being profitable", "in a nearby river bottom", "Issues of the American Civil War", "around 10 : 30am", "David Ben - Gurion", "RMS Titanic", "a warrior, Mage, or rogue coming from an elven, human, or dwarfven background", "Angel Island Immigration Station", "Eight", "a tradition in brass band parades in New Orleans, Louisiana", "Vasoepididymostomy", "October 1 to September 30", "James Bolam", "a feminine form of the Hebrew Yohannan, `` God forgave / God gratified ''", "in Broken Hill and Sydney", "the Reverse - Flash", "save, rescue, savior", "sedimentary rock", "Rabindranath Tagore", "NFL", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "post translational modification", "the 135th meridian west of the Greenwich Observatory", "near Camarillo, California", "Cordelia", "tomato", "Guru Nanak", "footballer", "mixed martial arts", "James Tinling", "Rima Fakih", "165-room", "David Bowie,", "Peach Melba", "the Queen Charlotte Sound", "Godiva", "Sri Lanka Freedom Party"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7008422694544019}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615385, 0.6666666666666666, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.0, 0.6153846153846153, 0.888888888888889, 0.0, 0.5, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1966", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-7489", "mrqa_hotpotqa-validation-4952", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-12527"], "SR": 0.59375, "CSR": 0.5578893442622951, "retrieved_ids": ["mrqa_squad-train-33779", "mrqa_squad-train-73845", "mrqa_squad-train-53818", "mrqa_squad-train-9152", "mrqa_squad-train-38556", "mrqa_squad-train-82568", "mrqa_squad-train-45233", "mrqa_squad-train-5132", "mrqa_squad-train-75644", "mrqa_squad-train-1336", "mrqa_squad-train-8881", "mrqa_squad-train-20253", "mrqa_squad-train-84781", "mrqa_squad-train-48461", "mrqa_squad-train-75038", "mrqa_squad-train-35966", "mrqa_squad-train-66163", "mrqa_squad-train-71162", "mrqa_squad-train-21092", "mrqa_squad-train-38064", "mrqa_squad-train-22253", "mrqa_squad-train-36403", "mrqa_squad-train-79627", "mrqa_squad-train-52157", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-1426", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-191", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6324", "mrqa_newsqa-validation-2671", "mrqa_naturalquestions-validation-1455", "mrqa_newsqa-validation-1544", "mrqa_searchqa-validation-16659", "mrqa_hotpotqa-validation-3442", "mrqa_squad-validation-4838", "mrqa_naturalquestions-validation-7855", "mrqa_newsqa-validation-3961", "mrqa_hotpotqa-validation-2256", "mrqa_naturalquestions-validation-7535", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-270", "mrqa_searchqa-validation-7780", "mrqa_newsqa-validation-2766", "mrqa_hotpotqa-validation-511"], "EFR": 1.0, "Overall": 0.7464216188524591}, {"timecode": 61, "before_eval_results": {"predictions": ["1994 season", "Tenochtitlan", "Conrad Lewis", "Bart Millard", "Pangaea or Pangea", "111", "Kiss", "Justice Harlan", "full '' sexual intercourse", "Rose Stagg ( Valene Kane )", "Georgia Groome", "The word autumn comes from the ancient Etruscan root autu - and has within it connotations of the passing of the year", "Gina Tognoni / to\u028an\u02c8jo\u028ani / ( born November 28, 1973 )", "T.J. Miller", "rural depopulation and the pursuit of excessive wealth", "Malina Weissman", "Pasek & Paul", "the state sector", "741 weeks", "the chief Senate spokespeople for the political parties respectively holding the majority and the minority in the United States Senate", "neutral", "1957", "cat in the hat knows a lot about space movie", "1999", "the American colonies", "the concentration of a compound exceeds its solubility", "February 9, 2018", "a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "Andrew Lloyd Webber", "Dollree Mapp", "the 15th century", "electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions", "Nehemiah", "Unwinding of DNA at the origin", "Detroit", "Mickey Mantle", "Shawn", "Simone Vangsness", "dress shop", "the First Epistle of John at 5 : 7 -- 8", "1603", "September 25, 1987", "the 1970s and'80s", "1939", "Randy Newman", "1956", "Ravi", "prokaryotic", "4", "an active supporter of the League of Nations", "New York City", "shoes", "sven goran eriksson", "horses", "Vanarama", "Polonius", "40 million", "Dr. Maria Siemionow", "Joe Harn", "gun charges,", "Ronald Reagan", "titanium", "Hastings", "(Urien) Urien"], "metric_results": {"EM": 0.53125, "QA-F1": 0.637640918109668}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false], "QA-F1": [0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.45454545454545453, 0.4444444444444445, 1.0, 0.7777777777777778, 1.0, 1.0, 0.5, 0.0, 0.2424242424242424, 0.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.25, 0.0, 0.20000000000000004, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-442", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-5479", "mrqa_newsqa-validation-4098", "mrqa_searchqa-validation-15585", "mrqa_searchqa-validation-13746", "mrqa_triviaqa-validation-3768"], "SR": 0.53125, "CSR": 0.5574596774193548, "retrieved_ids": ["mrqa_squad-train-49172", "mrqa_squad-train-26423", "mrqa_squad-train-24405", "mrqa_squad-train-61383", "mrqa_squad-train-31073", "mrqa_squad-train-26942", "mrqa_squad-train-24795", "mrqa_squad-train-37898", "mrqa_squad-train-54755", "mrqa_squad-train-10367", "mrqa_squad-train-6485", "mrqa_squad-train-75118", "mrqa_squad-train-28184", "mrqa_squad-train-5643", "mrqa_squad-train-31717", "mrqa_squad-train-35938", "mrqa_squad-train-51495", "mrqa_squad-train-74737", "mrqa_squad-train-72696", "mrqa_squad-train-78550", "mrqa_squad-train-60061", "mrqa_squad-train-55896", "mrqa_squad-train-28628", "mrqa_squad-train-68418", "mrqa_triviaqa-validation-4974", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-7812", "mrqa_squad-validation-1136", "mrqa_searchqa-validation-14371", "mrqa_hotpotqa-validation-62", "mrqa_triviaqa-validation-5726", "mrqa_newsqa-validation-3871", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-2926", "mrqa_searchqa-validation-217", "mrqa_newsqa-validation-2042", "mrqa_squad-validation-762", "mrqa_naturalquestions-validation-7351", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-339", "mrqa_hotpotqa-validation-1900", "mrqa_naturalquestions-validation-2060", "mrqa_newsqa-validation-1862", "mrqa_naturalquestions-validation-8699", "mrqa_newsqa-validation-1700", "mrqa_squad-validation-9136", "mrqa_naturalquestions-validation-5583", "mrqa_searchqa-validation-16021"], "EFR": 0.9666666666666667, "Overall": 0.7396690188172043}, {"timecode": 62, "before_eval_results": {"predictions": ["troopship", "horseracing", "Italy", "honeybee", "b\u00e9al Feirste", "linesider", "63 to 144 inches", "james boswell", "squash", "Jack London", "AFC Wimbledon", "london", "Edward VIII", "Bugs Bunny", "Botswana is a landlocked country located in Southern Africa", "ambidextrous", "bear Grylls", "Japan", "wake", "mercury", "y Yahoo!", "Klaus Barbie", "honey", "Joanne Harris", "quarrymen", "Kunigunde Mackamotski", "Moldova", "Chatsworth House", "India and Pakistan", "Bull Moose Party", "arctic", "eagle", "arlanda", "Harnoncourt", "Hercules", "Real Madrid", "Tennessee whiskey", "Matthew Pinsent", "Iran", "salsa", "Cuba", "John McEnroe", "kia", "Robert Stroud", "cat Stevens", "cuticle", "tyne", "oxygen", "mulberry", "trumpet", "Cockermouth", "more rural in its character and more readily adopted Latin as its common language", "October 1941", "Bharata Muni", "Amber Laura Heard", "Philip Billard Municipal Airport", "gull-wing doors", "July 8", "sexual harassment claims", "5,600", "a Cajun", "a natural at milking cows in", "A Moon for the Misbegotten", "If These Dolls Could Talk"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6461805555555555}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.5, 0.8, 1.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-2254", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6819", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-5993", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-988", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-2840", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-15132", "mrqa_searchqa-validation-9158"], "SR": 0.59375, "CSR": 0.5580357142857143, "retrieved_ids": ["mrqa_squad-train-78098", "mrqa_squad-train-38986", "mrqa_squad-train-30735", "mrqa_squad-train-2789", "mrqa_squad-train-20191", "mrqa_squad-train-69965", "mrqa_squad-train-77578", "mrqa_squad-train-73635", "mrqa_squad-train-37989", "mrqa_squad-train-15097", "mrqa_squad-train-24710", "mrqa_squad-train-11617", "mrqa_squad-train-63993", "mrqa_squad-train-60453", "mrqa_squad-train-41818", "mrqa_squad-train-12366", "mrqa_squad-train-14032", "mrqa_squad-train-81057", "mrqa_squad-train-80989", "mrqa_squad-train-81305", "mrqa_squad-train-38964", "mrqa_squad-train-76607", "mrqa_squad-train-38769", "mrqa_squad-train-45415", "mrqa_naturalquestions-validation-2855", "mrqa_triviaqa-validation-4599", "mrqa_searchqa-validation-13003", "mrqa_squad-validation-3130", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-809", "mrqa_triviaqa-validation-7030", "mrqa_triviaqa-validation-6896", "mrqa_triviaqa-validation-51", "mrqa_newsqa-validation-1514", "mrqa_squad-validation-3863", "mrqa_newsqa-validation-2976", "mrqa_triviaqa-validation-2632", "mrqa_squad-validation-2145", "mrqa_squad-validation-2911", "mrqa_squad-validation-10217", "mrqa_hotpotqa-validation-3742", "mrqa_triviaqa-validation-436", "mrqa_newsqa-validation-501", "mrqa_triviaqa-validation-4715", "mrqa_hotpotqa-validation-4828", "mrqa_newsqa-validation-3503", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-7661"], "EFR": 0.9230769230769231, "Overall": 0.7310662774725275}, {"timecode": 63, "before_eval_results": {"predictions": ["his mother.", "the southern city of Naples", "\"CNN Heroes: An All-Star Tribute\"", "for the rest of the year", "\"It was never our intention to offend anyone,\"", "Bob Bogle,", "salutes the \"People of Palestine\" and calls on them to fight back against Israel in Gaza.", "his business dealings", "Saturday", "People Against Switching Sides (PASS)", "Haiti,", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Darrin Tuck,", "Amsterdam, in the Netherlands,", "a review of state government practices completed in 100 days.", "prostate cancer,", "90", "a birdie four at the last hole", "Rima Fakih", "U.S. Department of Agriculture", "37", "her indictment in the 1969 \"Manson murders.\"", "33-year-old", "his son, Isaac, and daughter, Rebecca.", "12-hour-plus", "The judge", "President Obama's race in 2008.", "oldest documented bikinis", "free laundry service.", "angry over the treatment of Muslims,", "twice.", "learn in safer surroundings.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "Friday", "Gavin de Becker", "400 years ago", "U.S. Consulate in Rio de Janeiro", "Apple employees", "heavy turbulence", "$3 billion,", "Nkepile M abuse", "resources", "memories of his mother.", "pregnant soldier", "then-Sen. Obama", "Technological Institute of Higher Learning of Monterrey,", "female soldier,", "\"Now that we know Muhammad is an Ennis man, we will be back,\"", "David Russ,", "The former child actor was hospitalized briefly three months ago after suffering a seizure while being interviewed on a TV show in Los Angeles, California.", "Chinese", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "The stability, security, and predictability of British law and government", "six", "Israel", "algebra", "Chuck Yeager", "Detroit, Michigan", "the north bank of the North Esk", "singer", "a dinosaur", "temperature", "The Thief knot", "from Fort Kent, Maine, at the Canada -- US border, south to Key West, Florida"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6965172777672777}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.10810810810810811, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3087", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-1827", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-9739", "mrqa_searchqa-validation-6571", "mrqa_searchqa-validation-9458"], "SR": 0.640625, "CSR": 0.559326171875, "retrieved_ids": ["mrqa_squad-train-4947", "mrqa_squad-train-23165", "mrqa_squad-train-20538", "mrqa_squad-train-32310", "mrqa_squad-train-41902", "mrqa_squad-train-5166", "mrqa_squad-train-28622", "mrqa_squad-train-42177", "mrqa_squad-train-83539", "mrqa_squad-train-10669", "mrqa_squad-train-48343", "mrqa_squad-train-44405", "mrqa_squad-train-28430", "mrqa_squad-train-20089", "mrqa_squad-train-73106", "mrqa_squad-train-36907", "mrqa_squad-train-11469", "mrqa_squad-train-5099", "mrqa_squad-train-24", "mrqa_squad-train-32361", "mrqa_squad-train-73574", "mrqa_squad-train-9283", "mrqa_squad-train-65968", "mrqa_squad-train-4861", "mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-2487", "mrqa_triviaqa-validation-5229", "mrqa_hotpotqa-validation-3703", "mrqa_naturalquestions-validation-186", "mrqa_newsqa-validation-3605", "mrqa_hotpotqa-validation-1467", "mrqa_triviaqa-validation-5044", "mrqa_naturalquestions-validation-1912", "mrqa_searchqa-validation-16826", "mrqa_triviaqa-validation-899", "mrqa_hotpotqa-validation-5386", "mrqa_naturalquestions-validation-10554", "mrqa_squad-validation-9764", "mrqa_triviaqa-validation-1089", "mrqa_naturalquestions-validation-6015", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-2902", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-421", "mrqa_triviaqa-validation-206", "mrqa_newsqa-validation-3016", "mrqa_hotpotqa-validation-5117", "mrqa_squad-validation-3130"], "EFR": 1.0, "Overall": 0.746708984375}, {"timecode": 64, "before_eval_results": {"predictions": ["the Mongols", "beer", "sweepstakes", "General McClellan", "St. Agnes", "the Cabernet", "Dorothy", "calcite", "Bologna", "potatoes", "Princeton", "China", "the Knight", "Evian", "unicorns", "heaven", "the Andes", "Jim Jarmusch", "Martin Luther", "Miles Davis", "Tennessee", "Audrey Hepburn", "Falafel", "Aladdin", "history of Lake County, Indiana, and the Calumet region", "Derek Jeter", "Arthur C. Clarke", "Super Bowl XXII", "the Vietnam War", "The Accused", "Sydney", "Christian Louboutin", "monk seal", "beer", "letters", "less than 1% fat", "Ginger Rogers", "Beijing", "plumeria", "Lafayette", "Marie Osmond", "The Pickwick Club", "pemmican", "comet", "Chuck Yeager", "Newton's Second Law", "sheep", "Eragon", "Russia", "French toast", "the Fifth Amendment", "Elvis Presley", "permanent display at the Louvre Museum in Paris since 1797", "notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "bacall", "jaws", "denarius", "29 June 1941", "White Horse", "Constitution of Mexico", "Kurt Cobain's", "Glasgow, Scotland", "Christopher Savoie", "London"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6479461102622868}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.823529411764706, 0.7878787878787877, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-13851", "mrqa_searchqa-validation-214", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-6953", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-13675", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-6692", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-1377", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-13122", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10795", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-5831", "mrqa_hotpotqa-validation-4263", "mrqa_hotpotqa-validation-1876", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-2011"], "SR": 0.5625, "CSR": 0.559375, "retrieved_ids": ["mrqa_squad-train-43362", "mrqa_squad-train-71009", "mrqa_squad-train-1100", "mrqa_squad-train-19816", "mrqa_squad-train-63248", "mrqa_squad-train-66988", "mrqa_squad-train-69878", "mrqa_squad-train-57135", "mrqa_squad-train-43726", "mrqa_squad-train-51501", "mrqa_squad-train-28470", "mrqa_squad-train-32255", "mrqa_squad-train-79164", "mrqa_squad-train-83565", "mrqa_squad-train-23209", "mrqa_squad-train-29595", "mrqa_squad-train-32210", "mrqa_squad-train-25374", "mrqa_squad-train-85069", "mrqa_squad-train-13005", "mrqa_squad-train-26956", "mrqa_squad-train-1521", "mrqa_squad-train-80885", "mrqa_squad-train-24751", "mrqa_squad-validation-9166", "mrqa_newsqa-validation-3087", "mrqa_triviaqa-validation-3338", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-4351", "mrqa_hotpotqa-validation-4120", "mrqa_squad-validation-2538", "mrqa_squad-validation-2368", "mrqa_searchqa-validation-12527", "mrqa_newsqa-validation-2904", "mrqa_naturalquestions-validation-359", "mrqa_squad-validation-7845", "mrqa_naturalquestions-validation-9856", "mrqa_hotpotqa-validation-5573", "mrqa_searchqa-validation-13520", "mrqa_naturalquestions-validation-10156", "mrqa_triviaqa-validation-6121", "mrqa_newsqa-validation-1488", "mrqa_naturalquestions-validation-2196", "mrqa_newsqa-validation-3982", "mrqa_naturalquestions-validation-2558", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-8530"], "EFR": 0.9285714285714286, "Overall": 0.7324330357142858}, {"timecode": 65, "before_eval_results": {"predictions": ["Montana", "the McClatchy Company", "Spectre", "William Tell", "The Apprentice", "Aeschylus", "the College of William", "Intelligence Quotient", "Robert A. Heinlein", "a beehive- or barrel-shaped container of baked clay", "24", "Cowpoke", "Monty Python and the Holy Grail", "Ludwig van Beethoven", "Stalin", "E pluribus unum", "Portland", "China", "Abishalom", "Castle Rock", "Bollywood", "a doctor", "the Habsburg", "eudaimonia", "a Twinkie", "the altitude", "immoral", "Richard", "Anne of Cleves", "Siddur", "Won't you be mine", "Liliuokalani", "the pituitary", "the South African Boer War", "the pulp", "the high school girlfriend of an alcoholic student", "Aswan", "Billy Ray Cyrus", "quiver", "The Body", "Impostor syndrome", "displace", "Davy Crockett", "Sagittarius", "magma", "zinc", "Dubliners", "Jules Verne", "Cuba", "the Taliban", "Arlington", "Holly", "Otis Timson", "during prenatal development", "james stokley", "Maria do Carmo Miranda da Cunha", "Joan Crawford", "superhero Birdman", "Chief of the Operations Staff of the Armed Forces High Command", "Erich Maria Remarque", "Elena Kagan", "alleviation of their pain", "15-year-old's", "Qutab - ud - din Aibak"], "metric_results": {"EM": 0.390625, "QA-F1": 0.49810267857142854}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.8, 0.0, 0.25, 1.0, 0.25, 0.0, 0.5, 0.25, 0.09523809523809523, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15873", "mrqa_searchqa-validation-11355", "mrqa_searchqa-validation-9446", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-5265", "mrqa_searchqa-validation-15917", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-2194", "mrqa_searchqa-validation-5095", "mrqa_searchqa-validation-894", "mrqa_searchqa-validation-10927", "mrqa_searchqa-validation-10163", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-9120", "mrqa_searchqa-validation-11135", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-16276", "mrqa_searchqa-validation-3734", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-11604", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-8840", "mrqa_searchqa-validation-6273", "mrqa_searchqa-validation-1852", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-14981", "mrqa_searchqa-validation-8703", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-2440", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-1982", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1062"], "SR": 0.390625, "CSR": 0.5568181818181819, "retrieved_ids": ["mrqa_squad-train-51100", "mrqa_squad-train-19653", "mrqa_squad-train-48529", "mrqa_squad-train-54428", "mrqa_squad-train-4293", "mrqa_squad-train-38948", "mrqa_squad-train-64067", "mrqa_squad-train-21388", "mrqa_squad-train-11610", "mrqa_squad-train-3999", "mrqa_squad-train-31705", "mrqa_squad-train-36861", "mrqa_squad-train-64044", "mrqa_squad-train-32254", "mrqa_squad-train-12165", "mrqa_squad-train-17587", "mrqa_squad-train-45174", "mrqa_squad-train-48938", "mrqa_squad-train-63621", "mrqa_squad-train-55917", "mrqa_squad-train-71543", "mrqa_squad-train-33911", "mrqa_squad-train-52559", "mrqa_squad-train-12960", "mrqa_hotpotqa-validation-3136", "mrqa_triviaqa-validation-4966", "mrqa_naturalquestions-validation-2067", "mrqa_newsqa-validation-1688", "mrqa_triviaqa-validation-4750", "mrqa_newsqa-validation-1820", "mrqa_searchqa-validation-9148", "mrqa_hotpotqa-validation-4389", "mrqa_squad-validation-4572", "mrqa_naturalquestions-validation-4279", "mrqa_hotpotqa-validation-741", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-1550", "mrqa_searchqa-validation-3525", "mrqa_triviaqa-validation-1348", "mrqa_triviaqa-validation-1124", "mrqa_squad-validation-3863", "mrqa_triviaqa-validation-5858", "mrqa_hotpotqa-validation-4515", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6920", "mrqa_naturalquestions-validation-8707", "mrqa_newsqa-validation-3437"], "EFR": 0.9743589743589743, "Overall": 0.7410791812354313}, {"timecode": 66, "before_eval_results": {"predictions": ["eight", "function like an endocrine organ", "December 20, 1951", "the ninth w\u0101", "Terry Kath", "an inability to comprehend and formulate language", "hyperirritable spots in the fascia surrounding skeletal muscle", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "1546", "Banquo", "January 1923", "CeCe Drake", "a habitat", "lamb", "Geophysicists who specialize in paleomagnetism", "free floating", "the United States", "Help! is the fifth studio album by English rock band the Beatles", "30 October 1918", "Austria - Hungary", "Domhnall Gleeson", "13 to 22 June 2012", "T - Bone Walker", "Paul Baumer", "in Paradise, Nevada", "Executive Residence of the White House Complex", "Article Two", "April 13, 2018", "Bush", "Yuzuru Hanyu", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "across the Strait of Magellan", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "in Rome", "December 27, 2015", "Leonardo da Vinci", "absolute temperature", "Thawne", "Philippe Petit", "Proposition 103", "2008", "inside the cell nucleus", "Julie Adams", "The palace has 775 rooms", "Scotland", "100 % owned by Xiu Li Dai and Yongge Dai", "Norman Whitfield", "Americans who served in the armed forces and as civilians during World War II", "Uplokayukta", "James Fleet", "the year 1 BC", "David Davis", "Dirty Dancing", "mumps", "Delphi Lawrence", "2 May 2015", "International Boxing Hall of Fame (IBHOF)", "a dress", "Pakistani officials,", "Almost all British troops in Iraq", "Jamaica", "copra", "Python", "not guilty of affray"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6691652608012901}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.8, 0.8333333333333333, 0.8095238095238095, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.2, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-4419", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-47", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1259", "mrqa_searchqa-validation-10836", "mrqa_newsqa-validation-37"], "SR": 0.5625, "CSR": 0.5569029850746269, "retrieved_ids": ["mrqa_squad-train-69752", "mrqa_squad-train-31215", "mrqa_squad-train-35593", "mrqa_squad-train-79633", "mrqa_squad-train-23709", "mrqa_squad-train-50249", "mrqa_squad-train-10872", "mrqa_squad-train-17113", "mrqa_squad-train-77473", "mrqa_squad-train-28665", "mrqa_squad-train-15325", "mrqa_squad-train-39376", "mrqa_squad-train-37279", "mrqa_squad-train-62495", "mrqa_squad-train-827", "mrqa_squad-train-62117", "mrqa_squad-train-22345", "mrqa_squad-train-58695", "mrqa_squad-train-3490", "mrqa_squad-train-78000", "mrqa_squad-train-54002", "mrqa_squad-train-64937", "mrqa_squad-train-36602", "mrqa_squad-train-81720", "mrqa_naturalquestions-validation-9578", "mrqa_searchqa-validation-3613", "mrqa_triviaqa-validation-6915", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-12996", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-9848", "mrqa_triviaqa-validation-1583", "mrqa_searchqa-validation-1771", "mrqa_triviaqa-validation-2693", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-13003", "mrqa_naturalquestions-validation-5831", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-276", "mrqa_hotpotqa-validation-1112", "mrqa_searchqa-validation-123", "mrqa_triviaqa-validation-3735", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-8027"], "EFR": 0.9642857142857143, "Overall": 0.7390814898720682}, {"timecode": 67, "before_eval_results": {"predictions": ["in Pyeongchang County, Gangwon Province, South Korea", "Padawan", "in a liquid solution", "April 1917", "London", "Utah", "1969", "by October 1986", "the referee", "geographical area", "species capable of reproduction", "Michael Kopelow", "Reproductive system", "a land mass ( smaller than a continent ) that is surrounded by water", "the Parliament of the United Kingdom", "husky or other Nordic breed", "Gibraltar", "September 1947", "every year from 6 -- 14 July", "in the bone marrow", "Sophia Akuffo", "who the better fighters are relative to their weight", "each team has either selected a player or traded its draft position", "Sarah Josepha Hale", "Ingrid Bergman", "Jessica Simpson", "on the microscope's stage", "the Old Testament", "Daren Maxwell Kagasoff", "the country club pool was Eagle Ridge Outdoor pool in Coquitlam, BC", "Mary Elizabeth Ellis", "Michigan and surrounding states and provinces", "a recognized group of people who jointly oversee the activities of an organization", "Friedman Billings Ramsey", "Miami Heat of the National Basketball Association ( NBA )", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys", "Toronto Islands", "lighter", "the final episode of the series", "Richard Carpenter", "Konakuppakatil Gopinathan Balakrishnan", "Ryan Evancic", "Border Collie", "Vesta", "1665 to 1666", "sugars and amino acids", "Aegisthus", "about 1,300 km ( 800 mi ) from the nearest open sea at Bay of Whales", "Oklahoma", "during Christmas season in the late 1970s", "December 1349", "Ipswich Town", "most well-known post-impressionist", "British Airways", "gender queer", "14,673", "YouTube", "five", "Joel \"Taz\" DiGregorio", "detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "the Death Valley", "the 1972 Summer Olympics", "Ichabod Crane", "Thomas Jefferson"], "metric_results": {"EM": 0.453125, "QA-F1": 0.597254460331782}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, true, false, true, true], "QA-F1": [0.923076923076923, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5517241379310345, 0.3076923076923077, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.3076923076923077, 0.0, 0.1739130434782609, 1.0, 1.0, 0.4444444444444445, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.56, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9760", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-1269", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-4558", "mrqa_triviaqa-validation-263", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-621", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-4207", "mrqa_searchqa-validation-324"], "SR": 0.453125, "CSR": 0.5553768382352942, "retrieved_ids": ["mrqa_squad-train-38160", "mrqa_squad-train-46391", "mrqa_squad-train-70395", "mrqa_squad-train-21057", "mrqa_squad-train-15101", "mrqa_squad-train-85424", "mrqa_squad-train-74281", "mrqa_squad-train-64202", "mrqa_squad-train-1361", "mrqa_squad-train-56535", "mrqa_squad-train-72844", "mrqa_squad-train-663", "mrqa_squad-train-56562", "mrqa_squad-train-22274", "mrqa_squad-train-27408", "mrqa_squad-train-12348", "mrqa_squad-train-15781", "mrqa_squad-train-64065", "mrqa_squad-train-43402", "mrqa_squad-train-44309", "mrqa_squad-train-14982", "mrqa_squad-train-23329", "mrqa_squad-train-35965", "mrqa_squad-train-61152", "mrqa_searchqa-validation-3542", "mrqa_hotpotqa-validation-2262", "mrqa_triviaqa-validation-2036", "mrqa_naturalquestions-validation-421", "mrqa_triviaqa-validation-5874", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1782", "mrqa_triviaqa-validation-6834", "mrqa_newsqa-validation-720", "mrqa_triviaqa-validation-1951", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-927", "mrqa_hotpotqa-validation-4879", "mrqa_squad-validation-3435", "mrqa_triviaqa-validation-4454", "mrqa_squad-validation-257", "mrqa_searchqa-validation-7004", "mrqa_squad-validation-9484", "mrqa_newsqa-validation-1136", "mrqa_naturalquestions-validation-4824", "mrqa_searchqa-validation-14371", "mrqa_naturalquestions-validation-9675", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-3087"], "EFR": 0.9714285714285714, "Overall": 0.7402048319327731}, {"timecode": 68, "before_eval_results": {"predictions": ["Borneo", "nomadic pastoralism", "5.4%", "Parkinson's Disease", "Patrick Henry", "Warsaw", "(Prince) Albert", "a 2.03 carat diamond", "South Africa", "David", "Muhammad", "Oceania", "Joe Namath", "high and dry", "a doll", "the inquisition", "Cleopatra", "the International Space Station", "Iran", "Gaius Cassius Longinus", "the Charleston", "South Africa", "John Deere", "Thames", "Oxford", "William Wordsworth", "Elphaba", "Tuscaloosa", "Cyprus", "Sabino Canyon", "Frasier Crane", "Brian Slade", "Sicily", "Herbert Hoover", "Zhou Enlai", "ham", "Lake Geneva", "(Barbie) doll", "The Mole", "the HIV/AIDS so Botswana can achieve epidemic control.", "Today", "Golden", "liver cancer", "Bern", "hollandaise sauce", "Jackie Robinson", "Buzzbee", "Diane Arbus", "Willa Cather", "overrule", "the marathon", "Masha Skorobogatov", "Kyla Pratt", "Dumont d'Urville Station", "Union Gap", "Charlotte's Web", "china", "Ding Sheng", "May 5, 2015", "Massapequa", "Highway 18 near Grand Ronde, Oregon.", "1936,", "transit bombings", "acid phosphate"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6458333333333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2956", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-13698", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-8634", "mrqa_searchqa-validation-6130", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-2941", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-6816", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-12621", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3166", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1457", "mrqa_triviaqa-validation-3820"], "SR": 0.5625, "CSR": 0.5554800724637681, "retrieved_ids": ["mrqa_squad-train-28020", "mrqa_squad-train-2456", "mrqa_squad-train-81328", "mrqa_squad-train-48105", "mrqa_squad-train-80618", "mrqa_squad-train-50164", "mrqa_squad-train-65659", "mrqa_squad-train-84241", "mrqa_squad-train-60028", "mrqa_squad-train-22949", "mrqa_squad-train-78792", "mrqa_squad-train-61448", "mrqa_squad-train-54012", "mrqa_squad-train-64240", "mrqa_squad-train-75270", "mrqa_squad-train-20292", "mrqa_squad-train-38074", "mrqa_squad-train-63566", "mrqa_squad-train-9698", "mrqa_squad-train-60120", "mrqa_squad-train-60199", "mrqa_squad-train-83587", "mrqa_squad-train-71977", "mrqa_squad-train-56446", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9159", "mrqa_triviaqa-validation-5632", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-929", "mrqa_triviaqa-validation-602", "mrqa_naturalquestions-validation-1987", "mrqa_searchqa-validation-214", "mrqa_searchqa-validation-10163", "mrqa_newsqa-validation-831", "mrqa_triviaqa-validation-7115", "mrqa_squad-validation-7445", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-3774", "mrqa_triviaqa-validation-3361", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-837", "mrqa_newsqa-validation-3508", "mrqa_naturalquestions-validation-10163", "mrqa_searchqa-validation-12363", "mrqa_hotpotqa-validation-2819", "mrqa_newsqa-validation-3437", "mrqa_triviaqa-validation-3690", "mrqa_naturalquestions-validation-10412"], "EFR": 1.0, "Overall": 0.7459397644927537}, {"timecode": 69, "before_eval_results": {"predictions": ["an aqueduct", "a quark", "Christopher Reeve", "Romania", "ambition", "John Jacob Astor", "acting", "50th Street", "The Sun Also Rises", "Cherokee", "Ferrari", "Fig", "Burgundy", "Joe Hill", "Job", "Kentucky", "Supernatural", "Jean Foucault", "Montana", "Deep brain stimulation", "kissanhnta", "Amazon", "Oklahoma", "Anne Hathaway", "Model A", "Iraq", "Vietnam", "Ecocriticism", "hoquet", "Blake Day Lewis", "Isaac Newton", "Blue Ridge Mountain", "Frdric Chopin", "Susan B. Anthony", "Dexter", "kangaroo", "Washington Bullets", "Hutch", "Batman", "Knocked Up", "The disappearedeared", "Chatsworth House", "jazz", "Boston", "Han Solo", "Hans Christian Andersen", "a proscenium arch", "Emiliano Zapata", "veil", "Barry Goldwater", "Guinness", "Portugal. The Man.", "1983", "Moira Kelly", "pumas", "mike", "a Greek shipping magnate", "Academy Award for Best Animated Feature", "1937", "Stephen James Ireland", "a group of college students of Pakistani background", "Copts", "\"an eye for an eye,\"", "Retina display"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5640624999999999}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.8, 0.5, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16658", "mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-5269", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-6228", "mrqa_searchqa-validation-6770", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-6956", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-16813", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-14736", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-6185", "mrqa_searchqa-validation-5427", "mrqa_triviaqa-validation-3274", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-7182", "mrqa_hotpotqa-validation-987", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-2435"], "SR": 0.484375, "CSR": 0.5544642857142856, "retrieved_ids": ["mrqa_squad-train-30455", "mrqa_squad-train-34509", "mrqa_squad-train-54306", "mrqa_squad-train-68811", "mrqa_squad-train-82835", "mrqa_squad-train-10032", "mrqa_squad-train-13485", "mrqa_squad-train-77465", "mrqa_squad-train-71010", "mrqa_squad-train-32838", "mrqa_squad-train-29631", "mrqa_squad-train-4857", "mrqa_squad-train-18967", "mrqa_squad-train-34000", "mrqa_squad-train-52853", "mrqa_squad-train-3169", "mrqa_squad-train-56992", "mrqa_squad-train-30838", "mrqa_squad-train-55495", "mrqa_squad-train-25216", "mrqa_squad-train-23426", "mrqa_squad-train-59813", "mrqa_squad-train-82123", "mrqa_squad-train-23617", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2900", "mrqa_naturalquestions-validation-8907", "mrqa_searchqa-validation-10836", "mrqa_triviaqa-validation-4940", "mrqa_newsqa-validation-3767", "mrqa_squad-validation-5818", "mrqa_newsqa-validation-834", "mrqa_hotpotqa-validation-4119", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-3456", "mrqa_searchqa-validation-9120", "mrqa_hotpotqa-validation-1471", "mrqa_triviaqa-validation-5477", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8699", "mrqa_naturalquestions-validation-8161", "mrqa_hotpotqa-validation-3587", "mrqa_searchqa-validation-12552", "mrqa_searchqa-validation-3244", "mrqa_naturalquestions-validation-2309"], "EFR": 1.0, "Overall": 0.745736607142857}, {"timecode": 70, "UKR": 0.818359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2888", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-39", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-978", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1120", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4890", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-963", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14334", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3809", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-2467", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3832", "mrqa_squad-validation-3852", "mrqa_squad-validation-386", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-3994", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4467", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5493", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6638", "mrqa_squad-validation-6875", "mrqa_squad-validation-6957", "mrqa_squad-validation-7064", "mrqa_squad-validation-739", "mrqa_squad-validation-7549", "mrqa_squad-validation-7688", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-7917", "mrqa_squad-validation-8309", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-893", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-143", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5861", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-595", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917"], "OKR": 0.857421875, "KG": 0.50546875, "before_eval_results": {"predictions": ["Kathy Najimy", "2006 -- 07", "The Divergent Series : Ascendant", "Mel Tillis", "2026", "Pink Floyd", "Andrew Lloyd Webber", "at the TV studio in the Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Health or vitality", "Detective Superintendent Dave Kelly", "one", "1955", "Owen Hunt", "Parthenogenesis", "fertilization", "Golde", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "stems and roots", "a Czech word, robota", "in skeletal muscle and the brain", "Nazi Germany and Fascist Italy", "Gunpei Yokoi", "David Motl", "a simple majority", "September 9, 2010", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "scrolls", "Buddhist", "Kiss", "the eighth series of the UK version of The X Factor", "Trace Adkins", "the optic chiasm", "to manage the characteristics of the beer's head", "United States, the United Kingdom, and their respective allies", "1957", "James Intveld", "15 February 1998", "Christopher Allen Lloyd", "100,000", "January 2004", "Bartolomeu Dias", "Isabela Moner", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "potential of hydrogen", "Fall 1998", "the Qianlong Emperor", "Guwahati", "74", "South Africa", "Rufus and Chaka Khan", "eight", "Venado Tuerto, Argentina", "Jamaica", "mead", "Tomorrowland", "Tallahassee City Commission", "January 18, 1977", "National Infrastructure Program, as he called it,", "Jewish civil rights activists", "123 pounds of cocaine and 4.5 pounds of heroin, Tempe, Arizona,", "In Memoriam", "Mercury", "Law & Order: Special Victims Unit", "UNICEF"], "metric_results": {"EM": 0.515625, "QA-F1": 0.646886204073704}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.18181818181818182, 0.888888888888889, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.9, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 1.0, 0.0, 1.0, 0.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-527", "mrqa_triviaqa-validation-4646", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-236", "mrqa_searchqa-validation-9696"], "SR": 0.515625, "CSR": 0.5539172535211268, "retrieved_ids": ["mrqa_squad-train-31035", "mrqa_squad-train-14428", "mrqa_squad-train-72629", "mrqa_squad-train-30766", "mrqa_squad-train-20720", "mrqa_squad-train-62439", "mrqa_squad-train-9140", "mrqa_squad-train-38148", "mrqa_squad-train-23831", "mrqa_squad-train-45484", "mrqa_squad-train-71945", "mrqa_squad-train-49383", "mrqa_squad-train-7057", "mrqa_squad-train-39464", "mrqa_squad-train-35281", "mrqa_squad-train-80725", "mrqa_squad-train-11755", "mrqa_squad-train-27520", "mrqa_squad-train-40011", "mrqa_squad-train-19369", "mrqa_squad-train-12872", "mrqa_squad-train-31332", "mrqa_squad-train-74665", "mrqa_squad-train-48522", "mrqa_triviaqa-validation-3002", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-1156", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-10163", "mrqa_triviaqa-validation-3768", "mrqa_newsqa-validation-1003", "mrqa_searchqa-validation-4724", "mrqa_triviaqa-validation-2532", "mrqa_naturalquestions-validation-1649", "mrqa_newsqa-validation-2742", "mrqa_searchqa-validation-11621", "mrqa_triviaqa-validation-7072", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-9436", "mrqa_searchqa-validation-6145", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1318", "mrqa_squad-validation-739", "mrqa_searchqa-validation-14981", "mrqa_searchqa-validation-1279", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-7164"], "EFR": 0.967741935483871, "Overall": 0.7405818378009996}, {"timecode": 71, "before_eval_results": {"predictions": ["Easter Island", "George Balanchine", "Brad Pitt", "the Mesozoic Era", "Austen", "Leonardo DiCaprio", "the Basque", "Cherry Jones", "Happy feet", "a guardian angel", "the Army", "the Tame", "Law & Order: Special Victims Unit", "the Black and Caspian seas", "June Carter Cash", "Bloemfontein", "hermatypic", "David Glasgow Farragut", "1:24 a.m.", "salaried", "a Skull", "Side by Side", "Scrabble", "the suckers", "Roman Catholicism", "London", "Burgenland", "Halliburton", "the retina", "Boston", "Anamosa", "the spelling bee", "poetry", "the Battle of Fort Donelson", "1950s", "the Rich", "sucrose", "Shropshire", "Cuba", "The Prince and the Pauper", "Thomas Paine", "Abraham Lincoln", "Lord North", "Charles I", "Aunt Jemima", "Diane Arbus", "Palitana", "George Bernard Shaw", "Utah", "Humulin", "Kublai Khan", "pulmonary heart disease ( cor pulmonale )", "Kimberlin Brown", "Henry Selick", "Caviar", "July", "argentina", "the vicar of Wantage", "Marc Bolan", "Polish-Jewish", "apartment building", "Iraq's autonomous region of Kurdish.", "$40 and a loaf of bread.", "argentina"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5347222222222221}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-13029", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-5919", "mrqa_searchqa-validation-4309", "mrqa_searchqa-validation-4000", "mrqa_searchqa-validation-12776", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-6284", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-15548", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-9495", "mrqa_searchqa-validation-6350", "mrqa_searchqa-validation-15590", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-3746", "mrqa_hotpotqa-validation-3593", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-3002", "mrqa_triviaqa-validation-1782"], "SR": 0.484375, "CSR": 0.5529513888888888, "retrieved_ids": ["mrqa_squad-train-38553", "mrqa_squad-train-28067", "mrqa_squad-train-31087", "mrqa_squad-train-13279", "mrqa_squad-train-50023", "mrqa_squad-train-26164", "mrqa_squad-train-78349", "mrqa_squad-train-12429", "mrqa_squad-train-55958", "mrqa_squad-train-18455", "mrqa_squad-train-30703", "mrqa_squad-train-62074", "mrqa_squad-train-63234", "mrqa_squad-train-63587", "mrqa_squad-train-45728", "mrqa_squad-train-46593", "mrqa_squad-train-70427", "mrqa_squad-train-85515", "mrqa_squad-train-50101", "mrqa_squad-train-61195", "mrqa_squad-train-61552", "mrqa_squad-train-20862", "mrqa_squad-train-37440", "mrqa_squad-train-59586", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7080", "mrqa_newsqa-validation-3782", "mrqa_squad-validation-4838", "mrqa_searchqa-validation-13675", "mrqa_hotpotqa-validation-4018", "mrqa_triviaqa-validation-6271", "mrqa_naturalquestions-validation-801", "mrqa_squad-validation-4435", "mrqa_searchqa-validation-2971", "mrqa_triviaqa-validation-2516", "mrqa_newsqa-validation-1415", "mrqa_searchqa-validation-9730", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-3385", "mrqa_searchqa-validation-5265", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-809", "mrqa_newsqa-validation-3714", "mrqa_searchqa-validation-10614", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-2586"], "EFR": 0.9393939393939394, "Overall": 0.7347190656565656}, {"timecode": 72, "before_eval_results": {"predictions": ["Austria", "peninsulas", "bazaar", "Brasilia", "Applebee\\'s", "Rhode Island", "Backgammon", "Steely Dan", "Artemis", "Hobart", "Colorado Springs", "Cheap trick", "Spinach", "Islam", "Cerberus", "Robert E. Lee", "Tobago", "Brigadoon", "Columbus", "Elijah Muhammad", "(Prince) Albert", "Federico Fellini", "Fenway Park", "C.T. Eisler", "The Princess Diaries", "fluoridation", "Herman Melville", "Korea", "John Henry", "Babe Ruth", "Hillary Clinton", "Chicago", "Wallace & Gromit", "sesame", "Nike", "Jack Nicholson", "ammonia", "Omaha", "wild dogs", "Paul Gauguin", "Francis Scott Key", "Mexico", "the Peashooter", "Joe Pozzuoli", "money earned from recycling cans", "Massachusetts", "ACTIVE", "the box office", "Alfred Hitchcock", "the Basques", "Ambrose Bierce", "The president", "2.45 billion years ago", "Jennifer Morrison", "stanley coltrane", "meteoroids", "Argentina", "Edinburgh", "Campbellsville", "mathematician and physicist", "165-room", "Sixteen years ago", "Brian Smith.", "Ballon d'Or"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7486920426065162}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4210526315789474, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.0, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-4160", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-1079", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-10907", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-2889", "mrqa_triviaqa-validation-6507", "mrqa_triviaqa-validation-5187", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4856", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-1962"], "SR": 0.671875, "CSR": 0.5545804794520548, "retrieved_ids": ["mrqa_squad-train-47416", "mrqa_squad-train-84634", "mrqa_squad-train-57459", "mrqa_squad-train-55673", "mrqa_squad-train-32063", "mrqa_squad-train-52584", "mrqa_squad-train-45567", "mrqa_squad-train-40884", "mrqa_squad-train-28954", "mrqa_squad-train-65893", "mrqa_squad-train-50416", "mrqa_squad-train-7564", "mrqa_squad-train-85875", "mrqa_squad-train-51566", "mrqa_squad-train-38038", "mrqa_squad-train-76738", "mrqa_squad-train-46176", "mrqa_squad-train-27822", "mrqa_squad-train-46751", "mrqa_squad-train-13610", "mrqa_squad-train-18961", "mrqa_squad-train-83312", "mrqa_squad-train-77405", "mrqa_squad-train-37248", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-2903", "mrqa_naturalquestions-validation-5113", "mrqa_hotpotqa-validation-4870", "mrqa_naturalquestions-validation-4048", "mrqa_newsqa-validation-2477", "mrqa_naturalquestions-validation-3363", "mrqa_triviaqa-validation-6920", "mrqa_naturalquestions-validation-9791", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-5554", "mrqa_newsqa-validation-3174", "mrqa_hotpotqa-validation-5464", "mrqa_newsqa-validation-1512", "mrqa_squad-validation-589", "mrqa_triviaqa-validation-1917", "mrqa_hotpotqa-validation-1123", "mrqa_naturalquestions-validation-1798", "mrqa_searchqa-validation-3114", "mrqa_triviaqa-validation-2356", "mrqa_naturalquestions-validation-3329", "mrqa_newsqa-validation-3605", "mrqa_searchqa-validation-16480", "mrqa_newsqa-validation-593"], "EFR": 0.9523809523809523, "Overall": 0.7376422863666015}, {"timecode": 73, "before_eval_results": {"predictions": ["barcarole", "Sinclair Lewis", "Hilary Swank", "sun lust Pictures", "dutch", "Israel", "louis island", "Van Morrison", "carbon", "stuart bingham", "Frank Darabont", "proclamation of Neutrality", "Adam Smith", "latte", "in a pyramid", "Volkswagen", "Bedser", "Oldham, in Greater Manchester", "north of the equator", "jabba the hutt", "Billie Jean King", "Crystal Gayle", "Zachary Taylor", "baku", "Chechnya", "h Hodder & Stoughton", "green", "Chester", "Hippety Hopper", "a smock", "Mt Kenya", "a pumpkin", "Latvia", "Sicily", "Switzerland", "magic circle", "Julie Andrews Edwards", "Pancho Villa", "Nigeria", "leeds", "Passover", "c Cologne", "Oliver!", "nippon", "Ra\u00fal Castro", "indiopia", "louis denance", "impossible object", "Mexico", "n Carolina", "Friends", "water can flow from the sink into the faucet without modifying the system", "Charles Haley", "January to May 2014", "Forrest Gump", "Julianne Moore", "Mel Blanc", "Gary Coleman", "island's dining scene", "Abhisit Vejjajiva", "Jackie Moon", "Maria Callas", "Desperate Housewives", "intelligent design"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6604166666666667}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-6822", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-6862", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-4028", "mrqa_newsqa-validation-3432", "mrqa_searchqa-validation-16053"], "SR": 0.609375, "CSR": 0.5553209459459459, "retrieved_ids": ["mrqa_squad-train-14909", "mrqa_squad-train-18070", "mrqa_squad-train-64319", "mrqa_squad-train-35352", "mrqa_squad-train-34580", "mrqa_squad-train-1503", "mrqa_squad-train-85263", "mrqa_squad-train-7676", "mrqa_squad-train-73404", "mrqa_squad-train-79932", "mrqa_squad-train-69301", "mrqa_squad-train-76811", "mrqa_squad-train-27665", "mrqa_squad-train-52238", "mrqa_squad-train-45323", "mrqa_squad-train-61339", "mrqa_squad-train-72967", "mrqa_squad-train-56601", "mrqa_squad-train-31095", "mrqa_squad-train-61727", "mrqa_squad-train-46857", "mrqa_squad-train-47333", "mrqa_squad-train-69996", "mrqa_squad-train-38016", "mrqa_triviaqa-validation-6580", "mrqa_naturalquestions-validation-3442", "mrqa_squad-validation-7338", "mrqa_triviaqa-validation-6380", "mrqa_searchqa-validation-14569", "mrqa_triviaqa-validation-5690", "mrqa_newsqa-validation-4041", "mrqa_triviaqa-validation-2356", "mrqa_searchqa-validation-4000", "mrqa_hotpotqa-validation-5292", "mrqa_searchqa-validation-3932", "mrqa_triviaqa-validation-2669", "mrqa_searchqa-validation-792", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7464", "mrqa_newsqa-validation-3018", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4132", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-4017", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-511"], "EFR": 0.88, "Overall": 0.7233141891891892}, {"timecode": 74, "before_eval_results": {"predictions": ["sue bardon", "ewan McGregor", "Royal Navy", "towed by a tow truck", "apple", "yellow", "Dreamgirls", "France", "the Antilles Current", "Viola", "hay fever", "gin", "Canada", "it means that the rent doesn't include additional costs such as insurance or business rates", "whooping cough", "Peter Stuyvesant", "apple", "India and Pakistan", "palace", "Chiricahua", "sinus node", "blucher", "(Pius XII) Pius XII", "smell", "30", "george i", "Lincolnshire", "Zimbabwe", "island of Ireland", "orange, lemon, lime, grape and strawberry", "(Muhammad Anwar el-Sadat", "David Bowie", "Silent Spring", "diocese of bath and Wells", "Rade Serbedzija", "Michael Sheen", "The Archers", "p Tottenham Court Road", "Montmorency", "gymnogyps californianus", "12", "port of London", "Pinocchio", "pangaea", "president Jimmy Carter", "Jamie Oliver", "bicycle", "willy Russell", "Petula Clark", "New Democracy", "The Blue Boy", "Border Collie", "Kristy Swanson", "fourth season", "Charles Wesley", "Hermione Baddeley", "Floyd Casey Stadium", "David Bowie,", "spurring on economic growth and creating opportunity for our people.", "Robert Kimmitt.", "Mammoth Cave", "recessive", "Dutchman", "Virginia Beach, Virginia"], "metric_results": {"EM": 0.609375, "QA-F1": 0.659375}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-3482", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-4706", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-2699", "mrqa_triviaqa-validation-7353", "mrqa_triviaqa-validation-2180", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-1344", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-5738", "mrqa_naturalquestions-validation-8404", "mrqa_hotpotqa-validation-2801", "mrqa_newsqa-validation-3008", "mrqa_searchqa-validation-4464", "mrqa_hotpotqa-validation-3787"], "SR": 0.609375, "CSR": 0.5560416666666667, "retrieved_ids": ["mrqa_squad-train-14040", "mrqa_squad-train-8812", "mrqa_squad-train-78088", "mrqa_squad-train-49468", "mrqa_squad-train-16118", "mrqa_squad-train-76925", "mrqa_squad-train-13753", "mrqa_squad-train-6167", "mrqa_squad-train-14575", "mrqa_squad-train-81726", "mrqa_squad-train-51762", "mrqa_squad-train-28190", "mrqa_squad-train-26821", "mrqa_squad-train-82879", "mrqa_squad-train-979", "mrqa_squad-train-59717", "mrqa_squad-train-66041", "mrqa_squad-train-44331", "mrqa_squad-train-9966", "mrqa_squad-train-71583", "mrqa_squad-train-53626", "mrqa_squad-train-85336", "mrqa_squad-train-53380", "mrqa_squad-train-21885", "mrqa_searchqa-validation-1279", "mrqa_naturalquestions-validation-844", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-538", "mrqa_triviaqa-validation-189", "mrqa_squad-validation-6171", "mrqa_naturalquestions-validation-8896", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-5123", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-7827", "mrqa_triviaqa-validation-1802", "mrqa_hotpotqa-validation-876", "mrqa_searchqa-validation-2456", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-1719", "mrqa_triviaqa-validation-6819", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-645", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-12996", "mrqa_naturalquestions-validation-7535"], "EFR": 0.88, "Overall": 0.7234583333333333}, {"timecode": 75, "before_eval_results": {"predictions": ["new Zealand", "july 1961", "spain", "Amy", "duke orsino", "arthurian", "jimmy carter", "japan", "Gillette", "spain", "mediterranean", "Bash Street", "public square", "the forelimb", "swallow sidecar", "Chicago", "Brett Favre", "the Netherlands", "Gryffindor", "hallmarks", "john buchan", "Pyrenees", "17", "arthurian", "geneva", "Elysium", "algebra", "Eddie Murphy", "Crete", "spain", "denmark", "vena cava", "jimmy carter", "killer whale", "Christopher Nolan", "purple rain", "chess", "Ireland", "diana vickers", "july", "iain Duncan Smith", "argon", "bagel", "France", "South Dakota", "Alexander Dubcek", "denmark", "Chicago Cubs", "l", "iberia", "Rosetta", "in the basic curriculum", "the Saudi Arab kingdom", "the nucleus", "August 14, 1848", "1892", "American pharmaceutical company", "1,500", "in Shenzhen in southern China.", "Iran", "bone", "Washington", "sedimentary rock", "golf"], "metric_results": {"EM": 0.5, "QA-F1": 0.5763888888888888}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.888888888888889, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7442", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5653", "mrqa_triviaqa-validation-4799", "mrqa_triviaqa-validation-2012", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-156", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5531", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-3857", "mrqa_triviaqa-validation-6799", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-3745", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-366", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2495", "mrqa_searchqa-validation-10770", "mrqa_searchqa-validation-8445", "mrqa_searchqa-validation-2183"], "SR": 0.5, "CSR": 0.5553042763157895, "retrieved_ids": ["mrqa_squad-train-41423", "mrqa_squad-train-20327", "mrqa_squad-train-22508", "mrqa_squad-train-41152", "mrqa_squad-train-58704", "mrqa_squad-train-36607", "mrqa_squad-train-46353", "mrqa_squad-train-32208", "mrqa_squad-train-46437", "mrqa_squad-train-81559", "mrqa_squad-train-42879", "mrqa_squad-train-39221", "mrqa_squad-train-49747", "mrqa_squad-train-83664", "mrqa_squad-train-36205", "mrqa_squad-train-36978", "mrqa_squad-train-67201", "mrqa_squad-train-55803", "mrqa_squad-train-39975", "mrqa_squad-train-62112", "mrqa_squad-train-25970", "mrqa_squad-train-55701", "mrqa_squad-train-4553", "mrqa_squad-train-53770", "mrqa_naturalquestions-validation-9791", "mrqa_naturalquestions-validation-10412", "mrqa_hotpotqa-validation-3428", "mrqa_newsqa-validation-4100", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-2309", "mrqa_searchqa-validation-7620", "mrqa_newsqa-validation-1290", "mrqa_searchqa-validation-6004", "mrqa_naturalquestions-validation-953", "mrqa_squad-validation-7880", "mrqa_newsqa-validation-3652", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-66", "mrqa_triviaqa-validation-890", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-2923", "mrqa_naturalquestions-validation-9578", "mrqa_hotpotqa-validation-257", "mrqa_searchqa-validation-16908", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-2605"], "EFR": 0.9375, "Overall": 0.7348108552631578}, {"timecode": 76, "before_eval_results": {"predictions": ["English author Rudyard Kipling", "Andrew Garfield", "California, Utah and Arizona", "The Nurses'Health Study ( NHS )", "William Chatterton Dix", "1924", "September 27, 2017", "Alabama", "Scheria", "Sanchez Navarro", "Thomas Jefferson", "1990", "Joe Pizzulo and Leeza Miller", "Julie Adams", "Ian Hart", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "a Native American nation from the Great Plains", "capillaries, alveoli, glomeruli, outer layer of skin and other tissues where rapid diffusion is required", "the ARPANET", "April 1979", "Tbilisi", "a security feature for `` card not present '' payment card transactions instituted to reduce the incidence of credit card fraud", "Tom Robinson", "four of the 50 states of the United States", "Liam Cunningham", "2013", "ummat al - Islamiyah", "4", "1980", "2017 season", "W. Edwards Deming", "Saphira", "transmissions", "Galveston hurricane", "the final years of the Third Republic", "Ajay Tyagi", "Fr\u00e9d\u00e9ric Bazille", "Paul Revere", "Julius Caesar", "Thespis", "1990", "Zeus", "two degrees of freedom", "April 10, 2018", "Lee County, Florida, United States", "mid November", "Kevin Spacey", "Fa Ze Rug", "The Lightning Thief", "the French", "hyperinflation", "Lingerie", "Amy Dorrit", "Augustus", "Karolina Dean", "around four hundred", "Caesars Entertainment Corporation", "North Korea intends to launch a long-range missile in the near future,", "the sins of the members of the church,", "Marcus Schrenker,", "Gimli", "Amadeus", "ask for help", "Charice"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6689242742551567}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.6363636363636364, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5714285714285715, 0.28571428571428575, 1.0, 0.4, 0.5, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-1907", "mrqa_hotpotqa-validation-4503", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-8474", "mrqa_searchqa-validation-1590"], "SR": 0.578125, "CSR": 0.5556006493506493, "retrieved_ids": ["mrqa_squad-train-39931", "mrqa_squad-train-73080", "mrqa_squad-train-80296", "mrqa_squad-train-6407", "mrqa_squad-train-70479", "mrqa_squad-train-47418", "mrqa_squad-train-11910", "mrqa_squad-train-27477", "mrqa_squad-train-84709", "mrqa_squad-train-26395", "mrqa_squad-train-77569", "mrqa_squad-train-56508", "mrqa_squad-train-50749", "mrqa_squad-train-22391", "mrqa_squad-train-75302", "mrqa_squad-train-53015", "mrqa_squad-train-10768", "mrqa_squad-train-48689", "mrqa_squad-train-32299", "mrqa_squad-train-5283", "mrqa_squad-train-80458", "mrqa_squad-train-29358", "mrqa_squad-train-12545", "mrqa_squad-train-23703", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-9670", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-5024", "mrqa_searchqa-validation-354", "mrqa_naturalquestions-validation-138", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-7535", "mrqa_hotpotqa-validation-2256", "mrqa_triviaqa-validation-2735", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-2170", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-6228", "mrqa_searchqa-validation-6649", "mrqa_naturalquestions-validation-4924", "mrqa_hotpotqa-validation-230", "mrqa_triviaqa-validation-4577", "mrqa_newsqa-validation-1796", "mrqa_searchqa-validation-2183", "mrqa_newsqa-validation-1544", "mrqa_hotpotqa-validation-5588", "mrqa_triviaqa-validation-1194"], "EFR": 0.8148148148148148, "Overall": 0.7103330928330929}, {"timecode": 77, "before_eval_results": {"predictions": ["(senator Hillary Clinton)", "Istanbul", "Galilee", "fetch", "The Marriage of Figaro", "Valerie Bertolini", "Glitter", "Bayer", "picture book for children", "Karl Rove", "Poles", "Ireland", "Trinity College", "Portland", "Florida Keys", "Doctor Dolittle", "fish", "CVT", "hot air balloons", "vacuum tubes", "The Bridges of Madison County", "Italy", "ghee", "(Prince) XIV", "ice cream", "Louis XIV", "catfish", "Alien", "the JFK assassination", "Indira Gandhi", "rodents", "Stephen Decatur", "Mary J. Blige", "the squire", "Molly Brown", "Foot container", "hurricanes", "The Wall Street Journal", "fragmentation", "cream", "Virgin Atlantic", "Perrier", "Eastwick", "Richard III", "trout", "Afghanistan", "Minnesota", "San Francisco", "rabbit", "latte", "the M9", "Brazil, Turkey and Uzbekistan", "Nicole DuPort", "species", "farlake", "Leonardo da Vinci", "dada", "July 25 to August 4", "1755", "Trey Parker and Matt Stone", "more than 1.2 million people.", "president Luca di Montezemolo", "Roger Federer", "Agnolo Bronzino"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6323660714285715}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6923", "mrqa_searchqa-validation-692", "mrqa_searchqa-validation-8250", "mrqa_searchqa-validation-2938", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-15029", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-6226", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-15498", "mrqa_searchqa-validation-10162", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-5989", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-6955", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-2685", "mrqa_searchqa-validation-11923", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-2858", "mrqa_naturalquestions-validation-9830", "mrqa_triviaqa-validation-6146", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-1364", "mrqa_triviaqa-validation-5253"], "SR": 0.5625, "CSR": 0.5556891025641026, "retrieved_ids": ["mrqa_squad-train-53569", "mrqa_squad-train-86197", "mrqa_squad-train-53535", "mrqa_squad-train-30440", "mrqa_squad-train-51980", "mrqa_squad-train-2179", "mrqa_squad-train-27668", "mrqa_squad-train-5630", "mrqa_squad-train-42237", "mrqa_squad-train-797", "mrqa_squad-train-82138", "mrqa_squad-train-65644", "mrqa_squad-train-78079", "mrqa_squad-train-8309", "mrqa_squad-train-23488", "mrqa_squad-train-81080", "mrqa_squad-train-30679", "mrqa_squad-train-24275", "mrqa_squad-train-10397", "mrqa_squad-train-30477", "mrqa_squad-train-71475", "mrqa_squad-train-69695", "mrqa_squad-train-42660", "mrqa_squad-train-66885", "mrqa_newsqa-validation-1443", "mrqa_naturalquestions-validation-9081", "mrqa_triviaqa-validation-6233", "mrqa_newsqa-validation-1331", "mrqa_searchqa-validation-4898", "mrqa_triviaqa-validation-4643", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-1259", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-9578", "mrqa_newsqa-validation-2112", "mrqa_triviaqa-validation-6368", "mrqa_newsqa-validation-3681", "mrqa_hotpotqa-validation-2986", "mrqa_naturalquestions-validation-7164", "mrqa_newsqa-validation-1550", "mrqa_squad-validation-7296", "mrqa_hotpotqa-validation-4612", "mrqa_newsqa-validation-3781", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-473", "mrqa_searchqa-validation-13142", "mrqa_newsqa-validation-1893", "mrqa_squad-validation-2328"], "EFR": 0.9642857142857143, "Overall": 0.7402449633699634}, {"timecode": 78, "before_eval_results": {"predictions": ["Tycho Brahe", "Little Miss Sunshine", "Philadelphia", "Peter Rabbit", "Tommy Franks", "Ur", "Jonny Quest", "Rwanda and its neighboring kingdom to the south, Burundi", "Fort Sumter", "Love Story", "Captains Courageous", "Bryan Adams", "Moses", "engineering", "Chaucer", "the Toronto Blue Jays", "lieutenant", "Adam", "Sayonara", "Orient Express", "Dante", "Sir Walter Scott", "a face cord", "Louisiana", "The Maltese Falcon", "MacArthur", "Cherry Cherry", "breast", "PG-13", "occipital", "a spoon", "The True History of Little Golden-Hood", "The Jonas Brothers", "Iceland", "a popsicle", "Los Angeles", "paladin", "\"Chelsea Morning\"", "a comb", "Venice", "Paraguay", "Hoffmann", "debts", "Oz", "El Supremo", "Foot Locker", "Princess Leia", "thistle", "some concerts", "Hammurabi", "Alkalinity", "the ninth w\u0101", "Matt Monro", "on the two tablets", "Mt kenya", "Elvis Presley", "Boston Legal", "all-time", "Channel 4", "Mark Neary Donohue Jr.", "State Department", "share in the royalties for the tune.", "Arizona", "3D computer-animated comedy"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7321552579365079}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.4, 0.7499999999999999, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4423", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-6426", "mrqa_searchqa-validation-10152", "mrqa_searchqa-validation-12278", "mrqa_searchqa-validation-8259", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-13217", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-12891", "mrqa_naturalquestions-validation-10310", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-4688", "mrqa_hotpotqa-validation-5344", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-2151"], "SR": 0.640625, "CSR": 0.5567642405063291, "retrieved_ids": ["mrqa_squad-train-29215", "mrqa_squad-train-21056", "mrqa_squad-train-49731", "mrqa_squad-train-61322", "mrqa_squad-train-82130", "mrqa_squad-train-35709", "mrqa_squad-train-72174", "mrqa_squad-train-59630", "mrqa_squad-train-46404", "mrqa_squad-train-7485", "mrqa_squad-train-15399", "mrqa_squad-train-25312", "mrqa_squad-train-50843", "mrqa_squad-train-62720", "mrqa_squad-train-5032", "mrqa_squad-train-81710", "mrqa_squad-train-7479", "mrqa_squad-train-14009", "mrqa_squad-train-16846", "mrqa_squad-train-18100", "mrqa_squad-train-28929", "mrqa_squad-train-27265", "mrqa_squad-train-80323", "mrqa_squad-train-26441", "mrqa_squad-validation-9458", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-5792", "mrqa_searchqa-validation-2938", "mrqa_naturalquestions-validation-297", "mrqa_newsqa-validation-2238", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-3979", "mrqa_searchqa-validation-14310", "mrqa_triviaqa-validation-2959", "mrqa_hotpotqa-validation-45", "mrqa_triviaqa-validation-781", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-6999", "mrqa_hotpotqa-validation-3563", "mrqa_squad-validation-2328", "mrqa_newsqa-validation-2802", "mrqa_triviaqa-validation-6057", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-2582", "mrqa_searchqa-validation-15496", "mrqa_triviaqa-validation-3079", "mrqa_searchqa-validation-15973", "mrqa_naturalquestions-validation-868"], "EFR": 0.9130434782608695, "Overall": 0.7302115437534397}, {"timecode": 79, "before_eval_results": {"predictions": ["the Confederate", "Banquo", "Detroit Rock City", "a flower", "the word", "the Ford", "Joseph Campbell", "thesaurus", "Faith Hill", "a novel", "a broom", "Edinburgh", "engineering", "Cyprus", "savanna", "the tandoori", "a floatplane", "piano", "Sure", "oysters", "What's Eating Gilbert", "Mrs. Barbara Bush", "Cold Mountain", "The Jungle Book", "eggshells", "the F/A-18", "the Vic-Wells Ballet", "Pakistan", "the FBI's Delta Force", "Aaron Burr", "Johns Hopkins", "Jason", "the Mississippi River", "Damascus", "Oahu", "Devo", "zoology", "stuffing", "Reading", "George Eliot", "the Cotton Bowl", "Shiloh", "Old Time Radio", "the Takana", "apples", "a cedar", "an Almond Joy", "the Australian Outback", "Sam Houston", "Hail Caesar", "cable cars", "July 14, 1969", "on permanent display at the Louvre Museum in Paris", "1923", "dragonings castle", "teflonation Street", "The Boar", "Johnny Cash and Jennings", "Sarajevo", "Annie Ida Jenny No\u00eb Haesendonck", "Mother's Day", "Italian Serie A title", "The son of Gabon's former president", "Wildcats"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6001116071428572}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.75, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-9910", "mrqa_searchqa-validation-15449", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-12299", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-15892", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-2799", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-15134", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-3790", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-13472", "mrqa_searchqa-validation-1853", "mrqa_searchqa-validation-9372", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-6870", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-3155", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3923"], "SR": 0.515625, "CSR": 0.55625, "retrieved_ids": ["mrqa_squad-train-28725", "mrqa_squad-train-82762", "mrqa_squad-train-27863", "mrqa_squad-train-40600", "mrqa_squad-train-28300", "mrqa_squad-train-62161", "mrqa_squad-train-47542", "mrqa_squad-train-33025", "mrqa_squad-train-57043", "mrqa_squad-train-40574", "mrqa_squad-train-20972", "mrqa_squad-train-14683", "mrqa_squad-train-69150", "mrqa_squad-train-3953", "mrqa_squad-train-37458", "mrqa_squad-train-57700", "mrqa_squad-train-72617", "mrqa_squad-train-56053", "mrqa_squad-train-6332", "mrqa_squad-train-3064", "mrqa_squad-train-41124", "mrqa_squad-train-62935", "mrqa_squad-train-76050", "mrqa_squad-train-53869", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-1348", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-456", "mrqa_hotpotqa-validation-1182", "mrqa_triviaqa-validation-1782", "mrqa_newsqa-validation-2399", "mrqa_searchqa-validation-8598", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-6833", "mrqa_searchqa-validation-9789", "mrqa_triviaqa-validation-2063", "mrqa_naturalquestions-validation-2196", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-637", "mrqa_naturalquestions-validation-6461", "mrqa_searchqa-validation-6956", "mrqa_hotpotqa-validation-3428", "mrqa_newsqa-validation-3008", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-1712", "mrqa_triviaqa-validation-5993"], "EFR": 0.9032258064516129, "Overall": 0.7281451612903226}, {"timecode": 80, "UKR": 0.802734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3137", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8631", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-8803", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7052", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.880859375, "KG": 0.525, "before_eval_results": {"predictions": ["India", "bird", "long-term", "Czech Republic", "George IV", "azerbaijani", "alfa", "Sisyphus", "state of Italy", "syndicate", "Cambodia", "Moldova", "Taking of Pelham One Two Three", "egypt", "Frank McCourt", "Furby", "Arkansas", "Texas", "Norway", "archer", "paul Mellon", "cape horn", "Ernests Gulbis", "Charlie Chan", "Galileo Galilei", "Great British Bake Off", "World War I", "shekel", "George Sand", "Michael Caine", "Professor Brian Cox", "jack brabham", "knutsford", "casualty", "McDonnell Douglas", "tyne", "Missouri", "Emma Chambers", "Buckinghamshire", "Turkey", "cat", "elephant man", "nine", "One Direction", "Groucho Marx", "Brazil", "Idris Elba", "India", "1941", "Sweeney Todd", "Rio Grande", "If waivers are requested outside the playing season, or before November 1", "8 January 1999", "David Joseph Madden", "The Braes o' Bowhether", "Mary Astor", "al-Qaeda", "natural gas", "CNN's best ten golf movies ever made", "Madonna", "Hawaii", "Monaco", "Sally Jupp", "Carter Pewtersch Schmidt"], "metric_results": {"EM": 0.703125, "QA-F1": 0.703125}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3969", "mrqa_triviaqa-validation-1559", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-2733", "mrqa_naturalquestions-validation-215", "mrqa_hotpotqa-validation-2718", "mrqa_newsqa-validation-4110", "mrqa_searchqa-validation-12999", "mrqa_naturalquestions-validation-10523"], "SR": 0.703125, "CSR": 0.5580632716049383, "retrieved_ids": ["mrqa_squad-train-44664", "mrqa_squad-train-77646", "mrqa_squad-train-65954", "mrqa_squad-train-59854", "mrqa_squad-train-24771", "mrqa_squad-train-58393", "mrqa_squad-train-55629", "mrqa_squad-train-64884", "mrqa_squad-train-8540", "mrqa_squad-train-32019", "mrqa_squad-train-7359", "mrqa_squad-train-52028", "mrqa_squad-train-72148", "mrqa_squad-train-13490", "mrqa_squad-train-30405", "mrqa_squad-train-1246", "mrqa_squad-train-13569", "mrqa_squad-train-35548", "mrqa_squad-train-35677", "mrqa_squad-train-7477", "mrqa_squad-train-66988", "mrqa_squad-train-33970", "mrqa_squad-train-49496", "mrqa_squad-train-60101", "mrqa_searchqa-validation-2866", "mrqa_triviaqa-validation-7690", "mrqa_searchqa-validation-5501", "mrqa_naturalquestions-validation-681", "mrqa_searchqa-validation-14519", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1512", "mrqa_searchqa-validation-7774", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-12621", "mrqa_newsqa-validation-1232", "mrqa_hotpotqa-validation-3563", "mrqa_squad-validation-8229", "mrqa_naturalquestions-validation-5780", "mrqa_searchqa-validation-10163", "mrqa_triviaqa-validation-1314", "mrqa_naturalquestions-validation-10209", "mrqa_hotpotqa-validation-4842", "mrqa_newsqa-validation-3034", "mrqa_searchqa-validation-8465", "mrqa_hotpotqa-validation-1745", "mrqa_searchqa-validation-214", "mrqa_triviaqa-validation-4257", "mrqa_naturalquestions-validation-3288"], "EFR": 0.8421052631578947, "Overall": 0.7217524569525666}, {"timecode": 81, "before_eval_results": {"predictions": ["the main highway entrance at California State Route 1,", "the Latin alphabet", "New York Knickerbockers", "John Dalton", "San Antonio", "A rear - view mirror", "one of the most internationally recognized symbols", "Fighter Command", "BC Jean and Toby Gad", "UNESCO", "September 2017", "Universal Pictures and Focus Features", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "September 29, 2017", "nine", "Tbilisi, Georgia", "April 1917", "1900", "Bryan Cranston", "Geothermal gradient", "10 : 30am", "frontal lobe", "planned invasion of the United Kingdom", "potential of hydrogen", "volcanic activity", "held that `` a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Book of Exodus", "201", "pia mater", "members of the gay ( LGBT ) community", "Burbank, California", "April 20, 1983", "2018", "rapid destruction of the donor red blood cells by host antibodies", "1603", "English author Rudyard Kipling", "March 16, 2018", "Fusajiro Yamauchi", "the rez", "2013", "the breast or lower chest", "David Fitch", "business applications to be developed with Flash", "2018", "Saint Peter", "1963", "August 19, 2016", "Madison", "Washington, Jay and Franklin", "ABC", "Brevet Colonel Robert E. Lee", "glockenspiel", "Alaska", "Hercules", "Elbow", "County Louth", "NCAA Division II", "the Airbus A330-200", "about 50", "\"I always kind of admired him, oddly.\"", "Portugal", "gravity", "the Smith & Wesson Model 36", "Yemeni port city of Aden"], "metric_results": {"EM": 0.625, "QA-F1": 0.7339383046694199}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.2666666666666667, 0.8333333333333333, 1.0, 1.0, 0.7710843373493976, 1.0, 0.25, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.7000000000000001, 1.0, 1.0, 0.0, 1.0, 0.4, 0.4210526315789474, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-2319", "mrqa_newsqa-validation-1449", "mrqa_searchqa-validation-2328", "mrqa_newsqa-validation-4144"], "SR": 0.625, "CSR": 0.5588795731707317, "retrieved_ids": ["mrqa_squad-train-70280", "mrqa_squad-train-38517", "mrqa_squad-train-40163", "mrqa_squad-train-2129", "mrqa_squad-train-82699", "mrqa_squad-train-5528", "mrqa_squad-train-60418", "mrqa_squad-train-56177", "mrqa_squad-train-75742", "mrqa_squad-train-4380", "mrqa_squad-train-70114", "mrqa_squad-train-58485", "mrqa_squad-train-40177", "mrqa_squad-train-61255", "mrqa_squad-train-65642", "mrqa_squad-train-9514", "mrqa_squad-train-33669", "mrqa_squad-train-34683", "mrqa_squad-train-14855", "mrqa_squad-train-78124", "mrqa_squad-train-64025", "mrqa_squad-train-46972", "mrqa_squad-train-30196", "mrqa_squad-train-31876", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6084", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1761", "mrqa_triviaqa-validation-102", "mrqa_naturalquestions-validation-4863", "mrqa_searchqa-validation-6426", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-4931", "mrqa_searchqa-validation-13851", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-3181", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-4000", "mrqa_naturalquestions-validation-4073", "mrqa_hotpotqa-validation-3742", "mrqa_searchqa-validation-10353", "mrqa_naturalquestions-validation-2794", "mrqa_searchqa-validation-5989", "mrqa_hotpotqa-validation-5386", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-7458"], "EFR": 0.9166666666666666, "Overall": 0.7368279979674797}, {"timecode": 82, "before_eval_results": {"predictions": ["jon stanford", "king Henry I", "Ross Kemp", "jumanji", "Kirk Douglas", "William Shakespeare", "Christmas", "African violet", "davy", "Gerald Ford", "a wood wind musical instrument of low pitch", "pembrokeshire Coast National Park", "Imola", "South Africa", "sows", "Persistence of Memory", "orangutan", "The Time Machine", "uranus", "Tacitus", "Lady Gaga", "Mecca", "cirrus", "kiev", "myxomatosis", "Jasper Fforde", "Philippines", "xerophyte", "Blur", "getting to know you", "The Last King of Scotland", "jaws", "Pearson PLC", "John Steinbeck", "The Bulletin", "violin", "Ross Bagdasarian", "Mark Hamill", "pierce bacall", "Burma", "peasant", "cryonics", "j\u00f8rn Utzon", "Another Day in Paradise", "decorate", "ringstra\u00dfe", "Department of Justice", "South Africa", "stage 3", "Antonio Vivaldi", "Corfu", "Walter Brennan", "a solitary figure who is not understood by others, but is actually wise", "Continental drift", "Major Charles White Whittlesey", "2015 Orange Bowl", "White Knights of the Ku Klux Klan", "6,000", "fill a million sandbags and place 700,000 around our city,\"", "Judge Herman Thomas", "the processor", "Mazur", "a genie", "1945 to 1951"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6875}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3307", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2050", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4711", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-2044"], "SR": 0.65625, "CSR": 0.5600527108433735, "retrieved_ids": ["mrqa_squad-train-49778", "mrqa_squad-train-51030", "mrqa_squad-train-28371", "mrqa_squad-train-10850", "mrqa_squad-train-2053", "mrqa_squad-train-34743", "mrqa_squad-train-83158", "mrqa_squad-train-76212", "mrqa_squad-train-63749", "mrqa_squad-train-70372", "mrqa_squad-train-27666", "mrqa_squad-train-11685", "mrqa_squad-train-433", "mrqa_squad-train-48148", "mrqa_squad-train-74811", "mrqa_squad-train-19517", "mrqa_squad-train-19043", "mrqa_squad-train-28484", "mrqa_squad-train-8127", "mrqa_squad-train-79087", "mrqa_squad-train-30666", "mrqa_squad-train-24642", "mrqa_squad-train-76428", "mrqa_squad-train-39022", "mrqa_triviaqa-validation-4599", "mrqa_searchqa-validation-3000", "mrqa_newsqa-validation-4110", "mrqa_naturalquestions-validation-2016", "mrqa_squad-validation-2975", "mrqa_triviaqa-validation-6018", "mrqa_newsqa-validation-859", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-5928", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-10614", "mrqa_searchqa-validation-15548", "mrqa_searchqa-validation-6953", "mrqa_searchqa-validation-8840", "mrqa_searchqa-validation-6181", "mrqa_newsqa-validation-880", "mrqa_searchqa-validation-2941", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-421", "mrqa_naturalquestions-validation-1443", "mrqa_squad-validation-6638", "mrqa_newsqa-validation-2011", "mrqa_triviaqa-validation-1089"], "EFR": 0.7272727272727273, "Overall": 0.6991838376232201}, {"timecode": 83, "before_eval_results": {"predictions": ["Libya", "Syriza", "georgiaquo", "wrigley", "james Blake", "hawaii flinstone", "eight", "Charles Taylor", "palm sunday", "dollar", "The Wicker Man", "pleur\u00e6", "endgame", "a spider god", "Peter Nichols", "bear Grylls", "Count Basie", "john glenn", "Xenophon", "amundsen", "splendor", "Pensacola, Florida", "Ireland", "michael hordern", "Gerald Durrell", "ishmael", "francis", "climates", "tank", "bacall", "the Etruscan army", "James Van Allen", "sheffield", "Bulls Eye", "South Africa", "welding boots", "Helen Gurley Brown", "Luxor", "The Jungle Book", "joshua", "Massachusetts", "Josh Brolin", "Hamlet", "georgia", "paul", "r34", "georgia parker", "rock follies", "australia", "Ann Darrow", "bacall", "1996", "a Texas - style chili con carne", "16 June", "Squam Lake", "3D computer-animated comedy", "1902", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "The Impeccable", "Justicialist Party, or PJ by its Spanish acronym,", "the Ming dynasty", "Prince Albert", "a crossword clue", "al Qaeda."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6261642156862746}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-5095", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-5103", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-2611", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-3264", "mrqa_triviaqa-validation-925", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-352", "mrqa_triviaqa-validation-7713", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-4848", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-5596", "mrqa_newsqa-validation-3703", "mrqa_searchqa-validation-581", "mrqa_searchqa-validation-6285"], "SR": 0.546875, "CSR": 0.5598958333333333, "retrieved_ids": ["mrqa_squad-train-71794", "mrqa_squad-train-80111", "mrqa_squad-train-11980", "mrqa_squad-train-32306", "mrqa_squad-train-15577", "mrqa_squad-train-29389", "mrqa_squad-train-4627", "mrqa_squad-train-13083", "mrqa_squad-train-60254", "mrqa_squad-train-59445", "mrqa_squad-train-3406", "mrqa_squad-train-23535", "mrqa_squad-train-5204", "mrqa_squad-train-2147", "mrqa_squad-train-19482", "mrqa_squad-train-82915", "mrqa_squad-train-18356", "mrqa_squad-train-38299", "mrqa_squad-train-30826", "mrqa_squad-train-50567", "mrqa_squad-train-31205", "mrqa_squad-train-46678", "mrqa_squad-train-85194", "mrqa_squad-train-51380", "mrqa_triviaqa-validation-4940", "mrqa_newsqa-validation-1634", "mrqa_searchqa-validation-2038", "mrqa_hotpotqa-validation-1968", "mrqa_naturalquestions-validation-4675", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-2641", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-3491", "mrqa_newsqa-validation-2682", "mrqa_searchqa-validation-14736", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8409", "mrqa_searchqa-validation-1279", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-1479", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-4041", "mrqa_hotpotqa-validation-4950", "mrqa_squad-validation-9717", "mrqa_hotpotqa-validation-5403"], "EFR": 0.8620689655172413, "Overall": 0.7261117097701149}, {"timecode": 84, "before_eval_results": {"predictions": ["ganges", "David Hilbert", "Halifax", "laos", "Q", "Franklin Delano Roosevelt", "Buncefield Depot", "cesium", "coffee", "turangas", "prime minister of new Brunswick", "Oklahoma", "Brad Pitt", "florida", "william Lamb", "Jupiter Mining Corporation", "faye omdahl", "Nouakchott", "volcan", "verona", "weekly", "gail Webb", "noah", "jimoboam", "budge", "queen Victoria and Prince Albert", "Quentin Tarantino", "william", "comitium", "breaststroke", "ouwerks", "gin", "Supertramp", "leicestershire", "halogens", "Jackie Kennedy", "blue", "calcium carbonate", "cobalt", "cuba", "Lorraine", "Nicola Adams", "Leicester City", "Andes", "Essex Eagles", "carry On Cleo", "American History X", "dysmenorrhea", "alwatan", "Brighton", "jimmy redknapp", "approximately 26,000 years", "Travis Tritt and Marty Stuart", "Norway", "Leontes, King of Sicilia, and his wife Hermione", "Stormzy", "Tottenham Hotspur", "a potential military strike", "attempted murder", "Ma Khin Khin Leh", "Wii", "Germaine Greer", "Isaac and Esau", "Surrey"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6076388888888888}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3183", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7646", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3345", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-3125", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-2381", "mrqa_triviaqa-validation-4587", "mrqa_hotpotqa-validation-3085", "mrqa_hotpotqa-validation-875", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-729", "mrqa_searchqa-validation-4541", "mrqa_searchqa-validation-14852"], "SR": 0.546875, "CSR": 0.5597426470588236, "retrieved_ids": ["mrqa_squad-train-58961", "mrqa_squad-train-35471", "mrqa_squad-train-66683", "mrqa_squad-train-15511", "mrqa_squad-train-76209", "mrqa_squad-train-74306", "mrqa_squad-train-76375", "mrqa_squad-train-49619", "mrqa_squad-train-39561", "mrqa_squad-train-16466", "mrqa_squad-train-4196", "mrqa_squad-train-48717", "mrqa_squad-train-42777", "mrqa_squad-train-64121", "mrqa_squad-train-58918", "mrqa_squad-train-60397", "mrqa_squad-train-32896", "mrqa_squad-train-60428", "mrqa_squad-train-17048", "mrqa_squad-train-19385", "mrqa_squad-train-16514", "mrqa_squad-train-12395", "mrqa_squad-train-61107", "mrqa_squad-train-340", "mrqa_naturalquestions-validation-6787", "mrqa_triviaqa-validation-5406", "mrqa_naturalquestions-validation-10128", "mrqa_searchqa-validation-7662", "mrqa_naturalquestions-validation-2333", "mrqa_newsqa-validation-3970", "mrqa_searchqa-validation-5180", "mrqa_naturalquestions-validation-4592", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-13600", "mrqa_newsqa-validation-831", "mrqa_searchqa-validation-6273", "mrqa_searchqa-validation-3653", "mrqa_newsqa-validation-3290", "mrqa_naturalquestions-validation-3564", "mrqa_searchqa-validation-12552", "mrqa_triviaqa-validation-1922", "mrqa_newsqa-validation-3772", "mrqa_hotpotqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-1680", "mrqa_hotpotqa-validation-2025", "mrqa_newsqa-validation-722"], "EFR": 0.7931034482758621, "Overall": 0.712287969066937}, {"timecode": 85, "before_eval_results": {"predictions": ["eagle", "teacher", "Shaft", "a semicubical parabola", "Jets", "cradle of civilization", "flatter", "pause/Play button", "leek", "eat porridge", "Life and Opinions of Tristram Shandy", "vincenzo Nibali", "240", "god of Earth", "pram", "Department of Justice", "Cyprus", "sheep", "laos", "Toilet Lid Lock", "Andes Mountains of Chile and Argentina", "George Sand", "10", "tinker", "shepherd neame", "shoulder", "scrobbesbyrig", "legs", "leighton park School in Reading", "Saturday Night and Sunday Morning", "afterlife", "around May 1", "1982", "BEA", "Danish", "priesthood", "president Richard Nixon", "South Africa", "Microsoft", "Bolivian", "the British", "secretary", "Apocalypse Now", "Judy Garland", "Amnesty International", "Wizard", "treaty of Waitangi", "southern", "renzo Piano", "50", "florida", "the first year begins", "2,579 steps", "Hold On", "1919", "patosaurus", "La Scala, Milan", "Malaysia Airlines", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "Police", "Daredevil", "Dr. George Washington Carver", "panda", "California, Texas and Florida,"], "metric_results": {"EM": 0.5, "QA-F1": 0.566344246031746}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2922", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-3394", "mrqa_triviaqa-validation-4316", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-193", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-3319", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-3109", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-3040", "mrqa_triviaqa-validation-3672", "mrqa_triviaqa-validation-688", "mrqa_triviaqa-validation-3123", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1317", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-3561", "mrqa_hotpotqa-validation-372", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-237", "mrqa_newsqa-validation-2338"], "SR": 0.5, "CSR": 0.5590479651162791, "retrieved_ids": ["mrqa_squad-train-17537", "mrqa_squad-train-37904", "mrqa_squad-train-26610", "mrqa_squad-train-37254", "mrqa_squad-train-31383", "mrqa_squad-train-70033", "mrqa_squad-train-2129", "mrqa_squad-train-51821", "mrqa_squad-train-79889", "mrqa_squad-train-29426", "mrqa_squad-train-6679", "mrqa_squad-train-16702", "mrqa_squad-train-5831", "mrqa_squad-train-14879", "mrqa_squad-train-28001", "mrqa_squad-train-79206", "mrqa_squad-train-26421", "mrqa_squad-train-50538", "mrqa_squad-train-16199", "mrqa_squad-train-51436", "mrqa_squad-train-17409", "mrqa_squad-train-68848", "mrqa_squad-train-19043", "mrqa_squad-train-11861", "mrqa_triviaqa-validation-4019", "mrqa_newsqa-validation-1442", "mrqa_hotpotqa-validation-2695", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-8359", "mrqa_searchqa-validation-686", "mrqa_squad-validation-5818", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-2052", "mrqa_triviaqa-validation-6437", "mrqa_triviaqa-validation-5531", "mrqa_squad-validation-2595", "mrqa_newsqa-validation-1749", "mrqa_triviaqa-validation-1907", "mrqa_squad-validation-3985", "mrqa_triviaqa-validation-4740", "mrqa_newsqa-validation-3923", "mrqa_searchqa-validation-16658", "mrqa_hotpotqa-validation-4101", "mrqa_searchqa-validation-12527", "mrqa_newsqa-validation-3893", "mrqa_searchqa-validation-5573", "mrqa_naturalquestions-validation-10184", "mrqa_searchqa-validation-2495"], "EFR": 0.65625, "Overall": 0.6847783430232558}, {"timecode": 86, "before_eval_results": {"predictions": ["keeping malls safe", "money or other discreet aid", "41,", "top designers", "in a fair and independent manner and ratify successful efforts.", "suicide bombing", "iCloud service will now be integrated into the iOS 5 operating system.", "Seasons of My Heart", "at a relative's house,", "shot in the head", "at a house party in Crandon, Wisconsin,", "Kenneth Cole", "$17,000", "137", "teeth", "urged NATO to take a more active role in countering the spread of the", "School-age girls", "Theoneste Bagosora", "by Brown himself.", "Haitians", "54", "German authorities", "his brother to surrender.", "Roy Foster", "Mogadishu", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "16", "Jackson sitting in Renaissance-era clothes and holding a book.", "fighting charges of Nazi war crimes", "Big Brother.", "Argentina has always claimed sovereignty over the islands and invaded them in 1982, prompting a war in which more than 600 Argentinean and 255 British military personnel died.", "ALS6,", "public-television", "Dead Weather's \"Horehound\"", "The dance team with the lowest combined score", "parents", "an empty water bottle", "Juri Kibuishi,", "forged credit cards and identity theft", "human rights abuses against ethnic Somalis by rebels and Ethiopian troops are rampant.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.\"", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "five female pastors", "Facebook and Google,", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "NATO's International Security Assistance Force", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England this weekend.", "a U.S. military helicopter made a hard landing in eastern Afghanistan,", "mental health", "Body Works", "abusing its dominant market position", "Thomas Edison", "Donna", "Thomas Lennon", "1947", "bacall", "mariette", "Boston Celtics", "Australian", "Northwestern Hawaiian Islands", "florida", "cinnamon roll", "the Seine", "Mary Tyler Moore Show"], "metric_results": {"EM": 0.53125, "QA-F1": 0.598970364690639}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.07407407407407407, 1.0, 0.18181818181818182, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.19512195121951217, 0.9166666666666666, 0.04761904761904762, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4444444444444445, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-982", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-227", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-3913", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-4625", "mrqa_searchqa-validation-14248"], "SR": 0.53125, "CSR": 0.5587284482758621, "retrieved_ids": ["mrqa_squad-train-51473", "mrqa_squad-train-49637", "mrqa_squad-train-84227", "mrqa_squad-train-3729", "mrqa_squad-train-51106", "mrqa_squad-train-46537", "mrqa_squad-train-50743", "mrqa_squad-train-51130", "mrqa_squad-train-60567", "mrqa_squad-train-38673", "mrqa_squad-train-13046", "mrqa_squad-train-74059", "mrqa_squad-train-29634", "mrqa_squad-train-86495", "mrqa_squad-train-79375", "mrqa_squad-train-40486", "mrqa_squad-train-72467", "mrqa_squad-train-78722", "mrqa_squad-train-27075", "mrqa_squad-train-74750", "mrqa_squad-train-16226", "mrqa_squad-train-59146", "mrqa_squad-train-17512", "mrqa_squad-train-2564", "mrqa_searchqa-validation-2463", "mrqa_triviaqa-validation-2667", "mrqa_newsqa-validation-3437", "mrqa_naturalquestions-validation-6201", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-2371", "mrqa_squad-validation-4452", "mrqa_searchqa-validation-1049", "mrqa_newsqa-validation-2742", "mrqa_triviaqa-validation-3073", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-15498", "mrqa_searchqa-validation-14009", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-1446", "mrqa_hotpotqa-validation-558", "mrqa_naturalquestions-validation-243", "mrqa_newsqa-validation-2723", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-5538", "mrqa_triviaqa-validation-4361", "mrqa_newsqa-validation-2495", "mrqa_squad-validation-2145", "mrqa_triviaqa-validation-4834"], "EFR": 0.7333333333333333, "Overall": 0.7001311063218391}, {"timecode": 87, "before_eval_results": {"predictions": ["four", "yellow", "pertussis", "Kawasaki", "fred king", "Reservoir", "equator", "hawaii", "\"Sugar Baby Love\"", "1981", "Bernardo Bertolucci", "The Seven Year Itch", "dieppe", "mediterranean", "spinach", "libretto", "Nicky Morgan", "midsomer Murders", "Muriel", "Abraham", "Aquaman", "American Civil War", "Christian Louboutin", "cathedrals", "domestic chicken", "yankees blossom", "herpes zoster", "queen mother-in-law, Queen Mary", "rupiah", "lisping Violet- Elizabeth Bott", "charles II", "Illinois", "danelaw", "Monopoly", "fern", "Christine Keeler", "Silver Hatch", "magic", "guinea", "clogs", "butch skirmidge Kid", "louis", "edwina currie", "Baton Rouge", "spanish", "2010", "carole king", "drizzle", "casualty", "trimdon", "sleepless in seattle", "Telma Hopkins", "1624", "Milira", "Wiltshire", "Austrian Volksbanks", "1848 to 1852", "Africa's longest-serving ruler.", "the Kurdish militant group in Turkey", "Elena Kagan", "the abacus", "the Maine", "the Marquis de Lafayette", "Donny Osmond"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6094866071428571}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2178", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-7150", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-5775", "mrqa_naturalquestions-validation-2862", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-5487", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-1506", "mrqa_naturalquestions-validation-5696"], "SR": 0.578125, "CSR": 0.5589488636363636, "retrieved_ids": ["mrqa_squad-train-19997", "mrqa_squad-train-18364", "mrqa_squad-train-67844", "mrqa_squad-train-74830", "mrqa_squad-train-54079", "mrqa_squad-train-48869", "mrqa_squad-train-11271", "mrqa_squad-train-64310", "mrqa_squad-train-20755", "mrqa_squad-train-75229", "mrqa_squad-train-35009", "mrqa_squad-train-75484", "mrqa_squad-train-64982", "mrqa_squad-train-1511", "mrqa_squad-train-68796", "mrqa_squad-train-51972", "mrqa_squad-train-81693", "mrqa_squad-train-29672", "mrqa_squad-train-27080", "mrqa_squad-train-9091", "mrqa_squad-train-30713", "mrqa_squad-train-24274", "mrqa_squad-train-25481", "mrqa_squad-train-86445", "mrqa_newsqa-validation-1557", "mrqa_triviaqa-validation-2099", "mrqa_newsqa-validation-1614", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-56", "mrqa_searchqa-validation-13813", "mrqa_triviaqa-validation-5602", "mrqa_newsqa-validation-1331", "mrqa_naturalquestions-validation-9419", "mrqa_hotpotqa-validation-741", "mrqa_squad-validation-6957", "mrqa_triviaqa-validation-7690", "mrqa_newsqa-validation-1084", "mrqa_searchqa-validation-1770", "mrqa_hotpotqa-validation-3216", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-8175", "mrqa_newsqa-validation-2976", "mrqa_searchqa-validation-2568", "mrqa_newsqa-validation-2608", "mrqa_naturalquestions-validation-7549", "mrqa_triviaqa-validation-4715"], "EFR": 0.7777777777777778, "Overall": 0.7090640782828282}, {"timecode": 88, "before_eval_results": {"predictions": ["a turkey wing", "luau", "Pat Paulsen", "Paddington Bear", "Antarctica", "a casino", "Mensheviks", "prada", "Calvin Klein cologne", "a hatchet", "Hamlet", "baboons", "Chicken Little", "Bach", "Bangkok", "Eli Whitney", "John Smith", "James Buchanan Eads", "A Bug's Life", "boys", "quiver", "the queen of hearts", "Ambassador to Vietnam", "Benito Mussolini", "a sheepshank", "Robert Burns", "Ebony", "a Golden Bear", "b Brunswick", "Las Vegas", "fiber", "poppy seed", "Portrait", "Lord of the Flies", "The Pursuit of Happyness", "Nickelback", "Succotash", "George Freeth", "the Falkland Islands", "acetone", "jell-O", "steal", "frankfurter", "Roanoke Island", "Blackbeard", "Tiger Woods", "Borden", "SO2", "Amish TV", "the dachshund", "Robert Frost", "Virginia Dare", "Ole Einar Bj\u00f8rndalen", "the first quarter of the 19th century", "George Washington", "Puerto Rico", "David Graham", "1987", "Jacobite uprising", "east", "a tenement in the Mumbai suburb of Chembur,", "Monday", "Rod Blagojevich,", "Charles A. Carpenter"], "metric_results": {"EM": 0.53125, "QA-F1": 0.589657738095238}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16628", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-14289", "mrqa_searchqa-validation-3438", "mrqa_searchqa-validation-5949", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-2948", "mrqa_searchqa-validation-12850", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-14294", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-15658", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-6279", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-2110", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-5235", "mrqa_searchqa-validation-5816", "mrqa_triviaqa-validation-3013", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-220", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-3631", "mrqa_triviaqa-validation-813"], "SR": 0.53125, "CSR": 0.5586376404494382, "retrieved_ids": ["mrqa_squad-train-21385", "mrqa_squad-train-76317", "mrqa_squad-train-57541", "mrqa_squad-train-23780", "mrqa_squad-train-16888", "mrqa_squad-train-82846", "mrqa_squad-train-38394", "mrqa_squad-train-37699", "mrqa_squad-train-74508", "mrqa_squad-train-21599", "mrqa_squad-train-2595", "mrqa_squad-train-54956", "mrqa_squad-train-8785", "mrqa_squad-train-30003", "mrqa_squad-train-81174", "mrqa_squad-train-82767", "mrqa_squad-train-45852", "mrqa_squad-train-65899", "mrqa_squad-train-35876", "mrqa_squad-train-3873", "mrqa_squad-train-1750", "mrqa_squad-train-30877", "mrqa_squad-train-38768", "mrqa_squad-train-60766", "mrqa_hotpotqa-validation-2009", "mrqa_newsqa-validation-2103", "mrqa_naturalquestions-validation-7143", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-10372", "mrqa_newsqa-validation-4143", "mrqa_triviaqa-validation-4594", "mrqa_naturalquestions-validation-7849", "mrqa_squad-validation-1827", "mrqa_newsqa-validation-3002", "mrqa_hotpotqa-validation-1033", "mrqa_hotpotqa-validation-1884", "mrqa_searchqa-validation-2800", "mrqa_hotpotqa-validation-4625", "mrqa_naturalquestions-validation-10138", "mrqa_searchqa-validation-2858", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-6571", "mrqa_searchqa-validation-10927", "mrqa_searchqa-validation-15449", "mrqa_triviaqa-validation-3249", "mrqa_hotpotqa-validation-4311", "mrqa_searchqa-validation-4534", "mrqa_hotpotqa-validation-5140"], "EFR": 0.8333333333333334, "Overall": 0.7201129447565543}, {"timecode": 89, "before_eval_results": {"predictions": ["Bill Bryson", "pink Panther", "Jordan", "vikings", "sartre", "Motown", "Carl Johan", "Mars", "riyadh", "peggy hookham", "Diane Keaton", "plutocracy", "dominoes", "ringway", "the trust said the station's service licence will be amended so that \" commitment to comedy and drama is increased\"", "purse", "bass", "U2", "Barcelona", "Australia's", "seabirds", "weir crest", "hampshire", "soybeans", "George Best", "Time Bandits", "Jean-Paul Gaultier", "Red Rock West", "x", "zagreb", "handley Page", "Marine One", "Zachary Taylor", "Hitler", "All Holies Day", "Shaft", "bacall", "Louis Le Vau", "Scotland", "Tripoli's", "jubilee line", "Abbey Theatre", "maine", "willow", "cotswold road", "denver", "july", "Mel Blanc", "Lily Allen", "radicalization", "oats", "Wisconsin", "season seven", "Whig candidates William Henry Harrison ( the `` hero of Tippecanoe '' ) and John Tyler", "more than 230", "Serie B league", "Mark O'Connor", "Colombia.", "a federal judge in Mississippi", "Oscar Fernandes was criticized for saying Chaudhary's death was warning to management.", "Wade E. Pickren", "the Untouchables", "an abrade", "Amy Brenneman"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6213541666666667}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-5922", "mrqa_triviaqa-validation-6682", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-7107", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1265", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-7737", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-1687", "mrqa_newsqa-validation-3566", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-9329", "mrqa_searchqa-validation-15793"], "SR": 0.578125, "CSR": 0.5588541666666667, "retrieved_ids": ["mrqa_squad-train-8458", "mrqa_squad-train-73576", "mrqa_squad-train-6807", "mrqa_squad-train-9530", "mrqa_squad-train-20025", "mrqa_squad-train-14486", "mrqa_squad-train-38931", "mrqa_squad-train-49208", "mrqa_squad-train-11483", "mrqa_squad-train-53085", "mrqa_squad-train-46722", "mrqa_squad-train-40172", "mrqa_squad-train-72183", "mrqa_squad-train-50060", "mrqa_squad-train-80319", "mrqa_squad-train-63956", "mrqa_squad-train-49364", "mrqa_squad-train-82033", "mrqa_squad-train-28581", "mrqa_squad-train-75669", "mrqa_squad-train-14220", "mrqa_squad-train-55890", "mrqa_squad-train-85173", "mrqa_squad-train-26796", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-3168", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-7930", "mrqa_searchqa-validation-5989", "mrqa_hotpotqa-validation-2075", "mrqa_squad-validation-4452", "mrqa_triviaqa-validation-7103", "mrqa_searchqa-validation-12621", "mrqa_triviaqa-validation-6885", "mrqa_hotpotqa-validation-3563", "mrqa_naturalquestions-validation-10202", "mrqa_newsqa-validation-4006", "mrqa_triviaqa-validation-2902", "mrqa_squad-validation-6638", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-6181", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-1470", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-2957", "mrqa_searchqa-validation-324", "mrqa_triviaqa-validation-6396", "mrqa_naturalquestions-validation-6851"], "EFR": 0.8148148148148148, "Overall": 0.7164525462962963}, {"timecode": 90, "UKR": 0.82421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.865234375, "KG": 0.5171875, "before_eval_results": {"predictions": ["Hypervelocity star", "colleen McCullough", "lion king", "Cyprus", "f. Lee Bailey and Barry Scheck", "pokemon", "russia", "a dove", "giraffe", "wiesn", "b Bromley", "Venus", "sue leiter", "london", "colleen McCullough", "Egypt", "gioacchino Rossini", "drew Carey", "Three Mile Island", "s Sicily", "Sunset Boulevard", "Bombe", "brussels", "arrows", "quatermass experiment", "pasta harvest", "Frogmore Estate or Gardens", "Emmy Awards", "Caucasus", "36", "cold Comfort Farm", "new year", "iceland", "David Hilbert", "mediterranean", "Declaration of Independence", "kenry", "fish", "ptolemy Philadelphus", "labau", "Robert Schumann", "Whisky Galore", "Grace Slick", "michael caine", "fonds de la Recherche Scientifique", "Robert Boyle", "1929", "The Lone Gunmen", "Sue", "daily Herald", "pj harvey", "Victor Dhar", "a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "Philadelphia, which is Greek for brotherly love", "Cherokee River", "Benedict of Nursia", "Wichita", "all faiths", "Friday,", "hooked up with Mildred, a younger woman of about 80, in March.", "a dove", "Konstantin Stanislavsky", "Annie Proulx", "\"Nude, Green Leaves and Bust\""], "metric_results": {"EM": 0.65625, "QA-F1": 0.703125}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-5925", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-4815", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-219", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-7098", "mrqa_triviaqa-validation-4408", "mrqa_naturalquestions-validation-10257", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-508"], "SR": 0.65625, "CSR": 0.5599244505494505, "retrieved_ids": ["mrqa_squad-train-10991", "mrqa_squad-train-71465", "mrqa_squad-train-28881", "mrqa_squad-train-13642", "mrqa_squad-train-70749", "mrqa_squad-train-23599", "mrqa_squad-train-79529", "mrqa_squad-train-65943", "mrqa_squad-train-82054", "mrqa_squad-train-45621", "mrqa_squad-train-39173", "mrqa_squad-train-3590", "mrqa_squad-train-22351", "mrqa_squad-train-77758", "mrqa_squad-train-83338", "mrqa_squad-train-18349", "mrqa_squad-train-40675", "mrqa_squad-train-14085", "mrqa_squad-train-17342", "mrqa_squad-train-53633", "mrqa_squad-train-59548", "mrqa_squad-train-72246", "mrqa_squad-train-39862", "mrqa_squad-train-57397", "mrqa_naturalquestions-validation-10614", "mrqa_searchqa-validation-15590", "mrqa_triviaqa-validation-2214", "mrqa_searchqa-validation-3438", "mrqa_newsqa-validation-3961", "mrqa_hotpotqa-validation-4545", "mrqa_newsqa-validation-4054", "mrqa_naturalquestions-validation-1462", "mrqa_triviaqa-validation-2310", "mrqa_naturalquestions-validation-8339", "mrqa_searchqa-validation-14519", "mrqa_newsqa-validation-2163", "mrqa_triviaqa-validation-2611", "mrqa_triviaqa-validation-317", "mrqa_newsqa-validation-729", "mrqa_squad-validation-978", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-4599", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-8765", "mrqa_newsqa-validation-1175", "mrqa_triviaqa-validation-5342", "mrqa_newsqa-validation-1022", "mrqa_triviaqa-validation-5007"], "EFR": 0.7727272727272727, "Overall": 0.7078584696553446}, {"timecode": 91, "before_eval_results": {"predictions": ["evangelical Christian periodical", "2011", "John McClane", "Marika Nicolette Green", "Princeton University", "conservative", "Lombardy", "writer", "Elton John", "Newcastle upon Tyne, England", "Lev Ivanovich Yashin", "Blackwood Partners Management Corporation", "1958", "2007", "Robots Overlords", "1776", "Ashland is home to Scribner-Fellows State Forest", "public house", "1944", "Austria", "Ron Cowen and Daniel Lipman", "The Soloist", "indoor", "New York", "January 30, 1930", "Dr. Alberto Taquini", "John Gotti", "Anderson Silva", "north", "Harrison Ford", "C. H. Greenblatt", "Taoiseach", "Dissection", "The Killer", "seacoast region", "Ken Rutherford", "Dorothy", "2017", "people working in film and the performing arts", "June 2, 2008", "The 8th Habit", "one", "London", "New Zealand", "\"Ready Player One\"", "1981 World Rowing Championships", "1989", "15,023", "North Atlantic Conference", "highland regions of Scotland", "Jeanne Tripplehorn", "March 2018", "Bay of Plenty, Taupo and Wellington", "19", "fly", "Perry Mason", "Oliver Goldsmith", "Afghan lawmakers", "James Whitehouse,", "al-Shabaab", "Ford", "Matt Damon", "the Aktion Club", "bartering"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7151041666666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4948", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-4077", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-4208", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-3250", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-3737", "mrqa_triviaqa-validation-1178", "mrqa_newsqa-validation-3713", "mrqa_searchqa-validation-14674", "mrqa_searchqa-validation-9994"], "SR": 0.640625, "CSR": 0.5608016304347826, "retrieved_ids": ["mrqa_squad-train-35810", "mrqa_squad-train-79106", "mrqa_squad-train-30102", "mrqa_squad-train-13966", "mrqa_squad-train-3191", "mrqa_squad-train-58011", "mrqa_squad-train-4994", "mrqa_squad-train-76962", "mrqa_squad-train-62247", "mrqa_squad-train-62547", "mrqa_squad-train-40157", "mrqa_squad-train-47732", "mrqa_squad-train-28778", "mrqa_squad-train-68030", "mrqa_squad-train-65343", "mrqa_squad-train-16560", "mrqa_squad-train-75296", "mrqa_squad-train-84490", "mrqa_squad-train-25713", "mrqa_squad-train-63444", "mrqa_squad-train-54240", "mrqa_squad-train-1326", "mrqa_squad-train-85537", "mrqa_squad-train-18673", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-286", "mrqa_hotpotqa-validation-989", "mrqa_naturalquestions-validation-10331", "mrqa_newsqa-validation-3720", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-6170", "mrqa_naturalquestions-validation-2482", "mrqa_searchqa-validation-5501", "mrqa_naturalquestions-validation-5297", "mrqa_newsqa-validation-4017", "mrqa_triviaqa-validation-4316", "mrqa_searchqa-validation-14101", "mrqa_newsqa-validation-2024", "mrqa_searchqa-validation-10536", "mrqa_naturalquestions-validation-5168", "mrqa_searchqa-validation-15585", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-237", "mrqa_naturalquestions-validation-3253", "mrqa_searchqa-validation-10806", "mrqa_newsqa-validation-62", "mrqa_triviaqa-validation-6328", "mrqa_naturalquestions-validation-9002"], "EFR": 0.9130434782608695, "Overall": 0.7360971467391304}, {"timecode": 92, "before_eval_results": {"predictions": ["Margery Williams", "the First Indochina War", "Mary Ellen Mark", "one of the youngest publicly documented people to be identified as transgender", "the Matildas", "Odense Boldklub", "SpongeBob SquarePants 4-D", "Oldham County", "Grammar, logic, and rhetoric", "the Wright brothers", "a research university with high research activity", "O.T. Genasis", "science fiction drama", "Speedway World Championship", "Citric acid", "200", "liquidambar", "Jerry Buss", "Delacorte Press", "close range combat", "twice", "Eli Roth", "Marion", "Lincoln Riley", "December 13, 1920", "Mark Sinclair", "John McClane", "rural areas", "Orchard Central", "Art of Dying", "Book of Judges", "Ant Timpson, Ted Geoghegan and Tim League", "the Bahamian island of Great Exuma", "John Ford", "classical realism", "Marvel Comics", "between 7,500 and 40,000", "for crafting and voting on legislation", "Earvin \"Magic\" Johnson Jr.", "Nevada", "Yasir Hussain", "Victoria", "Jennifer Aniston", "Long Island", "Universal's volcano Bay", "25 December 2009", "the Hanna-Barbera show \"Birdman and the Galaxy Trio.\"", "Jango Fett", "\"Der Rosenkavalier\", \"Elektra\", \"Die Frau ohne Schatten\" and \"Four Last Songs\"", "Minnesota's 8th congressional district", "NBA 2K16", "Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Professor Eobard Thawne", "India", "14", "beetle", "australia", "fans", "Kearny, New Jersey.", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City,", "pasta and meatballs", "Tennessee Williams", "Illinois", "bacall"], "metric_results": {"EM": 0.578125, "QA-F1": 0.698662734324499}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.33333333333333326, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4211", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4346", "mrqa_hotpotqa-validation-41", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-1445", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-4979", "mrqa_hotpotqa-validation-4735", "mrqa_naturalquestions-validation-2472", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-8642", "mrqa_triviaqa-validation-4848"], "SR": 0.578125, "CSR": 0.5609879032258065, "retrieved_ids": ["mrqa_squad-train-36982", "mrqa_squad-train-70626", "mrqa_squad-train-13424", "mrqa_squad-train-51668", "mrqa_squad-train-46317", "mrqa_squad-train-2173", "mrqa_squad-train-32224", "mrqa_squad-train-73771", "mrqa_squad-train-4603", "mrqa_squad-train-18431", "mrqa_squad-train-76155", "mrqa_squad-train-26045", "mrqa_squad-train-24464", "mrqa_squad-train-65608", "mrqa_squad-train-24400", "mrqa_squad-train-30451", "mrqa_squad-train-14929", "mrqa_squad-train-28823", "mrqa_squad-train-73400", "mrqa_squad-train-52549", "mrqa_squad-train-28703", "mrqa_squad-train-77819", "mrqa_squad-train-29358", "mrqa_squad-train-2717", "mrqa_hotpotqa-validation-1489", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-9614", "mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-516", "mrqa_naturalquestions-validation-2813", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-5873", "mrqa_hotpotqa-validation-2802", "mrqa_newsqa-validation-1218", "mrqa_searchqa-validation-13710", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-4940", "mrqa_newsqa-validation-4207", "mrqa_triviaqa-validation-919", "mrqa_naturalquestions-validation-10209", "mrqa_triviaqa-validation-2414", "mrqa_newsqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_newsqa-validation-3871", "mrqa_naturalquestions-validation-2245", "mrqa_newsqa-validation-3130"], "EFR": 0.6296296296296297, "Overall": 0.6794516315710872}, {"timecode": 93, "before_eval_results": {"predictions": ["YIVO", "Archbishop of Canterbury", "Samuel Beckett", "January 28, 2016", "The Catholic Church in Ireland", "close range combat", "Iran", "Kate Millett", "Timothy Matthew Howard", "\"Lucky\"", "Do Kyung-soo", "John Hunt", "Kongo", "William Finn", "Sam Raimi", "The final of 2011 AFC Asian Cup", "main east-west", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "Marine Corps Air Station Kaneohe Bay", "August 17, 2017", "Montagues and Capulets", "Lancia Rally 037", "left", "Vladimir Menshov", "film", "Denmark and Norway", "Love and Theft", "C. W. Grafton", "V, an anarchist freedom fighter who attempts to ignite a revolution through elaborate terrorist acts", "\"My Love from the Star\"", "143,372", "Jack Kilby", "West Point Foundry", "Afghanistan", "Operation Aqueduct", "guitar feedback", "Flushed Away", "George Washington Bridge", "Reginald Engelbach", "Van Diemen's Land", "Tampa", "Sergeant First Class", "140 million", "SpongeBob SquarePants 4-D", "StubHub Center", "Argentinian", "the Americas and the entire South American temperate zone", "a large portion of rural Maine", "1998", "The More", "John F. Kennedy Jr.", "com TLD", "Andrew Lloyd Webber", "John Bull", "NASCAR", "The Avengers", "rick wakeman", "mental health", "Madhav Kumar Nepal", "hardship", "Berlin", "William Golding", "The Odd Couple", "Americans who served in the armed forces and as civilians during World War II"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8543166035353535}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.22222222222222224, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5676", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4235", "mrqa_naturalquestions-validation-4844", "mrqa_newsqa-validation-1063"], "SR": 0.796875, "CSR": 0.5634973404255319, "retrieved_ids": ["mrqa_squad-train-29577", "mrqa_squad-train-81936", "mrqa_squad-train-84704", "mrqa_squad-train-2565", "mrqa_squad-train-27467", "mrqa_squad-train-45054", "mrqa_squad-train-80169", "mrqa_squad-train-16884", "mrqa_squad-train-31652", "mrqa_squad-train-54222", "mrqa_squad-train-46223", "mrqa_squad-train-21136", "mrqa_squad-train-80389", "mrqa_squad-train-78499", "mrqa_squad-train-24887", "mrqa_squad-train-61432", "mrqa_squad-train-81620", "mrqa_squad-train-39272", "mrqa_squad-train-38670", "mrqa_squad-train-57349", "mrqa_squad-train-85321", "mrqa_squad-train-66", "mrqa_squad-train-25052", "mrqa_squad-train-35231", "mrqa_naturalquestions-validation-10549", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-5294", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-13384", "mrqa_squad-validation-1748", "mrqa_naturalquestions-validation-1953", "mrqa_searchqa-validation-6350", "mrqa_hotpotqa-validation-5114", "mrqa_triviaqa-validation-7646", "mrqa_naturalquestions-validation-9791", "mrqa_hotpotqa-validation-3944", "mrqa_searchqa-validation-12615", "mrqa_triviaqa-validation-7115", "mrqa_naturalquestions-validation-7659", "mrqa_newsqa-validation-2027", "mrqa_naturalquestions-validation-3828", "mrqa_newsqa-validation-3508", "mrqa_squad-validation-7357", "mrqa_naturalquestions-validation-3285", "mrqa_newsqa-validation-1148", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-1125"], "EFR": 0.46153846153846156, "Overall": 0.6463352853927986}, {"timecode": 94, "before_eval_results": {"predictions": ["Nutbush", "alfa", "6", "golf", "Einstein", "Manchester", "southampton", "Bleak House", "vienna", "Harry S. Truman", "york", "to make wrinkles", "Amy Tan", "Gatsby", "Charlie Chan", "1664", "Good Will Hunting", "marquess of Burlington", "Iain Duncan Smith", "engraver", "george orwell", "Jim Peters", "nitrogen", "oldpatrick", "god", "infante", "the cuckoo", "PPTH", "The Wicker Man", "yellow", "Canada", "giacomo Meyerbeer", "Guardian", "john Huston", "the Passenger Pigeon", "Anne Frank", "manchego", "florida", "pi\u00f1a colada", "fauntleroy", "chicken, beef, goat, shrimp, or vegetable", "Petula Clark", "michael colquhoun", "Flo Rida", "Comedy of Errors", "mead", "chemical origins of life", "Finland", "fructose", "stuffed grape leaves", "kempton", "Cress", "Vesta", "the Season 6 premiere", "Tiffany & Company", "2010 to 2012", "Nathan Bedford Forrest", "Friday,", "11", "\"Americans always believe things are better in their own lives than in the rest of the country,\"", "tanning", "Louisiana State University", "Lauren Conrad", "the Bactrian camels"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6337937801932367}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1739130434782609, 1.0, 0.0, 0.0, 0.22222222222222224]}}, "before_error_ids": ["mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-1767", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-7529", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-7655", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-4091", "mrqa_naturalquestions-validation-9903", "mrqa_hotpotqa-validation-2141", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1300", "mrqa_searchqa-validation-8477", "mrqa_searchqa-validation-5376", "mrqa_naturalquestions-validation-8046"], "SR": 0.578125, "CSR": 0.5636513157894737, "retrieved_ids": ["mrqa_squad-train-57214", "mrqa_squad-train-65326", "mrqa_squad-train-33370", "mrqa_squad-train-1818", "mrqa_squad-train-12866", "mrqa_squad-train-1993", "mrqa_squad-train-25248", "mrqa_squad-train-72568", "mrqa_squad-train-5347", "mrqa_squad-train-66541", "mrqa_squad-train-58350", "mrqa_squad-train-73957", "mrqa_squad-train-50412", "mrqa_squad-train-986", "mrqa_squad-train-19097", "mrqa_squad-train-23807", "mrqa_squad-train-82039", "mrqa_squad-train-71249", "mrqa_squad-train-34829", "mrqa_squad-train-75166", "mrqa_squad-train-70317", "mrqa_squad-train-70246", "mrqa_squad-train-60087", "mrqa_squad-train-63519", "mrqa_hotpotqa-validation-5529", "mrqa_triviaqa-validation-2414", "mrqa_searchqa-validation-13675", "mrqa_searchqa-validation-10907", "mrqa_naturalquestions-validation-2418", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3697", "mrqa_triviaqa-validation-1166", "mrqa_naturalquestions-validation-9572", "mrqa_searchqa-validation-16053", "mrqa_triviaqa-validation-1624", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-validation-4609", "mrqa_squad-validation-2000", "mrqa_searchqa-validation-14852", "mrqa_hotpotqa-validation-3136", "mrqa_triviaqa-validation-1470", "mrqa_squad-validation-6965", "mrqa_newsqa-validation-2671", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-15280", "mrqa_triviaqa-validation-3717", "mrqa_searchqa-validation-9291"], "EFR": 0.5925925925925926, "Overall": 0.6725769066764132}, {"timecode": 95, "before_eval_results": {"predictions": ["the Korean War", "$1.5 million", "Fernando Caceres", "Most of those who managed to survive the incident hid in a boiler room and storage closets", "opposition parties", "German citizen, one of an estimated 20,500 \"green-card warriors\" in the military.", "people give the United States abysmal approval ratings.", "Secretary of State", "wife's name", "do a mammogram", "U.S. senators", "to put a lid on the marking of Ashura", "Alexey Pajitnov,", "Spc. Megan Lynn Touma,", "regulators in the agency's Colorado office", "in the west African nation", "Leo Frank, a northern Jew who'd moved to Atlanta to supervise the National Pencil Company factory.", "Sri Lanka", "Johannesburg", "last year's Gaza campaign", "enormous suffering and massive displacement,\" the U.N. refugee agency said.", "heavy flannel or wool", "It is not something that has gotten lost,\"", "near Pakistan's border with Afghanistan", "walk", "the remaining rebel strongholds in the north of Sri Lanka,", "bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "longest domestic relay in the games' history,", "Kim Jong Un", "Arthur E. Morgan III,", "billboards with an image of the burning World Trade Center", "Adidas", "Too many glass shards left by beer drinkers in the city center,", "E! News", "Cologne, Germany,", "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "70,000", "The station", "Antioquia,", "he has not been as badly shaken by the scandal as many other European countries.", "large accumulations of ice", "East Java", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "billions of dollars", "between the ages of 14 to 17.", "Rod Blagojevich,", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Majid Movahedi,", "$1.45 billion", "Jonas", "billions of dollars in Chinese products each year,", "Windows Media Video ( WMV )", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Joe Pizzulo and Leeza Miller", "( Alan) Turing", "paisley", "Tiny", "three", "The iPod Classic", "Lithuanian-born", "Wayne's World", "Jon Krakauer", "Hemingway", "Rob Reiner"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6720758255225168}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.11764705882352941, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.9047619047619047, 0.8333333333333334, 1.0, 0.4, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.35714285714285715, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-3916", "mrqa_triviaqa-validation-4402", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-318", "mrqa_searchqa-validation-10142"], "SR": 0.53125, "CSR": 0.5633138020833333, "retrieved_ids": ["mrqa_squad-train-26781", "mrqa_squad-train-39992", "mrqa_squad-train-10832", "mrqa_squad-train-35832", "mrqa_squad-train-1478", "mrqa_squad-train-45204", "mrqa_squad-train-51319", "mrqa_squad-train-16653", "mrqa_squad-train-69783", "mrqa_squad-train-4452", "mrqa_squad-train-21365", "mrqa_squad-train-17009", "mrqa_squad-train-9738", "mrqa_squad-train-49473", "mrqa_squad-train-83464", "mrqa_squad-train-70800", "mrqa_squad-train-66113", "mrqa_squad-train-29497", "mrqa_squad-train-70142", "mrqa_squad-train-68103", "mrqa_squad-train-80049", "mrqa_squad-train-23398", "mrqa_squad-train-61953", "mrqa_squad-train-5922", "mrqa_newsqa-validation-3331", "mrqa_triviaqa-validation-6409", "mrqa_newsqa-validation-1022", "mrqa_triviaqa-validation-3532", "mrqa_newsqa-validation-2518", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-7585", "mrqa_naturalquestions-validation-8441", "mrqa_hotpotqa-validation-4546", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-5664", "mrqa_naturalquestions-validation-4740", "mrqa_triviaqa-validation-1948", "mrqa_searchqa-validation-6426", "mrqa_searchqa-validation-15033", "mrqa_triviaqa-validation-94", "mrqa_searchqa-validation-5038", "mrqa_squad-validation-1906", "mrqa_naturalquestions-validation-8652", "mrqa_newsqa-validation-2404", "mrqa_hotpotqa-validation-1437", "mrqa_triviaqa-validation-7039", "mrqa_newsqa-validation-1339"], "EFR": 0.43333333333333335, "Overall": 0.6406575520833333}, {"timecode": 96, "before_eval_results": {"predictions": ["UNICEF", "opium poppies", "security breach", "urged NATO to take a more active role in countering the spread of the", "public opinion in Turkey", "Christopher Savoie", "prisoners at the South Dakota State Penitentiary", "Tuesday afternoon.", "Iowa,", "Dr. Jennifer Arnold and husband Bill Klein,", "Chinese", "Mayor Michael Bloomberg", "more than 1.2 million people.", "the estate with its 18th-century sights, sounds, and scents.", "Keating Holland.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Flint, Michigan.", "the Revolutionary Armed Forces of Colombia, better known as FARC,", "Mexico", "Larry King", "Alberto Espinoza Barron,", "spiral into economic disaster.", "Four", "Mawise Gumba", "burned over 65 percent of his body", "Brian Smith.", "2-1", "motor scooter", "Hank Moody", "April 2010.", "\"Nothing But Love\"", "Mandi Hamlin", "people look at the content of the speech, not just the delivery.", "Yemen,", "Trevor Rees", "an American who entered the country illegally from China on Christmas Eve.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "the company has not yet managed to sell the concept to a buyer.", "Manny Pacquiao", "\"I tell [viewers] that all of the forthcoming anthology \"The Fat Studies Reader,\"", "\"in bad condition at the scene,\"", "President Thabo Mbeki", "Bright Automotive, a small carmaker from Anderson, Indiana,", "Jeffrey Jamaleldine", "Negotiators for Zelaya and Roberto Micheletti, the politician who was appointed president hours after Zelaya's June 28 removal, reached an agreement late Thursday", "Haiti,", "in Atlanta's Hartsfield-Jackson International Airport", "his business dealings", "hardship for terminally ill patients and their caregivers,", "nearly 28 years of rule.", "Hollywood", "A \u00d7 9", "ThonMaker", "parashah", "Syria", "the Benedictine Order", "Cecil", "sexy Star", "March 31, 1944", "Dutch", "the dragon", "Marcus Garvey", "zodiac", "obsessive-compulsive"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6650829968907483}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.888888888888889, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9166666666666666, 0.0, 1.0, 0.10526315789473685, 0.12500000000000003, 0.2, 0.4444444444444445, 0.6666666666666666, 0.25806451612903225, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-905", "mrqa_newsqa-validation-1138", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-6184", "mrqa_hotpotqa-validation-5312", "mrqa_searchqa-validation-2594", "mrqa_searchqa-validation-9869"], "SR": 0.578125, "CSR": 0.5634664948453608, "retrieved_ids": ["mrqa_squad-train-69395", "mrqa_squad-train-60016", "mrqa_squad-train-55617", "mrqa_squad-train-33309", "mrqa_squad-train-8766", "mrqa_squad-train-4566", "mrqa_squad-train-78450", "mrqa_squad-train-6326", "mrqa_squad-train-80290", "mrqa_squad-train-27770", "mrqa_squad-train-27172", "mrqa_squad-train-57774", "mrqa_squad-train-4909", "mrqa_squad-train-27385", "mrqa_squad-train-76316", "mrqa_squad-train-66396", "mrqa_squad-train-57816", "mrqa_squad-train-43727", "mrqa_squad-train-41039", "mrqa_squad-train-82413", "mrqa_squad-train-67755", "mrqa_squad-train-60806", "mrqa_squad-train-11983", "mrqa_squad-train-29339", "mrqa_newsqa-validation-1458", "mrqa_searchqa-validation-5984", "mrqa_newsqa-validation-3146", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-4870", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1658", "mrqa_hotpotqa-validation-5724", "mrqa_triviaqa-validation-5726", "mrqa_hotpotqa-validation-5531", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-10184", "mrqa_triviaqa-validation-1917", "mrqa_naturalquestions-validation-3559", "mrqa_newsqa-validation-868", "mrqa_naturalquestions-validation-10009", "mrqa_searchqa-validation-9822", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-6662", "mrqa_naturalquestions-validation-2907", "mrqa_newsqa-validation-1415", "mrqa_triviaqa-validation-4306", "mrqa_naturalquestions-validation-1085", "mrqa_triviaqa-validation-4490"], "EFR": 0.48148148148148145, "Overall": 0.6503177202653684}, {"timecode": 97, "before_eval_results": {"predictions": ["Johannes Gutenberg", "2018", "in the Hebrew Bible, in the books of Exodus and Deuteronomy", "Thomas Jefferson", "the end of the spinal cord is about the level of the third lumbar vertebra, or L3, at birth", "Rama", "Pakistan", "for the red - bed country of its watershed", "the Soviet Union and its satellite states", "Rashida Jones", "close by the hip, and under the left shoulder", "Most days are sunny throughout the year", "Peter Andrew Beardsley MBE", "Season two", "Terry Reid", "1260 cubic centimeters ( cm ) for men and 1130 cm for women", "May 3, 2005", "639", "the Rashidun Caliphs", "British Columbia, Canada", "a central place in Christian eschatology", "Pyeongchang County, Gangwon Province, South Korea", "diffuse interstellar medium ( ISM )", "a very long forward pass in American football, made in desperation, with only a small chance of success and time running out on the clock", "1943", "Tokyo for the 2020 Summer Olympics", "the head of Lituya Bay in Alaska", "San Jose, California", "Panzerkampfwagen VIII Maus", "July 2, 1776", "accomplish the objectives of the organization", "Domhnall Gleeson", "Simon Callow", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a pancake house which would destroy a colony of burrowing owls who live on the site", "Laura Jane Haddock", "May 2016", "Bacon", "1994", "Cam Clarke", "senators", "origins of replication, in the genome", "the fourth quarter of the preceding year", "April 2016", "Michael Schumacher", "Massachusetts", "Latitude", "2015", "post translational modification", "the rise of literacy, technological advances in printing, and improved economics of distribution", "petition for a writ of certiorari", "Jules Shear", "guinea", "paen\u012bnsula", "neck", "Benvolio", "De La Soul", "Delilah Rene", "July", "conviction of Peru's ex-president is a warning to those who deny human rights.", "$81,27014", "the Missouri", "jade", "Frank Sinatra", "Long troop deployments"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6770981208481208}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.19999999999999998, 1.0, 0.1111111111111111, 0.0, 1.0, 1.0, 0.5, 1.0, 0.2, 0.0, 1.0, 1.0, 0.0, 0.8148148148148148, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.39999999999999997, 0.08108108108108107, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06060606060606061, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-7950", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-2425", "mrqa_triviaqa-validation-7592", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2892"], "SR": 0.578125, "CSR": 0.5636160714285714, "retrieved_ids": ["mrqa_squad-train-30491", "mrqa_squad-train-57624", "mrqa_squad-train-5111", "mrqa_squad-train-35075", "mrqa_squad-train-70522", "mrqa_squad-train-29594", "mrqa_squad-train-49996", "mrqa_squad-train-4001", "mrqa_squad-train-66223", "mrqa_squad-train-4904", "mrqa_squad-train-73020", "mrqa_squad-train-78052", "mrqa_squad-train-85973", "mrqa_squad-train-32484", "mrqa_squad-train-39399", "mrqa_squad-train-45821", "mrqa_squad-train-18691", "mrqa_squad-train-67465", "mrqa_squad-train-13675", "mrqa_squad-train-16300", "mrqa_squad-train-34433", "mrqa_squad-train-44938", "mrqa_squad-train-83748", "mrqa_squad-train-17968", "mrqa_hotpotqa-validation-5464", "mrqa_triviaqa-validation-2180", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2497", "mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-2021", "mrqa_naturalquestions-validation-8884", "mrqa_hotpotqa-validation-5448", "mrqa_newsqa-validation-3503", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-12999", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-876", "mrqa_squad-validation-3019", "mrqa_naturalquestions-validation-4544", "mrqa_newsqa-validation-3446", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-7281", "mrqa_searchqa-validation-2141", "mrqa_naturalquestions-validation-9875"], "EFR": 0.4074074074074074, "Overall": 0.6355328207671957}, {"timecode": 98, "before_eval_results": {"predictions": ["hair", "Deimos", "Lana Turner", "a Polaroid picture", "Oklahoma City", "June Carter Cash", "Edward Lear", "Colleen", "fat", "poison ivy", "Denny McLain", "bicycle", "Edith Wharton", "Liberia", "rhythm and blues", "Queen Victoria", "AARP", "Arturo Toscanini", "Bangladesh", "Saturn", "Nancy Pelosi", "American funk rock band", "root beer", "misery", "the Black Swallower", "coal", "Tennessee", "kidnapping", "Pope John Paul II", "a photocopier", "Syria", "(William) Inge", "the grain", "the Bean Sidhe", "Japan", "Clinton", "ballistic missile", "Ambrose Bierce", "Walt Whitman", "frequency", "Macbeth", "Colorado River", "Vice President", "Tommy Franks", "Botswana", "Mousehunt", "the Dow Jones", "Sir Winston Churchill", "Vietnam", "a bell", "a Croque Madam", "Kyla Pratt", "Wisconsin", "March 11, 2018", "Top Cat", "cutis anserina", "Adrian Cronauer", "1 August 1971", "Australia", "Bronwyn Kathleen Bishop", "Jacob", "Madhav Kumar Nepal", "Joe Jackson", "About 200"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7008928571428572}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8500", "mrqa_searchqa-validation-14933", "mrqa_searchqa-validation-1461", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-15155", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-14445", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13824", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-1625", "mrqa_searchqa-validation-6160", "mrqa_searchqa-validation-2884", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-312", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-1955"], "SR": 0.640625, "CSR": 0.5643939393939394, "retrieved_ids": ["mrqa_squad-train-4324", "mrqa_squad-train-79861", "mrqa_squad-train-1628", "mrqa_squad-train-11788", "mrqa_squad-train-142", "mrqa_squad-train-46832", "mrqa_squad-train-6172", "mrqa_squad-train-59336", "mrqa_squad-train-32520", "mrqa_squad-train-61499", "mrqa_squad-train-44916", "mrqa_squad-train-41762", "mrqa_squad-train-73424", "mrqa_squad-train-23825", "mrqa_squad-train-57832", "mrqa_squad-train-52109", "mrqa_squad-train-13198", "mrqa_squad-train-10862", "mrqa_squad-train-79721", "mrqa_squad-train-49515", "mrqa_squad-train-74658", "mrqa_squad-train-40203", "mrqa_squad-train-40842", "mrqa_squad-train-18329", "mrqa_searchqa-validation-9536", "mrqa_newsqa-validation-339", "mrqa_squad-validation-7457", "mrqa_newsqa-validation-1229", "mrqa_triviaqa-validation-5294", "mrqa_searchqa-validation-13686", "mrqa_triviaqa-validation-3669", "mrqa_hotpotqa-validation-1851", "mrqa_newsqa-validation-1380", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-6507", "mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-9691", "mrqa_newsqa-validation-1994", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-2435", "mrqa_naturalquestions-validation-5599", "mrqa_newsqa-validation-3726", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4546", "mrqa_newsqa-validation-1454", "mrqa_naturalquestions-validation-808"], "EFR": 0.34782608695652173, "Overall": 0.6237721302700922}, {"timecode": 99, "UKR": 0.814453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1078", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3329", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3905", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8120", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-70", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12568", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-2884", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4848", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.87109375, "KG": 0.53515625, "before_eval_results": {"predictions": ["Mobile", "lost a guppy", "Minnesota", "the Rosetta Stone", "Japan", "a crumpets", "Lord Bill Astor", "peripheral vision", "Henry Wadsworth", "Canton", "Hormel Foods", "syllable", "Theodore", "South Carolina", "Roger Williams", "Niels Bohr", "the sun", "Ahab", "monsters", "Surf's Up", "Scorpio", "a cat", "Finding Nemo", "the International Space Station", "Shakira", "Candice Bergen", "a shark", "Ireland", "George J. Mitchell", "Henry Wadsworth Longfellow", "Gauguin", "Joan Beaufort", "a bamboos", "Animal Crackers", "Crete", "Frank Sinatra", "(George) Custer", "barney stinson", "March 18", "Marlee Matlin", "Ben- Hur: A Tale of the Christ", "Yu Darvish", "Dan Rather", "KLM", "food combining", "a private tutor", "elephants", "Arkansas", "Bank of America", "a piccolo", "the tuba", "Jason Marsden", "1998", "Garfield Sobers", "Germany", "president Jimmy Carter", "yellow-brown", "Detroit, Michigan", "the Troubles", "ARY", "Sri Lanka", "Ali Bongo", "an initial report outlining its findings and recommendations in about 100 days.", "Muhammad"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7063244047619047}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4116", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-9926", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-3988", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-1713", "mrqa_searchqa-validation-2630", "mrqa_searchqa-validation-9182", "mrqa_searchqa-validation-7727", "mrqa_triviaqa-validation-3439", "mrqa_hotpotqa-validation-4869", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-3631"], "SR": 0.609375, "CSR": 0.56484375, "retrieved_ids": ["mrqa_squad-train-57436", "mrqa_squad-train-64656", "mrqa_squad-train-59778", "mrqa_squad-train-44792", "mrqa_squad-train-79422", "mrqa_squad-train-1788", "mrqa_squad-train-1690", "mrqa_squad-train-70086", "mrqa_squad-train-86040", "mrqa_squad-train-69058", "mrqa_squad-train-53002", "mrqa_squad-train-32571", "mrqa_squad-train-6108", "mrqa_squad-train-19117", "mrqa_squad-train-52077", "mrqa_squad-train-16921", "mrqa_squad-train-16214", "mrqa_squad-train-67604", "mrqa_squad-train-84506", "mrqa_squad-train-22501", "mrqa_squad-train-21572", "mrqa_squad-train-160", "mrqa_squad-train-18857", "mrqa_squad-train-44858", "mrqa_searchqa-validation-11733", "mrqa_hotpotqa-validation-558", "mrqa_squad-validation-259", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-7212", "mrqa_triviaqa-validation-6746", "mrqa_naturalquestions-validation-7598", "mrqa_triviaqa-validation-4798", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-3358", "mrqa_hotpotqa-validation-5358", "mrqa_triviaqa-validation-7166", "mrqa_naturalquestions-validation-8699", "mrqa_squad-validation-9959", "mrqa_newsqa-validation-3376", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-4828", "mrqa_triviaqa-validation-7497", "mrqa_searchqa-validation-3970", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-276", "mrqa_triviaqa-validation-3208", "mrqa_hotpotqa-validation-2792"], "EFR": 0.36, "Overall": 0.629109375}]}